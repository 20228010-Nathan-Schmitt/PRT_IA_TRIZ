{
    "took": 437,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": {
            "value": 245,
            "relation": "eq"
        },
        "max_score": null,
        "hits": [
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11409263-20220809",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11409263-20220809",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2017-11-15",
                    "PUBLICATION_DATE": "2022-08-09",
                    "INVENTORS": [
                        "Zhijun Zhang",
                        "Ziyi Yan"
                    ],
                    "APPLICANTS": [
                        "South China University of Technology    ( Guangzhou , CN )"
                    ],
                    "INVENTION_TITLE": "Method for programming repeating motion of redundant robotic arm",
                    "DOMAIN": "G05B 194155",
                    "ABSTRACT": "A method is presented for programming a repeating motion of a redundant robotic arm on the basis of a variable parameter convergence differential neural network. The method may include establishing an inverse kinematics equation, creating an inverse kinematics problem, introducing a repeating motion indicator, converting a time-varying convex quadratic programming problem into a time-varying matrix equation, and integrating an optimal solution to obtain an optimal solution of a joint angle. The use of the variable parameter convergence differential neural network to solve the repeating redundant mechanical motion has the advantages of high computational efficiency, high real-time performance, and enhanced robot arm robustness.",
                    "CLAIMS": "1. A method for performing a repeating motion of a robotic arm on the basis of a variable parameter convergence differential neural network, comprising: providing a robotic arm, the robotic arm having an end and a plurality of joints; providing a system for operating the robotic arm by the following steps: a establishing an inverse kinematics equation of the robotic arm at a velocity level by tracking the end of the robotic arm; b generating an inverse kinematics problem as a time-varying convex quadratic programming problem constrained by an equality; c obtaining an initial joint state 0 of the robotic arm and a joint state t during the motion of the robotic arm, and introducing a repeating motion indicator into the time-varying convex quadratic programming problem; wherein the repeating motion indicator is obtainable by means of the performance indicator coefficient c, which is designed as c=t0, where represents the response coefficient to a joint offset; d converting the time-varying convex quadratic programming problem, into a time-varying matrix equation by using a Lagrangian function; e solving the time-varying matrix equation by the variable parameter convergence differential neural network; wherein the error of the time-varying matrix equation converges to zero based on the variable parameter convergence differential neural network, an error function is constructed as: t=Qyu where t represents the error of the time-varying matrix equation, and then based on a neurodynamic method, the error is designed to converge to zero in the following way: d t dt = - exp t t where is a parameter for adjusting a convergence rate, is an activation function, and the error function is substituted into the above formula to obtain a variable parameter convergence differential neural network solver, namely: Q{dot over y}={dot over Q}y exptQyu+{dot over u} where the variable parameter convergence differential neural network solver obtains an optimal solution y* of the time-varying matrix equation, and the first n terms thereof are an optimal solution x* of the time-varying convex quadratic programming problem, i. e. , an optimal solution of a joint angular velocity; f integrating the optimal solution x* of the robotic arm at the velocity level to obtain an optimal solution * of a joint angle; and g using the optimal solution * of the joint angle to operate the robotic arm to perform the repeating motion, so that after the end of the robotic arm completes a cycle of actions, all the joints of the robotic arm can return to an initial position. 2. The method for performing a repeating motion of a robotic arm on the basis of a variable parameter convergence differential neural network, as claimed in claim 1, wherein the inverse kinematics equation of the robotic arm is expressed as: f=r where r is a desired track of an end of the robotic arm, and f is a nonlinear equation of the joint angle of the robotic arm, and the inverse kinematics equation of the robotic arm at the velocity level is obtained by deriving the two sides of the equation with respect to time: J{dot over }={dot over r} where JRmn is an mn-dimensional matrix on the real number field, J is a Jacobian matrix of the redundant robotic arm, n is the number of the degrees of freedom of the robotic arm, and m is the number of the spatial dimensions of the track of the end of the robotic arm, and {dot over } and {dot over r} are respectively the derivatives of the joint angle and the track of the end of the redundant robotic arm with respect to time. 3. The method for performing a repeating motion of a robotic arm on the basis of a variable parameter convergence differential neural network, as claimed in claim 1, wherein the formula for creating the inverse kinematics problem as a time-varying convex quadratic programming problem constrained by an equality is: min x T Wx 2 + c T x s . t . J x = b where x={dot over }, b={dot over r}, W=I represents an identity matrix, J is the Jacobian matrix of the robotic arm, and c is a performance indicator coefficient. 4. The method for performing a repeating motion of a robotic arm on the basis of a variable parameter convergence differential neural network, as claimed in claim 1, wherein the Lagrangian function is constructed as: L x , , t = x T Wx 2 + c T x + J x - b where is a Lagrangian multiplier, and the partial derivatives of the Lagrangian function respectively with respect to x and are obtained: L x , , t x = Wx + c + J T = 0 L x , , t = J x - b = 0 the above system of equations can be expressed as the following time-varying matrix equation: Qy = u where Q = [ W J T J 0 ] E n + m n + m , y = [ x ] R n + m , u = [ - c b ] R n + m .",
                    "FIELD_OF_INVENTION": "The present disclosure relates to the field of redundant robotic arms, and in particular to a method for programming and operating a repeating motion of a redundant robotic arm on the basis of a variable parameter convergence differential neural network.",
                    "STATE_OF_THE_ART": "The term redundant robotic arm means that the number of degrees of freedom of the robotic arm is greater than the number of degrees of freedom required to complete a task, so that when completing the main tasks by an end effector, the redundant robotic arm can also perform additional tasks such as avoiding an obstacle avoidance, a shutdown extreme position, and a singular state of the robotic arm due to the more degrees of freedom. The term repeating motion means that after the end of the robotic arm completes a cycle of actions, all the joints thereof can return to the initial positions, not just the end of the robotic arm returns back to the initial position. In the automated industrial production, the robotic arm usually needs to carry out mass production activities, if the robotic arm completes a non-repeating motion, that is, the initial state of each cycle of motions is different, an error will occur, and after the error has been accumulated to a certain extent, additional reset operation of the robotic arm is required, and the production efficiency will be greatly reduced. Therefore, it is meaningful to study the repeating motion of the redundant robotic arm. The traditional solution to the inverse kinematics problem of the redundant robotic arm is based on a pseudo-inverse method, this method is computationally intensive, has poor real-time performance, and considers a single problem constraint, which is greatly restricted in the actual application of the robotic arm. A quadratic programming scheme has been proposed to solve the repeating motion of a redundant robotic arm, which is divided into a numerical method solver and a neural network solver. Compared with the numerical method solver, the neural network solver has the advantages of being more efficient and having better real-time performance.",
                    "DESCRIPTION": "The present disclosure will be further described in detail below in connection with embodiments and the accompanying drawings, but embodiments of the present disclosure are not limited thereto. An exemplary embodiment provides a method for programming a repeating motion of a redundant robotic arm 10 on the basis of a variable parameter convergence differential neural network, the flow chart thereof is shown in 1, and is composed of three parts: presentation of a problem 12, conversion of the problem 14 and creating a solution to the problem 16. Firstly, the inverse kinematics equation at a velocity level is established according to the desired track of an end of the redundant robotic arm and the Jacobian matrix. The repeating motion of the redundant robotic arm is designed as a time-varying convex quadratic programming problem constrained by an equality. The quadratic programming problem is converted into a matrix equation problem by means of a Lagrangian function. And finally, the matrix equation is solved by means of the variable parameter convergence differential neural network. The following steps are contemplated. It will be appreciated that more or fewer steps could be deployed:1 establishing an inverse kinematics equation 20 of the redundant robotic arm 10 at a velocity level by means of a track 18 of an end of the redundant robotic arm. In this step, the inverse kinematics equation 20 of the redundant robotic arm is expressed as: f=r where r is a desired track of the end of the redundant robotic arm, and f is a nonlinear equation of the joint angle of the redundant robotic arm, which is determined by the structure of the robotic arm 10, a Kinova Jaco six-axis robotic arm is simulated in this embodiment, and the inverse kinematics equation 20 of the redundant robotic arm at the velocity level is obtained by deriving the two sides of the equation with respect to time: J{dot over }={dot over r}Here, JRmn is an mn-dimensional matrix on the real number field, J is a Jacobian matrix of the redundant robotic arm, n is the number of the degrees of freedom of the robotic arm, and m is the number of the spatial dimensions of the track of the end of the robotic arm, and {dot over } and {dot over r} are respectively the derivatives of the joint angle and the track of the end of the redundant robotic arm 10 with respect to time;2 Designing or creating an inverse kinematics problem in step 1 as a time-varying convex quadratic programming problem constrained by an equality. An exemplary formula is: min x T Wx 2 + c T x s . t . J x = b where x={dot over }, b={dot over r}. W=I represents an identity matrix, J is the Jacobian matrix 24 of the redundant robotic arm 10, and c is a performance indicator coefficient. 3 Introducing a repeating motion indicator 26 into the time-varying convex quadratic programming problem of step 2. The repeating motion indicator in this step is obtainable by means of the performance indicator coefficient c, which is designed as c=t0, where represents the response coefficient to a joint offset, and in this embodiment =5; and t and 0 respectively represent a joint state during c the movement of the robotic arm and an initial joint state. When the repeating motion indicator c is not considered, =0, the simulation result is as shown in 2a. In this case, after the robotic arm 10 completes a cycle of motions, it is impossible to ensure that the end position 11 and the initial position 13 of each joint of the robotic arm 10 are consistent; and when the repeating motion indicator is considered, =5, the simulation result is as shown in 2b, the end position 11 of each joint of the robotic arm 10 is consistent with the initial position 13 thereof. This arrangement is preferred. 4 Converting the time-varying convex quadratic programming problem, into which the repeating motion indicator is introduced in step 3, into a time-varying matrix equation 30 by using a Lagrangian function 28. An example of a process of this step is that the Lagrangian function 28 is constructed as: L x , , t = x T Wx 2 + c T x + J x - b where is a Lagrangian multiplier, and the partial derivatives of the Lagrangian function 28 respectively with respect to x and are obtained: L x , , t x = Wx + c + J T = 0 L x , , t = J x - b = 0 The above system of equations can be expressed as the following time-varying matrix equation 30: Qy = u where Q = [ W J T J 0 ] E n + m n + m , y = [ x ] R n + m , u = [ - c b ] R n + m . 5 Solving the time-varying matrix equation 30 of step 4 by means of the variable parameter convergence differential neural network 32; and the specific process of this step is that the error of the time-varying matrix equation converges to zero based on the variable parameter convergence differential neural network. Firstly, an error function is constructed as: t=Qyu where t represents the error of the time-varying matrix equation, and then based on a neurodynamic method, the error is designed to converge to zero in the following way, the specific formula is: d t dt = - exp t t where is a parameter for adjusting a convergence rate, is an activation function, and the error function is substituted into the above formula to obtain a variable parameter convergence differential neural network solver 32, namely: Q{dot over y}={dot over Q}y exptQyu+{dot over u}In this way, the variable parameter convergence differential neural network solver 32 obtains an optimal solution y* of the time-varying matrix equation, and the first n terms thereof are the optimal solution x* of the time-varying convex quadratic programming problem in step 2 i. e. , the optimal solution of a joint angular velocity. 6 Integrating an optimal solution, obtained in step 5, of the redundant robotic arm 10 at the velocity level to obtain an optimal solution of a joint angle 34. The optimal solution * of the joint angle 34 in this step is obtained by integrating the optimal solution of the time-varying convex quadratic programming problem, i. e. the optimal solution x* of the joint angular velocity. The specific process of this step is that by means of the variable parameter convergence differential neural network solver in step 5, the optimal solution y* can be obtained, and the first n terms thereof are the optimal solution x* of the time-varying convex quadratic programming problem in step 2 i. e. , the optimal solution of a joint angular velocity, which can be integrated to obtain the optimal solution * of the joint angle of the redundant robotic arm. The foregoing description is merely illustrative of the disclosed embodiments of the present disclosure, but the scope of protection of the present disclosure is not limited thereto. Equivalent replacements or modifications made to the technical solutions and the inventive concept of the present disclosure by a person skilled in the art within the scope of the disclosure of the present disclosure fall into the scope of protection of the present disclosure.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/63/092/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9852713346481323,
                    "F_SPEC_PARAMS": [
                        "additional reset operation of the robotic arm is required,",
                        "production efficiency"
                    ],
                    "S_SPEC_PARAMS": [
                        "efficient",
                        "real-time performance"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "In the automated industrial production, the robotic arm usually needs to carry out mass production activities, if the robotic arm completes a non-repeating motion, that is, the initial state of each cycle of motions is different, an error will occur, and after the error has been accumulated to a certain extent, additional reset operation of the robotic arm is required, and the production efficiency will be greatly reduced."
                    ],
                    "S_SENTS": [
                        "Compared with the numerical method solver, the neural network solver has the advantages of being more efficient and having better real-time performance."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "F_SIM_SCORE": 0.7671475410461426,
                    "S_TRIZ_PARAMS": [
                        "Productivity",
                        "Speed"
                    ],
                    "S_SIM_SCORE": 0.6117801666259766,
                    "GLOBAL_SCORE": 1.8080685218175252
                },
                "sort": [
                    1.8080685
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11344372-20220531",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11344372-20220531",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2021-05-28",
                    "PUBLICATION_DATE": "2022-05-31",
                    "INVENTORS": [
                        "Maurice Bourlion",
                        "Olivier Frezal",
                        "Guillaume Morel",
                        "Stephane Bette",
                        "Thibault Chandanson",
                        "Florian Richer",
                        "Valentin Kerspern"
                    ],
                    "APPLICANTS": [
                        "SpineGuard    ( Vincennes , FR )"
                    ],
                    "INVENTION_TITLE": "Robotic surgical system",
                    "DOMAIN": "A61B 3410",
                    "ABSTRACT": "Systems, instruments, and methods are provided verifying that a robotic surgery is being performed in accordance with a surgical plan, wherein a surgical tool having a sensor outputs a data signal that enables the trajectory of the surgical tool to be displayed as an overlay on an image of an anatomical portion of a patient and a visual or audible signal that confirms the surgical tool is penetrating the anatomical portion in accordance with the surgical plan and/or that issues an alert indicating that the surgical tool is not being inserted into the anatomical portion according to the surgical plan.",
                    "CLAIMS": "1. A surgical system for executing a pre-operative surgical plan of penetrating an anatomical portion, the surgical system for use in conjunction with a navigation system having an imaging system, the surgical system comprising: a robotic arm; a surgical tool coupled to the robotic arm, the surgical tool having a sensor to measure electrical characteristics of an anatomical portion; a guidance system coupled to the imaging system, the robotic arm and the surgical tool, wherein the guidance system is configured to: receive the pre-operative surgical plan, wherein the pre-operative surgical plan identifies, based on previously-obtained data for the anatomical portion, a specified orientation, trajectory and depth for penetration of the anatomical portion; receive and display images from the imaging system of an anatomical portion; operate the robotic arm and the surgical tool to execute the pre-operative surgical plan; receive sensor data from the sensor during penetration of the anatomical portion; determine whether the sensor data corresponds to the pre-operative surgical plan; display an indication of the orientation, trajectory and depth of the surgical tool as an overlay on the images of the anatomical portion; and issue, in response to determining that the sensor data does not correspond to the pre-operative surgical plan, an alert indicating that the surgical tool is not penetrating the anatomical portion according to the pre-operative surgical plan. 2. The surgical system of claim 1, wherein the surgical tool is an elongated member having a cutting tip, wherein the cutting tip is configured to penetrate the anatomical portion, the anatomical portion including bone and tissue. 3. The surgical system of claim 2, wherein the surgical tool is a drill bit and the sensor comprises first and second electrodes disposed on the drill bit. 4. The surgical system of claim 3, wherein the sensor is configured to measure electrical impedance of the anatomical portion in contact with the cutting tip. 5. The surgical system of claim 1, wherein the alert comprises an audible tone that is configured to vary in accordance with electrical characteristics of the anatomical portion measured by the sensor. 6. The surgical system of claim 5, wherein the audible tone varies when the sensor measures a transition in the anatomical portion from bone to tissue. 7. The surgical system of claim 1, wherein the guidance system is further configured to stop the robotic arm in response to determining that the sensor data does not correspond to the pre-operative surgical plan. 8. The surgical system of claim 1, wherein the guidance system is configured to propose a revised trajectory, orientation or depth of the surgical tool in connection with the alert that the surgical tool is not penetrating the anatomical portion according to the pre-operative surgical plan. 9. The surgical system of claim 8, wherein the guidance system is configured to effectuate the revised trajectory, orientation or depth of the surgical tool responsive to a user input. 10. The surgical system of claim 1, wherein the guidance system is further configured to control the force at which the surgical tool is advanced to penetrate the anatomical portion. 11. The surgical system of claim 10, wherein the guidance system is further configured to emit an audible force signal corresponding to a torque exerted on the surgical tool during penetration of the anatomical portion. 12. The surgical system of claim 1, wherein the guidance system is further configured to instruct, in response to determining that the sensor data does correspond to the pre-operative surgical plan, the robotic arm to proceed penetrating into the anatomical portion. 13. The surgical system of claim 1, wherein the robotic arm is further configured to determine a depth of penetration of the surgical tool into the anatomical portion. 14. The surgical system of claim 1, wherein the guidance system is further configured to continuously update the overlay of the indication of the orientation, trajectory and depth of the surgical tool on the display. 15. The surgical system of claim 1, wherein the guidance system is further configured to receive and store images from the imaging system of the anatomical portion before execution of the pre-operative surgical plan. 16. The surgical system of claim 1, wherein the guidance system is further configured to store a plurality of predefined signatures, each signature comprising a reference warning signal corresponding to a variation in the electrical characteristics expected to be encountered during execution of the pre-operative surgical plan. 17. The surgical system of claim 16, wherein the predefined signatures includes a plurality of movement parameters for controlling the robotic arm, wherein the guidance system is further configured to, during penetration of the surgical tool into the anatomical portion, continuously compare a variation of sensor data to the predefined signatures. 18. The surgical system of claim 16, wherein each predefined signature further comprises at least one among: a reference force signal or a reference depth signal. 19. The surgical system of claim 16, wherein each one of the plurality of predefined signatures comprises at least one critical threshold. 20. The surgical system of claim 1, wherein the guidance system is further configured to advance in a penetration direction relative to the anatomical portion, as long as the alert has not reached a critical threshold, and to modify movement of the surgical device when the alert reaches the critical threshold. 21. The surgical system of claim 1, wherein the robotic arm comprises a duct suitable for receiving the surgical tool. 22. The surgical system of claim 1, wherein the guidance system further is configured to adjust real-time registration of the indication of the orientation, trajectory and depth of the surgical tool in the overlay responsive to the sensor data. 23. The surgical system of claim 22, wherein the guidance system further is configured so that the indication of the orientation, trajectory and depth of the surgical tool provides visual confirmation that a working end of the surgical tool is approaching a transition in the anatomical portion.",
                    "FIELD_OF_INVENTION": "The invention relates to a robotic surgical system in which a trajectory of a working end of a robotic arm is guided by an impedance sensing system and wherein progress of the working end may be visualized with a navigation system that includes an imaging system.",
                    "STATE_OF_THE_ART": "Image based surgical techniques have been used to aid physicians in performing a wide variety of delicate surgical procedures. These surgical procedures are used, for example, when patient anatomy obscures visualization of a surgical tool, or when a working end of a surgical tool is difficult to visualize in three dimensions. Surgical procedures in which such concerns frequently arise include, for example, spinal implant placement, alignment of broken bone fragments, and fixation of bone fractures. Surgical navigation and robotic systems are known that use x-ray or fluoroscopic images to assist a physician in visualizing the location of a working end of a surgical tool within patient anatomy. Such systems repeatedly acquire x-ray or CT images during a surgical procedure, thereby permitting real-time display of a position of the working end of the surgical tool relative to the patient anatomy. Fluoroscopically-based surgical navigation systems also may track a trajectory of the surgical tool and superimpose a representation of the surgical tool onto pre-acquired images of the patient anatomy, without x-rays being taken repeatedly during the surgical procedure. An example of such a system is the Stealth Station navigation system sold by Medtronic, Inc. , which provides a surgeon with CT-imaging feedback of the position and trajectory of handheld instruments. Adoption of previously known surgical navigation systems has been limited because the accuracy of such systems may be compromised by a variety of factors. For example, a patient or the surgeon may reposition the patient's body on the surgical table, thereby causing mis-registration of the working end of the surgical tool relative to pre-acquired images. One solution to this problem, to periodically update the fluoroscopic or CT-images during the surgical procedure, disadvantageously subjects the patient and the surgeon to greater exposure to ionizing radiation. Yet another inherent limitation of previously known navigation systems is the inability to provide a high degree of confidence of the location of the working end of a surgical tool in three-dimensions, based on two-dimensional displayed images. While navigation systems may provide multiple views of the patient's anatomy and the working end, it may be challenging for the surgeon to mentally integrate such views in real time to achieve a high degree of confidence about the location of the working end of the surgical tool. The foregoing drawbacks may become even more acute where the surgeon is directing a robotic arm to perform the surgery. While robotic systems provide important benefits, such as reducing surgeon fatigue and eliminating transmission of physician muscle tremors during delicate operations, such systems also typically reduce the physician's tactile feel during advancement of the working end, for example, when passing from bone into tissue. This concern is accentuated when working with delicate structures innervated with critical nervous system components, such as the spine, where maintaining high precision is vital to the patient's safety. Accordingly, it would be desirable to provide systems and methods that improve upon known surgical navigation systems, particularly those having a robotic component, and which enable a surgeon accurately to confirm the location of the working end of a surgical tool in three-dimensional space. Such improved systems and methods would offer a surgeon a high degree of confidence that a robot-assisted surgery is being conducted in accordance with pre-operative plans. It further would be desirable to provide systems and methods that improve real-time registration of a working end of a robotically controlled surgical tool with images displayed on a navigation system and which could alert the surgeon when the working end is approaching a transition in the anatomy. It still further would be desirable to provide systems and methods for use with a navigation system and robotic interface that not only alerts a surgeon when the working end is approaching a transition in the anatomy, but also provides a revised trajectory to the robotic arm or surgeon to avoid the anatomical transition.",
                    "SUMMARY": [
                        "In accordance with the principles of the present invention, systems and methods are provided that improve upon known surgical navigation systems, particularly those having a robotic component, by enabling a surgeon accurately to confirm the location of the working end of a surgical tool in three-dimensional space. The inventive systems and methods permit a surgeon to achieve a high degree of confidence that a robot-assisted surgery is being conducted according to pre-operative plans. Further in accordance with invention, systems and methods are provided that improve real-time registration of a working end of a robotically controlled surgical tool with images displayed on a navigation system. The inventive systems and methods may alert a surgeon when the working end is approaching a transition in the anatomy that is being traversed by the working end. Further, the inventive systems and methods may suggest to the surgeon, or directly implement via feedback to the mechanism controlling movement of the robotic arm, a revised trajectory for the working end of the surgical tool to avoid the anatomical transition. In one exemplary embodiment, the inventive medical system includes a robotic arm having a base and a surgical tool having an effector, the robotic arm configured to allow movement of the effector relative to the base, and a control unit connected to the robotic arm and configured to issue a signal that controls movement of the tool and effector relative to the base. In one embodiment, the robotic arm is controlled by the control unit and used by a surgeon to perform at least some of his gestures during a surgical procedure on an anatomical portion. In a preferred embodiment, the medical system includes a navigation system that displays an overlay of the tool and effector on images of the anatomical portion. The images of the anatomical portion may be pre-acquired or acquired in real time during the surgical procedure. In a preferred embodiment, the medical system is designed to improve the gesture precision and to prevent damage to particularly sensitive tissues of the anatomical portion. In an embodiment for use in orthopedic surgery or spine surgery, the medical system may be used to control movement and trajectory of a medical device, , a medical or surgical instrument, tool or implant, relative to boney anatomical structure. The inventive system may reduce the risk of damage caused by inadvertent intrusion into areas of functional tissue, such as nervous system tissue, near the boney structure. The inventive system and methods are particularly advantageous in controlling the movement of the medical device involved in attaching an implant in the pedicle of a vertebra of the spine, in immediate proximity to the functional tissue that include the spinal cord, nerve endings, and vascular structures. During the surgical procedure, movement of the medical device is generally controlled by a navigation system including a tracking device and a display device. The tracking device may comprise targets of any suitable nature, integral with of affixed to the medical device, and one or more target detection members, such as optical cameras. The control unit thus may detect a position of the medical device within a reference frame, determined by the tracking device, and display it on the display device superimposed on a representation of the anatomical portion. In accordance with one aspect of the invention, the medical system includes a device designed to penetrate an anatomical portion composed of different mediums having different electrical characteristics, which characteristics vary as a function of the capacities of the mediums to conduct electric current. Further in accordance with the inventive principles, the device is configured to sense the different electrical characteristics of the different mediums, and emit a warning signal when a transition in the electrical characteristics is sensed. The control unit controlling movement of the robotic arm, responsive to the detection of an impending transition in electrical characteristics, may cease movement of the device, or revise the trajectory of the device, within the anatomical portion to avoid the transition. The present invention also relates to a method of verifying a trajectory of a surgical tool during a procedure. The method includes receiving one or more images of an anatomical portion of a patient and executing a surgical plan to insert a medical device into the anatomical portion, during which sensor data is collected from one or more sensors embodied in the medical device as it is being inserted into the anatomical portion. The method further includes determining whether the sensor data corresponds to the surgical plan; and sending, in response to determining that the sensor data does not correspond to the surgical plan, an alert indicating that the surgical tool is not being inserted according to the surgical plan. In one embodiment, the medical device includes a bipolar electrode sensing arrangement, wherein a body of the medical device extends between a distal end designed to penetrate an anatomical portion and having an external surface, and a proximal end, opposite the distal end. Preferably, the body has at least one first electrode comprising a first contact surface arranged on the external surface at the distal end, configured to contact the anatomical portion, and at least one second electrode having a second contact surface, arranged on the external surface at the distal end, to contact the anatomical portion at a distance from the first contact surface. An electric generator may be connected to the first and second electrodes to apply a voltage between the first and second contact surfaces. A processing device is connected to the electric generator and the first and second electrodes and programmed to determine a measurement parameter related to the electrical characteristic based on a measured electric current between the first and second electrodes, and to emit a warning signal corresponding to the measurement parameter. The control unit is configured to issue a control signal to control movement of the medical device responsive to the warning signal. The inventive system and method thus makes it possible to control the movement of the robotic arm using a feedback loop based on a warning signal representative of the relative positions of the body of the medical device and the anatomical portion. Such control based on the actual position of the body of the medical device relative to the anatomical portion improves the reliability and safety of the medical system. In addition, the inventive system and method may be employed with previously-known navigation systems to improve precision of the representation of the medical device relative to the anatomical portion, for example, resulting from the acquisition of MRI or ultrasound images. In accordance with another aspect of the invention, a medical system includes a surgical navigation system for verifying a trajectory of a medical device during a procedure. The surgical navigation system includes at least one sensor arrangement attached to the medical device, an imaging system configured to capture images of an anatomical portion of a patient and to transmit the captured images to a control system, and a control system configured to operate the medical device. In a preferred embodiments, the control system is configured to receive the captured images and to execute a surgical plan to insert the medical device into the anatomical portion using a robotic arm. The control system also is configured to receive sensor data collected from the sensor arrangement as the medical device is being inserted into the anatomical portion, to determine whether the data provided by the sensor arrangement corresponds to the surgical plan. If the control system determines that the sensor data does not correspond to the surgical plan, an alert is generated indicating that the medical device is not being inserted according to the surgical plan. In yet another aspect of the invention, a computer program is provided that includes a non-transitory computer-readable storage medium containing program instructions for verifying a trajectory of a medical device during a surgical procedure, in which the program instructions are executable by at least one processor. The program instructions include receiving a captured image of an anatomical portion of a patient, and executing a surgical plan to insert a medical device into the anatomical portion. Further in accordance with the invention, the program instructions include receiving sensor data collected from a sensor arrangement being inserted into the anatomical portion, the sensor arrangement embodied in or affixed to the medical device. Preferably, the program instructions include determining whether the data generated by the sensor arrangement corresponds to the surgical plan. If it does not, the program instructions include sending an alert indicating that the medical device is not being inserted according to the surgical plan.",
                        "The foregoing and other objects, features and advantages of the invention will be apparent from the following more particular descriptions of exemplary embodiments of the invention as illustrated in the accompanying drawings wherein like reference numbers generally represent like parts of the disclosure. 1 illustrates an exemplary surgical system constructed in accordance with the principles of the present invention that includes a robotic arm having a medical device, control unit, surgical navigation system, imaging system and display. 2 is a detailed view of an exemplary robotic arm for use in the surgical system depicted in 1, in which the medical device includes a body suitable for penetrating an anatomical portion having variable electrical characteristics. 3 is a detailed view of the exemplary medical device of 2. 4 is a schematic illustration of an alternative embodiment of the inventive system, in which the body of the medical device is configured to be moved by external action exerted on the medical device and in which the control unit is configured to issue a control signal as a function of the warning signal. 5 is a schematic illustration of a further embodiment of the invention, in which the effector includes a duct and a support that is movable relative to the duct, such that the body of the medical device is mounted on the support and the control signal instructions direct movement of the support relative to the duct. 6 is a flowchart illustrating a method of a surgical navigation procedure of the present invention. 7A to 7C are illustrative examples of implementation of the surgical navigation procedure of the present invention. 8 is an exemplary illustration showing penetration of a spinal pedicle and vertebrae, and the corresponding transitions in the sensed electrical characteristics, and changes in control unit instructions provided to the medical device. 9 through 15 are an exemplary illustrations, similar to that of 8, for different initial trajectories of the medical device."
                    ],
                    "DESCRIPTION": "The present invention is particularly advantageous for use in the field of orthopedic surgery and spine surgery to assist a surgeon during a surgical procedure placing an implant in one or more vertebrae of a patient's spine. The assistance of the medical system may be partial, controlling only a portion of the surgeon's gestures, complete, controlling the gestures in place of the surgeon, or a combination of the two. The inventive medical system thus enables improving the precision of the gestures and preventing the risk of damage related to unintended intrusion into sensitive functional tissues, such as the spinal cord, nerve endings, and vascular structures. Although the system and methods of the invention are described herein with respect to an application in a vertebra, and more generally in a boney structure, they are not limited to such an application. Instead, the principles of the present invention advantageously may be applied to any anatomical portion comprising different mediums and having an electrical characteristic, such as a conductivity or resistivity, which varies as a function of the capacities of the mediums to conduct an electric current. Referring now to 1, an illustrative operating environment for spinal surgical procedures arranged in accordance with the principles of the present invention is described. Operating environment 100 includes precision guidance system 102 and imaging system 104. Operating environment 100 enables a surgeon to generate and display images of a trajectory of a surgical instrument within an anatomy of patient P during a surgical procedure. An exemplary surgical navigation system is described in 8,634,897 B2, the entirety of which is hereby incorporated by reference. Commercial embodiments of surgical navigation systems include, for example, the Fluor Nav. system, which utilizes the StealthStation Treatment Guidance Platform, both of which are available from Medtronic Sofamor Danek, Inc. The StealthStation Treatment Guidance Platform, and in particular the StealthStation Navigation System, is described in part in the StealthStation S8 Spinal Navigation Solution brochure published by Medtronic, Inc. , circa 2019, and in The Clinical and Economic Benefits of Using StealthStation Navigation and O-arm Imaging Systems for Spine Surgery brochure published by Medtronic, Inc. , circa 2014. Such surgical navigation systems may be used in combination with robotic systems, such as, for example, the Mazor X Stealth Edition, available from Medtronic, Inc. Precision guidance system 102 may include monitor 106, robotic 108, camera 110, and control unit 111, which has one or more processors and at least one computer readable medium. The computer may be any programmable electronic device or computing system capable of receiving and sending data and performing computer-readable program instructions on the at least one computer readable medium. The computer-readable program instructions may be instructions to operate a medical system, illustratively described as method 200 in 6. The one or more processors may be configured to execute program instructions to verify a trajectory of a surgical tool, such as a pedicle access needle, Jamshidi needle, guidewire, or probe 112, during a surgical procedure. Monitor 106, robot 108, and camera 110 each may be pivotably coupled to base 114 of precision guidance system 102. Camera 110 may be an integrated 3D camera with spatial tracking features. Camera 110 may be used to track the movements of robotic arm 108 to enable precision guidance system 102 to control movement instructions for robot 108. Monitor 106 may include a control panel for a user to interact with in a sterile area of operating environment 100. Monitor 106 may display a surgical plan and/or an actual trajectory of a surgical tool, for example, drill bit 112, within an anatomical portion of patient P. Monitor 106 may be configured to receive and display images from imaging system 104. Operation of robot 108 is controlled by control unit 111 of precision guidance system 102, which in one embodiment provides movement instructions to robot 108, for example, as determined by a surgeon or in accordance with a surgical plan. Robot 108 may be configured to rotate about one or more axes to perform a surgical procedure. Robot 108 also may include instrument holder 116, configured in a variety of shapes, to secure tools, such as surgical tools, to the distal end of robot 108. For example, instrument holder 116 may be configured to receive and hold a penetrating device, such as a drill bit. Imaging system 104 illustratively includes workstation 118 having workstation monitor 120, and image receiving section 122 coupled to image generating section 124 via arm 126. Arm 126 has a shape that enables image receiving section 122 and image generating section 124 to be positioned above and below patient P laying on surgical table 128. For example, arm 126 may be configured in a C shape such that image generating section 124 is positioned at a bottom distal end of the C-shape arm, and image receiving section 122 is positioned at an upper distal end of the C-shape arm. When imaging system 104 is positioned to take images of patient P, image generating section 124, patient P, and image receiving section 122 are linearly aligned with one another. Imaging system 104 may be a computed tomography CT fluoroscopic image-based surgical navigation system configured to acquire and display CT images and/or x-ray images appropriate for a given surgical procedure. However, it should be understood that imaging system 104 is not limited to use with any particular image guided surgical system. For example, imaging system 104 may acquire images from other modalities than CT fluoroscopic image-based surgical navigation system, including, for example, ultrasound, PET, or magnetic resonance imaging. CT images and/or x-ray images may be collected when patient P is positioned laying on surgical table 128 within arm 126 of imaging system 104, with the images preferably taken at a time prior to initiation of a surgical procedure. Preferably, the images may be taken from two orthogonal directions, such as anterior-posterior A-P and lateral, of the anatomical portion of patient P. Imaging system 104 may transmit the acquired images from image receiving section 122 to workstation 118, which may be configured to display the received images via workstation monitor 120. Imaging system 104 also may provide the received images to precision guidance system 102. Referring now to 2, robot 108 and probe 112 coupled to control unit 111 of 1 are described in greater detail. Robot 108 includes base 130 and effector 132 arranged, in the embodiment shown, opposite to base 130. Robotic arm 134 is configured to enable movement of effector 132 relative to base 130. In particular, the robotic arm 134 may include several segments linked together by joints. In the embodiment shown, a first segment constitutes base 130 on which a first end of a second segment 136 is mounted by means of joint 138 having an appropriate number of degrees of freedom. A third segment 140 carrying effector 132 is mounted on a second end of second segment 136 by means of a second joint 142, also having an appropriate number of degrees of freedom. At least one of joints 138, 142 is equipped with at least one actuator. As will be apparent from the following description, the actuators of the joints may be reversible, thus allowing relative movement of the segments relative to one another under the effect of an external action exerted on robot 108, , applied by a surgeon on robotic arm 134. At least one of the reversible actuators is controlled by control unit 114. Medical device or probe 112 is configured to penetrate an anatomical portion, such as a region that includes a vertebra and surrounding tissue. It is important to ensure precise positioning of medical device 112 to avoid damaging, or even worse, passing through, the inner layer of cortical bone delimiting the foramen, or the outer layer, of cortical bone near the nerve endings. Medical device 112 preferably is configured to emit a warning signal that varies as a function of the sensed electrical characteristic when it is moved within an anatomical portion. Still referring to 2, medical device 112 illustratively is instrumented drilling device 144, and operates analogously to the hand tool described in 7,580,743, which is incorporated herein by reference, which device is commercially available from the assignee of the present application under the tradename PediGuard. Although described in relation to drilling device 144, it is to be understood that the invention is not limited to this specific type of medical device, and may be implemented with other types of medical or surgical tools or instruments, such as a probe, a square tip rongeur, spatula, curette, or tap. Medical device 112 also may constitute an implant to be placed in the anatomical structure, such as a screw, and in particular a pedicle screw. Drilling device 144 includes drill bit 146 suitable for penetrating the boney structure of a vertebra. Drill bit 146 extends along longitudinal axis L between proximal end 146a and distal end 146b, forming tip 147 for penetrating boney structure. Drill bit 146 generally has a cylindrical external surface of circular cross-section around longitudinal axis L and is provided with one or more spiral cutting edges near tip 147. The body of drill bit 146 could, however, have any other shape, in particular cylindrical with a polygonal or other cross-section. Drill bit 146 comprises first electrode 148, cylindrical and of conductive material, extending inside drill bit 146 parallel to longitudinal axis L. In particular, first electrode 148 is arranged in a central bore of drill bit 146 and extends coaxially to longitudinal axis L up to a free end having first contact surface 149, which is flush with the external surface of drill bit 146 at tip 147. Drill bit 146 also comprises second electrode 150, annular and of conductive material, extending along longitudinal axis L around first electrode 148. In particular, second electrode 150 is formed by a portion of drill bit 146 itself, made in this case of a conductive material. Second electrode 150 has second contact surface 151 composed of a cylindrical portion parallel to longitudinal axis L and corresponding to a lateral surface of drill bit 146, and an annular portion transverse to longitudinal axis L corresponding to a distal surface of drill bit 146. A layer of electrically insulating material is interposed between first electrode 148 and second electrode 150 in such a manner that first contact surface 149 and second contact surface 151 can come into contact, at a distance from one another, with the anatomical portion during penetration of drill bit 146 into the anatomical portion. It should be understood, however, that the invention is not limited to the embodiment illustrated by drill bit 146, and other shapes are possible, such as, for example, that first electrode 148 and second electrode 150 are not arranged coaxially but may be formed from a rod of conductive material inserted into body 146. Furthermore, first electrode 148 and second electrode 150 each may have a point-like or other contact surface 149, 151 flush with the lateral surface or distal surface of drill bit 146. Alternatively, drill bit 146 could support two or more first electrodes 148 and two or more second electrodes 150. Medical device 112 includes casing 152 to which proximal end 146a of drill bit 146 is integrally secured. Casing 152 has a housing that may enclose electronic components that enable medical device 112 to emit an appropriate warning signal. Those components include electric generator 154 and electric processing device 156 mounted on circuit board 158. Electric generator 154 is connected to first electrode 148 and second electrode 150, and is suitable for applying one or more voltages across first contact surface 149 and second contact surface 151. Processing device 156 may be connected to electric generator 154 and to first electrode 148 and second electrode 150, and is suitable for determining a measurement parameter related to the electrical characteristic based on a measurement electric currents induced by the applied voltages, and for emitting the warning signal corresponding to the measurement parameter. The measurement parameter may in particular be a voltage, an intensity of the electric current, conductivity or resistivity, or may be the result of processing one or more measurement electric currents, such as by integration, averaging, or the like. Casing 152 also may enclose a device supplying electric power to electric generator 154 and processing device 156. It also may include a communication interface communicating with control unit 111 by any suitable means, wired or wirelessly, and optionally to workstation 118, , to re-center the image or correct the relative position of the instrument image as compared to the anatomy. In alternative embodiments, electric generator 154 and processing device 156, as well as the other electronic components of the medical device, could be located remote from the body of the medical device. For example, such components could be carried by robotic arm 134 or integrated into control unit 111. To rotate drill bit 146 about longitudinal axis L, medical device 112 includes a drive mechanism, such as a gear motor assembly. In the first embodiment of 2, the drive mechanism may be mounted in housing 160 integral to effector 132 of robotic arm 134 so that, once integrally secured to the drive device, drill bit 146 is mounted on effector 132 of robotic arm 134. Referring now to 4, an alternative embodiment of portions of robot 108 constructed in accordance with the principles of the present invention is described. This alternative embodiment differs from the embodiment of 2 in that it is especially adapted to be implemented in a context of co-manipulation. As indicated above, medical device 112 of 2 may be operated independently of robotic arm 134, and movement of the drill bit may be obtained by an external action exerted on medical device 112 by the surgeon, for example. The actuators, or at least a portion of them, are reversible, in other words, they follow movement of the drill bit imposed by the external action, outside of particular situations identified further below. Effector 132 of robotic arm 134 of 4 includes a stop member, making it possible to control movement of drill bit 146 at the appropriate time, as will be apparent from the following description. In particular, effector 132 of robotic arm 134 has duct 162 suitable for receiving drill bit 146. Duct 162 serves as a guide for drill bit 146 and also as a stop member. A portion of duct 162, and in particular upper edge 164, which defines an upper opening through which drill bit 146 is inserted into duct 162, may form the stop member that contacts the drilling device to control its movement when necessary. In this alternative embodiment, the drive mechanism for drill bit 146 is independent of effector 132 and, for example, may include a hand drill, not shown, held by the surgeon. It should be noted that co-manipulation also may be obtained with medical system 100 according to the embodiment of 2, such that the surgeon exerts an external action on drilling device 112 either directly, by manipulating the drilling device, or indirectly, by manipulating effector 132. Drilling device 112 also may be manipulated in co-manipulation scheme by means of a robotic arm that includes a stop member other than robotic arm 134. In addition, the stop member may be implemented in any other suitable manner so as to come into contact with drilling device 112 to control movement of drill bit 146. The other features of operating environment in which robot 108 according to the embodiment of 4 is used are similar to those of operating environment 100 according to the embodiment of 2, and such further details are provided herein above. 5 depicts robot 108 according to a further alternative embodiment of the invention. In the embodiment of 5, effector 132 of robotic arm 134 includes duct 152 similar to the one previously described with respect to the embodiment of 4. Effector 132 includes support 166 that is movable relative to duct 162, and on which drill bit 146 is mounted. In particular, in the embodiment of 5, support 166 is movable in translation along a central axis of duct 162. Alternatively, any other movement of support 166 relative to duct 162 could be provided. Support 166 carries housing 168, which contains the drive mechanism to which the drill bit can be secured for rotation through duct 162. As indicated above, duct 162 also may serve as a guide and stop member. In accordance with one aspect of the invention, control unit 111 preferably is configured to issue a control signal that controls the movement of effector 132 relative to base 130 as a function of the warning signal emitted by drilling device 112 when drill bit 146 is penetrating targeted tissue. Referring now to 6 and 7A-7C, a flowchart illustrating method 200 for conducting a surgical procedure according to one aspect of the present invention, for example as depicted in 7A-7C, is described. 7A, 7B and 7C illustrate exemplary surgical procedures 300a, 300b and 300c to be carried out in accordance with method 200. Referring still to 6, a plurality of images of a target anatomy are captured at step 202, for example, using imaging system 104 of 1. The captured images may be CT images and/or x-ray images of the anatomical portion of patient P where a surgical procedure will be performed. The CT images, and/or x-ray images may be collected when patient P is positioned lying on surgical table 128 within arm 126 of imaging system 104, or more preferably, are taken at a time prior to performing the surgical procedure. The images may be taken from two orthogonal directions, such as anterior-posterior A-P and lateral, of the anatomical portion, , a portion of 302 vertebra, such as a pedicle, of patient P. Imaging system 104 may transmit the acquired images from image receiving section 122 to workstation 118 and/or precision guidance system 102. A surgical plan is determined at step 204 and a user, , a surgeon, may input the surgical plan to be executed by precision guidance system 102. Based on the captured images, the surgeon may determine the surgical plan to implant and/or determine a trajectory placement of a surgical device or implant, such as a pedicle screw. The surgical plan may include the path, such as path 304, to implant the surgical device into the anatomical portion of patient P. For example, the surgeon may determine that a hole configured to receive a pedicle screw may be formed via path 304, as shown in 7A-7C. The surgeon may determine that path 304 enters into central portion 306 of left pedicle 308 of vertebra 302. The surgeon, using the captured images, may determine data related to the anatomical portion, , the bone type of the anatomical portion and/or the bone type around path 304. Having determined the surgical plan, the plan then may be imported into workstation 118 and/or precision guidance system 102. A mounting platform, such as a fixation bone clamp of precision guidance system 102 as described in 9,066,751, which is incorporated herein by reference in its entirety, may be rigidly attached to the patient at another anatomical portion, such as a spinous process of another vertebra either above or below the target vertebra, of the patient. Alternatively, a mounting platform, such as a percutaneous reference pin for use with precision guidance system 102, may be rigidly attached to the patient at another anatomical portion, such as the posterior superior iliac spine. Precision guidance system 102 may perform a 3D scan of the surgical location, via camera 110 and/or a 3D camera integrated into robotic 108, to reconstruct the 3D volume of patient P and assess the working area for the surgeon. The one or more captured images may be mapped to the 3D scan of the surgical location. A medical tool, such as a probe or drilling device, is inserted at step 206 into the anatomical location. Robot 108 may begin to insert the probe, drilling device 112, into the anatomical portion, , left pedicle 306 of vertebra 302 of patient P. As the probe is inserted into the anatomical portion, sensor data is collected at step 208, preferably by electrodes 148 and 150 of device 112. The probe may transmit the collected sensor data to precision guidance system 102, via a suitable wired or wireless arrangement. Precision guidance system 102 may be configured to convert the collected sensor data to impedance values or measures of bone density and/or tissue density. Medical device 112 may transmit the collected sensor data to precision guidance system 102, which analyzes the collected sensor data in real-time. Having analyzed the collected sensor data, a determination is made at step 210, preferably by precision guidance system 102, whether the collected sensor data corresponds to the surgical plan. For example, precision guidance system 102 may determine whether the collected sensor data corresponds to the surgical plan in periodic time increments. Precision guidance system 102 also may continuously evaluate whether the collected sensor data corresponds to the surgical plan while the drilling device penetrates into the anatomical location. When the surgical plan is created using the captured images, precision guidance system 102 may determine data related to the anatomical portion. For example, precision guidance system 102 may determine at least one of the bone density and the tissue density of the anatomical portion and/or at least one of the bone density and the tissue density around path 304. To determine whether the collected sensor data corresponds to the surgical plan, precision guidance system 102 may compare the collected sensor data to the data of the anatomical portion predicted in the surgical plan. For example, for the cases in which the collected sensor data relates to bone impedance values, the collected sensor data may be compared to the predetermined bone impedance value of the anatomical portion. For the cases in which the collected sensor data corresponds to the surgical plan at decision step 210: YES, the drilling device continues penetrating into anatomical portion at 212. For example, precision guidance system 102 determines that path 304 measured by the sensors corresponds to path 204 determined by the surgical plan. That is, the impedance values collected by electrodes 148, 150 corresponds to the electrical characteristics predetermined in the surgical plan. In example 300a, precision guidance system 102 determines that the collected sensor data places path 304 on an equivalent path or almost equivalent path as path 304 determined by the surgical plan. In this case, precision guidance system 102 determines that the impedance values measured by the sensors while traveling along path 304 correspond to the predetermined electrical characteristics of vertebral body 302. In example 300b, precision guidance system 102 determines that the collected sensor data places path 304 slightly angled and/or off center from path 204; however, precision guidance system 102 determines that the collected sensor data is within an acceptable range of path 304. That is, the impedance values of the anatomical portion measured by the sensors may not be equivalent to the predetermined electrical characteristics; however, the values measured by the sensors may be close enough to the predetermined values to be acceptable for precision guidance system 102, , within 5-10%. In this case, robot 108 continues to insert the drilling device into left pedicle 308 of the vertebra 302. For the cases in which the collected sensor data does not correspond to the surgical plan at decision step 210: NO, an alert is sent and the insertion of the probe into the anatomical location may be stopped at 214. For example, the collected sensor data may not correspond to the surgical plan when the impedance values collected by the sensors is not equivalent to the predetermined electrical characteristics in the surgical plan, and/or the data collected by the sensors falls outside of the acceptable range of the predetermined values. For these cases, precision guidance system 102 may send an alert to the surgeon that the collected sensor data does not correspond to the surgical plan, and/or stop robot 108 from further inserting the drilling device into the anatomical location. The alert may be provided as an alarm, message, feedback, or in other suitable manner. For example, the alert may constitute a graduated visual signal and/or audio signal. For cases in which the impedance data collected by the sensors deviates from the surgical plan, a visual signal may appear on monitor 106 and/or workstation monitor 120. For instance, if precision guidance system 102 determines that the data collected by the sensors deviates at or about 1% from the predetermined data in the surgical plan, a yellow warning light may appear on monitor 106. In another instance, if precision guidance system 102 determines that the data collected by the sensors deviates at or about 10% from the predetermined data in the surgical plan, an orange warning light may appear on monitor 106. Still further, if precision guidance system 102 determines that the data collected by the sensors deviates at or about 20% from the predetermined data in the surgical plan, a red warning light may appear on monitor 106. In an alternative embodiment, instead of visual warning signals, the alerts and warnings may manifest as audible tones, which enables the surgeon to monitor the progress of the surgical plan without diverting his attention to monitors 106 or 120. In such an embodiment, a simple tone may be emitted from the precision guidance system if the deviation between measured and plan impedance values differs by 1% or less. If the measured values deviate from plan by about 10%, a second audio signal may be emitted as a more complex tone than the tone of the first audio signal. Further, if the deviation from surgical plan exceeds about 20%, a third audio signal may be emitted as a more complex tone than the tone of the second audio signal. Precision guidance system 102 may automatically stop robot 108 in those cases for which the collected sensor data does not correspond to the surgical plan. In addition, the surgeon may manually stop, redirect or interrupt the automatic or manual insertion upon receiving or perceiving the alert. In the scenario depicted in 7C, precision guidance system 102 determines that the collected sensor data places path 310 outside an acceptable range of path 304. For instance, path 310 may be at a different insertion angle than path 304, and/or path 310 may extend beyond the predetermined end of path 304. That is, precision guidance system 102 may determine that impedance values measured by the sensors while traveling along path 310 do not correspond to the predetermined electrical characteristics of vertebral body 320. Determining that path 310 is outside the acceptable range for path 304, precision guidance system 102 stops advancement of drilling device 112 probe into left pedicle 308 and issues an alert to the surgeon to indicate that robot 108 is not following path 304 of the surgical plan. Precision guidance system 102 may issue instructions to stop inserting drilling device 112 when the insertion angle of path 310 is determined to be incorrect or path 310 extends beyond an end portion 312 of path 304 determined by the surgical plan. Precision guidance system 102 also may be configured to assess the collected sensor data to determine whether drilling device 112 is askew. For example, precision guidance system 102 may determine whether the impedance values from the sensors are within an acceptable range. If, for instance, precision guidance system 102 senses a deviation that is not within an acceptable range, precision guidance system 102 may determine that the probe is skewed and compute a revised trajectory to reduce the deviation to an acceptable level, for example, using a multiple electrode arrangement as described in commonly-assigned 8,419,746, the entirety of which is incorporated herein by reference. Such a revised trajectory may be communicated to the surgeon by directional arrows on monitor 106 showing a revised insertion angle. In this manner, data from the sensors may be used to provide an additional source of data to the surgeon to confirm or improve the accuracy of execution of the surgical plan. The sensor data may serve as a feedback mechanism for precision guidance system 102 to provide a high degree of confidence in the accuracy of path 304. Alternatively, control unit 111 may be programmed to generate a series of micro-motions of tip 147 during advancement of drill bit 146, for example, by vertical, angular and/or lateral displacements of tip 147 of up to 1 mm in directions spaced at 90 degrees around the bore hole created by drill tip 147 to determine impedance values in each of the corresponding quadrants. Such motions may include, for example, reversing drill bit 146, removing the shaft of the drill bit from a current bore, slightly changing the angulation or location of the entry point of 147, and again advancing the drill bit. By storing and comparing the sensed impedances resulting from the micro-motions, control unit may determine when tip 147 is nearing a transition in tissue electrical characteristics, and thus steer further advancement of drill bit 146 in the direction that most closely aligns with the predetermined path of the surgical plan. Control unit 111 again may be programmed to provide such feedback to the surgeon, or to assist in visualizing such trajectory corrections, by superimposing a revised computed trajectory on monitor 106 of precision guidance system 102. As a further alternative, if the trajectory corrections computed based on the micro-motion displacements are not directly provided to robot 108 to effectuate the trajectory, the suggested trajectory corrections may be visually communicated to the surgeon by directional arrows superimposed on the images of the anatomical portion displayed on monitor 106. Referring now to 8 to 15, vertebra 320 is a boney structure internally comprising foramen 321 traversed by the spinal cord and vascular structures. On a dorsal face, vertebra 320 has spinous process 322 extending in a sagittal plane and two transverse processes 323 extending substantially one on each side of foramen 321 in a frontal plane, with nerve endings passing nearby. Vertebra 320 is externally delimited by outer layer 324 of cortical bone. Foramen 321 is itself delimited by inner layer 325 of cortical bone. Between outer layer 324 and inner layer 325 of cortical bone is cancellous bone 326. Inner layer 325 and outer layer 324 of cortical bone each constitute a first medium having a first capacity to conduct electric current. The cancellous bone constitutes a second medium having a second capacity to conduct electric current, the second capacity being greater than the first capacity. Soft tissues and fluids, such as blood, surrounding outer layer 324 of cortical bone and inside inner layer 325 of cortical bone constitute a third medium having a third capacity to conduct current, the third capacity being greater than the first and second capacities. 8 illustrates an exemplary control signal issued by precision guidance system 102 during penetration of drill bit 146 into pedicle 327 of vertebra 320 from entry point A in outer layer 324 of cortical bone to first exit point C in outer layer 324 of cortical bone opposite the entry point A. Longitudinal axis L of drill bit 146 is placed along a predetermined penetration direction T and tip 147 of drill bit 146 is brought into contact with outer layer 324 of cortical bone at the entry point A. Examples of determining the entry point and penetration direction T of the body of a medical device intended to penetrate an anatomical portion are described, for example, in above-incorporated 8,419,746. Still referring to 8, the measurement parameter representative of the electrical characteristic taken into account is an intensity of a measurement electric current flowing between first contact surface 149 and second contact surface 151, representative of a conductivity of the medium in which tip 147 of drill bit 146 is located. The warning signal emitted therefore corresponds to this intensity. At entry point A, tip 147 of drill bit 146 is in contact with cortical bone. When approaching interface B at the transition between cortical bone 324 and cancellous bone 326 from the outer layer of the cortical bone, tip 147 of the drill bit 146 approaches the cancellous bone. Because the conductivity of cortical bone is lower than that of cancellous bone, the current intensity between first contact surface 149 and second contact surface 151 increases. While traversing the cancellous bone by passing through one of pedicles 327 and until encountering outer layer 324 of cortical bone at exit point C, the intensity remains substantially unchanged and the warning signal reaches a plateau. At exit point C, when tip 147 again approaches cortical bone 324 and begins penetrating into the outer layer of cortical bone, the measured current intensity decreases. While drilling through outer layer of cortical bone 324, tip 147 of drill bit 146 approaches interface D between the cortical bone and the medium composed of soft tissue and fluids, such as blood, which have a higher conductivity than that of cortical bone and cancellous bone. Thus, measured current intensity between electrodes 148 and 150 increases, until a new plateau is reached, when tip 147 crosses the outer layer of cortical bone 324. 9 depicts generation of a similar warning signal during penetration of drill bit 146 into pedicle 327 of vertebra 320 from entry point A into outer layer 324 of cortical bone up to a second exit point C into inner layer 325 of cortical bone defining foramen 321. In effect, drill bit 146 again successively crosses cortical bone in outer layer 324 of cortical bone, then cancellous bone, and then cortical bone in inner layer 325 of cortical bone, before reaching a medium composed of soft tissue and fluids, such as blood, in foramen 321. The warning signal thus may be used to determine a position of tip 147 of drill bit 146 relative to the boney structure of vertebra 320. By choosing one or more critical thresholds, each representative of a critical position of tip 147 of drill bit 146 relative to the boney structure of vertebra 320, and by comparing an absolute value at each instant or a variation over a defined period of the measurement parameter at the corresponding critical threshold, it is possible to control the movement of drill bit 146 by means of the control signal provided by precision guidance system 102. A critical position is understood to mean a position for which its differentiation from other positions is of interest. It may be a position presenting a risk to the patient, but not necessarily. In order to be able to make the comparison, the critical threshold is an absolute value or a variation of a reference parameter comparable to the measurement parameter. The reference parameter may be predetermined based on test results on reference anatomical structures, it may be chosen by a user, or may be defined in any suitable manner. For example, control unit 111 may detect interface B between cancellous bone and cortical bone when the warning signal corresponding to the current intensity between first contact surface 148 and second contact surface 151 varies by decreasing to below first critical threshold SC1. Control unit 111 also may detect a breach in one of inner 325 and outer 324 layers of cortical bone when the warning signal once again increases beyond a minimum value of the measurement parameter representative of cancellous bone with a defined deviation. The deviation from the minimum value of the measurement parameter representative of cancellous bone may constitutes second critical threshold SC2, as depicted in 8. The minimum value of the measurement parameter representative of cancellous bone may be defined in a differential manner during drilling of vertebra 320. For example, control unit 111 of precision guidance system 102 may assign an initial value to the minimum value of the measurement parameter and continuously measure a current value of the measurement parameter. As long as the warning signal does not exceed first critical threshold SC, if the current value of the measurement parameter is less than the minimum value of the measurement parameter, control unit 111 assigns the current value of the measurement parameter to the minimum value of the measurement parameter. More generally, when the anatomical portion includes a first medium having a first capacity to conduct electric current, a second medium having a second capacity to conduct electric current, the second capacity being greater than the first capacity, and a third medium delimited by the first medium and having a fluid with a third capacity to conduct current, the third capacity being greater than the first and second capacities, control unit 111 of precision guidance system 102 may be configured to: detect an interface between the second medium and the first medium, such that the warning signal varies in a first variation direction and passes beyond a first critical threshold; detect a breach in the first medium when, after having varied in the first variation direction with respect to the threshold, the warning signal varies in a second variation direction opposite to the first variation direction, such that the warning signal passes beyond a minimum value of the measurement parameter representative of the second medium with a defined deviation. The minimum value of the measurement parameter representative of the second medium may be obtained as described above in relation to the boney structure. The movement of drill bit 146 may be defined by several movement parameters including: penetration direction T, an advancement direction drawing closer to the bone structure, a reverse direction moving away from the bone structure which are opposite one another along penetration direction T, a trajectory adjustment away from a detected transition, a variable advancement or reverse speed and a variable advancement or reverse force. As long as the warning signal has not reached a critical threshold, the control unit issues a control signal authorizing movement of the drill bit in the advancement direction along penetration direction T relative to the boney structure of the vertebra. Referring now to 8 and 9, the control signal may include instructions for reducing the advancement speed of drill bit 146 in the advancement direction when the warning signal reaches first critical threshold SC1. In addition, the control signal may include instructions for reducing the drive speed of drill bit 146 in the first direction of rotation when the warning signal reaches first critical threshold SC1. In the case of controlling the force, it would be the advancement force that would be reduced. When the warning signal reaches second critical threshold SC2 deviation from the minimum value of the measurement parameter representative of cancellous bone, the control signal may include instructions for stopping the movement of drill bit 146 relative to the boney structure of the vertebra and interrupting the rotation of drill bit 146. Alternatively, any other control of the movement of drill bit 146, and more generally of the body of the medical device, could be provided by issuing the appropriate control signal with the corresponding movement parameters. In particular, when the warning signal reaches a critical threshold corresponding to breaching cortical bone, such as second critical threshold SC2 described above, control unit 111 may issue a control signal including instructions for stopping, moving in the advancement direction and reverse direction over specified ranges to follow the patient's respiratory movements. The control signal also may be adapted to the risks represented by damage to a given medium. For example, in the absence of an immediate major risk, detection of damage to a layer of cortical bone by crossing a corresponding critical threshold, such as first critical threshold SC1 described above, could cause control unit 111 to reduce the advancement speed but increase the drive speed in the first direction of rotation to take into account the greater hardness of cortical bone compared to that of cancellous bone. To ensure continuous and real-time control of the movement of drill bit 146, the measurement electric current has a measurement period that is less than the ratio of a critical distance of drill bit 146 in the advancement direction along penetration direction T, to the advancement speed of drill bit 146, the critical distance being in particular less than or equal to 1 mm. Electric generator 154 of drilling device 112 may be connected to control unit 111 and the control unit may be suitable for measuring the advancement speed of drill bit 146 and for controlling electric generator 154 so that it applies the appropriate measurement electric current. To improve control over the movement of drill bit 146, in addition to the warning signal, control unit 111 may issue the control signal as a function of one or more other signals. The combination of the warning signal providing information on the electrical characteristic of the medium along with other signals can enable differentiating between different mediums having similar capacities to conduct electric current. The actual position of drill bit 146 relative to the boney structure of the vertebra thus may be defined more precisely, and the control signal adapted accordingly. In accordance with another aspect of the invention, the medical system may comprise a depth detection device connected to control unit 111 and configured to emit a depth signal corresponding to the depth to which drill bit 146 has penetrated the boney structure of the vertebra. The depth detection device may be of any suitable type, and may include, for example, one or more position sensors integrated into robot 108 that make it possible to determine the depth based on displacements of the robot actuators. As an alternative, the depth detection device may comprise one or more external sensors, for example optical sensors, that detect markings on the exterior of drill bit 146. Accordingly, in 8 and 9, based on the combination of warning and depth signals, control unit 111 may determine which among inner layer 325 and outer layer 324 of cortical bone has been reached after having passed through the cancellous bone and, where appropriate, issue different control signals depending on the layer of cortical bone reached. For example, a reduction in the advancement speed could be greater when inner layer 325 of cortical bone is reached than when outer layer 324 of cortical bone is reached. Or, it could be provided to impose a reverse speed and drive drill bit 146 in a second direction of rotation upon reaching inner layer 325 of cortical bone, while it would be provided to reduce the advancement speed and the drive speed in the first direction of rotation upon reaching outer layer 324 of cortical bone. 10 depicts an example of a control signal starting from when a depth threshold SP1 is reached. The control signal modifies the movement of drill bit 146 when one among critical thresholds SC1 and SC2 and depth threshold SP is reached. The control signal may include instructions enabling movement of drill bit 146 in the advancement direction along the penetration direction T at an advancement speed, from entry point A of outer layer 324 of cortical bone, as long as the depth signal has not reached depth threshold SP1. In the absence of detecting that one of critical thresholds SC1, SC2 has been exceeded, movement of drill bit 146 continues until the depth signal reaches depth threshold SP1 indicating that tip 147 of drill bit 146 is positioned at a target depth, for example corresponding to a length of a pedicle screw to be implanted. In 10, when the depth threshold SP1 is reached, the control signal reduces the speed of drill bit 146 to stop it, it being understood that any other change to the movement or direction of drill bit 146 could be undertaken. For example, the control signal could include instructions for moving the drill bit in a reverse direction along penetration direction T, reduce either or both of the advancement speed and force of the drill bit, or redirect the drill bit. In accordance with yet another aspect of the invention, control unit 111 may include several predefined signatures. Each signature may include a reference warning signal resulting from a variation of the measurement parameter related to the electrical characteristic during penetration of drill bit 146 into a reference anatomical portion. Each signature also may combine the reference warning signal with a reference depth signal resulting from a variation of a depth parameter related to the depth to which drill bit 146 has entered the reference anatomical portion. Each signature further may include a corresponding set of movement parameters, at least some of them possibly different from the movement parameters of the other sets of movement parameters. These movement parameters also may include the critical thresholds, the depth thresholds, or other parameters. The warning signal thus may be analyzed differently, in particular with respect to the exceeding of certain critical thresholds, depending on the actual position of drill bit 146 relative to the boney structure of the vertebra. More specifically, control unit 111 may be configured so that, during penetration of drill bit 146 into a vertebra, it continuously saves the measurement parameter and compares the variation of the measurement parameter with the signatures. If the variation of the measurement parameter corresponds to one of the signatures, the control unit issues the control signal with the set of movement parameters corresponding to that signature. For example, the variation in intensity as the measurement parameter with respect to the depth illustrated in 9 may constitute an internal breach signature representative of drilling vertebra 320 from an entry point facing pedicle 327 and along a path leading to breaching inner layer 325 of cortical bone. The variation in intensity as the measurement parameter with respect to the depth illustrated in 10 may constitute an expected signature representative of drilling vertebra 320, which may be qualified as suitable for placement of a pedicle screw from an entry point facing pedicle 327 and along a path leading to a defined depth through the pedicle. 11 illustrates the detection of a sliding of drill bit 146 on outer layer 324 of cortical bone. Tip 147 of drill bit 146 is positioned on the predetermined entry point A, but slides to another location B on outer layer 325 of the cortical bone, at a distance from the predetermined entry point A. In accordance with the expected signature represented by the dotted line, the current intensity as a measurement parameter should be constant at a level corresponding to that of cortical bone and then should increase to the level of intensity of cancellous bone, such that the depth signal reaches third depth threshold SP3, for example about 5 mm, corresponding to the interface between cortical bone and cancellous bone. In the case of sliding of drill bit 146 as depicted in 11, instead of being located in cortical bone, tip 147 of the drill bit enters the soft tissue and fluids surrounding vertebra 320, where the conductivity is higher than that of cancellous bone. Before the depth signal reaches third depth threshold SP3, the warning signal exceeds second critical threshold SC2 representative of breaching the cortical bone and the control unit shuts down drill bit 146. 12 depicts detection of traversing one of transverse processes 323, based on internal breach and/or expected signatures as represented by a dotted line. According to the internal breach signature, no breach should be detected by the second critical threshold SC2 being exceeded by the warning signal before fourth depth threshold SP4 is reached. When traversing transverse process 323, tip 147 of drill bit 146 is positioned at a predetermined location A, but penetration direction T leads it to successively cross outer layer 324 of cortical bone, cancellous bone, and again the outer layer of cortical bone of transverse process 323. In this case, the warning signal exceeds second critical threshold SC2, representative of breaching the cortical bone, before fourth depth threshold SP4 is attained. Control unit 111, which may impose a reduced advancement speed when first critical threshold SC1 identifying the interface C between cancellous bone and cortical bone is detected, imposes a zero speed at second critical threshold SC2. 13 depicts the signature for tip 147 of drill bit 146 entering pedicle 327 where the density of the cancellous bone increases, leading to a reduction in conductivity. This reduction may be identified by third critical threshold SC3, for example defined in a differential manner, meaning by a variation of the measurement parameter, within a defined spatial window between fifth SP5 and sixth SP6 depth thresholds. Within this spatial window, the warning signal must not exceed second critical threshold SC2, representative of a breach resulting from reaching one among inner layer 325 and outer layer 324 of cortical bone. Upon passage through pedicle 323, the drive speed of drill bit 146 may, for example, increase. 14 depicts the signature corresponding to traversing one of transverse processes 323 followed by a new penetration into outer layer 324 of cortical bone. Although represented in the figures as a function of depth, variations in the measurement parameter also could be obtained as a function of time. In accordance with a yet further aspect of the invention, precision guidance system 102 also may include a force measurement device connected to control unit 111 that is configured to emit a force signal corresponding to a force exerted on drill bit 146. In this embodiment, the force exerted on drill bit 146 may comprise one or more forces in all relevant directions, one or more torques in all relevant directions, or a combination thereof. The force measurement device may be of any suitable type, for example, including one or more force sensors integrated into robot 108 that enable determination of the force on drill bit 146 based on the forces and/or torques exerted by the actuators. In the foregoing embodiment, control unit 111 may control the movement of drill bit 146 as a function of the force signal in addition to controlling it as a function of the warning signal, and if necessary of the depth signal. In particular, control unit 111 may allow movement of drill bit 146 in the advancement direction as long as the force signal has not reached a force threshold SF, and may modify movement of drill bit 146 when the force signal reaches force threshold SF. A reference force signal resulting from a variation of a force parameter related to the force exerted on drill bit 146 during penetration of the drill bit into the reference anatomical portion may be provided in each signature and combined with the reference warning signal, and where appropriate with the reference depth signal. 15 depicts operation of a system that combines warning and force signals to differentiate between two different positions of drill bit 146. In first penetration direction T1, drill bit 146 exits outer layer 324 of cortical bone at exit point B facing pedicle 327. In second penetration direction T2, drill bit 146 exits outer layer 324 of cortical bone at exit point D in one of the transverse processes 323. Traversing the outer layer of cortical bone to reach cancellous bone in first penetration direction T1 and traversing the outer layer of cortical bone to reach soft tissue in the second penetration direction may result in similar warning signals being issued. By contrast, cancellous bone, which has greater hardness than soft tissue, will result in an increase in the force signal that may be characterized by exceeding a defined force threshold SF. Under such conditions, when the warning signal increases due to a passage from cortical bone to either cancellous bone or soft tissue, the force signal may be monitored. If force signal FT2 does not exceed force threshold SF after reaching depth threshold SP7, for example equal to 5 mm, control unit 111 detects the position of drill bit 146 in second direction T2 and modifies the movement, for example by reducing the speed of drill bit 146 to zero at VT2. Such differentiation may be obtained for tissues other than cancellous bone and soft tissue. The force signal may, for example, be used to differentiate between cortical bone and a fatty cyst having similar electrical conductivities.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/72/443/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9913725852966309,
                    "F_SPEC_PARAMS": [
                        "accuracy",
                        "mis-registration of the working end of the surgical tool"
                    ],
                    "S_SPEC_PARAMS": [
                        "reducing surgeon fatigue",
                        "eliminating transmission of physician muscle tremors",
                        "reduce the physician's tactile feel",
                        "precision",
                        "safety"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Adoption of previously known surgical navigation systems has been limited because the accuracy of such systems may be compromised by a variety of factors.",
                        "For example, a patient or the surgeon may reposition the patient's body on the surgical table, thereby causing mis-registration of the working end of the surgical tool relative to pre-acquired images."
                    ],
                    "S_SENTS": [
                        "While robotic systems provide important benefits, such as reducing surgeon fatigue and eliminating transmission of physician muscle tremors during delicate operations, such systems also typically reduce the physician's tactile feel during advancement of the working end, for example, when passing from bone into tissue.",
                        "This concern is accentuated when working with delicate structures innervated with critical nervous system components, such as the spine, where maintaining high precision is vital to the patient's safety."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement"
                    ],
                    "F_SIM_SCORE": 0.7515109777450562,
                    "S_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Reliability"
                    ],
                    "S_SIM_SCORE": 0.5941378176212311,
                    "GLOBAL_SCORE": 1.7975303163131078
                },
                "sort": [
                    1.7975303
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10906182-20210202",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10906182-20210202",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-02-06",
                    "PUBLICATION_DATE": "2021-02-02",
                    "INVENTORS": [
                        "Hideaki Suzuki",
                        "Keita Dan",
                        "Naoki Tsukabe"
                    ],
                    "APPLICANTS": [
                        "CANON KABUSHIKI KAISHA    ( Tokyo , JP )"
                    ],
                    "INVENTION_TITLE": "Method of teaching robot and robot system",
                    "DOMAIN": "B25J 91692",
                    "ABSTRACT": "A robot system includes a robot, a vision sensor, and a controller. The vision sensor is configured to be detachably attached to the robot. The controller is configured to measure a reference object by using the vision sensor and calibrate a relative relationship between a sensor portion of the vision sensor and an engagement portion of the vision sensor, and teach the robot by referring to the relative relationship and by using the vision sensor, after the vision sensor is attached to the robot.",
                    "CLAIMS": "1. A method of teaching a robot system comprising a vision sensor that comprises a sensor portion and an engagement portion, a robot, and a control portion configured to control the vision sensor and the robot, the method comprising: a measuring step in which the control portion measures a reference object by using the vision sensor in a state where the vision sensor is not attached to the robot; a calibrating step in which the control portion calibrates a relative relationship between a first coordinate system and a second coordinate system in a state where the vision sensor is not attached to the robot, wherein the first coordinate system is a coordinate system with respect to the sensor portion, and the second coordinate system is a coordinate system with respect to the engagement portion; and a teaching step in which the control portion teaches the robot by referring to the relative relationship and by using the vision sensor in a state where the vision sensor is attached to the robot. 2. The method according to claim 1, wherein the robot comprises a robot hand, and the robot hand is configured to hold the vision sensor such that the vision sensor is in the state where the vision sensor is attached to the robot. 3. The method according to claim 2, wherein the vision sensor is positioned with respect to the robot hand by engaging the engagement portion of the vision sensor with an engagement portion of the robot hand when the robot hand holds the vision sensor. 4. The method according to claim 1, wherein the calibrating step comprises determining, as the relative relationship, a rotation matrix and a translation vector. 5. The method according to claim 1, wherein the vision sensor is configured to measure a three-dimensional position and orientation. 6. The method according to claim 5, wherein the vision sensor includes, as the sensor portion, a first camera and a second camera, and wherein the calibrating step is performed with respect to one of the first camera and the second camera. 7. The method according to claim 1, wherein the teaching step comprises adjusting a posture of the robot and causing a memory to store an adjusted posture of the robot as a teach point. 8. The method according to claim 7, wherein the teaching step comprises: transforming a coordinate value of a feature point with respect to the first coordinate system to a measured value with respect to the second coordinate system, wherein the feature point is contained in visual information obtained from the vision sensor, presenting, to a user, information on an amount of positional difference between the measured value and a preset target value, and adjusting a posture of the robot depending on a command from an operation device operated by the user. 9. The method according to claim 7, wherein the teaching step comprises: transforming a coordinate value of a feature point with respect to the first coordinate system to a measured value with respect to the second coordinate system, wherein the feature point is contained in visual information obtained from the vision sensor, and adjusting a posture of the robot depending on an amount of positional difference between the measured value and a preset target value. 10. A robot system comprising: a robot; a vision sensor that comprises a sensor portion and an engagement portion; and a control portion configured to control the vision sensor and the robot, wherein the control portion is configured to measure a reference object by using the vision sensor in a state where the vision sensor is not attached to the robot, and to calibrate a relative relationship between a first coordinate system and a second coordinate system in a state where the vision sensor is not attached to the robot, wherein the first coordinate system is a coordinate system with respect to the sensor portion, and the second coordinate system is a coordinate system with respect to the engagement portion, and wherein the control portion is configured to teach the robot by referring to the relative relationship and by using the vision sensor in a state where the vision sensor is attached to the robot. 11. A method of manufacturing a product comprising: fitting a first workpiece to a second workpiece using the robot system according to claim 10, wherein the control portion teaches the robot a position where the fitting of the first workpiece to the second workpiece starts, and wherein the control portion controls the robot so as to move the first workpiece from the position and assemble the first workpiece to the second workpiece. 12. The method according to claim 1, wherein the measuring step and the calibrating step are performed in a state where the engagement portion of the vision sensor is positioned with respect to the second coordinate system by using a jig. 13. The method according to claim 2, wherein the robot hand comprises a palm and a finger, with the finger comprising a tapered portion, wherein the vision sensor comprises a base portion configured to touch the palm, with the base portion comprising a tapered portion, and wherein the tapered portion of the finger and the tapered portion of the base portion are configured to engage each other. 14. The method according to claim 3, wherein the robot hand comprises a finger whose position is adjusted with respect to the engagement portion of the robot hand. 15. The method according to claim 2, wherein the vision sensor comprises a base portion, with the base portion comprising a pair of tapered portions and a pair of positioning pins, and wherein a symmetry plane of the vision sensor coincides with a symmetry plane of the pair of positioning pins and with a symmetry plane of the pair of tapered portions.",
                    "FIELD_OF_INVENTION": "The present invention relates to teaching a robot.",
                    "STATE_OF_THE_ART": "In factories, a robot system is used to automate work, such as transfer of parts, assembly, and processing. The robot system has a robot which has multiple degrees of freedom. Examples of the robot having multiple degrees of freedom include a vertically articulated robot, a horizontally articulated robot, a Cartesian coordinate robot, and a parallel link robot. A user can cause a robot to move in accordance with a robot program, and thereby can cause the robot to perform a variety of types of work for respective purposes. In order to cause a robot to perform production work, such as transfer of workpieces and assembly, a user needs to teach a robot a posture of the robot in advance. Teaching a robot is typically performed by a user using a teaching pendant to perform jog feed of the robot to cause the robot to take a posture for work and causing a storage unit to store the posture. When a user causes a robot to perform work which needs precision, the user needs to determine a relative position and orientation, with high accuracy, between an object such as a workpiece or a workpiece holding jig, and an end effector such as a hand or a tool attached to the robot, and teach the robot the relative position and orientation. Japanese Patent Application Publication H7-84631 proposes a technique to correct a teach position of a robot. In this technique, a three-dimensional vision sensor is attached to a welding robot in place of, or in parallel with a welding torch, a mark formed on an object to be worked is measured by using the vision sensor, and thereby a teach position of the robot is corrected. The technique of Japanese Patent Application Publication H7-84631 calibrates the relationship between the robot and the vision sensor, and corrects the teach position by using information on a mark position seen from the vision sensor, in a state where the vision sensor is attached to the robot. However, because the robot has an error in its posture, the vision sensor attached to the robot also has an error in its position and orientation. As a result, in the technique of Japanese Patent Application Publication H7-84631, the error in posture of the robot affects the accuracy of the calibration, thus reducing the accuracy of teaching the robot.",
                    "SUMMARY": [
                        "According to a first aspect of the present invention, a method of teaching a robot includes measuring a reference object by using a vision sensor and calibrating a relative relationship between a sensor portion of the vision sensor and an engagement portion of the vision sensor, attaching the engagement portion of the vision sensor to a robot, after the calibrating, and teaching the robot by referring to the relative relationship and by using the vision sensor. According to a second aspect of the present invention, a robot system includes a robot, a vision sensor configured to be detachably attached to the robot, and a controller. The controller is configured to measure a reference object by using the vision sensor and calibrate a relative relationship between a sensor portion of the vision sensor and an engagement portion of the vision sensor, and teach the robot by referring to the relative relationship and by using the vision sensor, after the vision sensor is attached to the robot. Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.",
                        "1 is an explanatory diagram of a robot system of a first embodiment. 2A is a perspective view of a workpiece holding jig of the first embodiment. 2B is a perspective view of the workpiece holding jig, a first workpiece, and a second workpiece of the first embodiment. 3A is a perspective view of a stereo camera of the first embodiment. 3B is a perspective view of the stereo camera of the first embodiment. 3C is a cross-sectional view of the stereo camera of the first embodiment. 4A is a perspective view of a robot hand and the stereo camera of the first embodiment. 4B is a perspective view of the robot hand and the stereo camera of the first embodiment. 4C is a perspective view of the robot hand and the stereo camera of the first embodiment. 5 is a flowchart illustrating a method of teaching a robot, according to the first embodiment. 6 is a flowchart illustrating a method of teaching the robot, according to the first embodiment. 7A is an explanatory diagram of a calibration jig of the first embodiment. 7B is a plan view of a reference pattern portion of the first embodiment. 8 is a schematic diagram for illustrating a relationship between target values and measured values of feature points. 9A is an explanatory diagram illustrating a relationship between a tool coordinate system positioned at a leading end of a robot arm and a base coordinate system of the stereo camera. 9B is an explanatory diagram illustrating a relationship between a tool coordinate system positioned at the leading end of the robot arm and the base coordinate system of the stereo camera. 10 is an explanatory diagram of a robot system of a second embodiment. 11A is a front view illustrating a state where a stereo camera is held by a robot hand in the second embodiment. 11B is a side view illustrating the state where the stereo camera is held by the robot hand in the second embodiment. 12 is a flowchart illustrating a method of teaching a robot, according to the second embodiment. 13A is an explanatory diagram of a calibration jig of a third embodiment. 13B is an explanatory diagram of a robot of the third embodiment."
                    ],
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/82/061/109/0.pdf",
                    "CONTRADICTION_SCORE": 0.9600251913070679,
                    "F_SPEC_PARAMS": [
                        "precision,",
                        "relative position and orientation,",
                        "accuracy,",
                        "relative position and orientation"
                    ],
                    "S_SPEC_PARAMS": [
                        "accuracy of the calibration,",
                        "accuracy"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "When a user causes a robot to perform work which needs precision, the user needs to determine a relative position and orientation, with high accuracy, between an object such as a workpiece or a workpiece holding jig, and an end effector such as a hand or a tool attached to the robot, and teach the robot the relative position and orientation."
                    ],
                    "S_SENTS": [
                        "As a result, in the technique of Japanese Patent Application Publication H7-84631, the error in posture of the robot affects the accuracy of the calibration, thus reducing the accuracy of teaching the robot."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Length of Moving Object"
                    ],
                    "F_SIM_SCORE": 0.6189589202404022,
                    "S_TRIZ_PARAMS": [
                        "Accuracy of Measurement"
                    ],
                    "S_SIM_SCORE": 0.7515109181404114,
                    "GLOBAL_SCORE": 1.778593443830808
                },
                "sort": [
                    1.7785934
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11275095-20220315",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11275095-20220315",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-03-28",
                    "PUBLICATION_DATE": "2022-03-15",
                    "INVENTORS": [
                        "Takashi Yoshida",
                        "Hiroaki Inokuchi",
                        "Takehiro Hasegawa",
                        "Chikako Murata",
                        "Yukio Iwasaki",
                        "Satoshi Ouchi"
                    ],
                    "APPLICANTS": [
                        "SYSMEX CORPORATION    ( Kobe , JP )"
                    ],
                    "INVENTION_TITLE": "Sample pretreatment apparatus, robotic arm, and sample pretreatment method",
                    "DOMAIN": "G01N 350099",
                    "ABSTRACT": "A sample pretreatment apparatus includes: a plurality of sample pretreatment sections, each of which executes a sample pretreatment prior to a measurement; and a robotic arm including: an articulated arm member; and a hand attached to the articulated arm member. The plurality of sample pretreatment sections includes a sample dispenser that dispenses a sample in a first sample container into a second sample container. For a first measurement, the robotic arm holds the second sample container with the hand and transports the second sample container to a pretreatment group of the plurality of sample pretreatment sections, the pretreatment group including the sample dispenser. For a second measurement, the robotic arm holds the second sample container with the hand and transports the second sample container to another pretreatment group of the plurality of sample pretreatment sections, the another pretreatment group including the sample dispenser.",
                    "CLAIMS": "1. A sample pretreatment apparatus comprising: a plurality of sample pretreatment devices, each of which executes a sample pretreatment prior to a measurement; a sample dispenser configured to dispense a sample in a first sample container into a second sample container; a controller; and a robotic arm comprising: an articulated arm member; and a hand attached to the articulated arm member, wherein, the controller is programed to, in response to determining that the sample in the first sample container is subject to first and second measurements different from each other, control the sample dispenser to dispense the sample from the first sample container to one of a plurality of second sample containers and to an other of the plurality of second sample containers, and control the robotic arm to transport the one of the plurality of second sample containers to a first pretreatment group of the plurality of sample pretreatment devices and to transport the other of the plurality of second sample containers to a second pretreatment group of the plurality of sample pretreatment devices. 2. The sample pretreatment apparatus according to claim 1, wherein the plurality of sample pretreatment devices comprise at least one of: an information attaching device that attaches identification information to the one and the other of the plurality of second sample containers; a centrifuge; a closing device that closes a lid of each of the one and the other of the plurality of second sample containers; a reagent dispenser that dispenses a reagent; a heater that heats the sample dispensed into each of the one and the other of the plurality of second sample containers; and a nucleic acid extracting device that extracts a nucleic acid from the sample dispensed into each of the one and the other of the plurality of second sample containers. 3. The sample pretreatment apparatus according to claim 2, the controller is programed to, in response to determining that the first measurement is a cell morphology measurement, control the robotic arm to transport the one of the plurality of second sample containers to the first pretreatment group of the plurality of sample pretreatment devices comprising at least the centrifuge. 4. The sample pretreatment apparatus according to claim 2, the controller is programed to, in response to determining that the first measurement is a biomarker measurement, control the robotic arm to transport the one of the plurality of second sample containers to the first pretreatment group of the plurality of sample pretreatment devices comprising at least the closing device. 5. The sample pretreatment apparatus according to claim 2, wherein the controller is programed to, in response to determining that the first measurement is a visual measurement, control the robotic arm to transport the one of the plurality of second sample containers to the first pretreatment group of the plurality of sample pretreatment devices comprising at least the centrifuge. 6. The sample pretreatment apparatus according to claim 2, wherein the controller is programed to, in response to determining that the first measurement is a measurement of a substance to be expressed on a surface of a cell, control the robotic arm to transport the one of the plurality of second sample containers to the first pretreatment group of the plurality of sample pretreatment devices comprising at least the reagent dispenser that dispenses the reagent that activates the substance. 7. The sample pretreatment apparatus according to claim 2, wherein the controller is programed to, in response to determining that the first measurement is a measurement of gene mutation in a cell, control the robotic arm to transport the one of the plurality of second sample containers to the first pretreatment group of the plurality of sample pretreatment devices comprising at least: the reagent dispenser that dispenses the reagent that dissolves the cell; the heater; and the nucleic acid extracting device. 8. The sample pretreatment apparatus according to claim 2, wherein the controller is programed to, in response to determining that the first measurement is a measurement of a protein in a cell, control the robotic arm to transport the one of the plurality of second sample containers to the first pretreatment group of the plurality of sample pretreatment devices comprising at least the reagent dispenser that dispenses ethanol, and the centrifuge. 9. The sample pretreatment apparatus according to claim 1, further comprising a reader that is configured to read identification information from the first sample container, wherein the plurality of sample pretreatment devices comprise an information attaching device that is configured to attach identification information based on the identification information read by the reader, and the controller is programed to control the reader to read the identification information from the first sample container and control the information attaching device to attach identification information based on the identification information read by the reader to the one of the plurality of second sample containers and to the other of the plurality of second sample containers. 10. The sample pretreatment apparatus according to claim 1, wherein the hand includes a pair of container holding parts that are capable of holding each of the first sample container and the second sample containers, wherein a shape or a size of the first sample container is different from that of the plurality of second sample containers. 11. The sample pretreatment apparatus according to claim 1, wherein the sample dispenser comprises: a plurality of dispensing pipettes; and a driving mechanism that is coupled to the plurality of dispensing pipettes and drives the plurality of dispensing pipettes concurrently, and the robotic arm operates the driving mechanism with the hand to concurrently dispense samples from two or more first sample containers to two or more of the plurality of second sample containers. 12. The sample pretreatment apparatus according to claim 1, further comprising a supplying part from which the first sample container in which the sample is accommodated is to be supplied, wherein the controller is programed to control the robotic arm to transport the first sample container from the supplying part to the sample dispenser. 13. The sample pretreatment apparatus according to claim 1, wherein the robotic arm is configured to hold and transport a dispensing pipette to the sample dispenser, wherein the controller is programed to control the sample dispenser to dispense the sample via the dispensing pipette. 14. The sample pretreatment apparatus according to claim 13, wherein the robotic arm is configured to attach the dispensing pipette to the sample dispenser and to detach the dispensing pipette from the sample dispenser, and the controller is programed to control the robotic arm to attach the dispensing pipette to the sample dispenser and to detach the dispensing pipette from the sample dispenser. 15. The sample pretreatment apparatus according to claim 1, further comprising: an agitator that agitates the sample in the first sample container; and an opening device that opens a lid of the first sample container, wherein the controller is programed to control the robotic arm to transport the first sample container containing the sample agitated by the agitator to the opening device, and to transport the first sample container whose lid is opened by the opening device to the sample dispenser. 16. The sample pretreatment apparatus according to claim 1, comprising the robotic arm and a second robotic arm that includes an articulated arm member and a hand attached to the articulated arm member of the second robotic arm. 17. The sample pretreatment apparatus according to claim 16, wherein the articulated arm member of each of the robotic arm and the second robotic arm comprises a first link member that is turnable about a first axis with respect to a base for the articulated arm member, and a second link member that is connected to the first link member such that the second link member is turnable about a second axis different from the first axis with respect to the first link member. 18. The sample pretreatment apparatus according to claim 1, wherein the controller is programed to control the robotic arm, based on a coordinate position of any of the plurality of sample pretreatment devices as a transport destination, so that the robotic arm transports the second sample container to the sample pretreatment devices. 19. A sample pretreatment method using a robotic arm, the sample pretreatment method comprising: dispending, in response to determining that a sample in a first sample container is subject to first and second measurements different from each other, the sample in the first sample container into one of a plurality of second sample containers and an other of the plurality of second sample containers by a dispenser; and transporting the one of the plurality of second sample container containers to a first pretreatment group by the robotic arm and transporting the other of the plurality of second sample container containers to a second pretreatment group by the robotic arm.",
                    "STATE_OF_THE_ART": "The disclosure relates to a sample pretreatment apparatus, a robotic arm, and a sample pretreatment method. Conventionally, it has been known to perform several kinds of measurements on samples from the same subject. For example, Japanese Patent Application Publication Japanese Translation of PCT International Application 2013-542452 Patent Literature 1 teaches that a test of cervical cancer involves a measurement of the morphology of cells and a measurement of a biomarker. In this test of cervical cancer, as sample pretreatments before the test, sample pretreatments such as preparation of samples for the respective measurements, a centrifugation process and removing an unnecessary component are conducted in accordance with the measurement methods. However, the test of cervical cancer in Patent Literature 1 requires a user to devote considerable time and effort to separately prepare samples for respective tests, and perform sample pretreatments appropriate for the respective tests. Moreover, when some components derived from the user are mixed into the sample while the user manually performs the sample pretreatment, those components may have different influences on the test methods. In the test of cervical cancer in Patent Literature 1, a test of a cell morphology and a test of a biomarker such as HPV, for example, are performed. A component adhered on the skin of the user, if it is mixed into the samples, has small influence on the test of the cell morphology, but has large influence on the biomarker test where the component from the user is incorrectly measured as the biomarker. For this reason, the correlation between the results of the two tests loses the reliability. Moreover, in order to increase the flexibility of the system design, there is a demand to use a general-purpose apparatus for sample pretreatments instead of using a dedicated sample pretreatment system integrally constructed.",
                    "SUMMARY": [
                        "A sample pretreatment apparatus according to one or more embodiments may include: a plurality of sample pretreatment sections, each of which executes a sample pretreatment prior to a measurement; and a robotic arm including: an articulated arm member; and a hand attached to the articulated arm member. The plurality of sample pretreatment sections may include a sample dispenser that dispenses a sample in a first sample container into a second sample container. In a case in which the sample dispensed into the second sample container is pretreated for a first measurement, the robotic arm may hold the second sample container with the hand and transport the second sample container to a pretreatment group of the plurality of sample pretreatment sections, the pretreatment group including the sample dispenser. In a case in which the sample dispensed into the second sample container is pretreated for a second measurement, the robotic arm may hold the second sample container with the hand and transport the second sample container to another pretreatment group of the plurality of sample pretreatment sections, the another pretreatment group including the sample dispenser. A robotic arm according to one or more embodiments performs a sample pretreatment by cooperating with plurality of sample pretreatment sections including a dispenser that dispenses a sample in a first sample container into a second sample container. The robotic arm may include: an articulated arm member; and a hand attached to the articulated arm member. In a case in which the sample dispensed into the second sample container is pretreated for a first measurement, the robotic arm may hold the second sample container with the hand and transport the second sample container to a pretreatment group of the plurality of sample pretreatment sections, the pretreatment group including the dispenser. In a case in which the sample dispensed into the second sample container is pretreated for a second measurement, the robotic arm may hold the second sample container with the hand and transport the second sample container to another pretreatment group of the plurality of sample pretreatment sections, the another pretreatment group including the dispenser. A sample pretreatment method according to one or more embodiments uses a robotic arm. The sample pretreatment method may include: transporting a first sample container and a second sample container to a dispenser by the robotic arm including an articulated arm member and a hand attached to the articulated arm member, while holding the first container and the second sample container with the hand of the robotic arm; dispensing a sample in the first sample container into the second sample container by the dispenser using the robotic arm; holding the second sample container with the hand and transporting the second sample container to a group of sample pretreatment sections by the robotic arm, in a case in which the sample dispensed into the second sample container is pretreated for a first measurement; and holding the second sample container with the hand and transporting the second sample container to another group of the sample pretreatment sections by the robotic arm, in a case in which the sample dispensed into the second sample container is to be used in a second measurement.",
                        "1 is a schematic diagram illustrating a sample pretreatment apparatus; 2 is a flow diagram illustrating sample pretreatment; 3 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a first embodiment; 4 is a diagram illustrating a communication process of a sample pretreatment apparatus; 5 is a diagram illustrating a position information table of a sample pretreatment apparatus; 6 is a perspective diagram illustrating a hand provided to a robotic arm; 7 is a diagram illustrating holding of a second sample container with a hand; 8 is a diagram illustrating holding of a first sample container with a hand; 9 is a diagram illustrating holding of dispensing pipettes with a hand; 10 is a diagram illustrating dispensing by dispensing pipettes held by a hand; 11 is a diagram illustrating a process of a sample pretreatment apparatus; 12 is a flow diagram illustrating a pretreatment operation; 13 is a flow diagram illustrating a first sample container accepting process, such as at step S41 in 12; 14 is a flow diagram illustrating a bar code reading process, such as at step S42 in 12; 15 is a flow diagram illustrating a sample agitating process, such as at step S43 in 12; 16 is a flow diagram illustrating a first sample container opening process, such as at step S44 in 12; 17 is a flow diagram illustrating a sample dispensing process, such as at step S45 in 12; 18 is a flow diagram illustrating a pretreatment operation on a second sample container, such as at step S46 in 12; 19 is a flowchart illustrating a pretreatment operation on a second sample container, such as at step S47 in 12; 20 is a flow diagram illustrating a first sample container closing process, such as at step S48 in 12; 21 is a diagram illustrating one example of an agitator; 22 is a diagram illustrating one example of an opening part and a closing part; 23 is a diagram illustrating one example of a centrifuge tube inputting part; 24 is a diagram illustrating one example of an information attaching part; 25 is a diagram illustrating one example of a dispenser; 26 is a diagram illustrating one example of a centrifuge; 27 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a second embodiment; 28 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a third embodiment; 29 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a fourth embodiment; 30 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a fifth embodiment; 31 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a sixth embodiment; 32 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a seventh embodiment; 33 is a flow diagram illustrating a work process performed by robotic arms of a sample pretreatment apparatus of a seventh embodiment; 34 is a diagram illustrating a configuration of a sample pretreatment apparatus according to an eighth embodiment; 35 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a ninth embodiment; 36 is a flow diagram illustrating a work process performed by robotic arms of a sample pretreatment apparatus of a ninth embodiment; 37 is a diagram illustrating a configuration of a sample pretreatment apparatus according to a tenth embodiment; and 38 is a diagram illustrating a configuration of a sample pretreatment apparatus according to an eleventh embodiment."
                    ],
                    "DESCRIPTION": "A sample pretreatment apparatus 100 according to a first aspect includes multiple sample pretreatment sections 50 that respectively execute multiple sample pretreatments, and a robotic arm 10 including an articulated arm member 10b to which a hand 15 is attached. The multiple sample pretreatment sections 50 include a dispenser 51 that dispenses a sample in a first sample container 21 to second sample containers 22, 23. The robotic arm 10 holds the second sample container 22 with hand 15, and transports the second sample container 22 to the sample pretreatment section 50 in one group including the dispenser 51 when the sample dispensed into the second sample container 22 is to be used in one measurement, and holds the second sample container 23 with the hand 15, and transports the second sample container 23 to the sample pretreatment section 50 in another group including the dispenser 51 when the sample dispensed into the second sample container 23 is to be used in another measurement. The sample pretreatment apparatus 100 according to a first aspect is configured as the above to enable the robotic arm 10 instead of a manual work to transport the second sample containers 22, 23 in order to respectively perform multiple sample pretreatments different from each other. This can reduce a work burden of the user when tests of several types are executed with respect to the sample of the same subject, and can suppress generation of contamination by the mixing of a component derived from the user. Moreover, using the robotic arm 10 having a wide workable range makes it possible to access various kinds of devices and execute sample pretreatments in accordance with measurements. Accordingly, general-purpose devices can be used. A robotic arm 10 according to a second aspect is the robotic arm 10 that performs a sample pretreatment by cooperating with multiple sample pretreatment sections 50 including at least a dispenser 51 that dispenses a sample in a first sample container 21 into second sample containers 22, 23, and includes an articulated arm member 10b to which a hand 15 is attached. The robotic arm 10 holds the second sample container 22 with hand 15, and transports the second sample container 22 to the sample pretreatment section 50 in one group including the dispenser 51 when the sample dispensed into the second sample container 22 is to be used in one measurement, and holds the second sample container 23 with the hand 15, and transports the second sample container 23 to the sample pretreatment section 50 in another group including the dispenser 51 when the sample dispensed into the second sample container 23 is to be used in another measurement. The robotic arm 10 according to a second aspect is configured as the above to enable the robotic arm 10 instead of a manual work to transport the second sample containers 22, 23 in order to respectively perform multiple sample pretreatments different from each other. This can reduce a work burden of the user when tests of several types are executed with respect to the sample of the same subject, and can suppress generation of contamination by the mixing of a component derived from the user. Moreover, using the robotic arm 10 having a wide workable range makes it possible to access various kinds of devices and execute sample pretreatments in accordance with measurements. Accordingly, general-purpose devices can be used. A sample pretreatment method according to a third aspect is a sample pretreatment method of respectively executing multiple sample pretreatments by a robotic arm 10 including an articulated arm member 10b to which a hand 15 is attached. The sample pretreatment method includes: transporting, by the robotic arm 10, a first sample container 21 and second sample containers 22, 23 by being held by the hand 15 to the dispenser 51, and causing a sample in the first sample container 21 to be dispensed into the second sample containers 22, 23; and transporting the second sample container 22 by being held by the hand 15 to the sample pretreatment section 50 in one group including the dispenser 51 when the sample dispensed into the second sample container 22 is to be used in one measurement, and transporting the second sample container 23 by being held by the hand 15 to the sample pretreatment section 50 in another group including the dispenser 51 when the sample dispensed into the second sample container 23 is to be used in another measurement. The sample pretreatment method according to a third aspect is configured as the above to enable the robotic arm 10 instead of a manual work to transport the second sample containers 22, 23 in order to respectively perform multiple sample pretreatments different from each other. This can reduce a work burden of the user when tests of several types are executed with respect to the sample of the same subject, and can suppress generation of contamination by the mixing of a component derived from the user. Moreover, using the robotic arm 10 having a wide workable range makes it possible to access various kinds of devices and execute sample pretreatments in accordance with measurements. Accordingly, general-purpose devices can be used. According to one or more aspects, it is possible to reduce a work burden of the user when tests of several types are executed with respect to the sample of the same subject, suppress generation of contamination by the mixing of a component derived from the user, and use general-purpose devices. Hereinafter, embodiments are explained based on the drawings. Overview of Sample Pretreatment ApparatusWith reference to 1, an overview of a sample pretreatment apparatus 100 is explained. The sample pretreatment apparatus 100 according to an embodiment performs multiple sample pretreatments. Moreover, the sample pretreatment apparatus 100 is a sample pretreatment apparatus that performs a pretreatment of preparing a sample to be supplied to a first measuring device 30, and a sample to be supplied to a second measuring device 40 that performs measurement relating to that in the first measuring device 30. Note that, in order to perform three types or more of measurements, three or more samples may be prepared by the pretreatment. A subject is mainly a human, however, an animal other than the human may be used. For example, a sample collected from a patient is measured for a clinical test or a medical study. The sample is derived from a living body. Examples of the sample derived from a living body include blood whole blood, serum or plasma, urine, or other liquids such as a bodily fluid, or a liquid obtained by subjecting the collected bodily fluid or blood to a prescribed pretreatment. Moreover, the sample may be a part of tissue other than the liquid or the cells, of the subject. The first measuring device 30 and the second measuring device 40 detect objects components that are related to each other in order to detect a specific disease and the like. The sample pretreatment apparatus 100 includes a robotic arm 10, and multiple sample pretreatment sections 50. The sample pretreatment section 50 includes at least a dispenser 51. Moreover, the sample pretreatment section 50 includes, for example, a centrifuge 52. The sample pretreatment apparatus 100 includes a supplying part 60 that supplies a sample in a first sample container 21. The robotic arm 10 includes an articulated arm member 10b to which a hand 15 is attached, and transports a second sample container 22 and a second sample container 23, which are held, grabbed or gripped by the hand 15, to the sample pretreatment sections 50. Then, different sample pretreatments are performed. The sample pretreatment sections 50 execute multiple sample pretreatments, which are different from one another. The dispenser 51 dispenses a sample in the first sample container 21 into the second sample container 22 and the second sample container 23. When a sample dispensed into the second sample container 22 is to be used in one measurement, the robotic arm 10 holds the second sample container 22 with the hand 15 and transports the second sample container 22 to the sample pretreatment section 50 in one group including the dispenser 51. Moreover, when a sample dispensed into the second sample container 23 is to be used in another measurement, the robotic arm 10 holds the second sample container 23 with the hand 15 and transports the second sample container 23 to the sample pretreatment section 50 in another group including the dispenser 51. For example, when the first sample container 21 is supplied from the supplying part 60, the robotic arm 10 transports the first sample container 21 to the dispenser 51. In the dispenser 51, a first sample and a second sample are respectively dispensed into the second sample container 22 and the second sample container 23, from the first sample container 21. Moreover, in accordance with respective measurement methods of the first measuring device 30 and the second measuring device 40, the first sample and the second sample are subjected to different pretreatments. The first sample that is subjected to a pretreatment is supplied to the first measuring device 30. Moreover, the second sample that is subjected to a pretreatment different from that to the first sample is supplied to the second measuring device 40. Sample Measurement ProcessWith reference to 2, an overview of a sample measurement process is explained. At step S1 in 2, a parent sample is provided. Specifically, the first sample container 21 in which a sample is accommodated or placed is provided to the supplying part 60. At step S2, child samples are dispensed from the parent sample. Specifically, the first sample container 21, the second sample container 22, and the second sample container 23 are transported to the dispenser 51. The samples are then dispensed from the first sample container 21 respectively to the second sample container 22 and the second sample container 23. At step S3, the child samples are subjected to respective pretreatments in accordance with the measurement methods thereof. Thereafter, the sample pretreatments are ended. Configuration of Sample Pretreatment ApparatusWith reference to 3 to 26, an example of a sample pretreatment apparatus 900 is explained according to a first embodiment. The sample pretreatment apparatus 900 executes a sample pretreatment for performing multiple measurements related to each other. Moreover, the sample pretreatment apparatus 900 performs a pretreatment of preparing, from a sample in the first sample container 21, a first sample that is used for a first measurement, and a second sample that is used for a second measurement relating to the first measurement. The sample pretreatment apparatus 900 is used, for example, a clinical test. The sample pretreatment apparatus 900 is provided in a hospital or an inspection institute. The first measuring device 30 measures the first sample. The second measuring device 40 measures the second sample. The first measurement includes a measurement of the cell morphology in the first sample, and the second measurement includes a measurement of a biomarker in the second sample. The first measuring device 30 is an exfoliated cell measuring device that measures exfoliated cells immobilized with a stock solution, by a flow cytometric method that uses laser. A prescribed amount of an exfoliated cell specimen measured by the exfoliated cell measuring device is aspirated out, undergoes a pretreatment process and a stain process, and is supplied in a flow cell by a sheath flow mechanism. The supplied exfoliated cell specimen is irradiated with laser light when passing through the flow cell, and scatter light and fluorescence are detected therefrom. The first measuring device 30 acquires, based on the detected scatter light and fluorescence, information on the size of cytoplasm, the size of nucleus, the amount of DNA in the cell, and the like, and determines whether and there is suspicion about cervical cancer. The second measuring device 40 measures a human papilloma virus HPV included in the second sample. The second measuring device 40 extracts and measures a nucleic acid from the second sample. The sample pretreatment apparatus 900 includes, as illustrated in 3, the robotic arm 10, a clean area 910, an exfoliated cell inputting part 920, a container placing part 930, an agitator 941, an opening part 942a, a closing part 942b, a centrifuge tube inputting part 943, an information attaching part 944, a dispenser 945, a centrifuge 946, a plug inputting part 947, an inputting part 950, and a reader 951. The exfoliated cell inputting part 920 includes a rack inputting part 921, and a transporter 922. The container placing part 930 includes an HPV inputting part 931, and a smear preparation inputting part 932. The inputting part 950 includes a rack unit 952. The robotic arm 10 is provided on a base 10a. The robotic arm 10 is provided in plurality, and includes, for example, a first robotic arm 11, and a second robotic arm 12. Here, the robotic arm 10 transports the first sample container 21 in which a sample is accommodated. Moreover, the robotic arm 10 transports the second sample container 22 in which the first sample is accommodated. Moreover, the robotic arm 10 transports the second sample container 23 in which the second sample is accommodated. Note that, a single or multiple, that is three or more, robotic arms 10 may be provided. Here, in an embodiment, the robotic arm 10 transports the first sample container 21 in which a sample is accommodated. Moreover, the robotic arm 10 transports the second sample container 22 in which the first sample is accommodated. Moreover, the robotic arm 10 transports the second sample container 23 in which the second sample is accommodated. The robotic arm 10 may transport a rack in which a container is placed. The robotic arm 10 is capable of transporting a container or a rack in the horizontal direction and in the vertical direction. The base 10a supports the robotic arm 10. Specifically, the base 10a supports the robotic arm 10 from the lower side. Moreover, the robotic arm 10 includes the articulated arm member 10b to which a hand 15 is attached. The robotic arm 10 operates by being driven by a driving part including an electric motor, for example. The driving part drives the hand 15 and the articulated arm member 10b. The driving part transports the first sample container 21 held by the hand 15 to the dispenser 945. Moreover, the driving part transports the second sample container 22 and the second sample container 23 to sample pretreatment sections 50 so as to be subjected to different sample pretreatments, respectively. The dispenser 945 dispenses the first sample to be used in a first measurement from the first sample container 21 into the second sample container 22, and dispenses the second sample to be used in a second measurement relating to the first measurement from the first sample container 21 into the second sample container 23. The robotic arm 10 includes an encoder or the like, which controls the amount of driving by the electric motor. Moreover, the rated output of the robotic arm 10 is equal to or lower than a prescribed power. Each of the first robotic arm 11 and the second robotic arm 12 includes a first link member 141 and a second link member 142. The first link member 141 and the second link member 142 constitute the articulated arm member 10b. The hand 15 is provided to each pair of the first robotic arm 11 and the second robotic arm 12. The hand 15 is attached to the articulated arm member 10b. The first link member 141 is turnable about a first axis line 14a. The second link member 142 is connected to the first link member 141 so as to be turnable about a second axis line 14b that is defined on the first link member 141 at a position different from that of the first axis line 14a. This enables the robotic arm 10 to turn about the multiple axis lines and operate, thereby obtaining a large workable range of the robotic arm 10. The first axis line 14a of the first robotic arm 11 and the first axis line 14a of the second robotic arm 12 are arranged by being overlapped with each other in plan view. Note that, the first axis line 14a of the first robotic arm 11 and the first axis line 14a of the second robotic arm 12 may be arranged by being separated from each other in plan view. The first link members 141 and the second link members 142 are turned to enable the first robotic arm 11 and the second robotic arm 12 to respectively move the hands 15 in the horizontal direction. In other words, the first robotic arm 11 and the second robotic arm 12 are respectively capable of causing the hands 15 to move translationally in the horizontal direction and rotate in the horizontal direction, by the first link members 141 and the second link members 142. Moreover, the first robotic arm 11 and the second robotic arm 12 are respectively capable of moving the hands 15 in the vertical direction. When the robotic arm 10 is brought into contact with an object other than an object to be transported during operation, the operation thereof is stopped. The robotic arm 10 may detect, for example, a drive current, and detect the contact with the object when the drive current exceeds a prescribed threshold. Moreover, a camera that monitors the operation of the robotic arm 10 may detect the contact between the robotic arm 10 and the object. Moreover, a sensor or the like may detect the contact between the robotic arm 10 and the object. The base 10a is movable relative to an installation surface. This enables a layout to be changed in accordance with other devices, and a configuration to be changed in accordance with the type of a sample measurement. Moreover, the base 10a does not need to be arranged with a clearance with a different device and a wall for performing maintenance. This can suppress the installation area from increasing, and further increase the flexibility of installation of the other devices. A controller 13 controls the operation of the robotic arm 10. In other words, the controller 13 controls operations of the first robotic arm 11 and the second robotic arm 12. The controller 13 includes, for example, a central process unit CPU, a memory, and the like. Moreover, the controller 13 is connected to the robotic arm 10 in a wired or wireless manner. In other words, the controller 13 transmits a signal for controlling the robotic arm 10 to the robotic arm 10 via a wired communication or wireless communication. This enables the controller 13 to cause the first robotic arm 11 and the second robotic arm 12 to easily cooperate with each other and operate, and thus enables multiple robotic arms 10 to operate with high efficiency. The controller 13 is housed in, for example, the base 10a. Note that, the controller 13 may be provided outside the base 10a. The controller 13 may acquire a relative position between a position of the base 10a and that of a different device by driving the robotic arm 10, and causing the robotic arm 10 to touch a jig provided to the different device. In other words, in a state where the jig is installed at a prescribed position of the different device, the robotic arm 10 moves to touch the jig, so that the controller 13 can detect the position at which the jig is arranged relative to the base 10a. This enables easy alignment between the robotic arm 10 and the different device. Note that, the alignment between the robotic arm 10 and the different device is performed when the robotic arm 10 and the different device are installed, when the robotic arm 10 is activated, or for every prescribed period. The dispenser 945 dispenses a liquid. Specifically, the dispenser 945 dispenses a sample from the first sample container 21 into the second sample container 22 or the second sample container 23. Moreover, the dispenser 945 dispenses a reagent necessary for the pretreatment. Moreover, the dispenser 945 dispenses, from a sample in the first sample container 21, a first sample to be used in the first measurement into the second sample container 22, and a second sample to be used in the second measurement into the second sample container 23. Moreover, the dispenser 945 dispenses a reagent necessary for the pretreatment. The dispenser 945 has an exchangeable dispensing pipette with which dispensing is performed. The dispenser 945 in a state of the dispensing pipette being attached thereto aspirates a liquid, and discharges the aspirated liquid. The dispenser 945 includes, for example, a pipette attaching part to which the dispensing pipette is detachably attached, and a driving part that performs an aspirating operation and a discharging operation of the dispensing pipette. The driving part includes a motor, an air cylinder, and the like. The dispenser 945 discharges the first sample and the second sample so as to differ the discharge amount of the first sample and the discharge amount of the second sample from each other. Specifically, the dispenser 945 is configured to enable a first dispensing pipette for dispensing the first sample and a second dispensing pipette for dispensing the second sample to be attached thereto. Note that, the first dispensing pipette and the second dispensing pipette may be provided to the dispenser 945 so as to be alternatively exchangeable, or both may be provided and selectively used in the dispensing process. The first dispensing pipette and the second dispensing pipette are configured to have mutually different dispensing amounts. For example, the first dispensing pipette and the second dispensing pipette have different aspirating amounts. The reader 951 reads identification information from the first sample container 21. For example, the reader 951 includes a bar code reader, a two-dimensional code reader, or an IC tag reader. Moreover, the reader 951 reads information attached to the first sample container 21. The identification information includes, for example, attribute information on a subject, collected date/time, a sample type, and the like. The information attaching part 944 attaches, based on the identification information read by the reader 951, information at least either one of the second sample container 22 and the second sample container 23. The information attaching part 944 includes, for example, a printing part that prints identification information on a label, and a posting part that posts or attaches the label onto a container. Note that, the information attaching part 944 may directly print the information on the container. The information to be attached includes, for example, attribute information on a subject, collected date/time, a sample type, and the like. The centrifuge tube inputting part 943 supplies an empty container centrifuge tube. Specifically, the centrifuge tube inputting part 943 supplies the empty second sample container 22 and the empty second sample container 23. In the centrifuge tube inputting part 943, the second sample container 22 and the second sample container 23 are placed on a conveyor, and the conveyor transports the second sample container 22 and the second sample container 23. Note that, containers of the same type may be used as the second sample container 22 and the second sample container 23. In this case, the centrifuge tube inputting part 943 supplies a container at the common position. Moreover, containers of mutually different types may be used as the second sample container 22 and the second sample container 23. In this case, the centrifuge tube inputting part 943 supplies the second sample container 22 and the second sample container 23 by being distinguished. The plug inputting part 947 supplies a plug to the opening part 942a. The robotic arm 10 transports the plug supplied from the plug inputting part 947 to the opening part 942a. A general-purpose product can be used as the plug inputting part 947. Here, in a first embodiment, the robotic arm 10 transports the second sample container 22 in which the first sample is accommodated and the second sample container 23 in which the second sample is accommodated so as to be subjected to different pretreatments. As for the first sample to be used in the measurement of the cell morphology, the robotic arm 10 transports the second sample container 22 to a sample pretreatment section in a group including the dispenser 945 and the centrifuge 946. Note that, this group includes the information attaching part 944. Moreover, as for the second sample to be used in the measurement of a biomarker, the robotic arm 10 transports the second sample container 23 to a sample pretreatment section in a group including the dispenser 945 and the closing part 942b. Note that, this group includes the information attaching part 944. In the example illustrated in 3, the first sample container 21 is inputted from the inputting part 950. The reader 951 reads identification information from the inputted first sample container 21. The first sample container 21 is transported to the rack unit 952 based on the identification information. The robotic arm 10 then transports the first sample container 21 to the agitator 941. After the agitator 941 agitates, shakes, or stirs the sample in the first sample container 21, the robotic arm 10 transports the first sample container 21 to the opening part 942a. After the opening part 942a opens a lid of the first sample container 21, the robotic arm 10 transports the first sample container 21 to the dispenser 945. A general-purpose product can be used as the inputting part 950. For example, IDS-CLASX-1 series manufactured by IDS Co. , Ltd. may be used as the inputting part 950. Moreover, a general-purpose product can be used as the reader 951. Moreover, the robotic arm 10 takes out the empty second sample container 22 from the centrifuge tube inputting part 943, and transports the empty second sample container 22 to the information attaching part 944. After the information attaching part 944 attaches information to the second sample container 22, the robotic arm 10 transports the second sample container 22 transports to the dispenser 945. When the dispenser 945 dispenses the sample into the second sample container 22, the robotic arm 10 transports the second sample container 22 to the centrifuge 946. After the centrifuge 946 centrifuges the first sample, the robotic arm 10 transports the second sample container 22 to a first sample inputting part 31. Moreover, the robotic arm 10 transports the second sample container 22 and the second sample container 23 to the information attaching part 944 in order to attach identification information to the second sample container 22 and the second sample container 23, and transports the second sample container 22 and the second sample container 23 to the dispenser 945 in order to dispense a sample into the second sample container 22 and the second sample container 23. Moreover, the robotic arm 10 takes out the empty second sample container 23 from the centrifuge tube inputting part 943, and transports the empty second sample container 23 to the information attaching part 944. After the information attaching part 944 attaches information to the second sample container 23, the robotic arm 10 the second sample container 23 transports to the dispenser 945. When the dispenser 945 dispenses the sample to the second sample container 23, the robotic arm 10 transports the second sample container 23 to a second child sample inputting part 41. Moreover, the robotic arm 10 transports the second sample container 22 to the centrifuge 946 serving as a sample pretreatment section, moves the second sample container 22 to which a centrifugation process serving as a sample pretreatment is executed to a first position P1, and does not transport the second sample container 23 to the centrifuge 946 but moves the second sample container 23 to a second position P2 that is different from the first position P1. In other words, the first sample may be subjected to the centrifugation process performed by the centrifuge 946. Meanwhile, the second sample may not be subjected to the centrifugation process. The first sample container 21, the second sample container 22, and the second sample container 23 may be transported so as to be subjected to other pretreatments. Moreover, the sample pretreatment apparatus 100 may be provided with another processor. Moreover, as illustrated in 4, the parts in the sample pretreatment apparatus 900 respectively include controllers. Specifically, the robotic arm 10 includes the controller 13. Moreover, the agitator 941 includes a controller 962. Moreover, the closing part 942b includes a controller 963. Moreover, the centrifuge tube inputting part 943 includes a controller 964. Moreover, the information attaching part 944 includes a controller 965. Moreover, the dispenser 945 includes a controller 966. Moreover, the centrifuge 946 includes a controller 967. Moreover, the plug inputting part 947 includes a controller 968. Moreover, the rack unit 952 includes a controller 969. Each of the controllers includes a central processing unit CPU and a storage unit such as a memory. The controller 13 of the robotic arm 10 can communicate with the controllers 962 to 969. The controller 13 communicates with the other units to transport the sample container and control for the pretreatments. As illustrated in 5, the controller 13 communicates with the controllers 962 to 969 to create a table of position information. In the example illustrated in 5, the controller 13 acquires positions of samples set as numbers 1, 2, 3 . . . . In the example illustrated in 5, the controller 13 communicates with the controller 969 to acquire read information on a bar code of a parent sample. Moreover, the controller 13 communicates with the controller 962 to acquire information on a start and an end of the agitation. Moreover, the controller 13 communicates with the controller 963 to acquire information on an opening of the first sample container 21. Moreover, the controller 13 communicates with the controller 966 to acquire a dispensing position of the parent sample. For example, when there are three dispensing positions, the controller 13 acquires the positions by being distinguished from one another. In the example of 5, the controller 13 acquires the dispensing positions by being distinguished as X1, Y1, Z1, X2, Y2, Z2, and X3, Y3, Z3. Note that, coordinate position of a dispensing position is set with respect to a reference position of the robotic arm 10. The reference position may be set, for example, based on a rotation axis of the robotic arm 10. Moreover, the controller 13 communicates with the controller 963 to acquire an input position for smear preparing of the parent sample. For example, when there are three input positions, the controller 13 acquires the positions by being distinguished from one another. In the example of 5, the controller 13 acquires the input positions by being distinguished as Xa, Ya, Za, Xb, Yb, Zb, and Xc, Yc, Zc. Note that, the coordinate position of the input position is set with respect to a reference position of the robotic arm 10. Moreover, the controller 13 communicates with the controller 964 to acquire inputting information on the second sample container 22 and the second sample container 23. Moreover, the controller 13 communicates with the controller 965 to transmit read information on a bar code to be attached to a child sample. Moreover, the controller 13 communicates with the controller 966 to acquire a dispensing position of the first sample. Moreover, the controller 13 communicates with the controller 966 to acquire a dispensing position of the second sample. For example, when there are three dispensing positions, the controller 13 acquires the positions by being distinguished from one another. In the example of 5, the controller 13 acquires the dispensing positions by being distinguished as X4, Y4, Z4, X5, Y5, Z5, X6, Y6, Z6, X7, Y7, Z7, X8, Y8, Z8, and X9, Y9, Z9. Note that, the coordinate position of a dispensing position is set with respect to a reference position of the robotic arm 10. Moreover, the controller 13 communicates with the controller 967 to acquire information on a start and an end of the centrifugation of the first sample. Moreover, the controller 13 communicates with the controller 963 to acquire information on a start and an end of the closing of the second sample container 23. Moreover, the controller 13 acquires information on inputting of a sample of HPV into a measuring device. Moreover, the controller 13 acquires information on inputting of a sample of exfoliated cells into the measuring device. HandWith reference to 6 to 10, the hand 15 that is provided to the robotic arm 10 is explained. The hand 15 can hold the first sample container 21, the second sample container 22, the second sample container 23, and a dispensing pipette 16. Moreover, the hand 15 is attached to the robotic arm 10 in an exchangeable manner. As illustrated in 6, the hand 15 includes a supporting part 151, a pair of pipette holding parts 152, a pair of container holding parts 153, sliders 154, and a rail 155. Recess parts 152a are respectively formed in the pair of pipette holding parts 152. Three abutting parts 153a in total are provided to the pair of container holding parts 153. The supporting part 151 is configured to be attached to the robotic arm 10. Moreover, the supporting part 151 supports the pair of pipette holding parts 152, the pair of container holding parts 153, the sliders 154, and the rail 155. The pair of pipette holding parts 152 is configured to hold the dispensing pipette 16. Specifically, the pair of pipette holding parts 152 can move in a direction to approach each other and in a direction to move away from each other, by the driving part. This enables the pipette holding parts 152 to hold or release the dispensing pipette 16. Note that, the driving part includes, for example, an air cylinder or a motor. The recess parts 152a are formed to be recessed in a shape along a shape of a held part 161 of the dispensing pipette 16. This enables the pipette holding parts 152 to reliably hold the dispensing pipette 16. The pair of container holding parts 153 is configured to hold a container. Specifically, the pair of container holding parts 153 is configured to hold the first sample container 21, the second sample container 22, or the second sample container 23. The pair of container holding parts 153 can move in a direction to approach each other and in a direction to move away from each other, by the driving part. This enables the container holding parts 153 to hold or release a container. Note that, the pair of container holding parts 153 moves in a direction to approach each other and in a direction to move away from each other, in conjunction with the pair of pipette holding parts 152. This enables the pair of container holding parts 153 and the pair of pipette holding parts 152 to use the common driving part, so that it is possible to simplify the hand 15 and to decrease the number of components. The abutting parts 153a are provided in such a manner that one is provided to one of the container holding parts 153, and two are provided to the other container holding part 153. Moreover, the abutting parts 153a are configured to abut on a container, thereby holding the container. Each of the abutting parts 153a includes a bar shape that extends in the vertical direction. In other words, the abutting parts 153a are formed so as to extend in the axis direction of the cylindrical container. A pair of the sliders 154 is provided. Moreover, the sliders 154 are configured to be movable along the rail 155. One of the pair of pipette holding parts 152 and one of the pair of container holding parts 153 are connected to one of the pair of the sliders 154. Moreover, the other the pair of pipette holding parts 152 and the other of the pair of container holding parts 153 are connected to the other of the pair of the slider 154. This moves the pair of sliders 154 along the rail 155, so that a mutual distance between the pair of pipette holding parts 152 and a mutual distance between the pair of container holding parts 153 can be changed. As illustrated in 7, when the second sample container 22 or the second sample container 23 is held by the hand 15, the three abutting parts 153a of the container holding parts 153 sandwich and hold the second sample container 22 or the second sample container 23. As illustrated in 8, when the first sample container 21 is held by the hand 15, the three abutting parts 153a of the container holding parts 153 sandwich and hold the first sample container 21. The hand 15 is capable of holding each of containers having different diameters with the pair of container holding parts 153. In other words, the first sample container 21 has a shape different from that of the second sample container 22 and the second sample container 23. Moreover, the hand 15 is configured to be capable of holding each of the first sample container 21, the second sample container 22 and the second sample container 23. As illustrated in 9, when the hand 15 holds the dispensing pipette 16, the pipette holding parts 152 sandwich and hold the dispensing pipette 16. In the example illustrated in 9, the dispensing pipette 16 includes the held part 161, a supporting part 162, multiple aspirators 163. A chip 164 is provided on a tip of each aspirator 163. The held part 161 has a cylindrical shape that extends in the vertical direction. The held part 161 is fitted into the recess parts 152a of the pipette holding parts 152, and is held by the pipette holding parts 152. The supporting part 162 connects the held part 161 to the multiple aspirators 163. The multiple aspirators 163 are configured to aspirate liquids, and discharge the aspirated liquids. Moreover, the aspirators 163 are configured to be exchangeable. Moreover, the robotic arm 10 may perform an attaching operation and a detaching operation of the dispensing pipette 16 with respect to the dispenser 945, by using the hand 15. As illustrated in 10, in a state where the dispensing pipette 16 is held by the hand 15, dispensing may be performed. In this case, the hand 15 further includes an arm 156, a gear wheel 157 that is provided on a tip of the arm 156, and a plunger 158 that abuts on the gear wheel 157. A lower end portion of the plunger 158 is connected to the dispensing pipette 16. The dispensing pipette 16 is pressed in the downward direction by the plunger 158, thereby discharging the liquid. Moreover, when the press by the plunger 158 is canceled, the dispensing pipette 16 is urged in the upward direction by an urging member such as a spring, thereby aspirating the liquid. When the driving mechanism causes the arm 156 to turn in the downward direction, the plunger 158 is pressed by the gear wheel 157 at the tip, and moves in the downward direction. As illustrated in 11, the first sample container 21, the second sample container 22, and the second sample container 23 are transported. Specifically, a sample is contained in the first sample container 21, which serves as a uterine cervical sample preservation solution. Moreover, the second sample container 22 is used as an exfoliated cell measurement centrifuge tube. Moreover, the second sample container 23 is used as an HPV measurement centrifuge tube. In order to test cervical cancer, the exfoliated cell measurement and the HPV measurement are performed, so that the test can be performed with higher accuracy. The first sample container 21 is inputted from the inputting part 950. The inputting part 950 has a conveyor, and transports the multiple first sample containers 21. The reader 951 reads a bar code that is adhered on an outer circumferential portion of the first sample container 21. Whether the first sample container 21 is a measurement object is determined based on information on the bar code. The first sample container 21 that is a measurement object is sent to the rack unit 952. In contrast, the first sample container 21 that is not a measurement object is continuously transported by the inputting part 950, and is sent to the downstream. When a parent sample is inputted, communication is performed between the controller 969 of the rack unit 952 and the controller 13 of the robotic arm 10. The first sample container 21 that is a measurement object is transported to the agitator 941 by the first robotic arm 11, and is subjected to an agitating process. In this case, communication is performed between the controller 962 of the agitator 941 and the controller 13 of the robotic arm 10. The first sample container 21 after the agitating process is then transported to the opening part 942a by the first robotic arm 11, and is subjected to an opening process. In this case, communication is performed between the controller 963 and the controller 13 of the robotic arm 10. The first sample container 21 after the opening process is transported to the dispenser 945 by the first robotic arm 11, and is subjected to a dispensing process. In this case, communication is performed between the controller 966 and the controller 13 of the robotic arm 10. Thereafter, the first sample container 21 is moved to the closing part 942b, and is subjected to a closing process. In this case, communication is performed between the controller 963 and the controller 13 of the robotic arm 10. After the closing process, the first sample container 21 is then transported to the smear preparation inputting part 932. The second sample container 22 is supplied from the centrifuge tube inputting part 943, and is transported by the second robotic arm 12. In this case, communication is performed between the controller 964 and the controller 13 of the robotic arm 10. The second sample container 22 is transported to the information attaching part 944, and is attached with information based on the bar code information of the first sample container 21. In this case, communication is performed between the controller 965 and the controller 13 of the robotic arm 10. Thereafter, the second sample container 22 is transported to the dispenser 945 by the second robotic arm 12, and is subjected to a dispensing process after waiting. Specifically, a prescribed amount of samples are transferred from the first sample container 21. Thereafter, the second sample container 22 is moved to a transport position, is transported to the centrifuge 946 by the second robotic arm 12, and is subjected to a centrifugation process. In this case, communication is performed between the controller 967 and the controller 13 of the robotic arm 10. The second sample container 22 after the centrifugation process is transported to the dispenser 945 by the second robotic arm 12, and is subjected to a supernatant removing process. In this case, communication is performed between the controller 966 and the controller 13 of the robotic arm 10. Thereafter, the second sample container 22 is moved to the exfoliated cell inputting part 920. The second sample container 23 is supplied from the centrifuge tube inputting part 943, and is transported by the second robotic arm 12. In this case, communication is performed between the controller 964 and the controller 13 of the robotic arm 10. The second sample container 23 is transported to the information attaching part 944, and is attached with information based on the bar code information of the first sample container 21. In this case, communication is performed between the controller 965 and the controller 13 of the robotic arm 10. Thereafter, the second sample container 23 is transported to the dispenser 945 by the second robotic arm 12, and is subjected to a dispensing process. Specifically, a prescribed amount of samples are transferred from the first sample container 21. Thereafter, the second sample container 23 is moved to a transport position, and is supplied with a plug from a plug inputting part. In this case, communication is performed between the controller 968 and the controller 13 of the robotic arm 10. The second sample container 23 is then transported to the closing part 942b by the second robotic arm 12, and is subjected to a closing process. In this case, communication is performed between the controller 963 and the controller 13 of the robotic arm 10. Thereafter, the second sample container 23 is moved to the HPV inputting part 931. Next, with reference to 12 to 20, a pretreatment operation is explained. Note that, the controller 13 performs control of the pretreatment operation while communicating with the controllers 962 to 969. At step S41 in 12, whether the first sample container 21 is accepted is determined. In other words, whether the first sample container 21 is supplied by the inputting part 950 and is transported to the rack unit 952 is determined. For example, communication is performed between the controller 969 of the rack unit 952 and the controller 13 of the robotic arm 10, and the acceptance of the first sample container 21 to the rack unit 952 is transmitted to the controller 13. If the first sample container 21 is accepted, the operation proceeds to step S42, whereas if the first sample container 21 is not accepted, the operation proceeds to step S49. Specifically, at step S411 in 13, whether the controller 13 receives an acceptance signal from the controller 969 of the rack unit 952 is determined. It the controller 13 receives an acceptance signal, the process proceeds to step S412, whereas if the controller 13 does not receive, the process proceeds to step S49 see 12. At step S412, the first sample container 21 is taken out from the rack unit 952, and is transported. Note that, a transport operation by the robotic arm 10 is performed based on coordinate positions, which are allocated to the respective sample pretreatment sections and the respective dispensing positions. At step S42 in 12, the reader 951 reads a bar code of the first sample container 21. For example, as illustrated in 5, information on bar code reading For example, A, B, C . . . is transmitted to the controller 13. Specifically, at step S421 in 14, the controller 13 receives identification information on the first sample container 21 from the reader 951. At step S422, parent sample identification information is recorded. At step S43 in 12, the agitator 941 performs an agitating process for the first sample container 21. For example, communication is performed between the controller 962 of the agitator 941 and the controller 13 of the robotic arm 10, and the agitator 941 starts the agitating process. Moreover, after the agitation is ended, a completion signal is transmitted to the controller 13. Moreover, as illustrated in 5, an end of the agitating process is recorded. Specifically, at step S431 in 15, the first sample container 21 is transported to the agitator 941. At step S432, the controller 13 transmits an agitation start signal to the controller 962 of the agitator 941. When the agitation is ended, at step S433, the controller 13 receives an agitation end signal from the controller 962 of the agitator 941. At step S434, the controller 13 records an end of the agitation. At step S44 in 12, the opening part 942a performs an opening process for the first sample container 21. For example, communication is performed between the controller 963 and the controller 13 of the robotic arm 10, and the opening part 942a starts the opening process. Moreover, after the opening is ended, a completion signal is transmitted to the controller 13. Moreover, as illustrated in 5, an end of opening is recorded. Specifically, at step S441 in 16, the robotic arm 10 transports the first sample container 21 to the opening part 942a. At step S442, the controller 13 transmits an opening start signal to the controller 963 of the opening part 942a. After the opening is ended, at step S443, the controller 13 receives an opening end signal from the controller 963 of the opening part 942a. At step S444, the controller 13 then records an end of the opening. At step S45 in 12, the dispenser 945 performs a dispensing process for the first sample container 21. For example, communication is performed between the controller 966 and the controller 13 of the robotic arm 10, and the controller 13 starts the dispensing process. Moreover, as illustrated in 5, a dispensing position is transmitted to the controller 13, and is recorded. Moreover, after the dispensing process is ended, a completion signal is transmitted to the controller 13. Specifically, at step S451 in 17, the first sample container 21 is transported to the dispenser 945. At step S452, the controller 13 transmits a dispensing start signal to the controller 966 of the dispenser 945. When the dispensing is ended, at step S453, the controller 13 receives a dispensing end signal from the controller 966 of the dispenser 945. At step S46 in 12, a pretreatment operation for the second sample container 22 is performed. Specifically, at step S461 in 18, the controller 13 transmits an acquisition signal of the second sample container 22 to the controller 964 of the centrifuge tube inputting part 943. The second sample container 22 is then supplied from the centrifuge tube inputting part 943, and the robotic arm 10 acquires the second sample container 22. At step S462, the robotic arm 10 transports the second sample container 22 to the information attaching part 944. Moreover, the controller 13 transmits the identification information acquired from the first sample container 21 to the controller 965 of the information attaching part 944. Moreover, the controller 13 transmits an information attaching start signal to the controller 965. When the information attaching to the second sample container 22 is ended, the controller 13 receives an information attaching end signal from the controller 965 of the information attaching part 944. The controller 13 then records an end of the attaching. At step S463, the robotic arm 10 transports the second sample container 22 to the centrifuge 946. Moreover, the controller 13 transmits a centrifugation process start signal to the controller 967 of the centrifuge 946. When the centrifugation process for the second sample container 22 is ended, the controller 13 receives a centrifugation process end signal from the controller 967 of the centrifuge 946. The controller 13 then records an end of the centrifugation process. At step S464, the robotic arm 10 transports the second sample container 22 to the dispenser 945. Moreover, the controller 13 transmits a supernatant removing process start signal to the controller 966 of the dispenser 945. When the supernatant removing process for the second sample container 22 is ended, the controller 13 receives a supernatant removing process end signal from the controller 966 of the dispenser 945. The controller 13 then records an end of the supernatant removing process. At step S465, the robotic arm 10 transports the second sample container 22 to the exfoliated cell inputting part 920. At step S47 in 12, a pretreatment operation for the second sample container 23 is performed. Specifically, at step S471 in 19, the controller 13 transmits an acquisition signal of the second sample container 23 to the controller 964 of the centrifuge tube inputting part 943. The second sample container 23 is then supplied from the centrifuge tube inputting part 943, and the robotic arm 10 acquires the second sample container 23. At step S472, the robotic arm 10 transports the second sample container 23 to the information attaching part 944. Moreover, the controller 13 transmits the identification information acquired from the first sample container 21 to the controller 965 of the information attaching part 944. Moreover, the controller 13 transmits an information attaching start signal to the controller 965. When the information attaching to the second sample container 23 is ended, the controller 13 receives an information attaching end signal from the controller 965 of the information attaching part 944. The controller 13 then records an end of the attaching. At step S473, the robotic arm 10 transports the second sample container 23 to the closing part 942b. Moreover, the controller 13 transmits a closing start signal to the controller 963 of the closing part 942b. When the closing process for the second sample container 23 is ended, the controller 13 receives a closing process end signal from the controller 963 of the closing part 942b. The controller 13 then records an end of the closing process. At step S474, the robotic arm 10 transports the second sample container 23 to the HPV inputting part 931. At step S48 in 12, the closing part 942b performs a closing process for the first sample container 21. For example, communication is performed between the controller 963 and the controller 13 of the robotic arm 10, and the closing part 942b starts the closing process. Moreover, after the closing is ended, a completion signal is transmitted to the controller 13. Moreover, as illustrated in 5, an end of the closing is recorded. Specifically, at step S481 in 20, the robotic arm 10 transports the first sample container 21 to the closing part 942b. At step S482, the controller 13 transmits a closing start signal to the controller 963 of the closing part 942b. When the closing is ended, at step S483, the controller 13 receives a closing end signal from the controller 963 of the closing part 942b. Thereafter, at step S49 in 12, a shutdown process is performed. As illustrated in 21, the agitator 941 includes a main body part 9411, and a rocking part 9412. The agitator 941 is configured to agitate a content of a container by abutting the container on the rocking part 9412, thereby vibrating the container. A general-purpose mixer may be used as the agitator 941. For example, Digital Vortex-Genie2 manufactured by Scientific Industries may be used as the agitator 941. Moreover, Vortex Mixer 6778 manufactured by Corning Incorporated may be used as the agitator 941. As illustrated in 22, each of the opening part 942a and the closing part 942b includes a rotating part 9421, and holding parts 9422 that are supported by the rotating part 9421. The opening part 942a and the closing part 942b are respectively configured to perform the opening process and the closing process by rotating the rotating part 9421 in a state where the holding parts 9422 hold a plug 21a of a container. The holding parts 9422 are capable of approaching each other to an arbitrary distance, and are capable of stably holding a plug having any diameter. A General-purpose automatic plug opening and closing device may be used as the opening part 942a and the closing part 942b. For example, Cap automatic opening and closing device CAPNERCPF-100 manufactured by Allied Flow Co. , Ltd. may be used as the opening part 942a and the closing part 942b. As illustrated in 23, the centrifuge tube inputting part 943 is configured to hold multiple containers in a state of being piled in the vertical direction. Moreover, a container to be supplied by the centrifuge tube inputting part 943 is held by the hand 15 of the robotic arm 10. The multiple containers are piled in an inclined manner. Note that, the centrifuge tube inputting part 943 may have such a configuration that containers are held by being piled in the vertical direction, and a container is supplied by a button provided on a side surface thereof being pressed. A general-purpose inputting part may be used as the centrifuge tube inputting part 943. As illustrated in 24, the information attaching part 944 includes a main body part 9441, and an adhering part 9442. The information attaching part 944 is, for example, an auto labeler. The information attaching part 944 is configured to attach a bar code including prescribed information to a container that is transported with the hand 15 of the robotic arm 10, by the container being inserted into the adhering part 9442. Note that, the information attaching part 944 may attach information other than the bar code to the container. For example, the information attaching part 944 may attach a two-dimensional code, a character, or a symbol to the container. Moreover, the information attaching part 944 may attach information with a seal, or may attach information by printing. A general-purpose auto labeler may be used as the information attaching part 944. For example, BC12TL Test Tube Labeler System individual print type manufactured by Autonics Corporation may be used as the information attaching part 944. As illustrated in 25, the dispenser 945 includes multiple dispensing pipettes 9451, chips 9452 that are provided on tips of the respective dispensing pipettes 9451, and a driving mechanism 9453 that abuts on upper portions of the multiple dispensing pipettes 9451. The driving mechanism 9453 is coupled to the multiple dispensing pipettes 9451, and drives the multiple dispensing pipettes 9451 concurrently. The driving mechanism 9453 is operated with the hand 15 of the robotic arm 10 to enable the dispenser 945 to perform dispensing processes concurrently by the multiple dispensing pipettes 9451. In other words, the robotic arm 10 operates the driving mechanism 9453 with the hand 15 to enable the first sample and the second sample to be respectively dispensed into the multiple second sample containers 22 and the multiple second sample containers 23, from the multiple first sample containers 21 concurrently. Multiple single pipettes of mL order are arranged in the dispensing pipette 9451. A commercialized product may be used as the dispensing pipette 9451. The robotic arm 10 may prepare the multiple second sample containers 22 and the multiple second sample container 23 by repeating an operation of respectively dispensing the first sample and the second sample, from the multiple first sample containers 21, into the second sample container 22 and the second sample container 23. As illustrated in 26, the centrifuge 946 includes a main body part 9461, a lid part 9462, a rotating part 9463, and multiple container arranging parts 9464 that are provided to the rotating part 9463. The lid part 9462 is configured to be openable and closable. In other words, the lid part 9462 is opened when a container is taken out and put in, and the lid part 9462 is closed in the centrifugation process. Multiple containers are capable of being arranged in each of the container arranging parts 9464. The rotating part 9463 rotates to perform the centrifugation process of contents of the containers that are arranged in the container arranging parts 9464. A container is transported with the hand 15 of the robotic arm 10, and is set to the centrifuge 946. A general-purpose centrifugation device may be used as the centrifuge 946. For example, Universal Refrigerated Centrifuge 5930 manufactured by KUBOTA CORPORATION may be used as the centrifuge 946. Moreover, ROTANTA460 ROBOTIC manufactured by Hettich may be used as the centrifuge 946. Configuration of Sample Pretreatment ApparatusWith reference to 27, an example of a sample pretreatment apparatus 1000 according to a second embodiment is explained. Note that, configurations similar to those in a first embodiment are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 1000 of a second embodiment includes, as illustrated in 27, the robotic arm 10, a urine measuring device 1010, a parent sample inputting part 1020, a first child sample inputting part 1030, a second child sample placing part 1040, a dispenser 1050, a test tube inputting part 1060, an auto labeler 1070, a disposal station 1080, an agitator 1090, and a centrifuge 1091. The parent sample inputting part 1020 includes a bar code reader 1021. The sample pretreatment apparatus 1000 of a second embodiment is a device for performing a measurement of urine. For example, the sample pretreatment apparatus 1000 performs tests of urobilinogen, occult blood, protein, glucose, ketone body, bilirubin, nitrite, white blood cells, pH, creatinine, albumin, and the like, or performs a pretreatment for these tests. For tests of urine, multiple measurements are performed to enable the tests to be performed with higher accuracy. A sample in a cup serving as the first sample container 21 is supplied from the parent sample inputting part 1020. The agitator 1090 agitates the supplied sample in the cup. The dispenser 1050 dispenses a predetermined amount of the parent sample thus agitated into each of the second sample container 22 and the second sample container 23. Thereafter, the cup serving as the first sample container 21 is disposed of to the disposal station 1080. Each of the second sample container 22 and the second sample container 23 is supplied from the test tube inputting part 1060, and is attached with information based on bar code information of the first sample container 21 by the auto labeler 1070. The second sample container 22 is then transported to the first child sample inputting part 1030, and is subjected to a measurement by the urine measuring device 1010. The first sample is automatically subjected to a urine qualitative measurement by the urine measuring device 1010. The second sample container 23 is subjected to a centrifugation process by the centrifuge 1091. Moreover, after the dispenser 1050 removes the supernatant of the second sample that is subjected to the centrifugation, the second sample is transported to the second child sample placing part 1040. A test for the second sample is manually executed using a microscope. In other words, as for the second sample that is used in a visual measurement, the robotic arm 10 transports the second sample container 23 to a sample pretreatment section in a group including the dispenser 1050 and the centrifuge 1091. Configuration of Sample Pretreatment ApparatusWith reference to 28, an example of a sample pretreatment apparatus 1100 according to a third embodiment is explained. Note that, configurations similar to those in a first embodiment are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 1100 of a third embodiment includes, as illustrated in 28, the robotic arm 10, a cell measuring device 1111, a gene measuring device 1112, a protein measuring device 1113, a parent sample inputting part 1120, child sample inputting parts 1131, 1132, and 1133, a dispenser 1140, an agitator 1150, a centrifuge 1151, a BF separator 1160, a test tube inputting part 1170, an auto labeler 1180, and a heater 1190. The sample pretreatment apparatus 1100 of a third embodiment is a device for performing a measurement of formalin-fixation paraffin embedding FFPE. For example, the sample pretreatment apparatus 1100 performs a measurement of PD-L1 on the surface of a cancer cell, a measurement of mutation of a PD-L1 gene in the cancer cell, a measurement of a PD-L1 protein separated from the cancer cell, and the like, or performs a pretreatment for these tests. Note that, the cell measurement, the gene measurement, and the protein measurement may be performed about the cultured cell or whole blood. For tests of the sample, multiple measurements are performed to enable the tests to be performed with higher accuracy. A sample in which FFPE is deparaffinized and hydrophilized is supplied from the parent sample inputting part 1120, as a parent sample. The agitator 1150 agitates the supplied parent sample. The dispenser 1140 dispenses a predetermined amount of a cell sample of the parent sample thus agitated with a cell pipette. Moreover, the dispenser 1140 dispenses a predetermined amount of a gene sample of the parent sample with a gene pipette. Moreover, the dispenser 1140 dispenses a predetermined amount of a protein sample of the parent sample with a protein pipette. Each of a sample container into which the cell sample is dispensed, a sample container into which the gene sample is dispensed, and a sample container into which the protein sample is dispensed is supplied from the test tube inputting part 1170, and is attached with information based on bar code information of the first sample container 21 by the auto labeler 1180. The cell sample is subjected to an antigen activating process. Specifically, the cell sample is dispensed into an antigen retrieval solution heated by the heater 1190. The cell sample is transported to the child sample inputting part 1131, and is measured by the cell measuring device 1111. In other words, when the child sample is used for a measurement of a substance that is expressed on the surface of the cell, the robotic arm 10 transports the sample container to a sample pretreatment section in a group including the dispenser 1140 and a reagent dispenser for example, the dispenser 1140 for dispensing a reagent that activates the substance. The gene sample is subjected to a cell dissolution process. Specifically, Lysisbuffer and ProteinaseK are dispensed into the gene sample. The heater 1190 then heats the gene sample. The gene sample is then subjected to a nucleic acid extracting process. Specifically, the dispenser 1140 adds magnetic beads. Nucleic acids are adsorbed by the added magnetic beads. Thereafter, the gene sample is subjected to a BF separating process by the BF separator 1160. This removes unnecessary components. Thereafter, an eluate is added, and the nucleic acid is eluted. The gene sample is transported to the child sample inputting part 1132, and is measured by the gene measuring device 1112. In other words, when the sample is used for a measurement of the gene mutation in the cell, the robotic arm 10 transports the sample container to a sample pretreatment section in a group including the dispenser 1140, a reagent dispenser for example, the dispenser 1140 for dispensing a reagent that dissolves the cell, the heater 1190, and a nucleic acid extracting part for example, the dispenser 1140 and the BF separator 1160. The dispenser 1140 adds ethanol to the protein sample. The agitator 1150 then agitates the protein sample. Thereafter, the protein sample is subjected to a centrifugation process by the centrifuge 1151. The dispenser 1140 removes the supernatant of the protein sample that is subjected to the centrifugation process. The protein sample is transported to the child sample inputting part 1133, and is measured by the protein measuring device 1113. In other words, when the sample is used for a measurement of a protein in the cell, the robotic arm 10 transports the sample container to a sample pretreatment section in a group including the dispenser 1140, a reagent dispenser For example, the dispenser 1140 for dispensing ethanol, and the centrifuge 1151. Note that, this group may include the agitator 1150. Moreover, the dispenser 1140 of the sample pretreatment apparatus 1100 may include a reagent dispenser, serving as a sample pretreatment section, which dispenses a first reagent in accordance with a first measurement to the second sample container 22, and dispenses a second reagent, different from the first reagent, in accordance with a second measurement to the second sample container 23. For example, the first reagent is a reagent that activates the cell, and the second reagent is a reagent that dissolves the cell. Configuration of Sample Pretreatment ApparatusWith reference to 29, an example of a sample pretreatment apparatus 1200 according to a fourth embodiment is explained. Note that, configurations similar to those in a first embodiment are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 1200 of a fourth embodiment includes, as illustrated in 29, the robotic arm 10, a blood coagulation measuring device 1210, a parent sample inputting part 1220, a first child sample inputting part 1230, a second child sample placing part 1240, a dispenser 1250, an opening and closing part 1260, a plug inputting part 1261, a test tube inputting part 1270, an auto labeler 1280, and a centrifuge 1290. The sample pretreatment apparatus 1200 of a fourth embodiment is a device for performing a coagulation measurement of blood. For example, the sample pretreatment apparatus 1200 performs tests of PT, APTT, Fbg, TT0, HpT, factor quantification, ATIII, APL, PIg, PC, D dimer, FDP, and the like, or performs a pretreatment for these tests. The centrifuge 1290 performs the centrifugation process to a sample that is accommodated in the first sample container 21 and supplied from the parent sample inputting part 1220. The dispenser 1250 dispenses a predetermined amount of the parent sample that is subjected to the centrifugation to the second sample container 22. The first sample contains blood platelets. The second sample container 22 is supplied from the test tube inputting part 1270, and is attached with information based on bar code information of the first sample container 21 by the auto labeler 1280. The second sample container 22 is then transported to the first child sample inputting part 1230, and is subjected to a measurement by the blood coagulation measuring device 1210. In other words, the first sample is subjected to a coagulation test of blood platelet agglutination by the blood coagulation measuring device 1210. The first sample container 21 into which the first sample is dispensed is subjected to the centrifugation process by the centrifuge 1290. The dispenser 1250 dispenses a predetermined amount of the parent sample that is subjected to the centrifugation to the second sample container 23. The second sample contains no blood platelet. The second sample container 23 is supplied from the test tube inputting part 1270, and is attached with information based on bar code information of the first sample container 21 by the auto labeler 1280. The second sample container 23 is transported to the second child sample placing part 1240. The second sample is stored for a coagulation test different from the blood platelet agglutination. Configuration of Sample Pretreatment ApparatusWith reference to 30, an example of a sample pretreatment apparatus 200 according to a fifth embodiment is explained. The sample pretreatment apparatus 200 performs a pretreatment that prepares a first sample to be supplied from a parent sample to the first measuring device 30 see 1, and a second sample to be supplied to the second measuring device 40 see 1 that performs a measurement based on a principle different from that of the first measuring device 30. The sample pretreatment apparatus 200 includes, as illustrated in 30, the robotic arm 10, a parent sample receiving part 91, a container stock 92, a sample pretreatment device 93, a sample pretreatment reagent 94, a mounting table 95, a pipette holder 96, a chip stock 97, a first child sample placement place 98, and a second child sample placement place 99. The parent sample receiving part 91 receives the first sample container 21 in which a parent sample is accommodated. The container stock 92 stocks the empty second sample containers 22 and empty the second sample containers 23. The sample pretreatment device 93 is a device necessary for a pretreatment of the sample. For example, the sample pretreatment device 93 includes a centrifuge, a thermostat bath, an agitation device, and the like. Note that, when the pretreatment only includes the dispensing and the mixing with the respective reagents, this device does not need to be provided. The sample pretreatment reagent 94 stores therein a reagent necessary for the pretreatment. The mounting table 95 is a working table on which items necessary for the work are mounted. The first sample container 21, the second sample container 22, and the second sample container 23 are mounted on the mounting table 95. Moreover, a reagent container may be mounted on the mounting table 95. The pipette holder 96 holds dispensing pipettes of multiple types. Multiple dispensing pipettes are prepared depending on the dispensing amounts. The dispensing pipette is held by the robotic arm 10, and is used for the dispensing. Multiple chips are prepared in the chip stock 97. The chip of the chip stock 97 is attached on the tip of the dispensing pipette, and is exchanged for every dispensing. The second sample container 22 in which the first sample that is subjected to the pretreatment is accommodated is placed in the first child sample placement place 98. The second sample container 22 that is placed in the first child sample placement place 98 is transported to the first measuring device 30, and is subjected to a measurement. The second sample container 23 in which the second sample that is subjected to the pretreatment is accommodated is placed in the second child sample placement place 99. The second sample container 23 that is placed in the second child sample placement place 99 is transported to the second measuring device 40, and is subjected to a measurement. Here, in a fifth embodiment, the robotic arm 10 transports the second sample container 22 in which the first sample is accommodated and the second sample container 23 in which the second sample is accommodated so as to be subjected to different pretreatments. Moreover, the robotic arm 10 is configured to receive a dispensing pipette from the pipette holder 96, and perform the dispensing process. Specifically, the robotic arm 10 uses the dispensing pipette to dispense the first sample and the second sample from the parent sample. Moreover, the robotic arm 10 uses the dispensing pipette to dispense a reagent. Configuration of Sample Pretreatment ApparatusWith reference to 31, an example of a sample pretreatment apparatus 300 according to a sixth embodiment is explained. Note that, configurations similar to those in a fifth embodiment are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 300 of a sixth embodiment includes, as illustrated in 31, two robotic arms 10, the parent sample receiving part 91, the container stock 92, the sample pretreatment device 93, the sample pretreatment reagent 94, the mounting table 95, the pipette holder 96, the chip stock 97, the first child sample placement place 98, and the second child sample placement place 99. Here, in a sixth embodiment, the two robotic arms 10 transport the second sample container 22 in which the first sample is accommodated and the second sample container 23 in which the second sample is accommodated so as to be subjected to different pretreatments. Moreover, the two robotic arms 10 cooperate with each other to perform the transport operation and the dispensing operation. Configuration of Sample Pretreatment ApparatusWith reference to 32, an example of a sample pretreatment apparatus 400 according to a seventh embodiment is explained. Note that, configurations similar to those in fifth and sixth embodiments are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 400 of a seventh embodiment includes, as illustrated in 32, the two robotic arms 10, the parent sample receiving part 91, the mounting table 95, the pipette holder 96, the chip stock 97, a cell detecting area 411, a cell measurement reagent 412, a protein detecting area 421, and a protein measurement reagent 422. The parent sample receiving part 91, the mounting table 95, the pipette holder 96, and the chip stock 97 are provided in a common area 401 that is used by the two robotic arms 10 in common. Moreover, the sample pretreatment apparatus 400 includes an area where a cell is measured and an area where a protein is measured with the common area 401 being sandwiched therebetween. The cell detecting area 411, the robotic arm 10, and the cell measurement reagent 412 are provided in the area where a cell is measured. The protein detecting area 421, the robotic arm 10, and the protein measurement reagent 422 are provided in the area where a protein is measured. An optical unit that detects a stained cell is provided in the cell detecting area 411. A stain reagent for detecting a cell is provided in the cell measurement reagent 412. A plate reader that detects a protein is provided in the protein detecting area 421. A reagent such as an antibody for detection is provided in the protein measurement reagent 422. Work Process by Robotic ArmsWith reference to 33, a work process by the robotic arms 10 in the sample pretreatment apparatus 400 according to a seventh embodiment is explained. One of the two robotic arms 10 performs step S11 to step S15. Moreover, the other robotic arm 10 performs step S16 to step S18. At step S11, a parent sample is placed on the mounting table 95. Specifically, the robotic arm 10 transports and places the first sample container 21 placed on the parent sample receiving part 91, on the mounting table 95. At step S12, a dispensing pipette is taken out from the pipette holder 96. Moreover, a chip of the chip stock 97 is attached on a tip of the dispensing pipette. At step S13, a liquid portion of a specimen is aspirated by the dispensing pipette, and is transferred to another container. At step S14, a staining solution that stains the cell surface is added to the remaining cells. At step S15, after the reaction time has elapsed, a reagent necessary for the detection of a cell is added, and the container is transported to a detecting device. Thereafter, the parent sample process, and the cell staining and detecting process are ended. In parallel with the processes from step S11 to step S15, the other robotic arm 10 performs processes from step S16 to step S18. At step S16, a dispensing pipette is taken out from the pipette holder 96. Moreover, a chip of the chip stock 97 is attached on a tip of the dispensing pipette. At step S17, a reagent for a protein analysis is added to the liquid portion of the specimen. At step S18, after the reaction time has elapsed, a reagent necessary for the detection of a protein is added, and the container is transported to a detecting device. Thereafter, the process of detection of a protein detect is ended. Configuration of Sample Pretreatment ApparatusWith reference to 34, an example of a sample pretreatment apparatus 500 according to an eighth embodiment is explained. Note that, configurations similar to those in fifth to seventh embodiments are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 500 of an eighth embodiment includes, as illustrated in 34, the two robotic arms 10, the parent sample receiving part 91, the mounting table 95, the pipette holder 96, the chip stock 97, a nucleic acid detecting area 511, a nucleic acid measurement reagent 512, the protein detecting area 421, and the protein measurement reagent 422. The parent sample receiving part 91, the mounting table 95, the pipette holder 96, and the chip stock 97 are provided in a common area 501 that is used by the two robotic arms 10 in common. Moreover, the sample pretreatment apparatus 500 includes an area where a nucleic acid is measured and an area where a protein is measured with the common area 501 being sandwiched therebetween. The nucleic acid detecting area 511, the robotic arm 10, and the nucleic acid measurement reagent 512 are provided in the area where a nucleic acid is measured. The protein detecting area 421, the robotic arm 10, and the protein measurement reagent 422 are provided in the area where a protein is measured. A spectrophotometer that detects a nucleic acid is provided in the nucleic acid detecting area 511. A reagent for processing a nucleic acid is provided in the nucleic acid measurement reagent 512. A plate reader that detects a protein is provided in the protein detecting area 421. A reagent such as an antibody for detection is provided in the protein measurement reagent 422. Configuration of Sample Pretreatment ApparatusWith reference to 35, an example of a sample pretreatment apparatus 600 according to a ninth embodiment is explained. Note that, configurations similar to those in fifth to eighth embodiments are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 600 of a ninth embodiment includes, as illustrated in 35, the two robotic arms 10, the parent sample receiving part 91, the mounting table 95, the pipette holder 96, the chip stock 97, the cell detecting area 411, the cell measurement reagent 412, the protein detecting area 421, the protein measurement reagent 422, the nucleic acid detecting area 511, and the nucleic acid measurement reagent 512. In the nucleic acid detecting area 511, the nucleic acid extraction and PCR are performed to perform a gene measurement. In the cell detecting area 411, immunostaining is performed to perform a cell measurement. In the protein detecting area 421, Enzyme-Linked ImmunoSorbent Assay ELISA is performed to perform a protein measurement. Work Process by Robotic ArmsWith reference to 36, a work process by the robotic arms 10 in the sample pretreatment apparatus 600 according to a ninth embodiment is explained. One of the two robotic arms 10 performs step S21 to step S26. Moreover, the other robotic arm 10 performs step S29 to step S31. At step S21, a parent sample is placed on the mounting table 95. Specifically, the robotic arm 10 transports and places the first sample container 21 that is placed on the parent sample receiving part 91, on the mounting table 95. At step S22, a dispensing pipette is taken out from the pipette holder 96. Moreover, a chip of the chip stock 97 is attached on a tip of the dispensing pipette. At step S23, a liquid portion of a specimen is aspirated by the dispensing pipette, and transferred to another container. At step S24, a reagent is added to the remaining cells to disperse the cells. The dispersed cells are then divided into two containers. At step S25, a staining solution that stains the cell surface is added to the cells within one of the divided two containers. At step S26, after the reaction time has elapsed, a reagent necessary for the detection of a cell is added, and the container is transported to a detecting device. At step S27, a nucleic acid is extracted from the cells in the other container, out of the two divided containers. At step S28, after a polymerase chain reaction PCR reaction is performed, the container is transported to a detecting device. Thereafter, the parent sample process, and the cell staining and detecting process are ended. In parallel with the processes from step S21 to step S28, the other robotic arm performs processes from step S29 to step S31. At step S29, a dispensing pipette is taken out from the pipette holder 96. Moreover, a chip of the chip stock 97 is attached on a tip of the dispensing pipette. At step S30, a reagent for a protein analysis is added to the liquid portion of the specimen. At step S31, after the reaction time has elapsed, a reagent necessary for the detection of a protein is added, and the container is transported to a detecting device. Thereafter, the process of detection of a protein detect is ended. Configuration of Sample Pretreatment ApparatusWith reference to 37, an example of a sample pretreatment apparatus 700 according to a tenth embodiment is explained. Note that, configurations similar to those in fifth to ninth embodiments are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 700 of a tenth embodiment includes as illustrated in 37, one robotic arm 10, the parent sample receiving part 91, the mounting table 95, the pipette holder 96, the chip stock 97, the cell detecting area 411, the cell measurement reagent 412, the protein detecting area 421, the protein measurement reagent 422, the nucleic acid detecting area 511, and the nucleic acid measurement reagent 512. In the sample pretreatment apparatus 700, using one robotic arm, a pretreatment of a gene measurement by nucleic acid extraction and PCR and a pretreatment of a protein measurement by ELISA are performed. Configuration of Sample Pretreatment ApparatusWith reference to 38, an example of a sample pretreatment apparatus 800 according to an eleventh embodiment is explained. Note that, configurations similar to those in fifth to tenth embodiments are assigned with the same reference numerals, and explanations thereof are omitted. The sample pretreatment apparatus 800 of an eleventh embodiment includes, as illustrated in 38, the robotic arm 10, the first measuring device 30, the second measuring device 40, the parent sample receiving part 91, the mounting table 95, a first reagent setting part 801, and a second reagent setting part 802. The robotic arm 10 includes the first robotic arm 11 and the second robotic arm 12. A first reagent container in which a first reagent is accommodated for a measurement by the first measuring device 30 is provided in the first reagent setting part 801. A second reagent container in which a second reagent is accommodated for a measurement by the second measuring device 40 is provided in the second reagent setting part 802. The first reagent setting part 801 and the second reagent setting part 802 have a function of keeping cool or keeping warm so as to have a temperature suitable for storing a reagent. The robotic arm 10 is arranged between the first reagent setting part 801 and the second reagent setting part 802. This enables the robotic arm 10 to easily access both of the first reagent setting part 801 and the second reagent setting part 802, so that the work efficiency of the robotic arm 10 can be improved. Moreover, the mounting table 95 and the parent sample receiving part 91 are arranged in a region between the first reagent setting part 801 and the second reagent setting part 802. The first robotic arm 11 has a function of holding the first dispensing pipette such that the first reagent is dispensed from the first reagent container into the second sample container 22. Moreover, the second robotic arm 12 has a function of holding the second dispensing pipette such that the second reagent is dispensed from the second reagent container to the second sample container 23. Moreover, the first robotic arm 11 has a function of holding and transporting the second sample container 22 to the mounting table 95. Moreover, the second robotic arm 12 has a function of holding and transporting the second sample container 23 to the mounting table 95. Moreover, the first robotic arm 11 and the second robotic arm 12 respectively have functions of holding again and transporting the second sample container 22 and the second sample container 23, from the mounting table 95. This enables the second sample container 22 and the second sample container 23 to be transferred from one of the first robotic arm 11 and the second robotic arm 12 to the other, so that in a range where a work range of the first robotic arm 11 and a work range of the second robotic arm are overlapped with each other, it is possible to easily transport the second sample container 22 and the second sample container 23. Note that, it should be considered that the embodiments disclosed herein are merely examples, and are not limited thereto. The scope of the present invention is indicated by not the explanations for the above-described embodiments but by the claims, and includes all the changes modifications within the meaning and the range equivalent to the scope of claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/95/750/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9855799674987793,
                    "F_SPEC_PARAMS": [
                        "requires a user to devote considerable time and effort to separately prepare samples for respective tests,"
                    ],
                    "S_SPEC_PARAMS": [
                        "reliability",
                        "flexibility"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "However, the test of cervical cancer in Patent Literature 1 requires a user to devote considerable time and effort to separately prepare samples for respective tests, and perform sample pretreatments appropriate for the respective tests.",
                        "Moreover, when some components derived from the user are mixed into the sample while the user manually performs the sample pretreatment, those components may have different influences on the test methods."
                    ],
                    "S_SENTS": [
                        "For this reason, the correlation between the results of the two tests loses the reliability.",
                        "Moreover, in order to increase the flexibility of the system design, there is a demand to use a general-purpose apparatus for sample pretreatments instead of using a dedicated sample pretreatment system integrally constructed."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Amount of Substance"
                    ],
                    "F_SIM_SCORE": 0.5066615343093872,
                    "S_TRIZ_PARAMS": [
                        "Reliability",
                        "Adaptability"
                    ],
                    "S_SIM_SCORE": 0.7978731691837311,
                    "GLOBAL_SCORE": 1.7711806525786717
                },
                "sort": [
                    1.7711806
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11383289-20220712",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11383289-20220712",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-11-23",
                    "PUBLICATION_DATE": "2022-07-12",
                    "INVENTORS": [
                        "Yuxin Hao",
                        "Jun Li",
                        "Xipeng Zhao",
                        "Qingshan Zhan"
                    ],
                    "APPLICANTS": [
                        "JINAN HAOZHONG AUTOMATION CO., LTD.    ( Jinan , CN )"
                    ],
                    "INVENTION_TITLE": "High-speed stamping transfer robot",
                    "DOMAIN": "B21D 43055",
                    "ABSTRACT": "A high-speed stamping transfer robot, including a Y-axis portion, a Z1-axis portion, a Z2-axis portion upper arm, a mechanical lower arm portion and a tooling portion, wherein the Z1-axis portion is fixedly connected to the Y-axis portion; the Z2-axis portion upper arm is in rotational connection with the Z1-axis portion and the Y-axis portion and is capable of moving vertically relative to the Z1-axis portion while moving horizontally relative to the Y-axis portion; the mechanical lower arm portion is in rotational connection with the Z2-axis portion upper arm and is capable of rotating relative to the Z2-axis portion upper arm; and the tooling portion is fixedly connected to the mechanical lower arm portion. Through linkage among the respective axis portions, high-speed and high-load stable transfer of sheets between presses can be realized.",
                    "CLAIMS": "1. A high-speed stamping transfer robot, comprising: a Y-axis portion, a Z1-axis portion, a Z2-axis portion, a mechanical lower arm portion and a tooling portion, wherein the Z1-axis portion is fixedly connected to the Y-axis portion; the Z2-axis portion is in rotational connection with the Z1-axis portion and the Y-axis portion and is capable of moving vertically relative to the Z1-axis portion while moving horizontally relative to the Y-axis portion; the mechanical lower arm portion is in rotational connection with the Z2-axis portion and is capable of rotating relative to the Z2-axis portion; and the tooling portion is fixedly connected to the mechanical lower arm portion, and further comprising: a suspension portion, wherein the suspension portion comprises a connecting base, a first connecting rod, a second connecting rod, a third connecting rod, a fourth connecting rod, a fifth connecting rod, a sixth connecting rod, a first lug, a second lug, a third lug and a fourth lug; one end of the first connecting rod and one end of the second connecting rod are hinged to one end portion at the bottom of the connecting base, and the other end of the first connecting rod and the other end of the second connecting rod are hinged to the first lug and the third lug respectively; one end of the fifth connecting rod and one end of the sixth connecting rod are hinged to the other end portion at the bottom of the connecting base, and the other end of the fifth connecting rod and the other end of the sixth connecting rod are hinged to the second lug and the fourth lug respectively; one end of the third connecting rod and one end of the fourth connecting rod are hinged to the top of the connecting base, and the other end of the third connecting rod and the other end of the fourth connecting rod are hinged to the first lug and the second lug respectively; and the connecting base is fixedly connected to the Y-axis portion. 2. The high-speed stamping transfer robot according to claim 1, wherein the Y-axis portion comprises: a first transmission shaft; synchronous pulleys disposed on the first transmission shaft and mounted on two sides of the interior of a Y-axis base; a first synchronous belt mounted on the synchronous pulley; a first sliding disposed on the first synchronous belt and connected to a first rotary seat through a bearing; first linear guide rails that are symmetrical and are disposed on two sides of the first synchronous belt; a first slider that corresponds to the first linear guide rail is disposed on the first sliding seat; the first sliding seat is capable of moving horizontally along the first linear guide rail which is equipped with a guide rail brake; and a first limiting block is disposed on the outer side of the first transmission shaft. 3. The high-speed stamping transfer robot according to claim 2, wherein the two synchronous pulleys are disposed on the first transmission shaft; the first synchronous belt is mounted on each of the two synchronous pulleys that are symmetrical; and bearing supports are mounted at the two ends and in the middle of the first transmission shaft. 4. The high-speed stamping transfer robot according to claim 1, wherein the Z1-axis portion comprises: a Z1-axis base; the bottom of the Z1-axis base is fixedly mounted on the Y-axis base; a first driving pulley is disposed at the top of the interior of the Z1-axis base; a first driven pulley is disposed at the bottom of the interior of the Z1-axis base; a second synchronous belt is mounted on the first driving pulley and the first driven pulley; a second sliding seat is disposed on the second synchronous belt and connected to a second rotary seat through a bearing; second linear guide rails are disposed on two sides of the second synchronous belt; a second slider that corresponds to the second linear guide rail is disposed on the second sliding seat; and the second sliding seat is capable of moving vertically along the second linear guide rail which is equipped with a guide rail brake. 5. The high-speed stamping transfer robot according to claim 4, wherein the Z1-axis portion further comprises: a speed-up pulley block and a balance cylinder; the speed-up pulley block is mounted on a base of the speed-up pulley block, which is located at the upper end of the Z1-axis base; a steel wire rope is connected to the second sliding seat and wound in a fixed pulley groove of the speed-up pulley block, and enlaces a movable pulley on the balance cylinder; and the other end of the steel wire rope is connected to the base of the speed-up pulley block. 6. The high-speed stamping transfer robot according to claim 1, wherein the Z2-axis portion comprises: a Z2-axis base; a second driving pulley is disposed at the top of the interior of the Z2-axis base; a second driven pulley is disposed at the bottom of the interior of the Z2-axis base; a third synchronous belt with a third sliding seat disposed thereon is mounted on the second driving pulley and the second driven pulley; third linear guide rails are disposed on the front side and the back side of the Z2-axis base and located on two sides of the third synchronous belt; the third sliding seat is mounted on and is capable of moving linearly along the third linear guide rail on the front side of the Z2-axis base; the third linear guide rail on the back side of the Z2-axis base is connected to the Y-axis portion in a sliding manner; and the back side of the Z2-axis base is connected to the second rotary seat. 7. The high-speed stamping transfer robot according to claim 1, wherein the mechanical lower arm portion comprises a D1-axis portion, an E-axis portion and a D2-axis portion; the D1-axis portion is in rotational connection with the Z2-axis portion; the E-axis portion comprises a driving mechanism, an inner sleeve and an outer sleeve, the outer sleeve and the inner sleeve are connected through a bearing, an output end of the driving mechanism is fixedly connected to one end of the inner sleeve, and the D1-axis portion is fixedly connected to the outer sleeve; and the D2-axis portion comprises a D2-axis fixed seat fixedly connected to the other end of the inner sleeve, a third driving pulley is disposed in the D2-axis fixed seat, a third driven pulley is disposed in the D2-axis fixed seat, a fourth synchronous belt is mounted on the third driving pulley and the third driving pulley, and connecting flange plates connected to the tooling portion are disposed at two ends of the third driven pulley. 8. The high-speed stamping transfer robot according to claim 7, wherein a tensioning device is further disposed in the D2-axis fixed seat and comprises a swing rod, a tensioning wheel and an ejector rod, one end of the swing rod is hinged to the inner wall of the D2-axis fixed seat, the tensioning wheel is disposed at the other end of the swing rod, one end of the ejector rod is fixedly connected to the inner wall of the D2-axis fixed seat, the other end of the ejector rod is propped in a concave hole in the middle of the swing rod, the ejector rod is in threaded connection with an adjusting nut, and the tensioning wheel is clung to the fourth synchronous belt. 9. The high-speed stamping transfer robot according to claim 1, wherein the tooling portion comprises two toolings each of which comprises a main rod and an auxiliary rod, a plurality of connecting pieces sleeves the main rod and is connected to the two auxiliary rods that are symmetrical, the auxiliary rod is perpendicular to the main rod, a sucker is disposed at the other end of the auxiliary rod, and a connecting flange connected to the mechanical lower arm portion is disposed at one end of the main rod.",
                    "FIELD_OF_INVENTION": "The present invention relates to the field of high-speed stamping automation, and in particular to a high-speed stamping transfer robot.",
                    "STATE_OF_THE_ART": "In the stamping industry of automobile panels, the production of a workpiece often needs continuous stamping by multiple presses. Thus, transfer of sheets between the presses is crucial and directly affects the quality of stamped products and the efficiency of an entire stamping production line. On a traditional manual production line, the sheets are manually transferred between the presses. At least two workers are required to transfer semi-finished products between the presses. Since the sheets may be warped, collided and inaccurately located by manual transfer, which severely impacts the quality of the stamped products and the product stamping efficiency, currently, more and more automobile manufacturers replace the traditional manual production line with an automatic robot stamping line that can realize automatic transfer of the sheets between the presses, thus replacing the workers with a robot and guaranteeing the product quality and stability. At present, a common automatic robot line employs such special robots for stamping as those manufactured by ABB, KUKA, FANUC and YASKAWA, with a maximum load of 100 kg and a maximum arm spread of 3,000 mm. The automatic production line can achieve the pace of 8 times/minute, and therefore, can meet basic requirements for production. With rapid and efficient development of the automobile industry, the annual output of automobiles continues to increase. The transfer of the sheets between the presses restricts the overall production cycle of the automatic stamping line. Nowadays, the production efficiency of the common automatic robot line cannot meet the needs for efficient production.",
                    "SUMMARY": [
                        "An objective of the present invention is to overcome the shortcomings in the prior art and to provide a high-speed stamping transfer robot. A technical solution adopted to solve the technical problem is as below. The high-speed stamping transfer robot comprises a Y-axis portion, a Z1-axis portion, a Z2-axis portion, a mechanical lower arm portion and a tooling portion. The Z1-axis portion is fixedly connected to the Y-axis portion. The Z2-axis portion is in rotational connection with the Z1-axis portion and the Y-axis portion and is capable of moving vertically relative to the Z1-axis portion while moving horizontally relative to the Y-axis portion. The mechanical lower arm portion is in rotational connection with the Z2-axis portion and is capable of rotating relative to the Z2-axis portion. The tooling portion is fixedly connected to the mechanical lower arm portion. Preferably, the Y-axis portion comprises a first transmission shaft. Synchronous pulleys are disposed on the first transmission shafts and are mounted on two sides of the interior of a Y-axis base. A first synchronous belt is mounted on the synchronous pulley. A first sliding seat is disposed on the first synchronous belt and connected to a first rotary seat through a bearing. First linear guide rails that are symmetrical are disposed on two sides of the first synchronous belt. A first slider that corresponds to the first linear guide rail is disposed on the first sliding seat. The first sliding seat is capable of moving horizontally along the first linear guide rail which is equipped with a guide rail brake. A first limiting block is disposed on the outer side of the first transmission shaft. Further, two synchronous pulleys are disposed on the first transmission shaft. The first synchronous belt is mounted on each of the two synchronous pulleys that are symmetrical. Bearing supports are mounted at the two ends and in the middle of the first transmission shaft. Preferably, the Z1-axis portion comprises a Z1-axis base. The bottom of the Z1-axis base is fixedly mounted on the Y-axis base. A second transmission shaft with a first driving pulley disposed thereon is mounted at the top of the interior of the Z1-axis base. A first driven pulley is disposed at the bottom of the interior of the Z1-axis base. A second synchronous belt is mounted on the first driving pulley and the first driven pulley. A second sliding seat is disposed on the second synchronous belt and connected to a second rotary seat through a bearing. Second linear guide rails are disposed on two sides of the second synchronous belt. A second slider that corresponds to the second linear guide rail is disposed on the second sliding seat. The second sliding seat is capable of moving vertically along the second linear guide rail which is equipped with a guide rail brake. Further, the Z1-axis portion comprises a speed-up pulley block and a balance cylinder. The speed-up pulley block is mounted on a base of the speed-up pulley block, which is located at the upper end of the Z1-axis base. A steel wire rope is connected to the second sliding seat and wound in a fixed pulley groove of the speed-up pulley block, and enlaces a movable pulley on the balance cylinder. The other end of the steel wire rope is connected to the base of the speed-up pulley block. Preferably, the Z2-axis portion comprises a Z2-axis base. A third transmission shaft with a second driving pulley disposed thereon is mounted at the top of the interior of the Z2-axis base. A second driven pulley is disposed at the bottom of the interior of the Z2-axis base. A third synchronous belt with a third sliding seat disposed thereon is mounted on the second driving pulley and the second driven pulley. Third linear guide rails are disposed on the front side and the back side of the Z2-axis base and located on two sides of the third synchronous belt. The third sliding seat is mounted on and is capable of moving linearly along the third linear guide rail on the front side of the Z2-axis base. The third linear guide rail on the back side of the Z2-axis base is connected to the Y-axis portion in a sliding manner. The back side of the Z2-axis base is connected to the second rotary seat. Preferably, the mechanical lower arm portion comprises a D1-axis portion, an E-axis portion and a D2-axis portion. The D1-axis portion is in rotational connection with the Z2-axis portion. The E-axis portion comprises a driving mechanism, an inner sleeve and an outer sleeve. The outer sleeve and the inner sleeve are connected through a bearing. An output end of the driving mechanism is fixedly connected to one end of the inner sleeve. The D1-axis portion is fixedly connected to the outer sleeve. The D2-axis portion comprises a D2-axis fixed seat fixedly connected to the other end of the inner sleeve. A fourth transmission shaft with a third driving pulley disposed thereon is mounted in the D2-axis fixed seat. A third driven pulley is disposed in the D2-axis fixed seat. A fourth synchronous belt is mounted on the third driving pulley and the third driven pulley. Connecting flange plates connected to the tooling portion are disposed at two ends of the third driven pulley. Further, a tensioning device is further disposed in the D2-axis fixed seat and comprises a swing rod, a tensioning wheel and an ejector rod. One end of the swing rod is hinged to the inner wall of the D2-axis fixed seat. The tensioning wheel is disposed at the other end of the swing rod. One end of the ejector rod is fixedly connected to the inner wall of the D2-axis fixed seat. The other end of the ejector rod is propped in a concave hole in the middle of the swing rod. The ejector rod is in threaded connection with an adjusting nut. The tensioning wheel is clung to the fourth synchronous belt. Preferably, the tooling portion comprises two toolings each of which comprises a main rod and an auxiliary rod. A plurality of connecting pieces sleeves the main rod and is connected to the two auxiliary rods that are symmetrical. The auxiliary rod is perpendicular to the main rod. A sucker is disposed at the other end of the auxiliary rod. A connecting flange connected to the mechanical lower arm portion is disposed at one end of the main rod. Preferably, the transfer robot further comprises a suspension portion. The suspension portion comprises a connecting base, a first connecting rod, a second connecting rod, a third connecting rod, a fourth connecting rod, a fifth connecting rod, a sixth connecting rod, a first lug, a second lug, a third lug and a fourth lug. One end of the first connecting rod and one end of the second connecting rod are hinged to one end portion at the bottom of the connecting base, and the other end of the first connecting rod and the other end of the second connecting rod are hinged to the first lug and the third lug respectively. One end of the fifth connecting rod and one end of the sixth connecting rod are hinged to the other end portion at the bottom of the connecting base, and the other end of the fifth connecting rod and the other end of the sixth connecting rod are hinged to the second lug and the fourth lug respectively. One end of the third connecting rod and one end of the fourth connecting rod are hinged to the top of the connecting base, and the other end of the third connecting rod and the other end of the fourth connecting rod are hinged to the first lug and the second lug respectively. The connecting base is fixedly connected to the Y-axis portion. The present invention has the following beneficial effects. 1 The overall structure adopts linkage motion of the upper arm and the lower arm. The motion space in a logistics direction is small. The presses are compact in layout. The sheet transmission distance is short. The factory area is saved. The tail end of the lower arm stably swings back and forth at a high-speed. The characteristics of high speed and efficiency, stable running, compact layout and the like of a high-speed robot are highlighted. Imperious demands for high grade, high efficiency and high pace of the automatic stamping industry are met. 2 The suspension portion adopts a fixation mode of a six-connecting-rod hinged structure, such that vibration caused when the presses and the robot move separately may be relieved, avoiding resonance and facilitating high-speed stable running of the high-speed stamping transfer robot. By adjusting the lengths of the connecting rods, not only may mounting requirements of the presses of different models and sizes be satisfied, but also the form and location tolerance of the high-speed robot is conveniently adjusted. 3 Main parts of the upper arm and the lower arm are made of aluminium alloy, so that the weight of the moving part is reduced, and the movement inertia is reduced. Correspondingly, the overall driving energy consumption is reduced. The running cost is lowered. 4 Since all the parts have simple motion loci, locus curves may be analyzed and optimized conveniently through kinematics. The loci of different dies may be optimized and processed quickly. The applicability is strong. Debugging after installation is facilitated. 5 The E-axis portion has a rotation function, delicate design and high structural strength, and not only is suitable for conveying heavy-load materials but also may realize transfer of materials between the presses in a direction inclined to the logistics direction. 6 Due to rotational motion of the D2-axis portion, the transferred sheet is always kept horizontal in a transfer process, so that air resistance is reduced, and the sheet is prevented from interference with ambient objects. The transmission design of the D2-axis portion adopts the synchronous belt for transmission, such that on the premise of guaranteeing accurate transmission, contact impact between the sucker and the sheet may be resisted during material taking by the tooling at the tail end, avoiding a damage caused by direct resistance by the speed reducer.",
                        "The accompanying drawings, which constitute a part of the present invention, serve to provide a further understanding of the present invention. In the accompanying drawing: 1 is a structural schematic view of a high-speed stamping transfer robot provided by the present invention; 2 is a structural schematic view of a Y-axis portion provided by the present invention; 3 is a structural schematic view of a Z1-axis portion provided by the present invention; 4 is a structural schematic view of a Z2-axis portion upper arm provided by the present invention; 5 is a structural schematic view of a mechanical lower arm portion provided by the present invention; 6 is a side view of a mechanical lower arm portion provided by the present invention; 7 is a structural schematic view of a tooling portion provided by the present invention; 8 is a structural schematic view of a suspension portion provided by the present invention; and 9 is a schematic view of a transfer system on an automatic stamping line provided by the present invention. In the drawings, the respective numbers represent the following respective elements: 1, Y-axis portion; 101, Y-axis base; 102, first servo motor; 103, first speed reducer; 104, first speed reducer mounting base; 105, first transmission shaft; 106, synchronous pulley; 107, first synchronous belt; 108, first sliding seat; 109, first rotary seat; 110, first linear guide rail; 111, first limiting block; 2, Z1-axis portion; 201, Z1-axis base; 202, second servo motor; 203, second speed reducer; 204, second speed reducer mounting base; 205, first driving pulley; 206, first driven pulley; 207, second synchronous belt; 208, second sliding seat; 209, second rotary seat; 210, second linear guide rail; 211, speed-up pulley block; 212, balance cylinder; 3, Z2-axis portion; 301, Z2-axis base; 302, third servo motor; 303, third speed reducer; 304, third speed reducer mounting base; 305, second driving pulley; 306, third synchronous belt; 307, third sliding seat; 308, third linear guide rail; 309, bumper; 310, second limiting block; 4, D1-axis portion; 401, fourth servo motor; 402, fourth speed reducer mounting base; 403, fourth speed reducer; 5, E-axis portion; 501, fifth servo motor; 502, fifth speed reducer; 503, outer sleeve; 504, inner sleeve; 505, bearing; 6, D2-axis portion; 601, D2-axis fixed seat; 602, sixth servo motor; 603, sixth speed reducer; 604, third driving pulley; 605, third driven pulley; 606, fourth synchronous belt; 607, swing rod; 608, tensioning wheel; 609, ejector rod; 610, connecting flange plate; 7, tooling portion; 701, main rod; 702, connecting piece; 703, auxiliary rod; 704, sucker; 705, connecting flange; 8, suspension portion; 801, connecting base; 802, first connecting rod; 803, second connecting rod; 804, third connecting rod; 805, fourth connecting rod; 806, fifth connecting rod; 807, sixth connecting rod, 808, first lug; 809. second lug; 810, third lug; 811, fourth lug; 812, connecting sleeve; 9, press; 10, stamping die; 11, high-speed stamping transfer robot."
                    ],
                    "DESCRIPTION": "The technical solutions in the embodiments of the present invention will be clearly and completely described in the following with reference to the accompanying drawings in the embodiments of the present invention. It is obvious that the described embodiments are only part of but not all of the embodiments of the present invention. All other embodiments obtained by those skilled in the art based on the embodiments of the present invention without creative work shall fall within the scope of protection of the present invention. As shown in 1-7, a high-speed stamping transfer robot comprises a Y-axis portion 1, a Z1-axis portion 2, a Z2-axis portion 3, a mechanical lower arm portion and a tooling portion 7. The Z1-axis portion 2 is fixedly connected to the Y-axis portion 1. The Z2-axis portion 3 is in rotational connection with the Z1-axis portion 2 and the Y-axis portion 1 and is capable of moving vertically relative to the Z1-axis portion 2 while moving horizontally relative to the Y-axis portion 1. The mechanical lower arm portion is in rotational connection with the Z2-axis portion 3 and is capable of rotating relative to the Z2-axis portion 3. The tooling portion 7 is fixedly connected to the mechanical lower arm portion. As shown in 2, the Y-axis portion 1 comprises a first servo motor 102, a first speed reducer 103, a first speed reducer mounting base 104, a Y-axis base 101 and a first transmission shaft 105. The first speed reducer mounting bases 104 are mounted on two sides of the Y-axis base 101. The first speed reducer 103 is mounted on the first speed reducer mounting base 104. The first servo motor 102 is directly connected to the first speed reducer 103. An output shaft of the first speed reducer 103 is connected to the first transmission shaft 105 by means of a locking disk, such that a big torque may be transferred, and mounting and dismounting are facilitated. A synchronous pulley 106 with a first synchronous belt 107 mounted thereon is disposed on the first transmission shaft 105. A first sliding seat 108 is disposed on the first synchronous belt 107 and connected to a first rotary seat 109 through a bearing to form a passive rotation pair. First linear guide rails 110 that are symmetrical are disposed on two sides of the first synchronous belt 107 and are perpendicular to an axis direction of the first servo motor 102. A first slider that corresponds to the first linear guide rail 110 is disposed on the first sliding seat 108. The first sliding seat 108 is capable of moving horizontally along the first linear guide rail 110 which is equipped with a guide rail brake. The passive rotation pair may conduct not only a linear motion but also a rotary motion. A first limiting block 111 is disposed on the outer side of the first transmission shaft 105. Preferably, two synchronous pulleys 106 may be disposed on the first transmission shaft 105. The first synchronous belt 107 is mounted on each of the two synchronous pulleys 106 that are symmetrical. Bearing supports are mounted at the two ends and in the middle of the first transmission shaft 105 and configured to bear transmission power of an entire moving part of the robot in a sheet conveying direction. The bearing support in the middle of the first transmission shaft 105 is driven by two synchronous belts to achieve uniform power transmission and to meet the demand for a greater conveying force. A power source adopts two motors with relatively lower power for driving and may also adopt one high-power motor for driving. Here, the two first servo motors 102 with relatively lower power are adopted for driving. The first synchronous belt 107 for driving and tensioning is mounted on the inner side of the first sliding seat 108. The tightness of the first synchronous belt 107 is adjusted by adjusting a long screw to reduce a reciprocating clearance error. A belt breakage detection switch of the first synchronous belt 107 of the Y-axis portion is mounted on the first sliding seat 108. The first limiting block 111 is arranged on the outer side of the first transmission shaft 105. The guide rail brake is mounted on the first linear guide rail 110. When the belt breakage detection switch detects that the first synchronous belt 107 is broken, the guide rail brake immediately locks the first linear guide rail 110 to prevent the first sliding seat 108 from movement, avoiding an equipment damage after the belt breakage. As shown in 3, the Z1-axis portion 2 comprises a Z1-axis base 201. The bottom of the Z1-axis base 201 is fixedly mounted in the middle of the first servo motor 102 on the Y-axis base 101. The Z1-axis base 201 is secured to the Y-axis portion 1 through a processed locating straight port to guarantee the mounting accuracy. Second speed reducer mounting bases 204 are disposed on two sides of the top of the Z1-axis base 201. A second speed reducer 203 is mounted on the second speed reducer mounting base 204 and directly connected to a second servo motor 202. An output shaft of the second speed reducer 203 is connected to a second transmission shaft by means of a locking disk, such that a big torque may be transferred, and mounting and dismounting are facilitated. A first driving pulley 205 is disposed on the second transmission shaft. A first driven pulley 206 is disposed at the bottom of the Z1-axis base 201. The first driving pulley 205 and the first driven pulley 206 are mounted in the Z1-axis base 201. A second synchronous belt 207 is mounted on the first driving pulley 205 and the second driving pulley 206. A second sliding seat 208 is disposed on the second synchronous belt 207 and connected to a second rotary seat 209 through a bearing to form a passive rotation pair. Second linear guide rails 210 are disposed on two sides of the second synchronous belt 207 and are perpendicular to an axis direction of the second servo motor 202. A second slider that corresponds to the second linear guide rail 210 is disposed on the second sliding seat 208. The second sliding seat 208 is capable of moving back and forth vertically along the second linear guide rail 210. In this way, the passive rotation pair may conduct not only a linear motion but also a rotary motion. The second linear guide rail 210 is equipped with a guide rail brake. The Z1-axis portion 2 further comprises a speed-up pulley block 211 and a balance cylinder 212. The speed-up pulley block 211 is mounted on a base of the speed-up pulley block 211, which is located at the upper end of the Z1-axis base 201. A steel wire rope is connected to the second sliding seat 208 and wound in a fixed pulley groove of the speed-up pulley block 211, and enlaces a movable pulley on the balance cylinder 212. The other end of the steel wire rope is connected to the base of the speed-up pulley block 211. The power source adopts two motors with relatively lower power for driving and may also adopt one high-power motor for driving. Here, the two second servo motors 202 with relatively lower power are adopted for driving. Tensioning of the second synchronous belt 207 is realized by adjusting a long screw on the inner side of the second sliding seat 208, such that the second synchronous belt 207 is moderately tightened. Thus, the stability of a driving system is guaranteed. Since the second linear guide rail 210 is equipped with the guide rail brake, when the second synchronous belt 207 is broken or the steel wire rope at the balance cylinder 212 is suddenly broken, the guide rail brake immediately locks the second linear guide rail to prevent the second sliding seat 208 from movement, avoiding an equipment damage after the belt breakage. The balance cylinder 212 balances the overall weight of the moving part of the high-speed stamping transfer robot 11 through the speed-up pulley block 211. Thus, the driving power of the Z1-axis portion 2 and the working energy consumption are reduced. The balance cylinder 212 is secured to a suspension portion 8. One end of the steel wire rope is secured to the second sliding seat 208, and the other end thereof is secured to the base of the speed-up pulley block 211. The speed-up pulley block 211 is sped up to ensure that the vertical movement speed of the second sliding seat 208 of the Z1-axis portion 2 is two times of the movement speed of the balance cylinder 212. Thus, the speed of the cylinder is halved. The movement distance of the cylinder is half of the movement distance of the second sliding seat 208 of the Z1-axis portion 2. In this way, both the mounting space and the movement speed of the cylinder are reduced. As shown in 4, for the high-speed stamping transfer robot 11, a Z2-axis portion 3 is equivalent to an upper arm of the robot and comprises a Z2-axis base 301. To reduce the weight of the moving part, the Z2-axis base 301 is formed by connecting aluminium plates, such that power required by the Z1-axis portion 2 and the Y-axis portion 1 is reduced. Third speed reducer mounting bases 304 are disposed on two sides of the top of the Z2-axis base 301. A third speed reducer 303 is mounted on the third speed reducer mounting base 304 and directly connected to a third servo motor 302. An output shaft of the third speed reducer 303 is connected to the third transmission shaft by means of a locking disk, such that a big torque may be transferred, and mounting and dismounting are facilitated. A second driving pulley 305 is disposed on the third transmission shaft. A second driven pulley is disposed at the bottom of the Z2-axis base 301. The second driving pulley 305 and the second driven pulley are mounted in the Z2-axis base 301. A third synchronous belt 306 with a third sliding seat 307 disposed thereon is mounted on the second driving pulley 305 and the second driven pulley. Third linear guide rails 308 are disposed on the front side and the back side of the Z2-axis base 301 and located on two sides of the third synchronous belt 306. A third slider that corresponds to the third linear guide rail 308 is disposed on the third sliding seat 307. The third sliding seat 307 is mounted on and is capable of moving linearly along the third linear guide rail 308 on the front side of the Z2-axis base 301. Bumpers 309 are disposed on two sides of the bottom of the Z2-axis base 301. Second limiting blocks 310 are disposed on two sides of the top of the Z2-axis base 301 and configured for safety protection of movement of the third sliding seat 307. A fourth slider that corresponds to the third linear guide rail 308 is disposed on the first rotary seat 109 of the Y-axis portion 1. The Z2-axis portion 3 and the Y-axis portion 1 are connected in a sliding manner through the third linear guide rail 308 on the back side of the Z2-axis base 301 and the fourth slider. In this way, the Z2-axis base 301 may not only move linearly along the first sliding seat 108 of the Y-axis portion 1 but also rotate around the first sliding seat 108 of the Y-axis portion 1. As the back side of the Z2-axis base 301 is connected to the second rotary seat 209, the Z2-axis portion 3 may not only move linearly and vertically along the second linear guide rail 210 of the Z1-axis portion 2 but also rotate around the second rotary seat 209 of the Z1-axis portion 2. Thus, the Z2-axis portion 3 may move within a plane formed by the Y-axis portion 1 and the Z1-axis portion 2. Here, two motors are adopted for driving and symmetrically distributed and mounted. The center of gravity is kept on a symmetric plane. Thus, stable driving and running during swinging of the Z2-axis portion 3 may not be impacted by a stress imbalance during reciprocating swinging of the Z2-axis portion 3. The second limiting block 310 and the bumper 309 are mounted on the Z2-axis base 301, such that the third sliding seat 307 may not be disengaged from the third linear guide rail 308 during mounting or abnormal running. Thus, the second limiting block 310 and the bumper 309 are configured for safety protection of movement of the third sliding seat 307, avoiding personal casualties and equipment damages. As shown in 5 and 6, the mechanical lower arm portion comprises a D1-axis portion 4, an E-axis portion 5 and a D2-axis portion 6. The D1-axis portion 4 comprises a fourth servo motor 401, a fourth speed reducer 403 and a fourth speed reducer mounting base 402. The fourth servo motor 401 is directly connected to the fourth speed reducer 403. The fourth speed reducer 403 is fixedly connected to the fourth speed reducer mounting base 402 and also connected to the third sliding seat 307 of the Z2-axis portion 3. The fourth speed reducer 403 is mounted on the third sliding seat 307 of the Z2-axis portion 3. The fourth servo motor 401 drives the fourth speed reducer 403 to rotate. When an output shaft of the fourth speed reducer 403 is secured, the fourth speed reducer 403 drives the fourth speed reducer mounting base 402 to rotate, such that the whole D1-axis portion 4 swings and a reciprocating swinging function of the lower arm is achieved. As the fourth speed reducer mounting base 402 is made of aluminium alloy, on the premise of meeting the requirement for strength, the weight of the front-section moving part of the robot and the driving power are reduced. The running energy consumption is reduced. The E-axis portion 5 comprises a driving mechanism. The driving mechanism comprises a fifth servo motor 501 and a fifth speed reducer 502 which are mounted in the fourth speed reducer mounting base 402. The fifth servo motor 501 is directly connected to the fifth speed reducer 502. An output end of the fifth speed reducer 502 is fixedly connected to one end of an inner sleeve 504. The fourth speed reducer mounting base 402 is fixedly connected to an outer sleeve 503. The outer sleeve 503 and the inner sleeve 504 are connected through a bearing 505. The fifth speed reducer 502 of the E-axis portion 5 is mounted in the fourth speed reducer mounting base 402 of the D1-axis portion 4 and conducts a rotational motion relative to the D1-axis portion 4. The fifth servo motor 501 for driving is covered in the fourth speed reducer mounting base 402, such that the size of the front section is reduced, and interference caused when the front section of the robot enters a press 9 is avoided. Owing to the rotating function of the E-axis portion 5, a sheet may be conveyed between the presses in a direction inclined to a logistics direction. The outer sleeve 503 is secured to the fourth speed reducer mounting base 402 of the D1-axis portion 4 is a matched manner. The inner sleeve 504 is connected to an output shaft of the fifth speed reducer 502 and conducts a rotational motion. As the bearing 505 is added between the inner sleeve and the outer sleeve, the stress bearing capacity of the front section of the lower arm is increased, guaranteeing the load of the high-speed stamping transfer robot 11. The D2-axis portion 6 comprises a D2-axis fixed seat 601 fixedly connected to the other end of the inner sleeve 504. A sixth speed reducer 603 is disposed on one side of the D2-axis fixed seat 601 and directly connected to a sixth servo motor 602. An output shaft of the sixth speed reducer 603 is connected to a fourth transmission shaft through a coupler. A third driving pulley 604 is disposed on the fourth transmission shaft. A third driven pulley 605 is disposed on the D2-axis fixed seat 601. The third driving pulley 604 and the third driven pulley 605 are mounted in the D2-axis fixed seat 601. A fourth synchronous belt 606 is mounted on the third driving pulley 604 and the third driven pulley 605. Connecting flange plates 610 connected to the tooling portion 7 are disposed at two ends of the third driven pulley 604. A tensioning device is further disposed in the D2-axis fixed seat 601 and comprises a swing rod 607, a tensioning wheel 608 and an ejector rod 609. One end of the swing rod 607 is hinged to the inner wall of the D2-axis fixed seat 601. The tensioning wheel 608 is disposed at the other end of the swing rod 607. One end of the ejector rod 609 is fixedly connected to the inner wall of the D2-axis fixed seat 601, and the other end thereof is propped in a concave hole in the middle of the swing rod 607. The ejector rod 609 is in threaded connection with an adjusting nut. The tensioning wheel 608 is clung to the fourth synchronous belt 606. The sixth speed reducer 603 is mounted on the D2-axis fixed seat 601. The sixth servo motor 602 drives the third driving pulley 604. The third driven pulley 605 is driven by the fourth synchronous belt 606. The tensioning wheel 608 in the middle is configured for tensioning and mounted at one end of the swing rod 607. The other end of the swing rod 607 is fixedly connected to the D2-axis fixed seat 601 by means of hinging. The distance of the ejector rod 609 is adjusted to tension the fourth synchronous belt 606. The operation is performed from the outer side. The adjustment is facilitated. Here, the fourth synchronous belt 606 is adopted for transmission. On the premise of guaranteeing accurate transmission, a contact impact between a sucker 704 and the sheet during material taking by the tooling portion 7 at the tail end may be resisted. Thus, damages caused by direct resistance of the sixth speed reducer 603 are avoided. Here, owing to the rotating function of the D2-axis portion 6, the sheet may be kept horizontal in a conveying process. Further, air resistance is reduced. The sheet is prevented from interfering with ambient objects. The connecting flange plate 610 mounted at the third driven pulley 605 may be replaced according to a connection mode of the tooling portion 7 at the tail end, thus facilitating connection of the tooling portion 7 at the tail end. The fourth speed reducer 403 and the fourth speed reducer mounting base 402 are an RV speed reducer and an RV speed reducer mounting base. The RV speed reducer has the characteristics of small size and light weight. Thus, the overall dimension and weight of the D1-axis portion 4 are reduced. As shown in 7, the tooling portion 7 comprises two toolings each of which comprises a main rod 701 and an auxiliary rod 703. Two connecting pieces 702 sleeve the main rod 701 and are connected to the two auxiliary rods 703 that are symmetrical. The auxiliary rod 703 is perpendicular to the main rod 701. A sucker 704 is disposed at the other end of the auxiliary rod 703. A connecting flange 705 connected to the connecting flange plate 610 is disposed at one end of the main rod 701 and configured to mount the tooling, and may be replaced with a quick-change device to facilitate automatic replacement of the tooling. The auxiliary rod 703 is mounted on the main rod 701 through the connecting piece 702. Arrangement of the auxiliary rod 703 is adjusted according to the shape and the weight of the sheet. The sucker 704 is mounted on the auxiliary rod 703 and configured to suck the sheet. In addition, for transfer of the two sheets machined by one mold and with different center distances, cylinders or servo motors are mounted at two ends of the tooling for driving. As shown in 8, the transfer robot further comprises a suspension portion 8. The suspension portion 8 comprises a connecting base 801, a first connecting rod 802, a second connecting rod 803, a third connecting rod 804, a fourth connecting rod 805, a fifth connecting rod 806, a sixth connecting rod 807, a first lug 808, a second lug 809, a third lug 810 and a fourth lug 811. The connecting base 801 is triangular. Lug joints are disposed at two ends of the first connecting rod 802, the second connecting rod 803, the third connecting rod 804, the fourth connecting rod 805, the fifth connecting rod 806 and the sixth connecting rod 807 respectively and adopt right-and-left threaded structures. The lug joints at one ends of the first connecting rod 802 and the second connecting rod 803 are hinged to one end portion at the bottom of the connecting base 801. The lug joints at the other ends of the first connecting rod 802 and the second connecting rod 803 are hinged to the first lug 808 and the third lug 810 respectively. The lug joints at one ends of the fifth connecting rod 806 and the sixth connecting rod 807 are hinged to the other end portion at the bottom of the connecting base 801. The lug joints at the other ends of the fifth connecting rod 806 and the sixth connecting rod 807 are hinged to the second lug 809 and the fourth lug 811 respectively. The lug joints at one ends of the third connecting rod 804 and the fourth connecting rod 805 are hinged to the top of the connecting base 801. The lug joints at the other ends of the third connecting rod 804 and the fourth connecting rod 805 are hinged to the first lug 808 and the second lug 809 respectively. A plurality of connecting sleeves 812 with external threads is disposed on the connecting base 801. Internal threads matched with external threads are formed on the connecting base 801. A connecting sleeve 812 passes through a lock nut to be in threaded connection with the connecting base 801. A screw passes through a through hole in the connecting sleeve 812. The connecting base 801 and the Y-axis base 101 are fixedly connected through the screw inside the connecting sleeve 812. According to the high-speed stamping transfer robot 11, the Z2-axis portion 3 upper arm is driven to swing by the Y-axis portion 1 and the Z1-axis portion 2. The lower arm portion moves linearly and rotationally on the Z2-axis portion 3 upper arm. Through linkage among the respective axis portions, high-speed and high-load stable transfer of the sheet between the presses is realized. Although the present invention has been described in detail with reference to the foregoing embodiments, those skilled in the art may still modify the technical solutions described in the foregoing embodiments, or equivalently replace part of the technical features. Any modifications, equivalent substitutions, improvements, etc. within the spirit and the principle of the present invention should fall within the scope of protection of the present invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/89/832/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9560223817825317,
                    "F_SPEC_PARAMS": [
                        "warped,",
                        "quality of the stamped products and the product stamping efficiency,",
                        "product quality",
                        "stability"
                    ],
                    "S_SPEC_PARAMS": [
                        "production efficiency",
                        "efficient production"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Since the sheets may be warped, collided and inaccurately located by manual transfer, which severely impacts the quality of the stamped products and the product stamping efficiency, currently, more and more automobile manufacturers replace the traditional manual production line with an automatic robot stamping line that can realize automatic transfer of the sheets between the presses, thus replacing the workers with a robot and guaranteeing the product quality and stability."
                    ],
                    "S_SENTS": [
                        "Nowadays, the production efficiency of the common automatic robot line cannot meet the needs for efficient production."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Shape",
                        "Accuracy of Manufacturing",
                        "Stability of Object"
                    ],
                    "F_SIM_SCORE": 0.6402877569198608,
                    "S_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.7713545560836792,
                    "GLOBAL_SCORE": 1.7618435382843018
                },
                "sort": [
                    1.7618436
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11364084-20220621",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11364084-20220621",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-11-21",
                    "PUBLICATION_DATE": "2022-06-21",
                    "INVENTORS": [
                        "Assaf Govari",
                        "Andres Claudio Altmann",
                        "Vadim Gliner",
                        "Yehuda Algawi"
                    ],
                    "APPLICANTS": [
                        "Biosense Webster (Israel) Ltd.    ( Yokneam , IL )"
                    ],
                    "INVENTION_TITLE": "Contact force compensation in a robot manipulator",
                    "DOMAIN": "A61B 3432",
                    "ABSTRACT": "A robotic manipulator is attachable to a proximal end of a medical instrument. A processor linked to the manipulator transmits control signals to the manipulator to cause the manipulator to displace the medical instrument to achieve a desired position and orientation of the medical instrument. A contact force sensor is disposed on the medical instrument and linked to the processor. The control signals are issued by the processor responsively to force indications received from the contact force sensor.",
                    "CLAIMS": "1. An apparatus, comprising, a a medical instrument having a proximal portion and a distal portion, the distal portion having a distal end; b a manipulator, the proximal portion of the medical instrument being attachable to the manipulator; c a processor, linked to the manipulator and transmitting control signals to the manipulator to cause the manipulator to displace the medical instrument to achieve a desired position and orientation of the medical instrument within a paranasal passageway of a patient, wherein the processor is further configured to transmit the control signals to the manipulator to cause the manipulator to displace the medical instrument to achieve a predetermined force against the patient; d a first contact force sensor disposed on the distal portion of the medical instrument and linked to the processor, wherein the processor is configured to issue the control signals responsively to force indications received from the first contact force sensor to maintain the predetermined force against the paranasal passageway of the patient; e a location sensor disposed on the distal portion of the medical instrument and linked to the processor, wherein the processor is configured to determine a location of the distal portion based on signals from the location sensor; f a second contact force sensor, the second contact force sensor being positioned proximal to the first contact force sensor on the medical instrument; and g a haptic interface device linked to the processor, wherein the processor is configured to transmit haptic signals to the haptic interface device, to thereby drive the haptic interface device, to thereby provide haptic feedback to a human operator, based on signals from the first and second contact force sensors and the location sensor. 2. The apparatus according to claim 1, wherein the distal portion comprises 25 percent of a length of the medical instrument. 3. The apparatus according to claim 1, wherein the first contact force sensor is disposed at the distal end of the medical instrument. 4. The apparatus according to claim 1, wherein the processor is configured to issue the control signals responsively to other force indications received from the second contact force sensor. 5. The apparatus according to claim 1, further comprising: an operator interface with the processor, wherein the control signals are modifiable by human operator input via the operator interface. 6. The apparatus according to claim 1, further comprising a wireless transmitter for transmitting the haptic signals to the haptic interface device. 7. The apparatus according to claim 6, further comprising a communications module directly coupled with the first contact force sensor, wherein the communications module is configured to transmit the haptic signals to the haptic interface device. 8. The apparatus according to claim 1, wherein the processor is operative to calculate a contact force vector between the medical instrument and a contacting surface. 9. A method, comprising: a attaching a proximal portion of a medical instrument to a manipulator; b linking a processor to the manipulator; c transmitting control signals from the processor to the manipulator to cause the manipulator to displace the medical instrument to achieve a desired position, orientation, and force vector of the medical instrument against a surface of an inner cavity within the head of a patient; d receiving data from a first contact force sensor on the medical instrument, the first contact force sensor being configured to measure force data, the data indicating the force vector measured by the first contact force sensor, the data being received by the processor; e receiving data from a second contact force sensor, the second contact force sensor being positioned proximal to the first contact force sensor on the medical instrument; f receiving data from a location sensor on the medical instrument, the data from the location sensor indicating a location of the medical instrument within a patient; g calculating in the processor a contact force vector between the medical instrument and a contacting surface, wherein the contact force vector is based on the force data received from the first and second contact force sensors and the location indications received from the location sensor; h in response to the data received from the first contact force sensor, the second contact force sensor, and the location sensor, issuing control signals from the processor to constantly maintain the calculated contact force vector of the medical instrument against the surface of the patient during an operation; and i issuing haptic signals from the processor to a haptic interface device, the haptic interface device providing haptic feedback to a human operator in response to the haptic signals, the haptic feedback indicating a magnitude and direction of the calculated contact force vector. 10. The method according to claim 9, wherein the first contact force sensor is disposed on a distal portion of the medical instrument, the distal portion comprising 25 percent of a length of the medical instrument. 11. The method according to claim 9, wherein the first contact force sensor is disposed at a distal end of the medical instrument. 12. The method according to claim 9, further comprising providing an operator interface with the processor, wherein the control signals are modifiable by human operator input via the operator interface. 13. The method according to claim 9, further comprising wirelessly transmitting the force indications as the haptic signals to the haptic receiver. 14. A method, comprising: a attaching a proximal portion of a medical instrument to a manipulator; b linking a processor to the manipulator; c transmitting control signals from the processor to the manipulator to cause the manipulator to displace the medical instrument to achieve a desired position and orientation of the medical instrument, wherein the control signals are received from a human operator via an operator interface; d receiving data from a location sensor on the medical instrument, the data from the location sensor indicating a location of the medical instrument within a patient; e receiving data from a first contact force sensor on a distal end of the medical instrument, the first contact force sensor being configured to measure a force exerted by the medical instrument within a paranasal passageway of a patient, the data indicating the force exerted by the medical instrument, the data being received by the processor; f receiving data from a second contact force sensor, the second contact force sensor being positioned proximal to the first contact force sensor on the medical instrument; g in response to the received data, issuing control signals from the processor so as to maintain a predetermined force of the medical instrument against the paranasal passageway of the patient; and h transmitting haptic signals from the medical instrument to a wearable haptic receiver, the wearable haptic receiver having an actuator that deforms responsively to the haptic signals responsively to the force indications received from the first and second contact force sensors and the location indications received from the location sensor. 15. The apparatus according to claim 1, wherein the haptic interface device includes a wearable wrist bracelet having a motor-driven actuator that is configured to deform in response to haptic signals from the processor. 16. The apparatus according to claim 1, wherein the haptic feedback device comprises a vibration generator, the haptic signals being configured to cause the vibration generator to impart vibrations to the human operator with a periodicity or intensity that varies based on signals from the first contact force sensor and the location sensor. 17. The method according to claim 9, wherein the haptic interface device includes a wearable wrist bracelet having a motor-driven actuator that is configured to deform in response to haptic signals from the processor, the actuator deforming to provide the haptic feedback to the human operator in response to the haptic signals. 18. The apparatus according to claim 9, wherein the haptic feedback device comprises a vibration generator, the vibration generator imparting vibrations to the human operator with a periodicity or intensity that varies based on the calculated contact force vector.",
                    "FIELD_OF_INVENTION": "This invention relates to manipulators or robots adapted for use in surgery. More particularly, this invention relates to provision of contact force feedback and compensation in manipulators or robots adapted for use in surgery.",
                    "STATE_OF_THE_ART": "Robotic manipulators typically comprise a controller that control the motion of surgical instruments at a site, which can be remote from the location of the controller and its operator. One such device is the da Vinci System, which consists of a surgeon's console that is typically in the same room as the patient, and a patient-side cart with four interactive robotic arms controlled from the console. Three of the arms are for tools that hold objects, and can also act as scalpels, scissors, bovies, or graspers. The surgeon uses the console's master controls to maneuver the patient-side cart's three or four robotic arms. The instruments' jointed-wrist design exceeds the natural range of motion of the human hand; motion scaling and tremor reduction further interpret and refine the surgeon's hand movements. The da Vinci System always requires a human operator, and incorporates multiple redundant safety features designed to minimize opportunities for human error when compared with traditional approaches. One of the characteristics of many of the current robots used in surgical applications, which make them error prone is that they use an articular arm based on a series of rotational joints. The use of an articular system may create difficulties in arriving at an accurately targeted location because the level of any error is increased over each joint in the articular system. One solution to this difficulty is proposed in 8,628,518 to Blumenkranz, which proposes an arrangement in which a wireless package on a surgical end effector includes a force sensor.",
                    "SUMMARY": [
                        "According to disclosed embodiments of the invention, a probe is placed in contact with the wall of a cavity such as the ear or nasal cavity of a patient by a robotic manipulator. The probe may be configured to perform a variety of functions, such as visual inspection, radiofrequency ablation, or debriding. A contact force sensor is disposed on the distal end of the probe, and is configured to measure the magnitude and direction of the force acting on the distal end when in contact with the cavity wall. The robotic manipulator is activated to position the probe according to feedback received from the contact force sensor. More specifically, during the activation the probe is moved by the robotic manipulator such that the vector force exerted on the sensor is constant when, for example, there is patient movement. Additionally, the measured values of the vector force can be used to provide tactile feedback to the operator. There is provided according to embodiments of the invention an apparatus, which includes a manipulator that is attachable to a proximal end of a medical instrument. A processor linked to the manipulator transmits control signals to the manipulator to cause the manipulator to displace the medical instrument to achieve a desired position and orientation of the medical instrument. A contact force sensor is disposed on the medical instrument and linked to the processor. The control signals are issued by the processor responsively to force indications received from the contact force sensor. According to one aspect of the apparatus, the contact force sensor is disposed on the distal 25 percent of the length medical instrument. According to a further aspect of the apparatus, the contact force sensor is disposed on the distal end of the medical instrument. In yet another aspect of the apparatus another contact force sensor is disposed on the medical instrument proximal to the contact force sensor, wherein the control signals are issued by the processor responsively to other force indications received from the other contact force sensor. Still another aspect of the apparatus includes an interface with the processor, wherein the control signals are modifiable by an operator input via the interface, and a wearable haptic receiver linked to the processor having an actuator that deforms responsively to received haptic signals. An additional aspect of the apparatus includes a wireless transmitter for transmitting the force indications as the haptic signals to the haptic receiver. According to one aspect of the apparatus, the haptic signals are transmitted to the haptic receiver via the processor. According to another aspect of the apparatus, the haptic signals are transmitted to the haptic receiver directly from the contact force sensor. According to another aspect of the apparatus, the processor is operative to calculate a contact force vector between the medical instrument and a contacting surface. There is further provided according to embodiments of the invention a method, which is carried out by attaching a manipulator to a proximal end of a medical instrument, linking a processor to the manipulator, transmitting control signals from the processor to the manipulator to cause the manipulator to displace the medical instrument in order to achieve a desired position and orientation of the medical instrument, disposing a contact force sensor linked to the processor on the medical instrument, and issuing the control signals from the processor responsively to force indications received from the contact force sensor.",
                        "For a better understanding of the present invention, reference is made to the detailed description of the invention, by way of example, which is to be read in conjunction with the following drawings, wherein like elements are given like reference numerals, and wherein: 1 is a schematic illustration of a robotic medical instrument system, which is constructed and operative in accordance with an embodiment of the invention; 2 is a block diagram of a haptic device in the system shown in 1, which is constructed and operative in accordance with an embodiment of the invention; 3 schematically illustrates a robotic extension, which can be controlled by a manipulator in the system shown in 1 in accordance with an embodiment of the invention; and 4 is a flow chart of a method of operating a robotic medical instrument system in accordance with an embodiment of the invention."
                    ],
                    "DESCRIPTION": "In the following description, numerous specific details are set forth in order to provide a thorough understanding of the various principles of the present invention. It will be apparent to one skilled in the art, however, that not all these details are necessarily needed for practicing the present invention. In this instance, well-known circuits, control logic, and the details of computer program instructions for conventional algorithms and processes have not been shown in detail in order not to obscure the general concepts unnecessarily. Documents incorporated by reference herein are to be considered an integral part of the application except that, to the extent that any terms are defined in these incorporated documents in a manner that conflicts with definitions made explicitly or implicitly in the present specification, only the definitions in the present specification should be considered. The terms link, and links, are intended to mean either an indirect or direct connection. Thus, if a first device links to a second device, that connection may be through a direct connection, or through an indirect connection via other devices and connections. Reference is now made to 1, which is a schematic illustration of a robotic medical instrument system 10, which is constructed and operative in accordance with an embodiment of the invention. System 10 comprises a console or workstation 12 from which a human operator 14 interacts with a processor 16, which in turn regulates the activities of a manipulator 18. The manipulator 18 is typically located remotely from the workstation 12 and is linked to the processor 16 by cables or wirelessly. The processor 16 presents a graphical user interface on displays 20 and accepts operator input on manipulation devices 22, 24. The manipulator 18 holds a medical instrument, such as a probe 26, and is capable of manipulating the probe 26 in 3 dimensions x,y,z into contact with target tissue 28, which may be the wall of a cavity in the body of the patient. The manipulator 18 may additionally be capable of varying the attitude of the probe 26 about pitch, yaw and roll axes. The location of the probe 26 with respect to a reference coordinate system is established using a location sensor 30, which can be a magnetic sensor of the type described in commonly assigned patent application Ser. 15/708,357, issued as 10,517,612 on Dec. 31, 2019; Patent Application Publication 2017/0128128, issued as 10,588,692 on Mar. 17, 2020; and PCT Patent Document WO96105768 by Ben Haim, all of which are herein incorporated by reference. The location sensor 30 may be capable of determining the position of the probe 26 with up to 6 degrees of freedom. In another embodiment adapted to cardiac instrumentation, the location sensor 30 may be an electrode for an impedance-based locating system, as taught in 7,536,218, issued to Govari et al. , which is herein incorporated by reference. In any case, the location sensor 30 may communicate with the processor 16 over a wired or wireless connection, as described in the above-noted patent application Ser. 15/708,357, issued as 10,517,612 om Dec. 31,2019. The probe 26 is provided with a contact force sensor 32, which is linked to the processor 16 over a wired or wireless connection. Suitable force sensors are described in commonly assigned Patent Application Publication 2017/0258530 by Beeckler et al. , issued as 10,555,776 on Feb. 11, 2020; and commonly assigned copending patent application Ser. 15/452,843 of Govari et al. , published as Patent Publication 2018/0256247 on Sep. 13, 2018, the disclosures of which are herein incorporated by reference. Force sensor 32 is disposed on the distal portion of the probe 26 as shown in 1. The force sensor 32 issues signals from which a force vector {right arrow over F} is calculated by the processor 16. In some embodiments, the operator 14 may wear a haptic device 34, such as a wrist bracelet. Data from the force sensor 32 and the location sensor 30 processed by the processor 16. Alternatively, in some embodiments, raw data from the probe 26 may be haptically provided via a haptic device 34 to the operator by the system 10. This arrangement minimizes a need to consult a visual display and avoids distraction caused by an audio alert that might provide contact information. An advantage of this arrangement is shortened reaction time to inappropriate contact force or position of the probe 26. It is often important that the position of the tip of the probe 26 correspond to coordinates of medical images. In one mode of operation raw or processed data from the location sensor 30 may be provided to the haptic device 34. The sensations produced by the haptic device 34 are perceived by the operator as though a virtual assistant were holding his hand while he performs a medical procedure. The processor 16 may include a wireless transmitter 36 that communicates with the haptic device 34,Reference is now made to 2, which is a block diagram of the haptic device 34, which is wearable by the operator 14, and constructed and operative in accordance with an embodiment of the invention. The haptic device 34 has a wireless communications module 38, which receives signals from another communications module 40. The communications module 40 is linked to the probe 26 1, generally via a signal processor 42. Telemetry signals produced by the signal processor 42 are reflective of the contact force between the probe 26 and the target tissue 28. They are transmitted by the communications module 40 to the communications module 38 using any suitable communications protocol. Within the haptic device 34 a signal processor 44 has control circuitry linked to an electric motor 44, which drives an actuator 46. The actuator 46 has an oscillatory, vibratory or reciprocating motion, indicated by arrows 48. The tactile sensation experienced by the operator and produced by the actuator 46, , by deforming is representative of the contact force of the probe 26 as communicated via the communications module 38. In operation, the actuator 46 creates a tactile sensation, which the operator can interpret as a measure of the contact force currently being applied by the probe 26 against the target tissue 28. Additionally or alternatively the, signal processor 50 may be configured to control the electric motor 44 so as to cause the actuator 46 to vibrate, the vibrations being felt with a periodicity by the operator whose strength or period correlates with the contact force. Further alternatively, combinations of the intensity, periodicity and intervals of the vibration may communicate the contact force of the probe 26 to the operator 14. Vibratory frequencies varying from about 40 Hz to 250 Hz are suitable to communicate different levels of contact force. For example, the actuator 46 may vibrate rapidly or slowly according to contact force levels, or may alternate between vibrating and not vibrating to produce tactile silence for perceptibly longer periods, the pattern encoding levels of contact force. In a further example, the actuator 46 may operate for a relatively long and perceptible interval, , 0. 25-2 sec, and then cease to operate for a similar interval. Alternatively, specific ranges may produce different sensations, for example as a step function of the contact force. Encoding of activity patterns of the actuator 46 in various other ways will occur to those skilled in the art. In any case, such patterns, when haptically perceived by the operator, indicate the magnitude of the catheter's contact force or other parameter. Additionally or alternatively, the patterns might constitute, for example, a binary signal, indicating whether or not the catheter is in a stable location. The signals may be configurable by the operator, who may choose the kind of feedback he prefers to tactilely receive. Further details of the haptic device 34 are disclosed in commonly assigned Patent Application Publication 20140276760, entitled Force Feedback Device and Method for Catheters, issued as 9,486,272 on Nov. 8, 2016, which is herein incorporated by reference. Reference is now made to 3, which schematically illustrates a robotic extension 52 that could be controlled by manipulator 18 1 in accordance with an embodiment of the invention. The extension 52 comprises a distal arm 54, which pivots about a proximal arm 56. The arm 54 grasps a medical instrument 58, such as a probe. In some embodiments the pivoting motion of the arm 54 may be resisted, for example by a spring 60, which causes a restorative motion from a working position 62 to a resting position 64. Placement of a force sensor 66 distally on the instrument 58 confers an advantage in accuracy and precision of the force measurement compared with placement on the arm 54 of the extension 52. The location of the force sensor 66 on the instrument 58 is application-dependent. It is generally desirable that the force sensor 66 be placed at the most distal location of the instrument 58 that is practical, for example in the distal 25 percent of the length of the instrument 58. Preferably, the instrument 58 is placed at the distal end of the instrument 58. This is due, at least in part to the fact that a sensor disposed on the arm 54 of manipulator 18 experiences a mechanical advantage of force amplification based on a leverage effect, in which a magnitude of movement at the distal end of the probe 26 is exchanged for enhanced force at the arm 54. However any errors would similarly be amplified. Placement of the force sensor 66 proximal to the distal end causes it to experience a superposition of all forces applied on to the instrument 58 distal to the force sensor 66. For example, an application of a leftward force of 10 grams on the distal tip, and somewhere on the shaft a rightward force of 5 grams, the indicated force will be a superposition of the two. The calculation of the superposition in practice is non-trivial and depends on mechanical structure of the tool, locations of the forces and the position of the force sensor 66. In the case of a flexible instrument, the weight of the segment distal to the force sensor 66 also becomes a factor. Additionally, the force sensor 66 experiences greater excursion when placed on the instrument 58 as compared with the arm 54. Measurement of such excursion within the force sensor 66 overcomes any jitter that might exist with as a result of the relatively smaller excursion of the arm 54. 4 is a flow chart of a method of operating a robotic medical instrument system in accordance with an embodiment of the invention. At initial step 68 robotic medical instrument system 10 is configured by attachment of a suitable medical instrument to an arm of the manipulator 18. A force sensor is installed at the distal portion of the medical instrument. Optionally, a second force sensor may be placed on a more proximal portion of the medical instrument, which increases the precision of the controlled movements. Next, at step 70 the probe 26 or other medical instrument is introduced in contact with an operative site of the patient, which is typically a hollow viscous or chamber of an organ such as a paranasal sinus. The introduction may be done using the manipulator 18 1 under guidance of the operator 14. Signals from the force sensor are enabled, and the sensor generates data from which the processor 16 calculates the magnitude and direction of the force vector. In some embodiments the magnitude and direction of the force vector may be perceived by the operator 14 using a haptic device. During the operative procedure, the probe 26 may require repositioning by the manipulator 18 at the direction of the operator, which is indicated by delay step 72 in which such requirement is awaited. When the position of the probe 26 changes, the processor 16 then recalculates the force vector at step 74 and determines whether there is a deviation from the desired values of the force vector at decision step 76. If there is such a deviation, then at step 78 the position of the medical instrument is adjusted so as to null out the deviation. Then step 74 is iterated. The feedback loop formed by step 74, decision step 76 and step 78 may iterate until the deviation found in decision step 76 is insignificant, i. e. , is less than a predetermined value. Feedback levels are chosen according to the precision needed for a particular medical use, using well known control methods. When there is no significant deviation found at decision step 76, then, at decision step 80, it is determined if the operative procedure is complete. If the determination at decision step 80 is negative, then control returns to delay step 72 to await another position change in the medical instrument. If the determination at decision step 80 is affirmative, then the procedure ends at final step 82. It will be appreciated by persons skilled in the art that the present invention is not limited to what has been particularly shown and described hereinabove. Rather, the scope of the present invention includes both combinations and sub-combinations of the various features described hereinabove, as well as variations and modifications thereof that are not in the prior art, which would occur to persons skilled in the art upon reading the foregoing description.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/84/640/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9084962010383606,
                    "F_SPEC_PARAMS": [
                        "error prone"
                    ],
                    "S_SPEC_PARAMS": [
                        "arriving at an accurately targeted location",
                        "level of any error"
                    ],
                    "A_PARAMS": [
                        "articular arm based on a series of rotational joints"
                    ],
                    "F_SENTS": [
                        "One of the characteristics of many of the current robots used in surgical applications, which make them error prone is that they use an articular arm based on a series of rotational joints."
                    ],
                    "S_SENTS": [
                        "The use of an articular system may create difficulties in arriving at an accurately targeted location because the level of any error is increased over each joint in the articular system."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Reliability"
                    ],
                    "F_SIM_SCORE": 0.6210874915122986,
                    "S_TRIZ_PARAMS": [
                        "Accuracy of Measurement"
                    ],
                    "S_SIM_SCORE": 0.677349865436554,
                    "GLOBAL_SCORE": 1.7577148795127868
                },
                "sort": [
                    1.7577149
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11439470-20220913",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11439470-20220913",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-10-03",
                    "PUBLICATION_DATE": "2022-09-13",
                    "INVENTORS": [
                        "James T. Spivey",
                        "Mark S. Ortiz",
                        "Frederick E. Shelton, IV"
                    ],
                    "APPLICANTS": [
                        "Ethicon LLC    ( Guaynabo , US )"
                    ],
                    "INVENTION_TITLE": "Robotically-controlled surgical instrument with selectively articulatable end effector",
                    "DOMAIN": "A61B 3430",
                    "ABSTRACT": "Devices and systems are provided for controlling movement of a working end of a surgical device by means of a robotic system. In one embodiment, systems and devices are provided for moving an end effector on a distal end of a surgical fastening device. Movement can include rotational movement of the end effector about an axis of the shaft, articulation of the end effector relative to the shaft, and actuation of an end effector, , closing, firing, and/or cutting.",
                    "CLAIMS": "1. A surgical instrument for use with a robotic surgical system, wherein said surgical instrument comprises: a mounting portion operably interfacing with the robotic surgical system; an elongate shaft extending from said mounting portion, wherein said elongate shaft defines a longitudinal shaft axis; a drive system comprising a push-to-fire knife, wherein said push-to-fire knife is operably responsive to said robotic surgical system, wherein said drive system advances said push-to-fire knife in response to a firing motion transmitted from said robotic surgical system; a first articulation joint extending from said elongate shaft, wherein said first articulation joint comprises a first articulation axis transverse to said longitudinal shaft axis; a second articulation joint extending from said first articulation joint, wherein said second articulation joint comprises a second articulation axis positioned distal to said first articulation axis, and wherein said second articulation axis is transverse to said first articulation axis and said longitudinal shaft axis; an end effector extending from said second articulation joint; a first pair of articulation cables operably engaged with said first articulation joint and actuatable in response to an output motion from the robotic surgical system, wherein said end effector is rotated about said first articulation axis when said first pair of articulation cables are actuated; and a second pair of articulation cables extending through said first articulation joint and said second articulation joint, wherein said second pair of articulation cables are operably engaged with said end effector and actuatable in response to an output motion from the robotic surgical system, and wherein said end effector is rotated about said first articulation axis and said second articulation axis when said first pair of articulation cables and one of said second pair of articulation cables are actuated simultaneously. 2. The surgical instrument of claim 1, wherein said end effector is not rotated about said second articulation axis when said first pair of articulation cables are actuated. 3. The surgical instrument of claim 1, wherein said end effector comprises: a first jaw; a second jaw movable relative to said first jaw between an open position and a closed position; and a staple cartridge comprising a plurality of staples removably stored therein. 4. The surgical instrument of claim 1, wherein said first articulation axis and said second articulation axis are approximately 90 degrees apart. 5. A surgical instrument for use with a robotic surgical system, wherein said surgical instrument comprises: a mounting portion operably interfacing with the robotic surgical system; an elongate shaft extending from said mounting portion, wherein said elongate shaft defines a longitudinal shaft axis; a drive system comprising a push-to-fire knife, wherein said push-to-fire knife is operably responsive to said robotic surgical system, wherein said drive system advances said push-to-fire knife in response to a firing motion transmitted from said robotic surgical system; a first articulation joint extending from said elongate shaft, wherein said first articulation joint comprises a first articulation axis transverse to said longitudinal shaft axis; a second articulation joint extending from said first articulation joint, wherein said second articulation joint comprises a second articulation axis positioned distal to said first articulation axis, and wherein said second articulation axis is transverse to said first articulation axis and said longitudinal shaft axis; an end effector extending from said second articulation joint; a first articulation actuator operably engaged with said first articulation joint and actuatable in response to an output motion from the robotic surgical system, wherein said end effector is rotated about said first articulation axis when said first articulation actuator is actuated; and a second articulation actuator extending through said first articulation joint and said second articulation joint, wherein said second articulation actuator is operably engaged with said end effector and actuatable in response to an output motion from the robotic surgical system, and wherein said end effector is rotated about said first articulation axis and said second articulation axis when said first articulation actuator and said second articulation actuator are actuated at the same time. 6. The surgical instrument of claim 5, wherein said end effector is not rotated about said second articulation axis when said first articulation actuator is actuated. 7. The surgical instrument of claim 5, wherein said end effector comprises: a first jaw; a second jaw movable relative to said first jaw between an unclamped position and a clamped position; and a staple cartridge comprising a plurality of staples removably stored therein. 8. The surgical instrument of claim 5, wherein said first articulation axis and said longitudinal shaft axis are approximately 90 degrees apart. 9. The surgical instrument of claim 8, wherein said first articulation axis and said second articulation axis are approximately 90 degrees apart.",
                    "FIELD_OF_INVENTION": "The present invention relates broadly to methods and devices for controlling movement of a working end of a surgical device.",
                    "STATE_OF_THE_ART": "Endoscopic surgical instruments are often preferred over traditional open surgical devices since the use of a natural orifice tends to reduce the post-operative recovery time and complications. Consequently, significant development has gone into a range of endoscopic surgical instruments that are suitable for precise placement of a working end of a tool at a desired surgical site through a natural orifice. These tools can be used to engage and/or treat tissue in a number of ways to achieve a diagnostic or therapeutic effect. Endoscopic surgery requires that the shaft of the device be flexible while still allowing the working end to be articulated to angularly orient the working end relative to the tissue, and in some cases to be actuated to fire or otherwise effect movement of the working end. Integration of the controls for articulating and actuating a working end of an endoscopic device tend to be complicated by the use of a flexible shaft and by the size constraints of an endoscopic instrument. Generally, the control motions are all transferred through the shaft as longitudinal translations, which can interfere with the flexibility of the shaft. There is also a desire to lower the force necessary to articulate and/or actuate the working end to a level that all or a great majority of surgeons can handle. One known solution to lower the force-to-fire is to use electrical motors. However, surgeons typically prefer to experience feedback from the working end to assure proper operation of the end effector. The user-feedback effects are not suitably realizable in present motor-driven devices. Accordingly, there remains a need for improved methods and devices for controlling movement of a working end of an endoscopic surgical device.",
                    "SUMMARY": [
                        "In one embodiment, a surgical device is provided having an elongate shaft with a proximal end having a handle movably coupled thereto, and a distal end having a flexible neck extending therefrom. The handle and the flexible neck can be operatively associated such that movement of the handle is effective to cause the flexible neck to articulate in multiple planes. In certain exemplary embodiments, movement of the handle can be mimicked by the flexible neck. The device can also include an actuator extending between the handle and the flexible neck and configured to transfer movement from the handle to the flexible neck. The handle of the device can have a variety of configurations, but in one embodiment the handle can be adapted to articulate relative to the proximal end of the elongate shaft. For example, the handle can be coupled to the proximal end of the elongate shaft by a joint, such as a ball and socket joint, a hinge joint, or a flexing joint. The actuator of the device can also have a variety of configurations, and in one embodiment the actuator can be at least one cable extending along a length of the elongate shaft. For example, the device can include a plurality of cables extending along a length of the shaft and equally spaced apart from one another around a circumference of the actuator. The cables are configured to slide relative to an axis of the elongate shaft and to apply tension to the elongate shaft to cause at least a portion of the elongate shaft to flex and bend. The handle and/or the cables can also optionally include a locking mechanism associated therewith and configured to maintain the handle and/or cables in a fixed position. In an exemplary embodiment, the elongate shaft is configured to passively flex and bend when it is inserted through a tortuous lumen. The elongate shaft can also have a variety of configurations, but in one embodiment the device can be in the form of a surgical stapler and the elongate shaft can include an end effector coupled to a distal end of the flexible neck and adapted to engage tissue and deliver at least one fastener into the engaged tissue. The handle and the end effector can be coupled such that movement of the handle is mimicked by the end effector. For example, the handle can be coupled to the proximal end of the elongate shaft by a joint, such as a ball and socket joint, a hinge joint, and a flexing joint, and the flexible neck can be formed on or coupled to the end effector to allow the end effector to proportionally mimic movement of the handle. The device can also include an actuator extending between the handle and the end effector and configured to transfer movement from the handle to the flexible neck. The actuator can be, for example, a plurality of cables extending along a length of the elongate shaft. The cables can be equally spaced apart from one another around a circumference of the elongate shaft. In another embodiment, the device can be in the form of an accessory channel and the elongate shaft can be in the form of a tube having an inner lumen adapted to receive a tool therethrough. The flexible neck extending from the distal end of the elongate tube can be configured to flex to orient a tool extending through the elongate tube. The flexible neck can have a variety of configurations, but in one embodiment it includes a plurality of slits formed therein to facilitate flexion thereof. The slits can be configured to cause the flexible neck to flex into a desired orientation. For example, the flexible neck can include a distal region of slits and a proximal region of slits, and the slits can be configured such that tension applied to the flexible neck will cause the flexible neck to bend at the proximal and distal regions. A handle can be coupled to the proximal end of the elongate tube, and it can operatively associate with the flexible neck such that movement of the handle is mimicked by the flexible neck. The handle can also have a variety of configurations, and in one embodiment the handle can include a stationary member and a movable member adapted to articulate relative to the stationary member. The movable member can be coupled to the stationary member by a joint, such as a ball and socket joint, a hinge joint, and a flexing joint. In use, the accessory channel can be configured to releasably attach to an endoscope. For example, a mating element can be formed on and extend along a length of an external surface thereof for mating to a complementary mating element formed on a sleeve adapted to receive an endoscope. The device can also include an actuator extending between the handle and the flexible neck. The actuator can be configured to transfer movement from the handle to the flexible neck. In certain exemplary embodiments, the actuator is in the form of at least one cable extending along a length of the elongate tube. Where the actuator includes multiple cables, the cables are preferably equally spaced apart from one another around a circumference of the elongate tube. The cables can extend along the elongate tube using various techniques. For example, the elongate tube can include at least one lumen formed in a sidewall thereof and extending along the length thereof, and the cables can be slidably disposed within the lumens. The device can also include a locking mechanism positioned to engage at least one of the handle and the cables to lock the handle and the cables in a fixed position. The present invention also provides an endoscopic system having an elongate sleeve configured to be disposed around an endoscope, and an accessory channel removably matable to the elongate sleeve. The accessory channel can have an inner lumen extending therethrough between proximal and distal ends thereof for receiving a tool, a flexible portion formed on a distal portion thereof and being made flexible by a plurality of slits formed therein, and at least one handle coupled to the proximal end thereof and operatively associated with the flexible portion such that the handles is configured to cause the flexible portion to articulate in at least one plane. The handles can be operatively associated with the flexible portion by at least one cable, and the handles can be configured to axially move the cables relative to the accessory channel to cause the cables to apply tension to the flexible portion of the accessory channel such that the flexible portion articulates in at least one plane. In one embodiment, the device can include a single handle configured to cause the flexible portion to articulate in multiple planes. The single handle can include a stationary member coupled to the proximal end of the accessory channel, and a movable member configured to articulate relative to the stationary member. The single handle and the flexible portion can be operatively associated such that movement of the single handle is mimicked by the flexible portion. In another embodiment, the handle can include a first member configured to cause the flexible portion to articulate in a first plane, and a second member configured to cause the flexible portion to articulate in a second plane. In particular, the handle can include a stationary member coupled to the proximal end of the accessory channel, and the first and second members can be rotatably coupled to the stationary member. The device can further include a first spool coupled to the first member and having at least one cable extending therefrom and coupled to the flexible portion, and a second spool coupled to the second member and having at least one cable extending therefrom and coupled to the flexible portion. The first and second members can be effective to rotate the first and second spools and thereby move the cables axially to cause the flexible portion to articulate. The surgical devices disclosed herein can also include a variety of other features. For example, the device can include an optical image gathering unit disposed on a distal end of the elongate shaft. The optical image gathering unit can be adapted to acquire images during endoscopic procedures. An image display screen can be disposed on a proximal portion of the device and adapted to communicate with the optical image gathering unit to display the acquired images. In other embodiments, the end effector of the device can include a cartridge removably disposed therein and containing a plurality of staples for stapling tissue and a blade for cutting stapled tissue. In other aspects, a surgical method is provided and includes inserting an elongate shaft into a body lumen to position a flexible neck coupled to a distal end of the elongate shaft adjacent to tissue to be treated, and moving a handle pivotally coupled to a proximal end of the elongate shaft to cause the flexible neck to mimic the motion of the handle. The flexible neck can mirror movement of the handle, or movement of the flexible neck can directly correspond to movement of the handle. In certain exemplary embodiments, the movement is proportional. In one exemplary embodiment, an end effector coupled to a distal end of the elongate shaft is positioned adjacent to tissue to be fastened, and a handle pivotally coupled to a proximal end of the elongate shaft is moved to cause the end effector to proportionally mimic the motion of the handle. The end effector can mirror movement of the handle, or movement of the end effector can directly correspond to movement of the handle. In an exemplary embodiment, the handle is pivotally articulated about the proximal end of the elongate shaft to cause the end effector to mimic the motion of the handle. The method can further include engaging tissue between opposed jaws of the end effector, and driving at least one fastener from the end effector into the tissue. Tissue can be engaging by moving a translating member formed on the handle from a first position to a second position to close the opposed jaws, and the fasteners can be fired by rotating a rotatable member formed on the handle to actuate a driver mechanism disposed within the end effector to cause the driver mechanism to drive a plurality of fasteners into the tissue. In another embodiment, prior to moving the translating member from the first position to the second position, the rotatable member can be rotated to rotate the end effector relative to the flexible neck without actuating the driver mechanism. In yet another aspect, the elongate shaft can be in the form of an accessory channel that is slidably mated to an endoscope disposed within a body cavity to position a distal end of the accessory channel in proximity to a distal end of the endoscope. A tool is inserted through a lumen in the accessory channel such that the tool extends distally beyond the distal end of the accessory channel, and a handle coupled to a proximal end of the accessory channel can be moved to cause a flexible neck on the distal end of the accessory channel to articulate, thereby causing a working end of the tool to be oriented in a desired position. The handle can be moved by pivotally articulating the handle relative to the accessory channel, or alternatively is can be moved by rotating at least one rotatable member on the handle. In accordance with other general aspects of the various embodiments of the present invention, there is provided a surgical device that includes an end effector that is configured to perform at least one surgical procedure in response to at least one control motion applied thereto from a control unit of a robotic system. An elongate shaft is coupled to the end effector and is configured to facilitate the transmission of at least one control motion to the end effector from the robotic system. The elongate shaft defines a shaft axis and is configured to facilitate articulation of the end effector in two planes that are substantially perpendicular to the shaft axis upon manipulation of the control unit relative to the elongate shaft such that movement of the control unit is mimicked by the end effector. In accordance with still other general aspects of various embodiments of the present invention, there is provided an accessory channel for releasable attachment to an endoscope. In various embodiments, the accessory channel comprises an elongate tube that has an inner lumen extending therethrough between proximal and distal ends thereof for receiving a tool. The accessory channel further comprises a flexible neck that extends from the distal end of the elongate tube and is configured to flex to orient a tool extending through the elongate tube. The flexible neck is configured to be operably coupled to at least a portion of a robotic system such that movement of the at least a portion of the robotic system is mimicked by the flexible neck. In accordance with still other general aspects of various embodiments of the present invention, there is provided an endoscopic system for use with a robotic system. In various forms, the endoscopic system comprises an elongate sleeve that is configured to be disposed around an endoscope. An accessory channel is removably matable to the elongate sleeve. The accessory channel has an inner lumen extending therethrough between proximal and distal ends thereof for receiving a tool. A flexible portion is formed on a distal portion thereof and is made flexible by a plurality of slits formed therein. The proximal end of the accessory channel is configured for operable attachment to at least a portion of the robotic system such that actuation of the at least a portion of the robotic system causes the flexible portion to articulate in at least one plane. In various embodiments, a surgical instrument for use with a robotic surgical system is disclosed. The surgical instrument comprises a mounting portion operably interfacing with the robotic surgical system, an elongate shaft extending from the mounting portion, a drive system, a first articulation joint extending from the elongate shaft, a second articulation joint extending from the first articulation joint, an end effector extending from the second articulation joint, a first pair of articulation cables, and a second pair of articulation cables. The elongate shaft defines a longitudinal shaft axis. The drive system comprises a push-to-fire knife. The push-to-fire knife is operably responsive to the robotic surgical system. The drive system advances the push-to-fire knife in response to a firing motion transmitted from the robotic surgical system. The first articulation joint comprises a first articulation axis transverse to the longitudinal shaft axis. The second articulation joint comprises a second articulation axis positioned distal to the first articulation axis. The second articulation axis is transverse to the first articulation axis and the longitudinal shaft axis. The first pair of articulation cables are operably engaged with the first articulation joint and actuatable in response to an output motion from the robotic surgical system. The end effector is rotated about the first articulation axis when the first pair of articulation cables are actuated. The second pair of articulation cables extend through the first articulation joint and the second articulation joint. The second pair of articulation cables are operably engaged with the end effector and actuatable in response to an output motion from the robotic surgical system. The end effector is rotated about the first articulation axis and the second articulation axis when the first pair of articulation cables and one of the second pair of articulation cables are actuated simultaneously. In various embodiments, a surgical instrument for use with a robotic surgical system is disclosed. The surgical instrument comprises a mounting portion operably interfacing with the robotic surgical system, an elongate shaft extending from the mounting portion, a drive system, a first articulation joint extending from the elongate shaft, a second articulation joint extending from the first articulation joint, an end effector extending from the second articulation joint, a first articulation actuator, and a second articulation actuator. The elongate shaft defines a longitudinal shaft axis. The drive system comprises a push-to-fire knife. The push-to-fire knife is operably responsive to the robotic surgical system. The drive system advances the push-to-fire knife in response to a firing motion transmitted from the robotic surgical system. The first articulation joint comprises a first articulation axis transverse to the longitudinal shaft axis. The second articulation joint comprises a second articulation axis positioned distal to the first articulation axis. The second articulation axis is transverse to the first articulation axis and the longitudinal shaft axis. The first articulation actuator is operably engaged with the first articulation joint and actuatable in response to an output motion from the robotic surgical system. The end effector is rotated about the first articulation axis when the first articulation actuator is actuated. The second articulation actuator extends through the first articulation joint and the second articulation joint. The second articulation actuator is operably engaged with the end effector and actuatable in response to an output motion from the robotic surgical system. The end effector is rotated about the first articulation axis and the second articulation axis when the first articulation actuator and the second articulation actuator are actuated at the same time.",
                        "The invention will be more fully understood from the following detailed description taken in conjunction with the accompanying drawings, in which: 1A is a perspective view of one embodiment of a surgical stapling and cutting device, showing a working end of the device in an initial position; 1B is a perspective view of the surgical stapling and cutting device of 1A, showing the working end of the device in an articulated position; 2 is a perspective view of a portion of a flexible neck of the device shown in 1A and 1B; 3A is a perspective view of a distal portion of the device shown in 1A and 1B, showing an end effector and the flexible neck of 2 coupled thereto; 3B is a cross-sectional view taken across line 3B-3B of the end effector shown in 3A; 4A is a perspective view of a proximal portion of the device shown in 1A and 1B, showing a handle movably coupled to a proximal end of a shaft of the device; 4B is an exploded view of the proximal portion of the device shown in 4A; 5 is a perspective view of coupling element disposed between the flexible neck and elongate shaft of the device shown in 1A and 1B, showing an optical image gathering apparatus; 6 is a perspective view of the handle of the device shown in 1A and 1B, showing an image display screen; 7 is a perspective view of an accessory channel for use with an endoscope; 8A is a perspective view of a flexible neck of the device shown in 7; 8B is a perspective view of the flexible neck shown in 8A, showing the neck articulated in a first direction; 8C is a perspective view of the flexible neck shown in 8A, showing the neck articulated in a second direction; 9A is a perspective view of another embodiment of a flexible neck for use with an accessory channel; 9B is a perspective view of the flexible neck shown in 9A, showing the neck articulated in a first direction; 9C is a perspective view of the flexible neck shown in 9A, showing the neck articulated in a second direction; 10 is a perspective view of a plurality of cable actuators for use with the device of 7; 11 is a cross-sectional view of a shaft of the accessory channel of 7; 12 is a perspective view of one embodiment of an end cap for use with the accessory channel of 7; 13A is an exploded view of the handle and a proximal portion of the elongate shaft of the device shown in 7; 13B is a cross-sectional view of the handle and the proximal portion of the elongate shaft of 13A in an assembled configuration; 14A is a perspective view of another embodiment of an accessory channel; 14B is a cross-sectional view of the accessory channel shown in 14A; 15A is a side view of a handle assembly of the device shown in 14A and 14B; 15B is an exploded view of the handle assembly of 15A; 16A is a perspective view of one embodiment of a locking mechanism; 16B is a perspective view of the locking mechanism of 16A coupled to the surgical stapling and cutting device of 1A and 1B; 17 is a perspective view of one robotic controller embodiment; 18 is a perspective view of one robotic surgical arm cart/manipulator of a robotic system operably supporting a plurality of surgical tool embodiments of the present invention; 19 is a side view of the robotic surgical arm cart/manipulator depicted in 18; 20 is a perspective view of an exemplary cart structure with positioning linkages for operably supporting robotic manipulators that may be used with various surgical tool embodiments of the present invention; 21 is a perspective view of a surgical tool embodiment of the present invention; 22 is an exploded assembly view of an adapter and tool holder arrangement for attaching various surgical tool embodiments to a robotic system; 23 is a side view of the adapter shown in 22; 24 is a bottom view of the adapter shown in 22; 25 is a top view of the adapter of 22 and 23; 26 is a partial bottom perspective view of the surgical tool embodiment of 21; 27 is a partial exploded view of a portion of an articulatable surgical end effector embodiment of the present invention; 28 is a perspective view of the surgical tool embodiment of 26 with the tool mounting housing removed; 29 is a rear perspective view of the surgical tool embodiment of 26 with the tool mounting housing removed; 30 is a front perspective view of the surgical tool embodiment of 26 with the tool mounting housing removed; 31 is a partial exploded perspective view of the surgical tool embodiment of 30; 32 is a partial cross-sectional side view of the surgical tool embodiment of 26; 33 is an enlarged cross-sectional view of a portion of the surgical tool depicted in 32; 34 is an exploded perspective view of a portion of the tool mounting portion of the surgical tool embodiment depicted in 26; 35 is an enlarged exploded perspective view of a portion of the tool mounting portion of 34; 36 is a partial cross-sectional view of a portion of the elongated shaft assembly of the surgical tool of 26; 37 is a side view of a half portion of a closure nut embodiment of a surgical tool embodiment of the present invention; 38 is a perspective view of another surgical tool embodiment of the present invention; 39 is a cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 38 with the anvil in the open position and the closure clutch assembly in a neutral position; 40 is another cross-sectional side view of the surgical end effector and elongated shaft assembly shown in 39 with the clutch assembly engaged in a closure position; 41 is another cross-sectional side view of the surgical end effector and elongated shaft assembly shown in 39 with the clutch assembly engaged in a firing position; 42 is a top view of a portion of a tool mounting portion embodiment of the present invention; 43 is a perspective view of another surgical tool embodiment of the present invention; 44 is a cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 43 with the anvil in the open position; 45 is another cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 43 with the anvil in the closed position; 46 is a perspective view of a closure drive nut and portion of a knife bar embodiment of the present invention; 47 is a top view of another tool mounting portion embodiment of the present invention; 48 is a perspective view of another surgical tool embodiment of the present invention; 49 is a cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 48 with the anvil in the open position; 50 is another cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 49 with the anvil in the closed position; 51 is a cross-sectional view of a mounting collar embodiment of a surgical tool embodiment of the present invention showing the knife bar and distal end portion of the closure drive shaft; 52 is a cross-sectional view of the mounting collar embodiment of 51; 53 is a top view of another tool mounting portion embodiment of another surgical tool embodiment of the present invention; 53A is an exploded perspective view of a portion of a gear arrangement of another surgical tool embodiment of the present invention; 53B is a cross-sectional perspective view of the gear arrangement shown in 53A; 54 is a cross-sectional side view of a portion of a surgical end effector and elongated shaft assembly of another surgical tool embodiment of the present invention employing a pressure sensor arrangement with the anvil in the open position; 55 is another cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 54 with the anvil in the closed position; 56 is a side view of a portion of another surgical tool embodiment of the present invention in relation to a tool holder portion of a robotic system with some of the components thereof shown in cross-section; 57 is a side view of a portion of another surgical tool embodiment of the present invention in relation to a tool holder portion of a robotic system with some of the components thereof shown in cross-section; 58 is a side view of a portion of another surgical tool embodiment of the present invention with some of the components thereof shown in cross-section; 59 is a side view of a portion of another surgical end effector embodiment of a portion of a surgical tool embodiment of the present invention with some components thereof shown in cross-section; 60 is a side view of a portion of another surgical end effector embodiment of a portion of a surgical tool embodiment of the present invention with some components thereof shown in cross-section; 61 is a side view of a portion of another surgical end effector embodiment of a portion of a surgical tool embodiment of the present invention with some components thereof shown in cross-section; 62 is an enlarged cross-sectional view of a portion of the end effector of 61; 63 is another cross-sectional view of a portion of the end effector of 61 and 62; 64 is a cross-sectional side view of a portion of a surgical end effector and elongated shaft assembly of another surgical tool embodiment of the present invention with the anvil in the open position; 65 is an enlarged cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 64; 66 is another cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of 64 and 65 with the anvil thereof in the closed position; 67 is an enlarged cross-sectional side view of a portion of the surgical end effector and elongated shaft assembly of the surgical tool embodiment of 64-66; 68 is a top view of a tool mounting portion embodiment of a surgical tool embodiment of the present invention; 69 is a perspective assembly view of another surgical tool embodiment of the present invention; 70 is a front perspective view of a disposable loading unit arrangement that may be employed with various surgical tool embodiments of the present invention; 71 is a rear perspective view of the disposable loading unit of 70; 72 is a bottom perspective view of the disposable loading unit of 70 and 71; 73 is a bottom perspective view of another disposable loading unit embodiment that may be employed with various surgical tool embodiments of the present invention; 74 is an exploded perspective view of a mounting portion of a disposable loading unit depicted in 70-72; 75 is a perspective view of a portion of a disposable loading unit and an elongated shaft assembly embodiment of a surgical tool embodiment of the present invention with the disposable loading unit in a first position; 76 is another perspective view of a portion of the disposable loading unit and elongated shaft assembly of 75 with the disposable loading unit in a second position; 77 is a cross-sectional view of a portion of the disposable loading unit and elongated shaft assembly embodiment depicted in 75 and 76; 78 is another cross-sectional view of the disposable loading unit and elongated shaft assembly embodiment depicted in 75-77; 79 is a partial exploded perspective view of a portion of another disposable loading unit embodiment and an elongated shaft assembly embodiment of a surgical tool embodiment of the present invention; 80 is a partial exploded perspective view of a portion of another disposable loading unit embodiment and an elongated shaft assembly embodiment of a surgical tool embodiment of the present invention; 81 is another partial exploded perspective view of the disposable loading unit embodiment and an elongated shaft assembly embodiment of 80; 82 is a top view of another tool mounting portion embodiment of a surgical tool embodiment of the present invention; 83 is a side view of another surgical tool embodiment of the present invention with some of the components thereof shown in cross-section and in relation to a robotic tool holder of a robotic system; 84 is an exploded assembly view of a surgical end effector embodiment that may be used in connection with various surgical tool embodiments of the present invention; 85 is a side view of a portion of a cable-driven system for driving a cutting instrument employed in various surgical end effector embodiments of the present invention; 86 is a top view of the cable-driven system and cutting instrument of 85; 87 is a top view of a cable drive transmission embodiment of the present invention in a closure position; 88 is another top view of the cable drive transmission embodiment of 87 in a neutral position; 89 is another top view of the cable drive transmission embodiment of 87 and 88 in a firing position; 90 is a perspective view of the cable drive transmission embodiment in the position depicted in 87; 91 is a perspective view of the cable drive transmission embodiment in the position depicted in 88; 92 is a perspective view of the cable drive transmission embodiment in the position depicted in 89; 93 is a perspective view of another surgical tool embodiment of the present invention; 94 is a side view of a portion of another cable-driven system embodiment for driving a cutting instrument employed in various surgical end effector embodiments of the present invention; 95 is a top view of the cable-driven system embodiment of 94; 96 is a top view of a tool mounting portion embodiment of another surgical tool embodiment of the present invention; 97 is a top cross-sectional view of another surgical tool embodiment of the present invention; 98 is a cross-sectional view of a portion of a surgical end effector embodiment of a surgical tool embodiment of the present invention; 99 is a cross-sectional end view of the surgical end effector of 103 taken along line 99-99 in 98; 100 is a perspective view of the surgical end effector of 98 and 99 with portions thereof shown in cross-section; 101 is a side view of a portion of the surgical end effector of 98-100; 102 is a perspective view of a sled assembly embodiment of various surgical tool embodiments of the present invention; 103 is a cross-sectional view of the sled assembly embodiment of 102 and a portion of the elongated channel of 101; 104-109 diagrammatically depict the sequential firing of staples in a surgical tool embodiment of the present invention; 110 is a partial perspective view of a portion of a surgical end effector embodiment of the present invention; 111 is a partial cross-sectional perspective view of a portion of a surgical end effector embodiment of a surgical tool embodiment of the present invention; 112 is another partial cross-sectional perspective view of the surgical end effector embodiment of 111 with a sled assembly axially advancing therethrough; 113 is a perspective view of another sled assembly embodiment of another surgical tool embodiment of the present invention; 114 is a partial top view of a portion of the surgical end effector embodiment depicted in 111 and 112 with the sled assembly axially advancing therethrough; 115 is another partial top view of the surgical end effector embodiment of 114 with the top surface of the surgical staple cartridge omitted for clarity; 116 is a partial cross-sectional side view of a rotary driver embodiment and staple pusher embodiment of the surgical end effector depicted in 111 and 112; 117 is a perspective view of an automated reloading system embodiment of the present invention with a surgical end effector in extractive engagement with the extraction system thereof; 118 is another perspective view of the automated reloading system embodiment depicted in 117; 119 is a cross-sectional elevational view of the automated reloading system embodiment depicted in 117 and 118; 120 is another cross-sectional elevational view of the automated reloading system embodiment depicted in 117-119 with the extraction system thereof removing a spent surgical staple cartridge from the surgical end effector; 121 is another cross-sectional elevational view of the automated reloading system embodiment depicted in 117-120 illustrating the loading of a new surgical staple cartridge into a surgical end effector; 122 is a perspective view of another automated reloading system embodiment of the present invention with some components shown in cross-section; 123 is an exploded perspective view of a portion of the automated reloading system embodiment of 122; 124 is another exploded perspective view of the portion of the automated reloading system embodiment depicted in 123; 125 is a cross-sectional elevational view of the automated reloading system embodiment of 122-124; 126 is a cross-sectional view of an orientation tube embodiment supporting a disposable loading unit therein; 127 is a perspective view of another surgical tool embodiment of the present invention; 128 is a partial perspective view of an articulation joint embodiment of a surgical tool embodiment of the present invention; 129 is a perspective view of a closure tube embodiment of a surgical tool embodiment of the present invention; 130 is a perspective view of the closure tube embodiment of 129 assembled on the articulation joint embodiment of 128; 131 is a top view of a portion of a tool mounting portion embodiment of a surgical tool embodiment of the present invention; 132 is a perspective view of an articulation drive assembly embodiment employed in the tool mounting portion embodiment of 131; 133 is a perspective view of another surgical tool embodiment of the present invention; and 134 is a perspective view of another surgical tool embodiment of the present invention."
                    ],
                    "DESCRIPTION": "Applicant of the present application also owns the following patent applications that were filed on May 27, 2011 and which are each herein incorporated by reference in their respective entireties: patent application Ser. 13/118,259, entitled SURGICAL INSTRUMENT WITH WIRELESS COMMUNICATION BETWEEN A CONTROL UNIT OF A ROBOTIC SYSTEM AND REMOTE SENSOR, now 8,684,253; patent application Ser. 13/118,210, entitled ROBOTICALLY-CONTROLLED DISPOSABLE MOTOR DRIVEN LOADING UNIT, now 8,752,749; patent application Ser. 13/118,253, entitled ROBOTICALLY-CONTROLLED MOTORIZED SURGICAL INSTRUMENT, now 9,386,983; patent application Ser. 13/118,278, entitled ROBOTICALLY-CONTROLLED SURGICAL STAPLING DEVICES THAT PRODUCE FORMED STAPLES HAVING DIFFERENT LENGTHS, now 9,237,891; patent application Ser. 13/118,190, entitled ROBOTICALLY-CONTROLLED MOTORIZED CUTTING AND FASTENING INSTRUMENT, now 9,179,912; patent application Ser. 13/118,223, entitled ROBOTICALLY-CONTROLLED SHAFT BASED ROTARY DRIVE SYSTEMS FOR SURGICAL INSTRUMENTS, now 8,931,682; patent application Ser. 13/118,263, entitled ROBOTICALLY-CONTROLLED SURGICAL INSTRUMENT HAVING RECORDING CAPABILITIES, now Patent Application Publication 2011/0295295; patent application Ser. 13/118,272, entitled ROBOTICALLY-CONTROLLED SURGICAL INSTRUMENT WITH FORCE FEEDBACK CAPABILITIES, now Patent Application Publication 2011/0290856; patent application Ser. 13/118,246, entitled ROBOTICALLY-DRIVEN SURGICAL INSTRUMENT WITH E-BEAM DRIVER, now 9,060,770; patent application Ser. 13/118,241, entitled SURGICAL STAPLING INSTRUMENTS WITH ROTATABLE STAPLE DEPLOYMENT ARRANGEMENTS, now 9,072,535. Certain exemplary embodiments will now be described to provide an overall understanding of the principles of the structure, function, manufacture, and use of the devices and methods disclosed herein. One or more examples of these embodiments are illustrated in the accompanying drawings. Those of ordinary skill in the art will understand that the devices and methods specifically described herein and illustrated in the accompanying drawings are non-limiting exemplary embodiments and that the scope of the various embodiments of the present invention is defined solely by the claims. The features illustrated or described in connection with one exemplary embodiment may be combined with the features of other embodiments. Such modifications and variations are intended to be included within the scope of the present invention. Uses of the phrases in various embodiments, in some embodiments, in one embodiment, or in an embodiment, or the like, throughout the specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics of one or more embodiments may be combined in any suitable manner in one or more other embodiments. Such modifications and variations are intended to be included within the scope of the present invention. The present invention provides method and devices for controlling a working end of an endoscopic surgical device. In general, the endoscopic surgical devices include an elongate shaft having a distal working end with a flexible neck, and a proximal end with a handle for controlling movement of the flexible neck on the distal working end. In certain exemplary embodiments, this can be achieved using, for example, one or more cables that extend between the handle and the flexible neck such that movement of the handle applies a force to one or more of the cables to cause the flexible portion to flex and thereby move the working end of the device. Various other features are also provided to facilitate use of the device. A person skilled in the art will appreciate that the particular device being controlled, and the particular configuration of the working end, can vary and that the various control techniques described herein can be used on virtually any surgical device in which it is desirable to control movement of the working end. 1A and 1B illustrate one exemplary embodiment of a technique for controlling articulation of the end effector, and in particular for causing the end effector to mimic and simultaneously move with the handle. In this embodiment, the device is in the form of a linear stapling and cutting device 10 for applying multiple linear rows of staples to tissue and for cutting the stapled tissue. As shown, the device 10 generally includes an elongate shaft 12 having a proximal end 12a with a handle 14 coupled thereto, and a distal, working end 12a having an end effector 16 coupled thereto or formed thereon, as will be discussed in more detail below. In use, the end effector 16 is configured to mimic movement of the handle 14. Mimicking motion between the handle 14 and the end effector 16 can generally be achieved using an actuator not shown that extends between the handle 14 and the end effector 16, and that is effective to transfer forces from the handle 14 to the end effector 16. In an exemplary embodiment, the actuator is in the form of several cables that are spaced around a circumference of the elongate shaft 12, and that extend along the length of the elongate shaft 12. Movement of the handle 14 about the proximal end 12a of the shaft 12 will apply a force to one or more of the cables to cause the cables to apply a force to the end effector 16, thereby causing the end effector 16 to mimic the motion of the handle 14. Mimicking motion can include corresponding motion, whereby the end effector 16 moves in the same direction and orientation as the handle 14, or mirrored motion, whereby the end effector 16 moves in an opposite direction and orientation as the handle 14. The mimicking motion can also be proportional to the movement of the handle. The elongate shaft 12 of the device 10 can have a variety of configurations. For example, it can be solid or hollow, and it can be formed from a single component or multiple segments. As shown in 2, the elongate shaft 12 is hollow and is formed from multiple connecting segments to allow the elongate shaft 12 to flex. The flexibility of the shaft 12, as well as a relatively small diameter, allows the shaft 12 to be used in endoscopic procedures, whereby the device is introduced transluminally through a natural orifice. The shaft can also vary in length depending on the intended application. 2 further illustrates one exemplary embodiment of an actuator 22 in the form of several cables 34a, 34b, 34c, 34d that are spaced around a circumference of the elongate shaft 12, and that extend along the length of the elongate shaft 12. The number and location of the cables can vary. For example, three cables can be spaced approximately 120 apart from one another around the circumference of the shaft 12. In the embodiment shown in 2, four cables 34a, 34b, 34c, 34d are spaced approximately 90 apart from one another around the circumference of the shaft 12. Each cable 34a-d can extend through a pathway, such as a lumen, formed on, in, or around the elongate shaft 12. 2 illustrates each cable 34a-d extending through a cut-out formed on an external surface of each segment of the shaft 12. Thus, each segment includes four cut-outs spaced equidistant around the circumference of the shaft 12 to maintain the cables 34a-d equidistant from one another. The cut-outs preferably have a size that is effective to retain the cables 34a-d therein while allowing the cables 34a-d to freely slide relative to the shaft 12. The distal end of the cables 34a-d can be mated to the end effector 16 to control movement of the end effector 16. While the end effector 16 can have a variety of configurations, and various end effectors known in the art can be used, 3A illustrates one exemplary embodiment of an end effector 16 which generally includes opposed first and second jaws 18, 20 that are adapted to receive tissue therebetween. The first jaw 18 is adapted to contain a staple cartridge having multiple staples disposed therein and configured to be driven into tissue, and the second jaw 20 forms an anvil for deforming the staples. The particular configuration and the basic operation of the end effector 16 can vary, and various end effectors 16 known in the art can be used. By way of non-limiting example, 6,978,921 entitled SURGICAL STAPLING INSTRUMENT INCORPORATING AN E-BEAM FIRING MECHANISM, which is incorporated herein in its entirety, discloses one embodiment of an end effector that can be used with the present invention. In order to allow movement of the end effector 16 relative to the elongate shaft 12, the end effector 16 can be movably coupled to the distal end 12b of the elongate shaft 12. For example, the end effector 16 can be pivotally coupled to the distal end 12b of the elongate shaft 12 by a pivoting or rotating joint. Alternatively, the end effector 16 can include a flexible neck 26 formed thereon, as shown, for allowing movement of the end effector 16 relative to the elongate shaft 12. The flexible neck 26 can be formed integrally with the distal end 12b of the shaft 12 and/or the proximal end of the jaws 18, 20, or it can be a separate member that extends between the shaft 12 and the jaws 18, 20. As shown in 3A, the flexible neck 26 includes a first coupler 28 for mating the flexible neck 26 to the proximal end of the opposed jaws 18, 20, and a second coupler 30 for mating the flexible neck 26 to the distal end of the elongate shaft 12. The couplers 28, 30 can be removably of fixedly mated to the flexible neck 26 and/or to the jaws 18, 20 and the shaft 12. The couplers 28, 30 also function to house certain components of the end effector 16. For example, the first coupler 28 can function to anchor the cables therein, as will be discussed below, and it can also function to house a gear and driver assembly for actuating , closing and firing the jaws 18, 20. In order to facilitate flexion of the flexible neck 26, the neck 26 can include one or more slits 32 formed therein. The quantity, location, and size of the slits 32 can vary to obtain a desired flexibility. In the embodiment shown in 3A, the flexible neck 26 includes multiple rows of slits 32, each row extending radially around the flexible neck 26 and each row being spaced axially along the length of the flexible neck 26. Each row of slits contains two slits extending around the circumference of the neck 26, and each row of slits 32 is axially offset from one another. As a result, the flexible neck 26 includes alternating slits 32. A person skilled in the particular pattern of the slits 32 can vary, and that 3A merely illustrates one pattern for forming slits 32 to allow flexion of the flexible neck 26. Other exemplary slit configurations will be discussed in more detail below. As indicated above, the cables 34a-d can be coupled to the end effector 16 to allow the end effector 16 to move in coordination with the handle 14. The connecting location of the cables 34a-d with the end effector 16 can vary depending on the desired movement. In the illustrated embodiment, the distal end of the cables 34a-d is connected to the distal end of the flexible neck 26, and in particular they extend into and connect to the first coupler 28. 3B illustrates a cross-sectional view of the first coupler 28 showing four bores 28a, 28b, 28c, 28d for receiving the four cables 34a, 34b, 34c, 34d, respectively. Virtually any technique known in the art can be used to connect the cables 34a-d to the coupler 28 including, for example, mechanical mating techniques such as adhesives, an interference fit, a ball-and-socket connection, threads, etc. In use, the connection of the cables 34a-d at the distal end of the flexible neck 26 will allow the cables 34a-d to apply a tension to the flexible neck 26 when an axial force is applied to the cables 34a-d by the handle 14. This tension will cause the neck 26 to flex in a direction dictated by the amount of tension applied to each cable 34a-d, as will be discussed in more detail below. The handle 14 of the device 10 can be used to control movement of the end effector 16, and in particular to articulate the end effector 16 and thus angularly orient it relative to a longitudinal axis A of the elongate shaft 12. While the handle 14 can have a variety of configurations, in one exemplary embodiment the handle 14 is movably coupled to the proximal end 12a of the elongate shaft 12 such that movement of the handle 14 can be mimicked by the end effector 16. While various techniques can be used to movably couple the handle 14 to the shaft 12, in the embodiment shown in 4A-4C, a ball-and-socket connection is formed between the handle 14 and the proximal end 12a of the elongate shaft 12. As best shown in 4B, the proximal end 12a of the elongate shaft 12 includes a socket 24 formed therein, and the handle 14 includes a hemi-spherical ball 13a formed on a distal end thereof and configured to be rotatably seated within the socket 24. The socket 24 can be integrally formed with the proximal end 12a of the elongate shaft, or it can be formed by coupling a hollow housing 12c, as shown, to the proximal end 12a of the elongate shaft 12. The hemi-spherical ball 13a can also be formed integrally with the handle 14, or it can be a separate member that is coupled to the handle 14. In order to movably mate the handle 14 to the shaft 12, the hemi-spherical ball 13a on the handle 14 can be retained within the socket 24 using the cables 34a-d, which attach to the handle 14 as will be discussed below. However, other mating techniques can be used to movably mate the handle 14 to the shaft 12. For example, the ball 13a can be spherical and it can be captured within a spherical socket formed in the proximal end 12a of the elongate shaft 12, or a mating element, such as a pin, can extend through the ball 13a to retain the ball 13a within the socket 24. While 4B illustrates a ball 13a formed on the handle 14 and a socket 24 formed in the shaft 12, the ball-and-socket connection can be reversed such that the ball is on the shaft 12 and the socket is in the handle 14. Moreover, a person skilled in the art will appreciate that a variety of other techniques can be used to movably couple the handle 14 to the proximal end 12a of the elongate shaft 12. In use, the handle 14 can articulate or pivotally move relative to the shaft 12 to cause the end effector 16 to mimic the movement of the handle 14. This can be achieved by coupling the proximal end of the cables 34a-d to the handle 14. The connecting location of the cables 34a-d with the handle 14 can vary depending on the desired movement. In the illustrated embodiment, the cables only three cables 34a, 34b and 34c are shown in 4A extend from the elongate shaft 12, through the hollow housing 12c, and out of slots or openings formed in a proximal end of the hollow housing 12c. The cables 34a-d then extend around the ball 13a on the handle 14 and connect to a distal-facing surface on the handle 14 that surrounds the ball 13a. Virtually any technique known in the art can be used to connect the cables 34a-d to the handle 14 including, for example, mechanical mating techniques such as adhesives, an interference fit, threads, etc. As shown in 4A, the handle 14 includes openings formed therein, and the proximal ends not shown of the cables 34a-d can have a ball or other element formed thereon and configured to be captured within the openings. As further shown in 4A, the cables only three cables 34a, 34b and 34c are shown can remain spaced circumferentially around the handle 14. This will allow movement of the handle 14 to be mirrored by the end effector 16, as will be discussed in more detail below. Alternatively, the cables 34a-d can be crossed before they connect to the handle 14 to cause the end effector 16 to move in the same direction as the handle 14. For example, opposed cables 34a and 34c can cross one another and can connect to opposed sides of the handle 14, and opposed cables 34b and 34d can likewise cross one another and can connect to opposed sides of the handle 14. The cables 34a-d can be crossed at any location, such as within the hollow housing 12c on the proximal end 12a of the shaft 12. As further shown in 4A and 4B, the handle 14 can also include other features to facilitate use of the device. For example, the handle 14 can include a translating member 38 that is effective to close the jaws 18, 20 on the end effector 16, and a rotating member 40 that is effective to selectively rotate and actuate the end effector 16. The translating and rotating members 38, 40 are described in more detail in application Ser. 11/277,320 filed on Mar. 23, 2006, entitled SURGICAL FASTENER AND CUTTER WITH SINGLE CABLE ACTUATOR, now 7,575,144, which is hereby incorporated by reference in its entirety. In other embodiments, the handle 14 can include triggers, knobs, etc. for rotating and/or actuating the end effector 16. Referring back to 1B, in use the handle 14 can be pivoted or angularly oriented relative to the proximal end 12a of the elongate shaft 12 to effect mimicking movement of the end effector 16. In particular, pivoting the handle 14 about the elongate shaft 12 in a first direction will apply a force to one or more of cables 34a-d to pull the cables axially. As a result, the actuated cables will apply tension to the flexible neck 26 to cause the neck 26 to flex. In order to prevent the elongate shaft 12 from flexing in response to tension applied to the cables 34a-d by the handle 14, the flexible neck 26 can have a greater flexibility than the elongate shaft 12. This can be achieved, for example, using the alternating slits 32 as previously described, or in other embodiments the material can differ, or the elongate shaft can include a stabilizing element, such as a rod extending therethrough to render the shaft more rigid than the flexible neck. The direction of movement of the handle 14 will be mimicked by the end effector 16, either in the same direction i. e. , corresponding movement or in an opposite direction i. e. , mirrored movement, thus allowing a user to precisely control the position of the end effector 16. In an exemplary embodiment, the particular amount of movement of the end effector 16 can be proportional to the amount of movement of the handle 14. That is, the amount of movement of the end effector 16 can be directly equivalent to the amount of movement of the handle 14, or it can be proportionally increased or decreased relative to the amount of movement of the handle 14. In certain embodiments, it may be desirable to have the amount of movement of the end effector 16 be increased relative to the amount of movement of the handle 14. As a result, only small movements of the handle 14 will be necessary to allow large movements of the end effector 16. While various techniques can be achieved to proportionally multiple or increase the movement of the end effector 16, one exemplary embodiment of a force multiplying mechanism is an eccentric cam that is coupled to the cables and that increases the mechanical advantage, either force or displacement, of the cables 34a-d as tension is applied to the cables 34a-d by the handle 14. A person skilled in the art will appreciate that, while the movement between the handle and the working end of the device can be proportional in theory, in practice some lose of force will likely occur as the force is transferred through the elongate shaft. Accordingly, proportional movement as used herein is intended to include applications in which the handle and working end are configured to move in proportionate amounts, but in which some lose of force may occur during actual operation of the device. The various devices disclosed herein can also include a variety of other features to facilitate use thereof. For example, the device 10 of 1A can include an optical image gathering unit disposed on a distal end of the elongate shaft 12 and configured to acquire images during endoscopic procedures. While the location of the unit can vary, in one embodiment the optical image gathering unit can be disposed on the second coupler 30. In particular, 5 illustrates a ramp-shaped housing 42 that protrudes from an outer surface of the coupler 30, and that contains the optical image gathering unit therein. A viewing window 44 is formed on a distal-facing surface of the housing 42 to allow the unit to acquire images of the end effector 16 and surrounding surgical site. The images from the optical image gathering unit can be transferred to an external image display screen, or alternatively the device 10 can include image display screen disposed on or coupled to a proximal portion of the device. 6 illustrates one embodiment of an image display screen 46 protruding outward from the handle 14. As previously indicated, the various techniques disclosed herein for controlling movement of a working end of an endoscopic surgical device can be used in conjunction with a variety of medical devices. 7 illustrates another embodiment of a medical device having an actuator for controlling movement of the working end thereof. In this embodiment, the medical device is in the form of an accessory channel 100 for use with an endoscope. An accessory channel 100 is an external device that can mate to and slide along an endoscope to allow other tools, such as grasper, cutters, etc. , to be introduced therethrough and positioned in proximity to the viewing end of the endoscope. While the accessory channel 100 can have virtually any configuration, shape, and size, in the embodiment illustrated in 7 the accessory channel 100 includes an elongate tube or shaft 102 having an inner lumen extending between proximal and distal ends 102a, 102b thereof for receiving a tool therethrough. The accessory channel 100 can also include a mating element formed thereon for mating the accessory channel 100 directly to an endoscope or to a sleeve or other device disposed around an endoscope. While virtually any mating technique can be used, in the illustrated embodiment the mating element on the accessory channel 100 is in the form of a rail 104 that extends along a length of the elongate shaft 102. The rail 104 is configured to be received in a complementary track formed on an endoscope or a device disposed around an endoscope, such as a sleeve. A person skilled in the art will appreciate that a variety of other techniques can be used to mate the accessory channel either directly or indirectly to an endoscope. In order to control movement of a working end of the accessory channel 100, the device 100 can include features similar to those previously described. In particular, the device 100 can a flexible neck 108 formed on or coupled to the distal end 102b of the elongate shaft 102, a handle 106 formed on or coupled to the proximal end 102a of the elongate shaft 102, and an actuator extending between the handle 106 and the flexible neck 108. In this embodiment, the actuator is configured to transfer forces from the handle 106 to the flexible neck 108 such that movement of the handle 106 is mimicked by the flexible neck 108, thus allowing a tool extending through the accessory channel 100 to be positioned at a desired angular orientation. The flexible neck 108 can have a variety of configurations, and it can be a separate member that is coupled to the elongate shaft 102, or it can be formed integrally with the elongate shaft 102, as shown in 7. The neck 108 can be made flexible using various techniques. For example, the neck 108 can be formed from one or more segments that move relative to one another, and/or it can be formed from a flexible material. In the exemplary embodiment shown in 8A, the neck 108 includes several slits 112 formed therein and configured to provide maximum flexibility of the neck 108. While the size, quantity, and orientation of the slits 112 can vary to obtain the desired results, in the illustrated embodiment the flexible neck 108 includes four columns of slits only three columns of slits, indicated by arrows 112a, 112b, 112c, are shown. Each column extends axially along a length of the flexible neck 108, and each column includes four row of slits spaced radially around circumference of the neck 108. Each column of slits 112 is also axially offset from one another to allow the slits 112 to overlap. In use, when tension is applied to the actuator, the slits 112 will allow the neck 108 to bend or assume a curved configuration such that the neck 108 articulates relative to the remainder of the elongate shaft 102, as shown in 8B and 8C. In other embodiments, the slits can be positioned to allow flexion of the neck at multiple locations or bend points, or to otherwise allow the neck to flex into a predetermined position. By way of non-limiting example, 9A illustrates another embodiment of a flexible neck 108 having two regions of slits 112 formed therein. In particular, the flexible neck 108 includes a distal region of slits 112a and a proximal region of slits 112b. Each region 112a, 112b can include any number of slits positioned at any location to provide a desired degree of flexibility in one or more desired directions. As shown in 9A, the proximal end distal regions of slits 112a, 112b each include two rows of slits formed on opposed sides of and extending along the length of the flexible neck 108. In use, when tension is applied to the flexible neck 108, as will be discussed in more detail below, the neck 108 will flex at both the proximal and distal regions 112a, 112b and thereby articulate relative to the remainder of the elongate shaft 102. As shown in 9B, flexion can occur first in the distal region 112a of the neck 108. Further tension applied to the neck 108 can then cause the proximal region 112b to flex, as shown in 9C. In other embodiments, the slits positioning and/or size of the slits can be configured to cause flexion to occur in the proximal region 112b before it occurs in the distal region 112a, or alternatively the slits can be configured to cause simultaneous flexion of the proximal and distal regions 112b, 112a. A person skilled in the art will appreciate that the quantity, position, size, and shape of the slits can be adjusted to obtain the desired results. The particular configuration of the cut used to form each slit can also vary. For example, the width and length of the slit can remain constant from an outer surface of the elongate shaft to an inner surface of the elongate shaft, or alternatively the width and length can increase or decrease such that the slit tapers or otherwise varies. By way of non-limiting example, a tapering configuration can be formed by forming a slit having triangular configuration, where the length and width of the slit decrease from the outer surface to the inner surface of the elongate shaft. As indicated above, the actuator is configured to apply tension to the flexible neck 108 to cause the neck 108 to articulate. The actuator can have a variety of configurations, but in one exemplary embodiment the actuator is similar to the aforementioned actuator and includes one or more cables that extend between the handle 106 and the distal end of the flexible neck 108 such that the handle 106 and the flexible neck 108 are operatively associated. Each cable can be configured to apply tension to the flexible neck 108 to cause the neck 108 to articulate in a plane of motion. Thus, where the device 100 includes only one cable, the flexible neck 108 can articulate in a single plane of motion. Each additional cable can allow the neck 108 to articulate in a different plane of motion. Where multiple cables are provided, the neck 108 can articulate in multiple planes of motion. Moreover, the cables can be simultaneously tensioned, potentially allow for 360 articulation of the flexible neck 108. While the number of cables can vary, and the device 100 can include only one cable, in the embodiment shown in 7 the device 100 includes four cables only three cables 110a, 110b, 110c are shown. A portion of the cables 110a, 110b, 110c, 110d is shown in more detail in 10. As noted above, the cables 110a-d extend along a length of the elongate shaft 102 between the handle 106 and the flexible neck 108. The particular location of the cables 110a-d can vary, but in an exemplary embodiment the cables 110a-d are spaced radially around a circumference of the elongate shaft 102 and they extend between the distal-most end of the flexible neck 108 and the handle 106. The cables 110a-d can extend internally through or externally along the elongate shaft 102, or they can extend through lumens or pathways formed in the sidewall of the elongate shaft 102. 11 illustrates a cross-sectional view of the elongate shaft 102, showing four lumens 103a, 103b, 103c, 103d formed therein. The lumens 103a-d preferably have a size that allows the cables 116a-d to slide therein, and they are spaced circumferentially about the elongate shaft 102. The lumens 103a-d extend between the proximal and distal ends 102a, 102b of the elongate shaft 102 to allow the cables 110a-d to extend between the handle 106 and the distal-most end of the flexible neck 108. The distal end of the cables 110a-d can mate to the distal most end of the flexible neck 108 using a variety of techniques, but in one embodiment, shown in 12, the flexible neck 108 includes an end cap 114 coupled to or formed on the distal-most end thereof. While the configuration of the end cap 114 can vary depending on the configuration of the actuator, in the illustrated embodiment the end cap 114 includes four bores 114a, 114b, 114c, 114d formed therein and spaced around a circumference of the end cap 114 such that the bores 114a-d align with the lumens 103a-d in the elongate shaft 102. Each bore 114a-d is configured to receive one of the cables 110a-d. Various mating techniques can be used to retain the cables 110a-d within the bores 114a-d. For example, 10 illustrates ball formed on the end of each cable 110a-d for retaining the ends of the cables 110a-d in the bores 114a-d in the end cap 114. The end cap 114 can also include a central lumen 116 formed therein for receiving a tool therethrough. The lumen 116 can also function to facilitate positioning of a tool inserted through the accessory channel 100. The proximal end of the cables 110a-d can be mated to a handle 106 that is coupled to a proximal end of the shaft 102. While the handle 106 can have a variety of configurations, in one exemplary embodiment, previously shown in 7, the handle 106 can be in the form of a joystick that is movably coupled to the proximal end 102a of the elongate shaft 102, and in particular that is configured to articulate relative to the proximal end 102a of the elongate shaft 102. The articulating movement of the handle 106 can allow the motion of the handle 106 to be mimicked by the flexible neck 108, as will be discussed below. While articulating movement can be achieved using a variety of types of joints, in the illustrated embodiment a ball-and-socket connection is formed between the handle 106 and the elongate shaft 102. In particular, as shown in more detail in 13A and 13B, the proximal end 102a of the elongate shaft 102 includes a housing 103 formed thereon and defining a socket 118 in a proximal end thereof. The handle 106 includes a ball 120 that is movably disposed within the socket 118, and the joystick extends proximally from the ball 120 thus allowing the handle 106 to articulate relative to the elongate shaft 102. A pin or other mechanism can be used to movably retain the ball 120 within the socket 118. A person skilled in the art will appreciate that the handle can have a variety of other shapes, and that various other techniques can be used to movably connect the handle 106 to the elongate shaft 102. As indicated above, the proximal end of the cables 110a-d is configured to mate to the handle 106. Thus, the handle 106 can include features for mating to the cables 110a-d. While the particular mating features can vary depending on the configuration of the actuator, in an exemplary embodiment the joystick 122 on the handle 106 includes four legs 124a, 124b, 124c, 124d formed thereon. The legs 124a-d are spaced around a circumference of the joystick 122, such that they are substantially aligned with the cables, and each leg 124a-d is configured to mate to a terminal end of one of the cables 110a-d. A ball-and-socket connection, as previously described with respect to the distal ends of the cables 110a-d, can be used to mate the cables 110a-d to the legs, or alternatively any other mating technique known in the art can be used. Referring back to 7, in use the handle 106 can be pivoted or angularly oriented relative to the proximal end 102a of the elongate shaft 102 to effect mimicking movement of the flexible neck 108, and to thereby position a tool extending through the flexible neck 108. As shown in 7 and 13B, the joystick on the handle 106 can include a lumen 107 formed therethrough and axially aligned with the lumen 102c in the elongate shaft 102 for allowing a tool to be introduced through the device 100. In other embodiments, the handle 106 can be offset from the proximal end 102a of the elongate shaft 102 such that the handle 106 is coupled to the cables, but does not interfere with direct access to the lumen 102c in the elongate shaft 102. In order to control movement of the flexible neck 108 and thus a tool positioned therethrough, the handle 106 is pivoted or articulated about the proximal end 102a of the elongate shaft 102. For example, movement of the handle 106 in a first direction will cause the legs 124a-d on the handle 106 to apply a force to one or more of cables 110a-d to pull the cables axially. As a result, the actuated cables will apply a tension force to the flexible neck 108 to cause the neck 108 to flex. In order to prevent the elongate shaft 102 from flexing in response to tension applied to the cables 110a-d by the handle 106, the flexible neck 108 can have a greater flexibility than the elongate shaft 102. This can be achieved, for example, using the slits as previously described, or in other embodiments the shaft 102 can include a stabilizing element, such as a rod, extending therethrough to make the shaft 102 more rigid than the flexible neck 108. The direction of movement of the handle 106 will be mimicked by the flexible neck 108, either in the same direction i. e. , corresponding movement or in an opposite direction i. e. , mirrored movement, thus allowing a user to precisely control the position of the flexible neck 108, and thus to control the position of a tool extending through the flexible neck 108. In an exemplary embodiment, the particular amount of movement of the flexible neck 108 can be proportional to the amount of movement of the handle 106. That is, the amount of movement of the flexible neck 108 can be directly equivalent to the amount of movement of the handle 106, or it can be proportionally increased or decreased relative to the amount of movement of the handle 106. In certain embodiments, it may be desirable to have the amount of movement of the flexible neck 108 be increased relative to the amount of movement of the handle 106. As a result, only small movements of the handle 106 will be necessary to allow large movements of the flexible neck 108. While various techniques can be achieved to proportionally multiple or increase the movement of the flexible neck 108, one exemplary embodiment of a force multiplying mechanism is an eccentric cam that is coupled to the cables and that increases the mechanical advantage, either force or displacement, of the cables 110a-d as tension is applied to the cables 110a-d by the handle 106. As previously explained, while the movement between the handle and the working end of the device can be proportional in theory, in practice some lose of force will likely occur as the force is transferred through the elongate shaft. Accordingly, proportional movement as used herein is intended to include applications in which the handle and working end are configured to move in proportionate amounts, but in which some lose of force may occur during actual operation of the device. While 1A and 7 illustrate devices in which the working end mimics movement of the handle, the handle can have a variety of other configurations in which it is effective to articulate the working end of the device without having the working end of the device mimic movement of the handle. 14A and 14B illustrate another embodiment of a device 200 having a handle 204 that includes a rotatable member that is effective to articulate a flexible neck 206 in one or more planes of motion relative to an elongate shaft 202 of the device. In general, the elongate shaft 202 of the device 200 is very similar to the elongate shaft 102 previously described, and it generally includes a flexible neck 206 coupled to or formed on a distal end thereof. Four cable actuators not shown extend through the elongate shaft between the handle 106 and the flexible neck 206. The shaft 102 and the cable actuators are similar to the shaft 102 and cable actuators 110a-d previously described with respect to device 100, and thus they will not be described in detail. The handle 204 of the device 200 is shown in more detail in 15A and 15B. In general, the handle 204 includes one or more spools rotatably disposed therein. Each spool is configured to mate to and control one of the cable actuators. Thus, rotation of each spool will wind up or release the cable, thereby causing the flexible neck 108 to flex and articulate in a particular direction. While the number of spools can vary depending on the number of cable actuators, in the embodiment shown in 15A and 15B, the handle 204 includes four spools 208a, 208b, 210a, 210b. The first two spools 208a, 208b are coupled to one another, and the second two spools 210a, 210b are coupled to one another. A first cable 212a is coupled to and wound around the first spool 208a, and a second cable 212b is coupled to and wound around the second spool 208b. The first and second cables 212a, 212b are positioned on and extend along opposite sides of the elongate shaft 202. As a result, tension applied to the first cable 212a will cause the flexible neck 206 to articulate in direction within a first plane of motion, and tension applied to the second cable 212b will cause the flexible neck 206 to articulate in the opposite direction within the same plane of motion. To allow tension to be applied to only one of the cables 212a, 212b, the first and second cables 212a, 212b are wound around the first and second spools 208a, 208b in opposite directions. Thus, rotation of the first and second spools 208a, 208b will wind and apply tension to one of the cables 212a, 212b while unwinding and releasing tension on the other one of the cables 212a, 212b. Third and fourth cables 212c, 212d are likewise wound around the third and fourth spools 210a, 210b such that rotation of the third and fourth and second spools 210a, 210b will wind and apply tension to one of the cables 212c, 212d while unwinding and releasing tension on the other one of the cables 212c, 212d. The third and fourth cables 212c, 212d can extend along the shaft 102 at a position that is radially offset from the first and second cables 212a, 212b such that the third and fourth cables 212c, 212d cause articulation of the flexible neck 206 in a second, different plane of motion. For example, the third and fourth cables 212c, 212d can be offset from the first and second cables 212a, 212b by about 90 such that the cables 212a-d are all spaced substantially equidistant around the circumference of the elongate shaft 202. A person skilled in the art will appreciate that the handle 204 can include any number of spools and cables to effect articulation in a desired number of planes. In order to control the spools 208a, 208b, 210a, 210b, the device can include one or more grasping members. As shown in 15A and 15B, a first rotatable knob 214 is coupled to the first and second spools 208a, 208b, and a second rotatable knob 216 is coupled to the third and fourth spools 210a, 210b. The knobs 214, 216 can be integrally formed with the spools 208a, 208b, 210a, 210b, or they can be coupled to the spools 208a, 208b, 210a, 210b by a shaft that extends through the spools 208a, 208b, 210a, 210b. In the illustrated embodiment, the first knob 214 is formed on or coupled directly to the first spool 208a, and the second knob 216 is coupled to the third and fourth spools 210a, 210b by a shaft 218 that extends from the knob 216 through the first and second spools 208a, 208b, and that couples to the third and fourth spools 210a, 210b. In other words, the first and second spools 208a, 208b are rotatably disposed around the shaft 218. In certain exemplary embodiments, the spools and the rotatable knobs can also differ in size. In the embodiment shown in 15A and 15B, the first and second spools 208a, 208b, as well as the first rotatable knob 214, have a diameter that is greater than a diameter of the third and fourth spools 210a, 210b and the second rotatable knob 216. While not necessary, such a configuration can be advantageous as it spaces the cables 212a-d apart to prevent the cables 212a-d from coming into contact with one another. In use, a tool can be positioned through the elongate shaft 202, and the knobs 214, 216 can be rotated to articulate the flexible neck 206 on the shaft 202 and thereby position the tool as desired. As shown in 14A and 14B, the handle 204 can include a lumen 205 extending therethrough and in alignment with the lumen in the elongate shaft 202 for allowing a tool to be passed through the handle 204 and the shaft 202. In other embodiments, the handle 204 can be offset from the elongate shaft 202 to provide direct access to the lumen in the elongate shaft 202. Once the tool is positioned through the shaft 202, the knobs 214, 214 can be rotated to articulate the flexible neck 206 on the distal end of the elongate shaft 202. In particular, the first knob 214 can be rotated in a first direction, , clockwise, to apply tension to one of the cables, , the first cable 212a, while releasing or unwinding the other cable, , the second cable 212b. As a result, the tension applied to the first cable 212a will pull the distal-most end of the flexible neck 206 in a proximal direction, causing the flexible neck 206 to flex and thereby articulate in a first direction. Rotation of the first knob 214 in an opposite direction, , counterclockwise, will unwind the first cable 212a while winding the second cable 212b. The flexible neck 206 will return to its initial, linear configuration. Further rotation of the first knob 214 will continue to wind the second cable 212b while unwinding the first cable 212a, thereby causing the flexible neck 206 to flex and articulate in an opposite direction along the same plane of motion. The second knob 216 can be likewise rotated to articulate the flexible in a different plane of motion. The knobs 214, 216 can also optionally be rotated simultaneously to articulate the flexible neck 206 in additional planes of motion different than the first and second planes of motion. In other embodiments, the various devices disclosed herein can include a locking mechanism for locking the handles and/or actuator in a fixed position to maintain the working end of a device in desired articulated or angular orientation. While the locking mechanism can have a variety of configurations, in one exemplary embodiment the locking mechanism can be in the form of a clamp that is effective to clamp down onto the cables and thereby prevent movement of the cables to lock the working end in a desired orientation. The clamp can have a variety of shapes and sizes, and it can be positioned at various locations on the device. 16A and 16B illustrate one exemplary embodiment of a clamp 300 that is disposed around the hollow housing 12c on the surgical fastening and cutting device 10 of 1A and 1B. The clamp 300 is generally ring-shaped and can be configured to be slidably or rotatably mated to the hollow housing 12c adjacent to the openings through which the cables only three cables 34a, 34b, 34c are shown in 16B extend. In an initial position, the clamp 300 is spaced apart from the openings to allow free movement of the cables 34a-d therethrough. Once the working end of the device, , the end effector 16, is articulated into a desired position, the clamp 300 can moved axially along the hollow housing 12c until it extends over the openings and engages the cables 34a-d extending therefrom. The clamp 300 will thus prevent movement of the cables 34a-d when the clamp 300 is in the locked position. In order to move the clamp 300 axially and to lock the clamp 300 to the housing 12c, the clamp 300 can include a mating element formed thereon and configured to engage a corresponding mating element formed on the housing 12c. As shown in 16A and 16B, the clamp includes threads 302 formed therein that are configured to mate with corresponding threads not shown formed on the housing 12c. As a result, rotation of the clamp 300 about the housing 12c will cause the clamp 300 to move between the initial and locked positions. A person skilled in the art will appreciate that various other mating techniques can be used. Moreover, the locking mechanism can have a variety of other configurations. For example, the handle can include a locking element formed thereon and configured to lock the handle in a fixed, articulated position. In other embodiments, the cables can be used to passively allow articulation of the elongate shaft through a body lumen, and the clamp 300 or other locking mechanism can be used to lock the working end of the device into position when desired. In such a configuration, the handle can merely be used to facilitate grasping of the device. In other embodiments, the cable actuators disclosed herein used to effect articulation of a working end of a device can be formed from an electroactive polymer material. Electroactive polymers EAPs, also referred to as artificial muscles, are materials that exhibit piezoelectric, pyroelectric, or electrostrictive properties in response to electrical or mechanical fields. In particular, EAPs are a set of conductive doped polymers that change shape when an electrical voltage is applied. The conductive polymer can be paired to some form of ionic fluid or gel and electrodes, and the flow of ions from the fluid/gel into or out of the conductive polymer can induce a shape change of the polymer. Typically, a voltage potential in the range of about 1V to 4 kV can be applied depending on the particular polymer and ionic fluid or gel used. It is important to note that EAPs do not change volume when energized, rather they merely expand in one direction and contract in a transverse direction. Thus, the cable actuators previously disclosed herein can be replaced by EAP actuators, and the handle can be configured to activate an energy source to selectively deliver energy to one or more of the cables. In an exemplary embodiment, movement of the handle can be configured to dictate the amount of the energy source, as well as the cables receiving the energy source. As a result, movement of the handle can still be mimicked by the working end of the device to provide the user with the same, precise control over the position of the working end. The energy source can be an internal source, such as a battery, or it can be an external source. In other embodiments, the EAP cable actuators can supplement the axial force applied to the cables by movement of the handle and thereby proportionally increase the amount of movement of the working end relative to the handle. In other aspects, the cable actuators can be formed from a shape-memory material, such as Nitinol. Such a configuration allows tension to be applied to the cables to articulate the end effector, yet allows the cables to return to an initial linear configuration without having to manipulate the handle. In yet another embodiment, the various devices disclosed herein, including portions thereof, can be designed to be disposed of after a single use, or they can be designed to be used multiple times. In either case, the device can be reconditioned for reuse after at least one use. Reconditioning can include any combination of the steps of disassembly of the device, followed by cleaning or replacement of particular pieces, and subsequent reassembly. By way of example, the surgical stapling and fastening device shown in 1A and 1B can be reconditioned after the device has been used in a medical procedure. The device can be disassembled, and any number of the particular pieces can be selectively replaced or removed in any combination. For example, for the surgical stapling and cutting device, a cartridge disposed within the end effector and containing a plurality of fasteners can be replaced by adding a new fastener cartridge to the end effector. Upon cleaning and/or replacement of particular parts, the device can be reassembled for subsequent use either at a reconditioning facility, or by a surgical team immediately prior to a surgical procedure. Those skilled in the art will appreciate that reconditioning of a device can utilize a variety of techniques for disassembly, cleaning/replacement, and reassembly. Use of such techniques, and the resulting reconditioned device, are all within the scope of the present application. One skilled in the art will appreciate further features and advantages of the invention based on the above-described embodiments. Accordingly, the invention is not to be limited by what has been particularly shown and described, except as indicated by the appended claims. All publications and references cited herein are expressly incorporated herein by reference in their entirety. Over the years a variety of minimally invasive robotic or telesurgical systems have been developed to increase surgical dexterity as well as to permit a surgeon to operate on a patient in an intuitive manner. Many of such systems are disclosed in the following patents which are each herein incorporated by reference in their respective entirety: 5,792,135, entitled ARTICULATED SURGICAL INSTRUMENT FOR PERFORMING MINIMALLY INVASIVE SURGERY WITH ENHANCED DEXTERITY AND SENSITIVITY, 6,231,565, entitled ROBOTIC ARM DLUS FOR PERFORMING SURGICAL TASKS, 6,783,524, entitled ROBOTIC SURGICAL TOOL WITH ULTRASOUND CAUTERIZING AND CUTTING INSTRUMENT, 6,364,888, entitled ALIGNMENT OF MASTER AND SLAVE IN A MINIMALLY INVASIVE SURGICAL APPARATUS, 7,524,320, entitled MECHANICAL ACTUATOR INTERFACE SYSTEM FOR ROBOTIC SURGICAL TOOLS, 7,691,098, entitled PLATFORM LINK WRIST MECHANISM, 7,806,891, entitled REPOSITIONING AND REORIENTATION OF MASTER/SLAVE RELATIONSHIP IN MINIMALLY INVASIVE TELESURGERY, and 7,824,401, entitled SURGICAL TOOL WITH WRISTED MONOPOLAR ELECTROSURGICAL END EFFECTOR Many of such systems, however, have in the past been unable to generate the magnitude of forces required to effectively cut and fasten tissue. 17 depicts one version of a master controller 1001 that may be used in connection with a robotic arm slave cart 1100 of the type depicted in 18. Master controller 1001 and robotic arm slave cart 1100, as well as their respective components and control systems are collectively referred to herein as a robotic system 1000. Examples of such systems and devices are disclosed in 7,524,320 which has been herein incorporated by reference. Thus, various details of such devices will not be described in detail herein beyond that which may be necessary to understand various embodiments and forms of the present invention. As is known, the master controller 1001 generally includes master controllers generally represented as 1003 in 17 which are grasped by the surgeon and manipulated in space while the surgeon views the procedure via a stereo display 1002. The master controllers 1001 generally comprise manual input devices which preferably move with multiple degrees of freedom, and which often further have an actuatable handle for actuating tools for example, for closing grasping saws, applying an electrical potential to an electrode, or the like. As can be seen in 18, in one form, the robotic arm cart 1100 is configured to actuate a plurality of surgical tools, generally designated as 1200. Various robotic surgery systems and methods employing master controller and robotic arm cart arrangements are disclosed in 6,132,368, entitled MULTI-COMPONENT TELEPRESENCE SYSTEM AND METHOD, the full disclosure of which is incorporated herein by reference. In various forms, the robotic arm cart 1100 includes a base 1002 from which, in the illustrated embodiment, three surgical tools 1200 are supported. In various forms, the surgical tools 1200 are each supported by a series of manually articulatable linkages, generally referred to as set-up joints 1104, and a robotic manipulator 1106. These structures are herein illustrated with protective covers extending over much of the robotic linkage. These protective covers may be optional, and may be limited in size or entirely eliminated in some embodiments to minimize the inertia that is encountered by the servo mechanisms used to manipulate such devices, to limit the volume of moving components so as to avoid collisions, and to limit the overall weight of the cart 1100. Cart 1100 will generally have dimensions suitable for transporting the cart 1100 between operating rooms. The cart 1100 may be configured to typically fit through standard operating room doors and onto standard hospital elevators. In various forms, the cart 1100 would preferably have a weight and include a wheel or other transportation system that allows the cart 1100 to be positioned adjacent an operating table by a single attendant. Referring now to 19, in at least one form, robotic manipulators 1106 may include a linkage 1108 that constrains movement of the surgical tool 1200. In various embodiments, linkage 1108 includes rigid links coupled together by rotational joints in a parallelogram arrangement so that the surgical tool 1200 rotates around a point in space 1110, as more fully described in issued 5,817,084, the full disclosure of which is herein incorporated by reference. The parallelogram arrangement constrains rotation to pivoting about an axis 1112a, sometimes called the pitch axis. The links supporting the parallelogram linkage are pivotally mounted to set-up joints 1104 18 so that the surgical tool 1200 further rotates about an axis 1112b, sometimes called the yaw axis. The pitch and yaw axes 1112a, 1112b intersect at the remote center 1114, which is aligned along a shaft 1208 of the surgical tool 1200. The surgical tool 1200 may have further degrees of driven freedom as supported by manipulator 1106, including sliding motion of the surgical tool 1200 along the longitudinal tool axis LT-LT. As the surgical tool 1200 slides along the tool axis LT-LT relative to manipulator 1106 arrow 1112c, remote center 1114 remains fixed relative to base 1116 of manipulator 1106. Hence, the entire manipulator is generally moved to re-position remote center 1114. Linkage 1108 of manipulator 1106 is driven by a series of motors 1120. These motors actively move linkage 1108 in response to commands from a processor of a control system. As will be discussed in further detail below, motors 1120 are also employed to manipulate the surgical tool 1200. An alternative set-up joint structure is illustrated in 20. In this embodiment, a surgical tool 1200 is supported by an alternative manipulator structure 1106 between two tissue manipulation tools. Those of ordinary skill in the art will appreciate that various embodiments of the present invention may incorporate a wide variety of alternative robotic structures, including those described in 5,878,193, entitled AUTOMATED ENDOSCOPE SYSTEM FOR OPTIMAL POSITIONING, the full disclosure of which is incorporated herein by reference. Additionally, while the data communication between a robotic component and the processor of the robotic surgical system is primarily described herein with reference to communication between the surgical tool 1200 and the master controller 1001, it should be understood that similar communication may take place between circuitry of a manipulator, a set-up joint, an endoscope or other image capture device, or the like, and the processor of the robotic surgical system for component compatibility verification, component-type identification, component calibration such as off-set or the like communication, confirmation of coupling of the component to the robotic surgical system, or the like. An exemplary non-limiting surgical tool 1200 that is well-adapted for use with a robotic system 1000 that has a tool drive assembly 1010 22 that is operatively coupled to a master controller 1001 that is operable by inputs from an operator i. e. , a surgeon is depicted in 21. As can be seen in that Figure, the surgical tool 1200 includes a surgical end effector 2012 that comprises an endocutter. In at least one form, the surgical tool 1200 generally includes an elongated shaft assembly 2008 that has a proximal closure tube 2040 and a distal closure tube 2042 that are coupled together by an articulation joint 2011. The surgical tool 1200 is operably coupled to the manipulator by a tool mounting portion, generally designated as 1300. The surgical tool 1200 further includes an interface 1230 which mechanically and electrically couples the tool mounting portion 1300 to the manipulator. One form of interface 1230 is illustrated in 22-26. In various embodiments, the tool mounting portion 1300 includes a tool mounting plate 1302 that operably supports a plurality of four are shown in 26 rotatable body portions, driven discs or elements 1304, that each include a pair of pins 1306 that extend from a surface of the driven element 1304. One pin 1306 is closer to an axis of rotation of each driven elements 1304 than the other pin 1306 on the same driven element 1304, which helps to ensure positive angular alignment of the driven element 1304. Interface 1230 includes an adaptor portion 1240 that is configured to mountingly engage the mounting plate 1302 as will be further discussed below. The adaptor portion 1240 may include an array of electrical connecting pins 1242 24 which may be coupled to a memory structure by a circuit board within the tool mounting portion 1300. While interface 1230 is described herein with reference to mechanical, electrical, and magnetic coupling elements, it should be understood that a wide variety of telemetry modalities might be used, including infrared, inductive coupling, or the like. As can be seen in 22-25, the adapter portion 1240 generally includes a tool side 1244 and a holder side 1246. In various forms, a plurality of rotatable bodies 1250 are mounted to a floating plate 1248 which has a limited range of movement relative to the surrounding adaptor structure normal to the major surfaces of the adaptor 1240. Axial movement of the floating plate 1248 helps decouple the rotatable bodies 1250 from the tool mounting portion 1300 when the levers 1303 along the sides of the tool mounting portion housing 1301 are actuated See 21. Other mechanisms/arrangements may be employed for releasably coupling the tool mounting portion 1300 to the adaptor 1240. In at least one form, rotatable bodies 1250 are resiliently mounted to floating plate 1248 by resilient radial members which extend into a circumferential indentation about the rotatable bodies 1250. The rotatable bodies 1250 can move axially relative to plate 1248 by deflection of these resilient structures. When disposed in a first axial position toward tool side 1244 the rotatable bodies 1250 are free to rotate without angular limitation. However, as the rotatable bodies 1250 move axially toward tool side 1244, tabs 1252 extending radially from the rotatable bodies 1250 laterally engage detents on the floating plates so as to limit angular rotation of the rotatable bodies 1250 about their axes. This limited rotation can be used to help drivingly engage the rotatable bodies 1250 with drive pins 1272 of a corresponding tool holder portion 1270 of the robotic system 1000, as the drive pins 1272 will push the rotatable bodies 1250 into the limited rotation position until the pins 1234 are aligned with and slide into openings 1256. Openings 1256 on the tool side 1244 and openings 1256 on the holder side 1246 of rotatable bodies 1250 are configured to accurately align the driven elements 1304 26 of the tool mounting portion 1300 with the drive elements 1271 of the tool holder 1270. As described above regarding inner and outer pins 1306 of driven elements 1304, the openings 1256, 1256 are at differing distances from the axis of rotation on their respective rotatable bodies 1250 so as to ensure that the alignment is not 180 degrees from its intended position. Additionally, each of the openings 1256 is slightly radially elongated so as to fittingly receive the pins 1306 in the circumferential orientation. This allows the pins 1306 to slide radially within the openings 1256, 1256 and accommodate some axial misalignment between the tool 1200 and tool holder 1270, while minimizing any angular misalignment and backlash between the drive and driven elements. Openings 1256 on the tool side 1244 are offset by about 90 degrees from the openings 1256 shown in broken lines on the holder side 1246, as can be seen most clearly in 25. Various embodiments may further include an array of electrical connector pins 1242 located on holder side 1246 of adaptor 1240, and the tool side 1244 of the adaptor 1240 may include slots 1258 25 for receiving a pin array not shown from the tool mounting portion 1300. In addition to transmitting electrical signals between the surgical tool 1200 and the tool holder 1270, at least some of these electrical connections may be coupled to an adaptor memory device 1260 24 by a circuit board of the adaptor 1240. A detachable latch arrangement 1239 may be employed to releasably affix the adaptor 1240 to the tool holder 1270. As used herein, the term tool drive assembly when used in the context of the robotic system 1000, at least encompasses various embodiments of the adapter 1240 and tool holder 1270 and which has been generally designated as 1010 in 22. For example, as can be seen in 22, the tool holder 1270 may include a first latch pin arrangement 1274 that is sized to be received in corresponding clevis slots 1241 provided in the adaptor 1240. In addition, the tool holder 1270 may further have second latch pins 1276 that are sized to be retained in corresponding latch devises 1243 in the adaptor 1240. See 24. In at least one form, a latch assembly 1245 is movably supported on the adapter 1240 and is biasable between a first latched position wherein the latch pins 1276 are retained within their respective latch clevis 1243 and an unlatched position wherein the second latch pins 1276 may be into or removed from the latch devises 1243. A spring or springs not shown are employed to bias the latch assembly into the latched position. A lip on the tool side 1244 of adaptor 1240 may slidably receive laterally extending tabs of tool mounting housing 1301. Turning next to 26-33, in at least one embodiment, the surgical tool 1200 includes a surgical end effector 2012 that comprises in this example, among other things, at least one component 2024 that is selectively movable between first and second positions relative to at least one other component 2022 in response to various control motions applied thereto as will be discussed in further detail below. In various embodiments, component 2022 comprises an elongated channel 2022 configured to operably support a surgical staple cartridge 2034 therein and component 2024 comprises a pivotally translatable clamping member, such as an anvil 2024. Various embodiments of the surgical end effector 2012 are configured to maintain the anvil 2024 and elongated channel 2022 at a spacing that assures effective stapling and severing of tissue clamped in the surgical end effector 2012. As can be seen in 37, the surgical end effector 2012 further includes a cutting instrument 2032 and a sled 2033. The cutting instrument 2032 may be, for example, a knife. The surgical staple cartridge 2034 operably houses a plurality of surgical staples not show therein that are supported on movable staple drivers not shown. As the cutting instrument 2032 is driven distally through a centrally-disposed slot not shown in the surgical staple cartridge 2034, it forces the sled 2033 distally as well. As the sled 2033 is driven distally, its wedge-shaped configuration contacts the movable staple drivers and drives them vertically toward the closed anvil 2024. The surgical staples are formed as they are driven into the forming surface located on the underside of the anvil 2024. The sled 2033 may be part of the surgical staple cartridge 2034, such that when the cutting instrument 2032 is retracted following the cutting operation, the sled 2033 does not retract. The anvil 2024 may be pivotably opened and closed at a pivot point 2025 located at the proximal end of the elongated channel 2022. The anvil 2024 may also include a tab 2027 at its proximal end that interacts with a component of the mechanical closure system described further below to facilitate the opening of the anvil 2024. The elongated channel 2022 and the anvil 2024 may be made of an electrically conductive material such as metal so that they may serve as part of an antenna that communicates with sensors in the end effector, as described above. The surgical staple cartridge 2034 could be made of a nonconductive material such as plastic and the sensor may be connected to or disposed in the surgical staple cartridge 2034, as was also described above. As can be seen in 26-33, the surgical end effector 2012 is attached to the tool mounting portion 1300 by an elongated shaft assembly 2008 according to various embodiments. As shown in the illustrated embodiment, the shaft assembly 2008 includes an articulation joint generally indicated as 2011 that enables the surgical end effector 2012 to be selectively articulated about an articulation axis AA-AA that is substantially transverse to a longitudinal tool axis LT-LT. See 27. In other embodiments, the articulation joint is omitted. In various embodiments, the shaft assembly 2008 may include a closure tube assembly 2009 that comprises a proximal closure tube 2040 and a distal closure tube 2042 that are pivotably linked by a pivot links 2044 and operably supported on a spine assembly generally depicted as 2049. In the illustrated embodiment, the spine assembly 2049 comprises a distal spine portion 2050 that is attached to the elongated channel 2022 and is pivotally coupled to the proximal spine portion 2052. The closure tube assembly 2009 is configured to axially slide on the spine assembly 2049 in response to actuation motions applied thereto. The distal closure tube 2042 includes an opening 2045 into which the tab 2027 on the anvil 2024 is inserted in order to facilitate opening of the anvil 2024 as the distal closure tube 2042 is moved axially in the proximal direction PD. The closure tubes 2040, 2042 may be made of electrically conductive material such as metal so that they may serve as part of the antenna, as described above. Components of the main drive shaft assembly , the drive shafts 2048, 2050 may be made of a nonconductive material such as plastic. In use, it may be desirable to rotate the surgical end effector 2012 about the longitudinal tool axis LT-LT. In at least one embodiment, the tool mounting portion 1300 includes a rotational transmission assembly 2069 that is configured to receive a corresponding rotary output motion from the tool drive assembly 1010 of the robotic system 1000 and convert that rotary output motion to a rotary control motion for rotating the elongated shaft assembly 2008 and surgical end effector 2012 about the longitudinal tool axis LT-LT. In various embodiments, for example, the proximal end 2060 of the proximal closure tube 2040 is rotatably supported on the tool mounting plate 1302 of the tool mounting portion 1300 by a forward support cradle 1309 and a closure sled 2100 that is also movably supported on the tool mounting plate 1302. In at least one form, the rotational transmission assembly 2069 includes a tube gear segment 2062 that is formed on or attached to the proximal end 2060 of the proximal closure tube 2040 for operable engagement by a rotational gear assembly 2070 that is operably supported on the tool mounting plate 1302. As can be seen in 29, the rotational gear assembly 2070, in at least one embodiment, comprises a rotation drive gear 2072 that is coupled to a corresponding first one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 1302 when the tool mounting portion 1300 is coupled to the tool drive assembly 1010. See 26. The rotational gear assembly 2070 further comprises a rotary driven gear 2074 that is rotatably supported on the tool mounting plate 1302 in meshing engagement with the tube gear segment 2062 and the rotation drive gear 2072. Application of a first rotary output motion from the tool drive assembly 1010 of the robotic system 1000 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 2072. Rotation of the rotation drive gear 2072 ultimately results in the rotation of the elongated shaft assembly 2008 and the surgical end effector 2012 about the longitudinal tool axis LT-LT represented by arrow R in 29. It will be appreciated that the application of a rotary output motion from the tool drive assembly 1010 in one direction will result in the rotation of the elongated shaft assembly 2008 and surgical end effector 2012 about the longitudinal tool axis LT-LT in a first direction and an application of the rotary output motion in an opposite direction will result in the rotation of the elongated shaft assembly 2008 and surgical end effector 2012 in a second direction that is opposite to the first direction. In at least one embodiment, the closure of the anvil 2024 relative to the staple cartridge 2034 is accomplished by axially moving the closure tube assembly 2009 in the distal direction DD on the spine assembly 2049. As indicated above, in various embodiments, the proximal end 2060 of the proximal closure tube 2040 is supported by the closure sled 2100 which comprises a portion of a closure transmission, generally depicted as 2099. In at least one form, the closure sled 2100 is configured to support the closure tube 2009 on the tool mounting plate 1320 such that the proximal closure tube 2040 can rotate relative to the closure sled 2100, yet travel axially with the closure sled 2100. In particular, as can be seen in 34, the closure sled 2100 has an upstanding tab 2101 that extends into a radial groove 2063 in the proximal end portion of the proximal closure tube 2040. In addition, as can be seen in 31 and 34, the closure sled 2100 has a tab portion 2102 that extends through a slot 1305 in the tool mounting plate 1302. The tab portion 2102 is configured to retain the closure sled 2100 in sliding engagement with the tool mounting plate 1302. In various embodiments, the closure sled 2100 has an upstanding portion 2104 that has a closure rack gear 2106 formed thereon. The closure rack gear 2106 is configured for driving engagement with a closure gear assembly 2110. See 31. In various forms, the closure gear assembly 2110 includes a closure spur gear 2112 that is coupled to a corresponding second one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 1302. See 26. Thus, application of a second rotary output motion from the tool drive assembly 1010 of the robotic system 1000 to the corresponding second driven element 1304 will cause rotation of the closure spur gear 2112 when the tool mounting portion 1300 is coupled to the tool drive assembly 1010. The closure gear assembly 2110 further includes a closure reduction gear set 2114 that is supported in meshing engagement with the closure spur gear 2112. As can be seen in 30 and 31, the closure reduction gear set 2114 includes a driven gear 2116 that is rotatably supported in meshing engagement with the closure spur gear 2112. The closure reduction gear set 2114 further includes a first closure drive gear 2118 that is in meshing engagement with a second closure drive gear 2120 that is rotatably supported on the tool mounting plate 1302 in meshing engagement with the closure rack gear 2106. Thus, application of a second rotary output motion from the tool drive assembly 1010 of the robotic system 1000 to the corresponding second driven element 1304 will cause rotation of the closure spur gear 2112 and the closure transmission 2110 and ultimately drive the closure sled 2100 and closure tube assembly 2009 axially. The axial direction in which the closure tube assembly 2009 moves ultimately depends upon the direction in which the second driven element 1304 is rotated. For example, in response to one rotary output motion received from the tool drive assembly 1010 of the robotic system 1000, the closure sled 2100 will be driven in the distal direction DD and ultimately drive the closure tube assembly 1009 in the distal direction. As the distal closure tube 2042 is driven distally, the end of the closure tube segment 2042 will engage a portion of the anvil 2024 and cause the anvil 2024 to pivot to a closed position. Upon application of an opening out put motion from the tool drive assembly 1010 of the robotic system 1000, the closure sled 2100 and shaft assembly 2008 will be driven in the proximal direction PD. As the distal closure tube 2042 is driven in the proximal direction, the opening 2045 therein interacts with the tab 2027 on the anvil 2024 to facilitate the opening thereof. In various embodiments, a spring not shown may be employed to bias the anvil to the open position when the distal closure tube 2042 has been moved to its starting position. In various embodiments, the various gears of the closure gear assembly 2110 are sized to generate the necessary closure forces needed to satisfactorily close the anvil 2024 onto the tissue to be cut and stapled by the surgical end effector 2012. For example, the gears of the closure transmission 2110 may be sized to generate approximately 70-120 pounds. In various embodiments, the cutting instrument 2032 is driven through the surgical end effector 2012 by a knife bar 2200. See 32 and 34. In at least one form, the knife bar 2200 may be fabricated from, for example, stainless steel or other similar material and has a substantially rectangular cross-sectional shape. Such knife bar configuration is sufficiently rigid to push the cutting instrument 2032 through tissue clamped in the surgical end effector 2012, while still being flexible enough to enable the surgical end effector 2012 to articulate relative to the proximal closure tube 2040 and the proximal spine portion 2052 about the articulation axis AA-AA as will be discussed in further detail below. As can be seen in 35 and 36, the proximal spine portion 2052 has a rectangular-shaped passage 2054 extending therethrough to provide support to the knife bar 2200 as it is axially pushed therethrough. The proximal spine portion 2052 has a proximal end 2056 that is rotatably mounted to a spine mounting bracket 2057 attached to the tool mounting plate 1032. See 34. Such arrangement permits the proximal spine portion 2052 to rotate, but not move axially, within the proximal closure tube 2040. As shown in 32, the distal end 2202 of the knife bar 2200 is attached to the cutting instrument 2032. The proximal end 2204 of the knife bar 2200 is rotatably affixed to a knife rack gear 2206 such that the knife bar 2200 is free to rotate relative to the knife rack gear 2206. See 34. As can be seen in 28-33, the knife rack gear 2206 is slidably supported within a rack housing 2210 that is attached to the tool mounting plate 1302 such that the knife rack gear 2206 is retained in meshing engagement with a knife gear assembly 2220. More specifically and with reference to 31, in at least one embodiment, the knife gear assembly 2220 includes a knife spur gear 2222 that is coupled to a corresponding third one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 1302. See 26. Thus, application of another rotary output motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding third driven element 1304 will cause rotation of the knife spur gear 2222. The knife gear assembly 2220 further includes a knife gear reduction set 2224 that includes a first knife driven gear 2226 and a second knife drive gear 2228. The knife gear reduction set 2224 is rotatably mounted to the tool mounting plate 1302 such that the first knife driven gear 2226 is in meshing engagement with the knife spur gear 2222. Likewise, the second knife drive gear 2228 is in meshing engagement with a third knife drive gear 2230 that is rotatably supported on the tool mounting plate 1302 in meshing engagement with the knife rack gear 2206. In various embodiments, the gears of the knife gear assembly 2220 are sized to generate the forces needed to drive the cutting element 2032 through the tissue clamped in the surgical end effector 2012 and actuate the staples therein. For example, the gears of the knife drive assembly 2230 may be sized to generate approximately 40 to 100 pounds. It will be appreciated that the application of a rotary output motion from the tool drive assembly 1010 in one direction will result in the axial movement of the cutting instrument 2032 in a distal direction and application of the rotary output motion in an opposite direction will result in the axial travel of the cutting instrument 2032 in a proximal direction. In various embodiments, the surgical tool 1200 employs and articulation system 2007 that includes an articulation joint 2011 that enables the surgical end effector 2012 to be articulated about an articulation axis AA-AA that is substantially transverse to the longitudinal tool axis LT-LT. In at least one embodiment, the surgical tool 1200 includes first and second articulation bars 2250a, 2250b that are slidably supported within corresponding passages 2053 provided through the proximal spine portion 2052. See 39 and 41. In at least one form, the first and second articulation bars 2250a, 2250b are actuated by an articulation transmission generally designated as 2249 that is operably supported on the tool mounting plate 1032. Each of the articulation bars 2250a, 2250b has a proximal end 2252 that has a guide rod protruding therefrom which extend laterally through a corresponding slot in the proximal end portion of the proximal spine portion 2052 and into a corresponding arcuate slot in an articulation nut 2260 which comprises a portion of the articulation transmission. 35 illustrates articulation bar 2250a. It will be understood that articulation bar 2250b is similarly constructed. As can be seen in 35, for example, the articulation bar 2250a has a guide rod 2254 which extends laterally through a corresponding slot 2058 in the proximal end portion 2056 of the distal spine portion 2050 and into a corresponding arcuate slot 2262 in the articulation nut 2260. In addition, the articulation bar 2250a has a distal end 2251a that is pivotally coupled to the distal spine portion 2050 by, for example, a pin 2253a and articulation bar 2250b has a distal end 2251b that is pivotally coupled to the distal spine portion 2050 by, for example, a pin 2253b. In particular, the articulation bar 2250a is laterally offset in a first lateral direction from the longitudinal tool axis LT-LT and the articulation bar 2250b is laterally offset in a second lateral direction from the longitudinal tool axis LT-LT. Thus, axial movement of the articulation bars 2250a and 2250b in opposing directions will result in the articulation of the distal spine portion 2050 as well as the surgical end effector 2012 attached thereto about the articulation axis AA-AA as will be discussed in further detail below. Articulation of the surgical end effector 2012 is controlled by rotating the articulation nut 2260 about the longitudinal tool axis LT-LT. The articulation nut 2260 is rotatably journaled on the proximal end portion 2056 of the distal spine portion 2050 and is rotatably driven thereon by an articulation gear assembly 2270. More specifically and with reference to 29, in at least one embodiment, the articulation gear assembly 2270 includes an articulation spur gear 2272 that is coupled to a corresponding fourth one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 1302. See 26. Thus, application of another rotary input motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding fourth driven element 1304 will cause rotation of the articulation spur gear 2272 when the interface 1230 is coupled to the tool holder 1270. An articulation drive gear 2274 is rotatably supported on the tool mounting plate 1302 in meshing engagement with the articulation spur gear 2272 and a gear portion 2264 of the articulation nut 2260 as shown. As can be seen in 34 and 35, the articulation nut 2260 has a shoulder 2266 formed thereon that defines an annular groove 2267 for receiving retaining posts 2268 therein. Retaining posts 2268 are attached to the tool mounting plate 1302 and serve to prevent the articulation nut 2260 from moving axially on the proximal spine portion 2052 while maintaining the ability to be rotated relative thereto. Thus, rotation of the articulation nut 2260 in a first direction, will result in the axial movement of the articulation bar 2250a in a distal direction DD and the axial movement of the articulation bar 2250b in a proximal direction PD because of the interaction of the guide rods 2254 with the spiral slots 2262 in the articulation gear 2260. Similarly, rotation of the articulation nut 2260 in a second direction that is opposite to the first direction will result in the axial movement of the articulation bar 2250a in the proximal direction PD as well as cause articulation bar 2250b to axially move in the distal direction DD. Thus, the surgical end effector 2012 may be selectively articulated about articulation axis AA-AA in a first direction FD by simultaneously moving the articulation bar 2250a in the distal direction DD and the articulation bar 2250b in the proximal direction PD. Likewise, the surgical end effector 2012 may be selectively articulated about the articulation axis AA-AA in a second direction SD by simultaneously moving the articulation bar 2250a in the proximal direction PD and the articulation bar 2250b in the distal direction DD. See 27. The tool embodiment described above employs an interface arrangement that is particularly well-suited for mounting the robotically controllable medical tool onto at least one form of robotic arm arrangement that generates at least four different rotary control motions. Those of ordinary skill in the art will appreciate that such rotary output motions may be selectively controlled through the programmable control systems employed by the robotic system/controller. For example, the tool arrangement described above may be well-suited for use with those robotic systems manufactured by Intuitive Surgical, Inc. of Sunnyvale, Calif. , A. , many of which may be described in detail in various patents incorporated herein by reference. The unique and novel aspects of various embodiments of the present invention serve to utilize the rotary output motions supplied by the robotic system to generate specific control motions having sufficient magnitudes that enable end effectors to cut and staple tissue. Thus, the unique arrangements and principles of various embodiments of the present invention may enable a variety of different forms of the tool systems disclosed and claimed herein to be effectively employed in connection with other types and forms of robotic systems that supply programmed rotary or other output motions. In addition, as will become further apparent as the present Detailed Description proceeds, various end effector embodiments of the present invention that require other forms of actuation motions may also be effectively actuated utilizing one or more of the control motions generated by the robotic system. 38-42 illustrate yet another surgical tool 2300 that may be effectively employed in connection with the robotic system 1000 that has a tool drive assembly that is operably coupled to a controller of the robotic system that is operable by inputs from an operator and which is configured to provide at least one rotary output motion to at least one rotatable body portion supported on the tool drive assembly. In various forms, the surgical tool 2300 includes a surgical end effector 2312 that includes an elongated channel 2322 and a pivotally translatable clamping member, such as an anvil 2324, which are maintained at a spacing that assures effective stapling and severing of tissue clamped in the surgical end effector 2312. As shown in the illustrated embodiment, the surgical end effector 2312 may include, in addition to the previously-mentioned elongated channel 2322 and anvil 2324, a cutting instrument 2332 that has a sled portion 2333 formed thereon, a surgical staple cartridge 2334 that is seated in the elongated channel 2322, and a rotary end effector drive shaft 2336 that has a helical screw thread formed thereon. The cutting instrument 2332 may be, for example, a knife. As will be discussed in further detail below, rotation of the end effector drive shaft 2336 will cause the cutting instrument 2332 and sled portion 2333 to axially travel through the surgical staple cartridge 2334 to move between a starting position and an ending position. The direction of axial travel of the cutting instrument 2332 depends upon the direction in which the end effector drive shaft 2336 is rotated. The anvil 2324 may be pivotably opened and closed at a pivot point 2325 connected to the proximate end of the elongated channel 2322. The anvil 2324 may also include a tab 2327 at its proximate end that operably interfaces with a component of the mechanical closure system described further below to open and close the anvil 2324. When the end effector drive shaft 2336 is rotated, the cutting instrument 2332 and sled 2333 will travel longitudinally through the surgical staple cartridge 2334 from the starting position to the ending position, thereby cutting tissue clamped within the surgical end effector 2312. The movement of the sled 2333 through the surgical staple cartridge 2334 causes the staples therein to be driven through the severed tissue and against the closed anvil 2324, which turns the staples to fasten the severed tissue. In one form, the elongated channel 2322 and the anvil 2324 may be made of an electrically conductive material such as metal so that they may serve as part of the antenna that communicates with sensors in the end effector, as described above. The surgical staple cartridge 2334 could be made of a nonconductive material such as plastic and the sensor may be connected to or disposed in the surgical staple cartridge 2334, as described above. It should be noted that although the embodiments of the surgical tool 2300 described herein employ a surgical end effector 2312 that staples the severed tissue, in other embodiments different techniques for fastening or sealing the severed tissue may be used. For example, end effectors that use RF energy or adhesives to fasten the severed tissue may also be used. 5,709,680, entitled ELECTROSURGICAL HEMOSTATIC DEVICE, and 5,688,270, entitled ELECTROSURGICAL HEMOSTATIC DEVICE WITH RECESSED AND/OR OFFSET ELECTRODES, which are incorporated herein by reference, discloses cutting instruments that use RF energy to fasten the severed tissue. patent application Ser. 11/267,811, now 7,673,783, and patent application Ser. 11/267,383, now 7,607,557, which are also incorporated herein by reference, disclose cutting instruments that use adhesives to fasten the severed tissue. Accordingly, although the description herein refers to cutting/stapling operations and the like, it should be recognized that this is an exemplary embodiment and is not meant to be limiting. Other tissue-fastening techniques may also be used. In the illustrated embodiment, the surgical end effector 2312 is coupled to an elongated shaft assembly 2308 that is coupled to a tool mounting portion 2460 and defines a longitudinal tool axis LT-LT. In this embodiment, the elongated shaft assembly 2308 does not include an articulation joint. Those of ordinary skill in the art will understand that other embodiments may have an articulation joint therein. In at least one embodiment, the elongated shaft assembly 2308 comprises a hollow outer tube 2340 that is rotatably supported on a tool mounting plate 2462 of a tool mounting portion 2460 as will be discussed in further detail below. In various embodiments, the elongated shaft assembly 2308 further includes a distal spine shaft 2350. Distal spine shaft 2350 has a distal end portion 2354 that is coupled to, or otherwise integrally formed with, a distal stationary base portion 2360 that is non-movably coupled to the channel 2322. See 39-41. As shown in 39, the distal spine shaft 2350 has a proximal end portion 2351 that is slidably received within a slot 2355 in a proximal spine shaft 2353 that is non-movably supported within the hollow outer tube 2340 by at least one support collar 2357. As can be further seen in 39 and 40, the surgical tool 2300 includes a closure tube 2370 that is constrained to only move axially relative to the distal stationary base portion 2360. The closure tube 2370 has a proximal end 2372 that has an internal thread 2374 formed therein that is in threaded engagement with a transmission arrangement, generally depicted as 2375 that is operably supported on the tool mounting plate 2462. In various forms, the transmission arrangement 2375 includes a rotary drive shaft assembly, generally designated as 2381. When rotated, the rotary drive shaft assembly 2381 will cause the closure tube 2370 to move axially as will be describe in further detail below. In at least one form, the rotary drive shaft assembly 2381 includes a closure drive nut 2382 of a closure clutch assembly generally designated as 2380. More specifically, the closure drive nut 2382 has a proximal end portion 2384 that is rotatably supported relative to the outer tube 2340 and is in threaded engagement with the closure tube 2370. For assembly purposes, the proximal end portion 2384 may be threadably attached to a retention ring 2386. Retention ring 2386, in cooperation with an end 2387 of the closure drive nut 2382, defines an annular slot 2388 into which a shoulder 2392 of a locking collar 2390 extends. The locking collar 2390 is non-movably attached , welded, glued, to the end of the outer tube 2340. Such arrangement serves to affix the closure drive nut 2382 to the outer tube 2340 while enabling the closure drive nut 2382 to rotate relative to the outer tube 2340. The closure drive nut 2382 further has a distal end 2383 that has a threaded portion 2385 that threadably engages the internal thread 2374 of the closure tube 2370. Thus, rotation of the closure drive nut 2382 will cause the closure tube 2370 to move axially as represented by arrow D in 40. Closure of the anvil 2324 and actuation of the cutting instrument 2332 are accomplished by control motions that are transmitted by a hollow drive sleeve 2400. As can be seen in 39 and 40, the hollow drive sleeve 2400 is rotatably and slidably received on the distal spine shaft 2350. The drive sleeve 2400 has a proximal end portion 2401 that is rotatably mounted to the proximal spine shaft 2353 that protrudes from the tool mounting portion 2460 such that the drive sleeve 2400 may rotate relative thereto. See 39. As can also be seen in 39-41, the drive sleeve 2400 is rotated about the longitudinal tool axis LT-LT by a drive shaft 2440. The drive shaft 2440 has a drive gear 2444 that is attached to its distal end 2442 and is in meshing engagement with a driven gear 2450 that is attached to the drive sleeve 2400. The drive sleeve 2400 further has a distal end portion 2402 that is coupled to a closure clutch 2410 portion of the closure clutch assembly 2380 that has a proximal face 2412 and a distal face 2414. The proximal face 2412 has a series of proximal teeth 2416 formed thereon that are adapted for selective engagement with corresponding proximal teeth cavities 2418 formed in the proximal end portion 2384 of the closure drive nut 2382. Thus, when the proximal teeth 2416 are in meshing engagement with the proximal teeth cavities 2418 in the closure drive nut 2382, rotation of the drive sleeve 2400 will result in rotation of the closure drive nut 2382 and ultimately cause the closure tube 2370 to move axially as will be discussed in further detail below. As can be most particularly seen in 39 and 40, the distal face 2414 of the drive clutch portion 2410 has a series of distal teeth 2415 formed thereon that are adapted for selective engagement with corresponding distal teeth cavities 2426 formed in a face plate portion 2424 of a knife drive shaft assembly 2420. In various embodiments, the knife drive shaft assembly 2420 comprises a hollow knife shaft segment 2430 that is rotatably received on a corresponding portion of the distal spine shaft 2350 that is attached to or protrudes from the stationary base 2360. When the distal teeth 2415 of the closure clutch portion 2410 are in meshing engagement with the distal teeth cavities 2426 in the face plate portion 2424, rotation of the drive sleeve 2400 will result in rotation of the drive shaft segment 2430 about the stationary shaft 2350. As can be seen in 44-46, a knife drive gear 2432 is attached to the drive shaft segment 2430 and is meshing engagement with a drive knife gear 2434 that is attached to the end effector drive shaft 2336. Thus, rotation of the drive shaft segment 2430 will result in the rotation of the end effector drive shaft 2336 to drive the cutting instrument 2332 and sled 2333 distally through the surgical staple cartridge 2334 to cut and staple tissue clamped within the surgical end effector 2312. The sled 2333 may be made of, for example, plastic, and may have a sloped distal surface. As the sled 2333 traverses the elongated channel 2322, the sloped forward surface of the sled 2333 pushes up or drive the staples in the surgical staple cartridge 2334 through the clamped tissue and against the anvil 2324. The anvil 2324 turns or forms the staples, thereby stapling the severed tissue. As used herein, the term fire refers to the initiation of actions required to drive the cutting instrument and sled portion in a distal direction through the surgical staple cartridge to cut the tissue clamped in the surgical end effector and drive the staples through the severed tissue. In use, it may be desirable to rotate the surgical end effector 2312 about the longitudinal tool axis LT-LT. In at least one embodiment, the transmission arrangement 2375 includes a rotational transmission assembly 2465 that is configured to receive a corresponding rotary output motion from the tool drive assembly 1010 of the robotic system 1000 and convert that rotary output motion to a rotary control motion for rotating the elongated shaft assembly 2308 and surgical end effector 2312 about the longitudinal tool axis LT-LT. As can be seen in 42, a proximal end 2341 of the outer tube 2340 is rotatably supported within a cradle arrangement 2343 attached to the tool mounting plate 2462 of the tool mounting portion 2460. A rotation gear 2345 is formed on or attached to the proximal end 2341 of the outer tube 2340 of the elongated shaft assembly 2308 for meshing engagement with a rotation gear assembly 2470 operably supported on the tool mounting plate 2462. In at least one embodiment, a rotation drive gear 2472 is coupled to a corresponding first one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 2462 when the tool mounting portion 2460 is coupled to the tool drive assembly 1010. See 26 and 42. The rotation drive assembly 2470 further comprises a rotary driven gear 2474 that is rotatably supported on the tool mounting plate 2462 in meshing engagement with the rotation gear 2345 and the rotation drive gear 2472. Application of a first rotary output motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 2472 by virtue of being operably coupled thereto. Rotation of the rotation drive gear 2472 ultimately results in the rotation of the elongated shaft assembly 2308 and the end effector 2312 about the longitudinal tool axis LT-LT primary rotary motion. Closure of the anvil 2324 relative to the staple cartridge 2034 is accomplished by axially moving the closure tube 2370 in the distal direction DD. Axial movement of the closure tube 2370 in the distal direction DD is accomplished by applying a rotary control motion to the closure drive nut 2382. To apply the rotary control motion to the closure drive nut 2382, the closure clutch 2410 must first be brought into meshing engagement with the proximal end portion 2384 of the closure drive nut 2382. In various embodiments, the transmission arrangement 2375 further includes a shifter drive assembly 2480 that is operably supported on the tool mounting plate 2462. More specifically and with reference to 42, it can be seen that a proximal end portion 2359 of the proximal spine portion 2353 extends through the rotation gear 2345 and is rotatably coupled to a shifter gear rack 2481 that is slidably affixed to the tool mounting plate 2462 through slots 2482. The shifter drive assembly 2480 further comprises a shifter drive gear 2483 that is coupled to a corresponding second one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 2462 when the tool mounting portion 2460 is coupled to the tool holder 1270. See 26 and 42. The shifter drive assembly 2480 further comprises a shifter driven gear 2478 that is rotatably supported on the tool mounting plate 2462 in meshing engagement with the shifter drive gear 2483 and the shifter rack gear 2482. Application of a second rotary output motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding driven element 1304 will thereby cause rotation of the shifter drive gear 2483 by virtue of being operably coupled thereto. Rotation of the shifter drive gear 2483 ultimately results in the axial movement of the shifter gear rack 2482 and the proximal spine portion 2353 as well as the drive sleeve 2400 and the closure clutch 2410 attached thereto. The direction of axial travel of the closure clutch 2410 depends upon the direction in which the shifter drive gear 2483 is rotated by the robotic system 1000. Thus, rotation of the shifter drive gear 2483 in a first rotary direction will result in the axial movement of the closure clutch 2410 in the proximal direction PD to bring the proximal teeth 2416 into meshing engagement with the proximal teeth cavities 2418 in the closure drive nut 2382. Conversely, rotation of the shifter drive gear 2483 in a second rotary direction opposite to the first rotary direction will result in the axial movement of the closure clutch 2410 in the distal direction DD to bring the distal teeth 2415 into meshing engagement with corresponding distal teeth cavities 2426 formed in the face plate portion 2424 of the knife drive shaft assembly 2420. Once the closure clutch 2410 has been brought into meshing engagement with the closure drive nut 2382, the closure drive nut 2382 is rotated by rotating the closure clutch 2410. Rotation of the closure clutch 2410 is controlled by applying rotary output motions to a rotary drive transmission portion 2490 of transmission arrangement 2375 that is operably supported on the tool mounting plate 2462 as shown in 42. In at least one embodiment, the rotary drive transmission 2490 includes a rotary drive assembly 2490 that includes a gear 2491 that is coupled to a corresponding third one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 2462 when the tool mounting portion 2460 is coupled to the tool holder 1270. See 26 and 42. The rotary drive transmission 2490 further comprises a first rotary driven gear 2492 that is rotatably supported on the tool mounting plate 2462 in meshing engagement with a second rotary driven gear 2493 and the rotary drive gear 2491. The second rotary driven gear 2493 is coupled to a proximal end portion 2443 of the drive shaft 2440. Rotation of the rotary drive gear 2491 in a first rotary direction will result in the rotation of the drive shaft 2440 in a first direction. Conversely, rotation of the rotary drive gear 2491 in a second rotary direction opposite to the first rotary direction will cause the drive shaft 2440 to rotate in a second direction. As indicated above, the drive shaft 2440 has a drive gear 2444 that is attached to its distal end 2442 and is in meshing engagement with a driven gear 2450 that is attached to the drive sleeve 2400. Thus, rotation of the drive shaft 2440 results in rotation of the drive sleeve 2400. A method of operating the surgical tool 2300 will now be described. Once the tool mounting portion 2462 has been operably coupled to the tool holder 1270 of the robotic system 1000 and oriented into position adjacent the target tissue to be cut and stapled, if the anvil 2334 is not already in the open position 39, the robotic system 1000 may apply the first rotary output motion to the shifter drive gear 2483 which results in the axial movement of the closure clutch 2410 into meshing engagement with the closure drive nut 2382 if it is not already in meshing engagement therewith. See 40. Once the controller 1001 of the robotic system 1000 has confirmed that the closure clutch 2410 is meshing engagement with the closure drive nut 2382 , by means of sensors in the surgical end effector 2312 that are in communication with the robotic control system, the robotic controller 1001 may then apply a second rotary output motion to the rotary drive gear 2492 which, as was described above, ultimately results in the rotation of the rotary drive nut 2382 in the first direction which results in the axial travel of the closure tube 2370 in the distal direction DD. As the closure tube 2370 moved in the distal direction, it contacts a portion of the anvil 2323 and causes the anvil 2324 to pivot to the closed position to clamp the target tissue between the anvil 2324 and the surgical staple cartridge 2334. Once the robotic controller 1001 determines that the anvil 2334 has been pivoted to the closed position by corresponding sensors in the surgical end effector 2312 in communication therewith, the robotic system 1000 discontinues the application of the second rotary output motion to the rotary drive gear 2491. The robotic controller 1001 may also provide the surgeon with an indication that the anvil 2334 has been fully closed. The surgeon may then initiate the firing procedure. In alternative embodiments, the firing procedure may be automatically initiated by the robotic controller 1001. The robotic controller 1001 then applies the primary rotary control motion 2483 to the shifter drive gear 2483 which results in the axial movement of the closure clutch 2410 into meshing engagement with the face plate portion 2424 of the knife drive shaft assembly 2420. See 41. Once the controller 1001 of the robotic system 1000 has confirmed that the closure clutch 2410 is meshing engagement with the face plate portion 2424 by means of sensors in the end effector 2312 that are in communication with the robotic controller 1001, the robotic controller 1001 may then apply the second rotary output motion to the rotary drive gear 2492 which, as was described above, ultimately results in the axial movement of the cutting instrument 2332 and sled portion 2333 in the distal direction DD through the surgical staple cartridge 2334. As the cutting instrument 2332 moves distally through the surgical staple cartridge 2334, the tissue clamped therein is severed. As the sled portion 2333 is driven distally, it causes the staples within the surgical staple cartridge to be driven through the severed tissue into forming contact with the anvil 2324. Once the robotic controller 1001 has determined that the cutting instrument 2324 has reached the end position within the surgical staple cartridge 2334 by means of sensors in the end effector 2312 that are in communication with the robotic controller 1001, the robotic controller 1001 discontinues the application of the second rotary output motion to the rotary drive gear 2491. Thereafter, the robotic controller 1001 applies the secondary rotary output motion to the rotary drive gear 2491 which ultimately results in the axial travel of the cutting instrument 2332 and sled portion 2333 in the proximal direction PD to the starting position. Once the robotic controller 1001 has determined that the cutting instrument 2324 has reached the starting position by means of sensors in the surgical end effector 2312 that are in communication with the robotic controller 1001, the robotic controller 1001 discontinues the application of the secondary rotary output motion to the rotary drive gear 2491. Thereafter, the robotic controller 1001 applies the primary rotary output motion to the shifter drive gear 2483 to cause the closure clutch 2410 to move into engagement with the rotary drive nut 2382. Once the closure clutch 2410 has been moved into meshing engagement with the rotary drive nut 2382, the robotic controller 1001 then applies the secondary output motion to the rotary drive gear 2491 which ultimately results in the rotation of the rotary drive nut 2382 in the second direction to cause the closure tube 2370 to move in the proximal direction PD. As can be seen in 39-41, the closure tube 2370 has an opening 2345 therein that engages the tab 2327 on the anvil 2324 to cause the anvil 2324 to pivot to the open position. In alternative embodiments, a spring may also be employed to pivot the anvil 2324 to the open position when the closure tube 2370 has been returned to the starting position 39. 43-47 illustrate yet another surgical tool 2500 that may be effectively employed in connection with the robotic system 1000. In various forms, the surgical tool 2500 includes a surgical end effector 2512 that includes a first portion in the form of an elongated channel 2522 and a second movable portion in the form of a pivotally translatable clamping member, such as an anvil 2524, which are maintained at a spacing that assures effective stapling and severing of tissue clamped in the surgical end effector 2512. As shown in the illustrated embodiment, the surgical end effector 2512 may include, in addition to the previously-mentioned elongated channel 2522 and anvil 2524, a third movable portion in the form of a cutting instrument 2532, a sled not shown, and a surgical staple cartridge 2534 that is removably seated in the elongated channel 2522. The cutting instrument 2532 may be, for example, a knife. The anvil 2524 may be pivotably opened and closed at a pivot point 2525 connected to the proximate end of the elongated channel 2522. The anvil 2524 may also include a tab 2527 at its proximate end that is configured to operably interface with a component of the mechanical closure system described further below to open and close the anvil 2524. When actuated, the knife 2532 and sled travel longitudinally along the elongated channel 2522, thereby cutting tissue clamped within the surgical end effector 2512. The movement of the sled along the elongated channel 2522 causes the staples of the surgical staple cartridge 2534 to be driven through the severed tissue and against the closed anvil 2524, which turns the staples to fasten the severed tissue. In one form, the elongated channel 2522 and the anvil 2524 may be made of an electrically conductive material such as metal so that they may serve as part of the antenna that communicates with sensors in the surgical end effector, as described above. The surgical staple cartridge 2534 could be made of a nonconductive material such as plastic and the sensor may be connected to or disposed in the surgical staple cartridge 2534, as described above. It should be noted that although the embodiments of the surgical tool 2500 described herein employ a surgical end effector 2512 that staples the severed tissue, in other embodiments different techniques for fastening or sealing the severed tissue may be used. For example, end effectors that use RF energy or adhesives to fasten the severed tissue may also be used. 5,709,680, entitled ELECTROSURGICAL HEMOSTATIC DEVICE, and 5,688,270, entitled ELECTROSURGICAL HEMOSTATIC DEVICE WITH RECESSED AND/OR OFFSET ELECTRODES, which are incorporated herein by reference, discloses cutting instruments that use RF energy to fasten the severed tissue. patent application Ser. 11/267,811, now 7,673,783, and patent application Ser. 11/267,383, now 7,607,557, which are also incorporated herein by reference, disclose cutting instruments that use adhesives to fasten the severed tissue. Accordingly, although the description herein refers to cutting/stapling operations and the like, it should be recognized that this is an exemplary embodiment and is not meant to be limiting. Other tissue-fastening techniques may also be used. In the illustrated embodiment, the elongated channel 2522 of the surgical end effector 2512 is coupled to an elongated shaft assembly 2508 that is coupled to a tool mounting portion 2600. In at least one embodiment, the elongated shaft assembly 2508 comprises a hollow spine tube 2540 that is non-movably coupled to a tool mounting plate 2602 of the tool mounting portion 2600. As can be seen in 44 and 45, the proximal end 2523 of the elongated channel 2522 comprises a hollow tubular structure configured to be attached to the distal end 2541 of the spine tube 2540. In one embodiment, for example, the proximal end 2523 of the elongated channel 2522 is welded or glued to the distal end of the spine tube 2540. As can be further seen in 44 and 45, in at least one non-limiting embodiment, the surgical tool 2500 further includes an axially movable actuation member in the form of a closure tube 2550 that is constrained to move axially relative to the elongated channel 2522 and the spine tube 1540. The closure tube 2550 has a proximal end 2552 that has an internal thread 2554 formed therein that is in threaded engagement with a rotatably movable portion in the form of a closure drive nut 2560. More specifically, the closure drive nut 2560 has a proximal end portion 2562 that is rotatably supported relative to the elongated channel 2522 and the spine tube 2540. For assembly purposes, the proximal end portion 2562 is threadably attached to a retention ring 2570. The retention ring 2570 is received in a groove 2529 formed between a shoulder 2527 on the proximal end 2523 of the elongated channel 2522 and the distal end 2541 of the spine tube 1540. Such arrangement serves to rotatably support the closure drive nut 2560 within the elongated channel 2522. Rotation of the closure drive nut 2560 will cause the closure tube 2550 to move axially as represented by arrow D in 44. Extending through the spine tube 2540 and the closure drive nut 2560 is a drive member which, in at least one embodiment, comprises a knife bar 2580 that has a distal end portion 2582 that is rotatably coupled to the cutting instrument 2532 such that the knife bar 2580 may rotate relative to the cutting instrument 2582. As can be seen in 44-46, the closure drive nut 2560 has a slot 2564 therein through which the knife bar 2580 can slidably extend. Such arrangement permits the knife bar 2580 to move axially relative to the closure drive nut 2560. However, rotation of the knife bar 2580 about the longitudinal tool axis LT-LT will also result in the rotation of the closure drive nut 2560. The axial direction in which the closure tube 2550 moves ultimately depends upon the direction in which the knife bar 2580 and the closure drive nut 2560 are rotated. As the closure tube 2550 is driven distally, the distal end thereof will contact the anvil 2524 and cause the anvil 2524 to pivot to a closed position. Upon application of an opening rotary output motion from the robotic system 1000, the closure tube 2550 will be driven in the proximal direction PD and pivot the anvil 2524 to the open position by virtue of the engagement of the tab 2527 with the opening 2555 in the closure tube 2550. In use, it may be desirable to rotate the surgical end effector 2512 about the longitudinal tool axis LT-LT. In at least one embodiment, the tool mounting portion 2600 is configured to receive a corresponding first rotary output motion from the robotic system 1000 and convert that first rotary output motion to a rotary control motion for rotating the elongated shaft assembly 2508 about the longitudinal tool axis LT-LT. As can be seen in 42, a proximal end 2542 of the hollow spine tube 2540 is rotatably supported within a cradle arrangement 2603 attached to a tool mounting plate 2602 of the tool mounting portion 2600. Various embodiments of the surgical tool 2500 further include a transmission arrangement, generally depicted as 2605, that is operably supported on the tool mounting plate 2602. In various forms the transmission arrangement 2605 include a rotation gear 2544 that is formed on or attached to the proximal end 2542 of the spine tube 2540 for meshing engagement with a rotation drive assembly 2610 that is operably supported on the tool mounting plate 2602. In at least one embodiment, a rotation drive gear 2612 is coupled to a corresponding first one of the rotational bodies, driven discs or elements 1304 on the adapter side of the tool mounting plate 2602 when the tool mounting portion 2600 is coupled to the tool holder 1270. See 26 and 47. The rotation drive assembly 2610 further comprises a rotary driven gear 2614 that is rotatably supported on the tool mounting plate 2602 in meshing engagement with the rotation gear 2544 and the rotation drive gear 2612. Application of a first rotary output motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding driven rotational body 1304 will thereby cause rotation of the rotation drive gear 2612 by virtue of being operably coupled thereto. Rotation of the rotation drive gear 2612 ultimately results in the rotation of the elongated shaft assembly 2508 and the end effector 2512 about the longitudinal tool axis LT-LT. Closure of the anvil 2524 relative to the surgical staple cartridge 2534 is accomplished by axially moving the closure tube 2550 in the distal direction DD. Axial movement of the closure tube 2550 in the distal direction DD is accomplished by applying a rotary control motion to the closure drive nut 2382. In various embodiments, the closure drive nut 2560 is rotated by applying a rotary output motion to the knife bar 2580. Rotation of the knife bar 2580 is controlled by applying rotary output motions to a rotary closure system 2620 that is operably supported on the tool mounting plate 2602 as shown in 47. In at least one embodiment, the rotary closure system 2620 includes a closure drive gear 2622 that is coupled to a corresponding second one of the driven rotatable body portions discs or elements 1304 on the adapter side of the tool mounting plate 2462 when the tool mounting portion 2600 is coupled to the tool holder 1270. See 26 and 47. The closure drive gear 2622, in at least one embodiment, is in meshing driving engagement with a closure gear train, generally depicted as 2623. The closure gear drive rain 2623 comprises a first driven closure gear 2624 that is rotatably supported on the tool mounting plate 2602. The first closure driven gear 2624 is attached to a second closure driven gear 2626 by a drive shaft 2628. The second closure driven gear 2626 is in meshing engagement with a third closure driven gear 2630 that is rotatably supported on the tool mounting plate 2602. Rotation of the closure drive gear 2622 in a second rotary direction will result in the rotation of the third closure driven gear 2630 in a second direction. Conversely, rotation of the closure drive gear 2483 in a secondary rotary direction opposite to the second rotary direction will cause the third closure driven gear 2630 to rotate in a secondary direction. As can be seen in 47, a drive shaft assembly 2640 is coupled to a proximal end of the knife bar 2580. In various embodiments, the drive shaft assembly 2640 includes a proximal portion 2642 that has a square cross-sectional shape. The proximal portion 2642 is configured to slideably engage a correspondingly shaped aperture in the third driven gear 2630. Such arrangement results in the rotation of the drive shaft assembly 2640 and knife bar 2580 when the third driven gear 2630 is rotated. The drive shaft assembly 2640 is axially advanced in the distal and proximal directions by a knife drive assembly 2650. One form of the knife drive assembly 2650 comprises a rotary drive gear 2652 that is coupled to a corresponding third one of the driven rotatable body portions, discs or elements 1304 on the adapter side of the tool mounting plate 2462 when the tool mounting portion 2600 is coupled to the tool holder 1270. See 26 and 47. The rotary driven gear 2652 is in meshing driving engagement with a gear train, generally depicted as 2653. In at least one form, the gear train 2653 further comprises a first rotary driven gear assembly 2654 that is rotatably supported on the tool mounting plate 2602. The first rotary driven gear assembly 2654 is in meshing engagement with a third rotary driven gear assembly 2656 that is rotatably supported on the tool mounting plate 2602 and which is in meshing engagement with a fourth rotary driven gear assembly 2658 that is in meshing engagement with a threaded portion 2644 of the drive shaft assembly 2640. Rotation of the rotary drive gear 2652 in a third rotary direction will result in the axial advancement of the drive shaft assembly 2640 and knife bar 2580 in the distal direction DD. Conversely, rotation of the rotary drive gear 2652 in a tertiary rotary direction opposite to the third rotary direction will cause the drive shaft assembly 2640 and the knife bar 2580 to move in the proximal direction. A method of operating the surgical tool 2500 will now be described. Once the tool mounting portion 2600 has been operably coupled to the tool holder 1270 of the robotic system 1000, the robotic system 1000 can orient the surgical end effector 2512 in position adjacent the target tissue to be cut and stapled. If the anvil 2524 is not already in the open position 44, the robotic system 1000 may apply the second rotary output motion to the closure drive gear 2622 which results in the rotation of the knife bar 2580 in a second direction. Rotation of the knife bar 2580 in the second direction results in the rotation of the closure drive nut 2560 in a second direction. As the closure drive nut 2560 rotates in the second direction, the closure tube 2550 moves in the proximal direction PD. As the closure tube 2550 moves in the proximal direction PD, the tab 2527 on the anvil 2524 interfaces with the opening 2555 in the closure tube 2550 and causes the anvil 2524 to pivot to the open position. In addition or in alternative embodiments, a spring not shown may be employed to pivot the anvil 2354 to the open position when the closure tube 2550 has been returned to the starting position 44. The opened surgical end effector 2512 may then be manipulated by the robotic system 1000 to position the target tissue between the open anvil 2524 and the surgical staple cartridge 2534. Thereafter, the surgeon may initiate the closure process by activating the robotic control system 1000 to apply the second rotary output motion to the closure drive gear 2622 which, as was described above, ultimately results in the rotation of the closure drive nut 2382 in the second direction which results in the axial travel of the closure tube 2250 in the distal direction DD. As the closure tube 2550 moves in the distal direction, it contacts a portion of the anvil 2524 and causes the anvil 2524 to pivot to the closed position to clamp the target tissue between the anvil 2524 and the staple cartridge 2534. Once the robotic controller 1001 determines that the anvil 2524 has been pivoted to the closed position by corresponding sensors in the end effector 2512 that are in communication therewith, the robotic controller 1001 discontinues the application of the second rotary output motion to the closure drive gear 2622. The robotic controller 1001 may also provide the surgeon with an indication that the anvil 2524 has been fully closed. The surgeon may then initiate the firing procedure. In alternative embodiments, the firing procedure may be automatically initiated by the robotic controller 1001. After the robotic controller 1001 has determined that the anvil 2524 is in the closed position, the robotic controller 1001 then applies the third rotary output motion to the rotary drive gear 2652 which results in the axial movement of the drive shaft assembly 2640 and knife bar 2580 in the distal direction DD. As the cutting instrument 2532 moves distally through the surgical staple cartridge 2534, the tissue clamped therein is severed. As the sled portion not shown is driven distally, it causes the staples within the surgical staple cartridge 2534 to be driven through the severed tissue into forming contact with the anvil 2524. Once the robotic controller 1001 has determined that the cutting instrument 2532 has reached the end position within the surgical staple cartridge 2534 by means of sensors in the surgical end effector 2512 that are in communication with the robotic controller 1001, the robotic controller 1001 discontinues the application of the second rotary output motion to the rotary drive gear 2652. Thereafter, the robotic controller 1001 applies the secondary rotary control motion to the rotary drive gear 2652 which ultimately results in the axial travel of the cutting instrument 2532 and sled portion in the proximal direction PD to the starting position. Once the robotic controller 1001 has determined that the cutting instrument 2524 has reached the starting position by means of sensors in the end effector 2512 that are in communication with the robotic controller 1001, the robotic controller 1001 discontinues the application of the secondary rotary output motion to the rotary drive gear 2652. Thereafter, the robotic controller 1001 may apply the secondary rotary output motion to the closure drive gear 2622 which results in the rotation of the knife bar 2580 in a secondary direction. Rotation of the knife bar 2580 in the secondary direction results in the rotation of the closure drive nut 2560 in a secondary direction. As the closure drive nut 2560 rotates in the secondary direction, the closure tube 2550 moves in the proximal direction PD to the open position. 48-53B illustrate yet another surgical tool 2700 that may be effectively employed in connection with the robotic system 1000. In various forms, the surgical tool 2700 includes a surgical end effector 2712 that includes a first portion in the form of an elongated channel 2722 and a second movable portion in on form comprising a pivotally translatable clamping member, such as an anvil 2724, which are maintained at a spacing that assures effective stapling and severing of tissue clamped in the surgical end effector 2712. As shown in the illustrated embodiment, the surgical end effector 2712 may include, in addition to the previously-mentioned channel 2722 and anvil 2724, a third movable portion in the form of a cutting instrument 2732, a sled not shown, and a surgical staple cartridge 2734 that is removably seated in the elongated channel 2722. The cutting instrument 2732 may be, for example, a knife. The anvil 2724 may be pivotably opened and closed at a pivot point 2725 connected to the proximal end of the elongated channel 2722. The anvil 2724 may also include a tab 2727 at its proximal end that interfaces with a component of the mechanical closure system described further below to open and close the anvil 2724. When actuated, the knife 2732 and sled to travel longitudinally along the elongated channel 2722, thereby cutting tissue clamped within the surgical end effector 2712. The movement of the sled along the elongated channel 2722 causes the staples of the surgical staple cartridge 2734 to be driven through the severed tissue and against the closed anvil 2724, which turns the staples to fasten the severed tissue. In one form, the elongated channel 2722 and the anvil 2724 may be made of an electrically conductive material such as metal so that they may serve as part of the antenna that communicates with sensors in the surgical end effector, as described above. The surgical staple cartridge 2734 could be made of a nonconductive material such as plastic and the sensor may be connected to or disposed in the surgical staple cartridge 2734, as described above. It should be noted that although the embodiments of the surgical tool 2500 described herein employ a surgical end effector 2712 that staples the severed tissue, in other embodiments different techniques for fastening or sealing the severed tissue may be used. For example, end effectors that use RF energy or adhesives to fasten the severed tissue may also be used. 5,709,680, entitled ELECTROSURGICAL HEMOSTATIC DEVICE, and 5,688,270, entitled ELECTROSURGICAL HEMOSTATIC DEVICE WITH RECESSED AND/OR OFFSET ELECTRODES, which are incorporated herein by reference, discloses cutting instruments that use RF energy to fasten the severed tissue. patent application Ser. 11/267,811, now 7,673,783, and patent application Ser. 11/267,383, now 7,607,557, which are also incorporated herein by reference, disclose cutting instruments that use adhesives to fasten the severed tissue. Accordingly, although the description herein refers to cutting/stapling operations and the like, it should be recognized that this is an exemplary embodiment and is not meant to be limiting. Other tissue-fastening techniques may also be used. In the illustrated embodiment, the elongated channel 2722 of the surgical end effector 2712 is coupled to an elongated shaft assembly 2708 that is coupled to a tool mounting portion 2900. Although not shown, the elongated shaft assembly 2708 may include an articulation joint to permit the surgical end effector 2712 to be selectively articulated about an axis that is substantially transverse to the tool axis LT-LT. In at least one embodiment, the elongated shaft assembly 2708 comprises a hollow spine tube 2740 that is non-movably coupled to a tool mounting plate 2902 of the tool mounting portion 2900. As can be seen in 49 and 50, the proximal end 2723 of the elongated channel 2722 comprises a hollow tubular structure that is attached to the spine tube 2740 by means of a mounting collar 2790. A cross-sectional view of the mounting collar 2790 is shown in 51. In various embodiments, the mounting collar 2790 has a proximal flanged end 2791 that is configured for attachment to the distal end of the spine tube 2740. In at least one embodiment, for example, the proximal flanged end 2791 of the mounting collar 2790 is welded or glued to the distal end of the spine tube 2740. As can be further seen in 54 and 55, the mounting collar 2790 further has a mounting hub portion 2792 that is sized to receive the proximal end 2723 of the elongated channel 2722 thereon. The proximal end 2723 of the elongated channel 2722 is non-movably attached to the mounting hub portion 2792 by, for example, welding, adhesive, etc. As can be further seen in 49 and 50, the surgical tool 2700 further includes an axially movable actuation member in the form of a closure tube 2750 that is constrained to move axially relative to the elongated channel 2722. The closure tube 2750 has a proximal end 2752 that has an internal thread 2754 formed therein that is in threaded engagement with a rotatably movable portion in the form of a closure drive nut 2760. More specifically, the closure drive nut 2760 has a proximal end portion 2762 that is rotatably supported relative to the elongated channel 2722 and the spine tube 2740. For assembly purposes, the proximal end portion 2762 is threadably attached to a retention ring 2770. The retention ring 2770 is received in a groove 2729 formed between a shoulder 2727 on the proximal end 2723 of the channel 2722 and the mounting hub 2729 of the mounting collar 2790. Such arrangement serves to rotatably support the closure drive nut 2760 within the channel 2722. Rotation of the closure drive nut 2760 will cause the closure tube 2750 to move axially as represented by arrow D in 49. Extending through the spine tube 2740, the mounting collar 2790, and the closure drive nut 2760 is a drive member, which in at least one embodiment, comprises a knife bar 2780 that has a distal end portion 2782 that is coupled to the cutting instrument 2732. As can be seen in 49 and 50, the mounting collar 2790 has a passage 2793 therethrough for permitting the knife bar 2780 to slidably pass therethrough. Similarly, the closure drive nut 2760 has a slot 2764 therein through which the knife bar 2780 can slidably extend. Such arrangement permits the knife bar 2780 to move axially relative to the closure drive nut 2760. Actuation of the anvil 2724 is controlled by a rotary driven closure shaft 2800. As can be seen in 49 and 50, a distal end portion 2802 of the closure drive shaft 2800 extends through a passage 2794 in the mounting collar 2790 and a closure gear 2804 is attached thereto. The closure gear 2804 is configured for driving engagement with the inner surface 2761 of the closure drive nut 2760. Thus, rotation of the closure shaft 2800 will also result in the rotation of the closure drive nut 2760. The axial direction in which the closure tube 2750 moves ultimately depends upon the direction in which the closure shaft 2800 and the closure drive nut 2760 are rotated. For example, in response to one rotary closure motion received from the robotic system 1000, the closure tube 2750 will be driven in the distal direction DD. As the closure tube 2750 is driven distally, the opening 2745 will engage the tab 2727 on the anvil 2724 and cause the anvil 2724 to pivot to a closed position. Upon application of an opening rotary motion from the robotic system 1000, the closure tube 2750 will be driven in the proximal direction PD and pivot the anvil 2724 to the open position. In various embodiments, a spring not shown may be employed to bias the anvil 2724 to the open position 49. In use, it may be desirable to rotate the surgical end effector 2712 about the longitudinal tool axis LT-LT. In at least one embodiment, the tool mounting portion 2900 is configured to receive a corresponding first rotary output motion from the robotic system 1000 for rotating the elongated shaft assembly 2708 about the tool axis LT-LT. As can be seen in 53, a proximal end 2742 of the hollow spine tube 2740 is rotatably supported within a cradle arrangement 2903 and a bearing assembly 2904 that are attached to a tool mounting plate 2902 of the tool mounting portion 2900. A rotation gear 2744 is formed on or attached to the proximal end 2742 of the spine tube 2740 for meshing engagement with a rotation drive assembly 2910 that is operably supported on the tool mounting plate 2902. In at least one embodiment, a rotation drive gear 2912 is coupled to a corresponding first one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 2602 when the tool mounting portion 2600 is coupled to the tool holder 1270. See 26 and 53. The rotation drive assembly 2910 further comprises a rotary driven gear 2914 that is rotatably supported on the tool mounting plate 2902 in meshing engagement with the rotation gear 2744 and the rotation drive gear 2912. Application of a first rotary control motion from the robotic system 1000 through the tool holder 1270 and the adapter 1240 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 2912 by virtue of being operably coupled thereto. Rotation of the rotation drive gear 2912 ultimately results in the rotation of the elongated shaft assembly 2708 and the end effector 2712 about the longitudinal tool axis LT-LT primary rotary motion. Closure of the anvil 2724 relative to the staple cartridge 2734 is accomplished by axially moving the closure tube 2750 in the distal direction DD. Axial movement of the closure tube 2750 in the distal direction DD is accomplished by applying a rotary control motion to the closure drive nut 2760. In various embodiments, the closure drive nut 2760 is rotated by applying a rotary output motion to the closure drive shaft 2800. As can be seen in 53, a proximal end portion 2806 of the closure drive shaft 2800 has a driven gear 2808 thereon that is in meshing engagement with a closure drive assembly 2920. In various embodiments, the closure drive system 2920 includes a closure drive gear 2922 that is coupled to a corresponding second one of the driven rotational bodies or elements 1304 on the adapter side of the tool mounting plate 2462 when the tool mounting portion 2900 is coupled to the tool holder 1270. See 26 and 53. The closure drive gear 2922 is supported in meshing engagement with a closure gear train, generally depicted as 2923. In at least one form, the closure gear rain 2923 comprises a first driven closure gear 2924 that is rotatably supported on the tool mounting plate 2902. The first closure driven gear 2924 is attached to a second closure driven gear 2926 by a drive shaft 2928. The second closure driven gear 2926 is in meshing engagement with a planetary gear assembly 2930. In various embodiments, the planetary gear assembly 2930 includes a driven planetary closure gear 2932 that is rotatably supported within the bearing assembly 2904 that is mounted on tool mounting plate 2902. As can be seen in 53 and 53B, the proximal end portion 2806 of the closure drive shaft 2800 is rotatably supported within the proximal end portion 2742 of the spine tube 2740 such that the driven gear 2808 is in meshing engagement with central gear teeth 2934 formed on the planetary gear 2932. As can also be seen in 53A, two additional support gears 2936 are attached to or rotatably supported relative to the proximal end portion 2742 of the spine tube 2740 to provide bearing support thereto. Such arrangement with the planetary gear assembly 2930 serves to accommodate rotation of the spine shaft 2740 by the rotation drive assembly 2910 while permitting the closure driven gear 2808 to remain in meshing engagement with the closure drive system 2920. In addition, rotation of the closure drive gear 2922 in a first direction will ultimately result in the rotation of the closure drive shaft 2800 and closure drive nut 2760 which will ultimately result in the closure of the anvil 2724 as described above. Conversely, rotation of the closure drive gear 2922 in a second opposite direction will ultimately result in the rotation of the closure drive nut 2760 in an opposite direction which results in the opening of the anvil 2724. As can be seen in 53, the proximal end 2784 of the knife bar 2780 has a threaded shaft portion 2786 attached thereto which is in driving engagement with a knife drive assembly 2940. In various embodiments, the threaded shaft portion 2786 is rotatably supported by a bearing 2906 attached to the tool mounting plate 2902. Such arrangement permits the threaded shaft portion 2786 to rotate and move axially relative to the tool mounting plate 2902. The knife bar 2780 is axially advanced in the distal and proximal directions by the knife drive assembly 2940. One form of the knife drive assembly 2940 comprises a rotary drive gear 2942 that is coupled to a corresponding third one of the rotatable bodies, driven discs or elements 1304 on the adapter side of the tool mounting plate 2902 when the tool mounting portion 2900 is coupled to the tool holder 1270. See 26 and 53. The rotary drive gear 2942 is in meshing engagement with a knife gear train, generally depicted as 2943. In various embodiments, the knife gear train 2943 comprises a first rotary driven gear assembly 2944 that is rotatably supported on the tool mounting plate 2902. The first rotary driven gear assembly 2944 is in meshing engagement with a third rotary driven gear assembly 2946 that is rotatably supported on the tool mounting plate 2902 and which is in meshing engagement with a fourth rotary driven gear assembly 2948 that is in meshing engagement with the threaded portion 2786 of the knife bar 2780. Rotation of the rotary drive gear 2942 in one direction will result in the axial advancement of the knife bar 2780 in the distal direction DD. Conversely, rotation of the rotary drive gear 2942 in an opposite direction will cause the knife bar 2780 to move in the proximal direction. Tool 2700 may otherwise be used as described above. 54 and 55 illustrate a surgical tool embodiment 2700 that is substantially identical to tool 2700 that was described in detail above. However tool 2700 includes a pressure sensor 2950 that is configured to provide feedback to the robotic controller 1001 concerning the amount of clamping pressure experienced by the anvil 2724. In various embodiments, for example, the pressure sensor may comprise a spring biased contact switch. For a continuous signal, it would use either a cantilever beam with a strain gage on it or a dome button top with a strain gage on the inside. Another version may comprise an off switch that contacts only at a known desired load. Such arrangement would include a dome on the based wherein the dome is one electrical pole and the base is the other electrical pole. Such arrangement permits the robotic controller 1001 to adjust the amount of clamping pressure being applied to the tissue within the surgical end effector 2712 by adjusting the amount of closing pressure applied to the anvil 2724. Those of ordinary skill in the art will understand that such pressure sensor arrangement may be effectively employed with several of the surgical tool embodiments described herein as well as their equivalent structures. 56 illustrates a portion of another surgical tool 3000 that may be effectively used in connection with a robotic system 1000. The surgical tool 3003 employs on-board motors for powering various components of a surgical end effector cutting instrument. In at least one non-limiting embodiment for example, the surgical tool 3000 includes a surgical end effector in the form of an endocutter not shown that has an anvil not shown and surgical staple cartridge arrangement not shown of the types and constructions described above. The surgical tool 3000 also includes an elongated shaft not shown and anvil closure arrangement not shown of the types described above. Thus, this portion of the Detailed Description will not repeat the description of those components beyond that which is necessary to appreciate the unique and novel attributes of the various embodiments of surgical tool 3000. In the depicted embodiment, the end effector includes a cutting instrument 3002 that is coupled to a knife bar 3003. As can be seen in 56, the surgical tool 3000 includes a tool mounting portion 3010 that includes a tool mounting plate 3012 that is configured to mountingly interface with the adaptor portion 1240 which is coupled to the robotic system 1000 in the various manners described above. The tool mounting portion 3010 is configured to operably support a transmission arrangement 3013 thereon. In at least one embodiment, the adaptor portion 1240 may be identical to the adaptor portion 1240 described in detail above without the powered rotation bodies and disc members employed by adapter 1240. In other embodiments, the adaptor portion 1240 may be identical to adaptor portion 1240. Still other modifications which are considered to be within the spirit and scope of the various forms of the present invention may employ one or more of the mechanical motions i. e. , rotary motions from the tool holder portion 1270 as described hereinabove to power/actuate the transmission arrangement 3013 while also employing one or more motors within the tool mounting portion 3010 to power one or more other components of the surgical end effector. In addition, while the end effector of the depicted embodiment comprises an endocutter, those of ordinary skill in the art will understand that the unique and novel attributes of the depicted embodiment may be effectively employed in connection with other types of surgical end effectors without departing from the spirit and scope of various forms of the present invention. In various embodiments, the tool mounting plate 3012 is configured to at least house a first firing motor 3011 for supplying firing and retraction motions to the knife bar 3003 which is coupled to or otherwise operably interfaces with the cutting instrument 3002. The tool mounting plate 3012 has an array of electrical connecting pins 3014 which are configured to interface with the slots 1258 25 in the adapter 1240. Such arrangement permits the controller 1001 of the robotic system 1000 to provide control signals to the electronic control circuit 3020 of the surgical tool 3000. While the interface is described herein with reference to mechanical, electrical, and magnetic coupling elements, it should be understood that a wide variety of telemetry modalities might be used, including infrared, inductive coupling, or the like. Control circuit 3020 is shown in schematic form in 56. In one form or embodiment, the control circuit 3020 includes a power supply in the form of a battery 3022 that is coupled to an on-off solenoid powered switch 3024. Control circuit 3020 further includes an on/off firing solenoid 3026 that is coupled to a double pole switch 3028 for controlling the rotational direction of the motor 3011. Thus, when the controller 1001 of the robotic system 1000 supplies an appropriate control signal, switch 3024 will permit battery 3022 to supply power to the double pole switch 3028. The controller 1001 of the robotic system 1000 will also supply an appropriate signal to the double pole switch 3028 to supply power to the motor 3011. When it is desired to fire the surgical end effector i. e. , drive the cutting instrument 3002 distally through tissue clamped in the surgical end effector, the double pole switch 3028 will be in a first position. When it is desired to retract the cutting instrument 3002 to the starting position, the double pole switch 3028 will be moved to the second position by the controller 1001. Various embodiments of the surgical tool 3000 also employ a gear box 3030 that is sized, in cooperation with a firing gear train 3031 that, in at least one non-limiting embodiment, comprises a firing drive gear 3032 that is in meshing engagement with a firing driven gear 3034 for generating a desired amount of driving force necessary to drive the cutting instrument 3002 through tissue and to drive and form staples in the various manners described herein. In the embodiment depicted in 56, the driven gear 3034 is coupled to a screw shaft 3036 that is in threaded engagement with a screw nut arrangement 3038 that is constrained to move axially represented by arrow D. The screw nut arrangement 3038 is attached to the firing bar 3003. Thus, by rotating the screw shaft 3036 in a first direction, the cutting instrument 3002 is driven in the distal direction DD and rotating the screw shaft in an opposite second direction, the cutting instrument 3002 may be retracted in the proximal direction PD. 57 illustrates a portion of another surgical tool 3000 that is substantially identical to tool 3000 described above, except that the driven gear 3034 is attached to a drive shaft 3040. The drive shaft 3040 is attached to a second driver gear 3042 that is in meshing engagement with a third driven gear 3044 that is in meshing engagement with a screw 3046 coupled to the firing bar 3003. 58 illustrates another surgical tool 3200 that may be effectively used in connection with a robotic system 1000. In this embodiment, the surgical tool 3200 includes a surgical end effector 3212 that in one non-limiting form, comprises a component portion that is selectively movable between first and second positions relative to at least one other end effector component portion. As will be discussed in further detail below, the surgical tool 3200 employs on-board motors for powering various components of a transmission arrangement 3305. The surgical end effector 3212 includes an elongated channel 3222 that operably supports a surgical staple cartridge 3234. The elongated channel 3222 has a proximal end 3223 that slidably extends into a hollow elongated shaft assembly 3208 that is coupled to a tool mounting portion 3300. In addition, the surgical end effector 3212 includes an anvil 3224 that is pivotally coupled to the elongated channel 3222 by a pair of trunnions 3225 that are received within corresponding openings 3229 in the elongated channel 3222. A distal end portion 3209 of the shaft assembly 3208 includes an opening 3245 into which a tab 3227 on the anvil 3224 is inserted in order to open the anvil 3224 as the elongated channel 3222 is moved axially in the proximal direction PD relative to the distal end portion 3209 of the shaft assembly 3208. In various embodiments, a spring not shown may be employed to bias the anvil 3224 to the open position. As indicated above, the surgical tool 3200 includes a tool mounting portion 3300 that includes a tool mounting plate 3302 that is configured to operably support the transmission arrangement 3305 and to mountingly interface with the adaptor portion 1240 which is coupled to the robotic system 1000 in the various manners described above. In at least one embodiment, the adaptor portion 1240 may be identical to the adaptor portion 1240 described in detail above without the powered disc members employed by adapter 1240. In other embodiments, the adaptor portion 1240 may be identical to adaptor portion 1240. However, in such embodiments, because the various components of the surgical end effector 3212 are all powered by motors in the tool mounting portion 3300, the surgical tool 3200 will not employ or require any of the mechanical i. e. , non-electrical actuation motions from the tool holder portion 1270 to power the surgical end effector 3200 components. Still other modifications which are considered to be within the spirit and scope of the various forms of the present invention may employ one or more of the mechanical motions from the tool holder portion 1270 as described hereinabove to power/actuate one or more of the surgical end effector components while also employing one or more motors within the tool mounting portion to power one or more other components of the surgical end effector. In various embodiments, the tool mounting plate 3302 is configured to support a first firing motor 3310 for supplying firing and retraction motions to the transmission arrangement 3305 to drive a knife bar 3335 that is coupled to a cutting instrument 3332 of the type described above. As can be seen in 58, the tool mounting plate 3212 has an array of electrical connecting pins 3014 which are configured to interface with the slots 1258 25 in the adapter 1240. Such arrangement permits the controller 1001 of the robotic system 1000 to provide control signals to the electronic control circuits 3320, 3340 of the surgical tool 3200. While the interface is described herein with reference to mechanical, electrical, and magnetic coupling elements, it should be understood that a wide variety of telemetry modalities might be used, including infrared, inductive coupling, or the like. In one form or embodiment, the first control circuit 3320 includes a first power supply in the form of a first battery 3322 that is coupled to a first on-off solenoid powered switch 3324. The first firing control circuit 3320 further includes a first on/off firing solenoid 3326 that is coupled to a first double pole switch 3328 for controlling the rotational direction of the first firing motor 3310. Thus, when the robotic controller 1001 supplies an appropriate control signal, the first switch 3324 will permit the first battery 3322 to supply power to the first double pole switch 3328. The robotic controller 1001 will also supply an appropriate signal to the first double pole switch 3328 to supply power to the first firing motor 3310. When it is desired to fire the surgical end effector i. e. , drive the cutting instrument 3232 distally through tissue clamped in the surgical end effector 3212, the first switch 3328 will be positioned in a first position by the robotic controller 1001. When it is desired to retract the cutting instrument 3232 to the starting position, the robotic controller 1001 will send the appropriate control signal to move the first switch 3328 to the second position. Various embodiments of the surgical tool 3200 also employ a first gear box 3330 that is sized, in cooperation with a firing drive gear 3332 coupled thereto that operably interfaces with a firing gear train 3333. In at least one non-limiting embodiment, the firing gear train 333 comprises a firing driven gear 3334 that is in meshing engagement with drive gear 3332, for generating a desired amount of driving force necessary to drive the cutting instrument 3232 through tissue and to drive and form staples in the various manners described herein. In the embodiment depicted in 58, the driven gear 3334 is coupled to a drive shaft 3335 that has a second driven gear 3336 coupled thereto. The second driven gear 3336 is supported in meshing engagement with a third driven gear 3337 that is in meshing engagement with a fourth driven gear 3338. The fourth driven gear 3338 is in meshing engagement with a threaded proximal portion 3339 of the knife bar 3235 that is constrained to move axially. Thus, by rotating the drive shaft 3335 in a first direction, the cutting instrument 3232 is driven in the distal direction DD and rotating the drive shaft 3335 in an opposite second direction, the cutting instrument 3232 may be retracted in the proximal direction PD. As indicated above, the opening and closing of the anvil 3224 is controlled by axially moving the elongated channel 3222 relative to the elongated shaft assembly 3208. The axial movement of the elongated channel 3222 is controlled by a closure control system 3339. In various embodiments, the closure control system 3339 includes a closure shaft 3340 which has a hollow threaded end portion 3341 that threadably engages a threaded closure rod 3342. The threaded end portion 3341 is rotatably supported in a spine shaft 3343 that operably interfaces with the tool mounting portion 3300 and extends through a portion of the shaft assembly 3208 as shown. The closure system 3339 further comprises a closure control circuit 3350 that includes a second power supply in the form of a second battery 3352 that is coupled to a second on-off solenoid powered switch 3354. Closure control circuit 3350 further includes a second on/off firing solenoid 3356 that is coupled to a second double pole switch 3358 for controlling the rotation of a second closure motor 3360. Thus, when the robotic controller 1001 supplies an appropriate control signal, the second switch 3354 will permit the second battery 3352 to supply power to the second double pole switch 3354. The robotic controller 1001 will also supply an appropriate signal to the second double pole switch 3358 to supply power to the second motor 3360. When it is desired to close the anvil 3224, the second switch 3348 will be in a first position. When it is desired to open the anvil 3224, the second switch 3348 will be moved to a second position. Various embodiments of tool mounting portion 3300 also employ a second gear box 3362 that is coupled to a closure drive gear 3364. The closure drive gear 3364 is in meshing engagement with a closure gear train 3363. In various non-limiting forms, the closure gear train 3363 includes a closure driven gear 3365 that is attached to a closure drive shaft 3366. Also attached to the closure drive shaft 3366 is a closure drive gear 3367 that is in meshing engagement with a closure shaft gear 3360 attached to the closure shaft 3340. 63 depicts the end effector 3212 in the open position. As indicated above, when the threaded closure rod 3342 is in the position depicted in 58, a spring not shown biases the anvil 3224 to the open position. When it is desired to close the anvil 3224, the robotic controller 1001 will activate the second motor 3360 to rotate the closure shaft 3340 to draw the threaded closure rod 3342 and the channel 3222 in the proximal direction PD. As the anvil 3224 contacts the distal end portion 3209 of the shaft 3208, the anvil 3224 is pivoted to the closed position. A method of operating the surgical tool 3200 will now be described. Once the tool mounting portion 3302 has be operably coupled to the tool holder 1270 of the robotic system 1000, the robotic system 1000 can orient the end effector 3212 in position adjacent the target tissue to be cut and stapled. If the anvil 3224 is not already in the open position, the robotic controller 1001 may activate the second closure motor 3360 to drive the channel 3222 in the distal direction to the position depicted in 58. Once the robotic controller 1001 determines that the surgical end effector 3212 is in the open position by sensors in the and effector and/or the tool mounting portion 3300, the robotic controller 1001 may provide the surgeon with a signal to inform the surgeon that the anvil 3224 may then be closed. Once the target tissue is positioned between the open anvil 3224 and the surgical staple cartridge 3234, the surgeon may then commence the closure process by activating the robotic controller 1001 to apply a closure control signal to the second closure motor 3360. The second closure motor 3360 applies a rotary motion to the closure shaft 3340 to draw the channel 3222 in the proximal direction PD until the anvil 3224 has been pivoted to the closed position. Once the robotic controller 1001 determines that the anvil 3224 has been moved to the closed position by sensors in the surgical end effector 3212 and/or in the tool mounting portion 3300 that are in communication with the robotic control system, the motor 3360 may be deactivated. Thereafter, the firing process may be commenced either manually by the surgeon activating a trigger, button, etc. on the controller 1001 or the controller 1001 may automatically commence the firing process. To commence the firing process, the robotic controller 1001 activates the firing motor 3310 to drive the firing bar 3235 and the cutting instrument 3232 in the distal direction DD. Once robotic controller 1001 has determined that the cutting instrument 3232 has moved to the ending position within the surgical staple cartridge 3234 by means of sensors in the surgical end effector 3212 and/or the motor drive portion 3300, the robotic controller 1001 may provide the surgeon with an indication signal. Thereafter the surgeon may manually activate the first motor 3310 to retract the cutting instrument 3232 to the starting position or the robotic controller 1001 may automatically activate the first motor 3310 to retract the cutting element 3232. The embodiment depicted in 58 does not include an articulation joint. 64 and 65 illustrate surgical tools 3200 and 3200 that have end effectors 3212, 3212, respectively that may be employed with an elongated shaft embodiment that has an articulation joint of the various types disclosed herein. For example, as can be seen in 59, a threaded closure shaft 3342 is coupled to the proximal end 3223 of the elongated channel 3222 by a flexible cable or other flexible member 3345. The location of an articulation joint not shown within the elongated shaft assembly 3208 will coincide with the flexible member 3345 to enable the flexible member 3345 to accommodate such articulation. In addition, in the above-described embodiment, the flexible member 33345 is rotatably affixed to the proximal end portion 3223 of the elongated channel 3222 to enable the flexible member 3345 to rotate relative thereto to prevent the flexible member 3229 from winding up relative to the channel 3222. Although not shown, the cutting element may be driven in one of the above described manners by a knife bar that can also accommodate articulation of the elongated shaft assembly. 60 depicts a surgical end effector 3212 that is substantially identical to the surgical end effector 3212 described above, except that the threaded closure rod 3342 is attached to a closure nut 3347 that is constrained to only move axially within the elongated shaft assembly 3208. The flexible member 3345 is attached to the closure nut 3347. Such arrangement also prevents the threaded closure rod 3342 from winding-up the flexible member 3345. A flexible knife bar 3235 may be employed to facilitate articulation of the surgical end effector 3212. The surgical tools 3200, 3200, and 3200 described above may also employ anyone of the cutting instrument embodiments described herein. As described above, the anvil of each of the end effectors of these tools is closed by drawing the elongated channel into contact with the distal end of the elongated shaft assembly. Thus, once the target tissue has been located between the staple cartridge 3234 and the anvil 3224, the robotic controller 1001 can start to draw the channel 3222 inward into the shaft assembly 3208. In various embodiments, however, to prevent the end effector 3212, 3212, 3212 from moving the target tissue with the end effector during this closing process, the controller 1001 may simultaneously move the tool holder and ultimately the tool such to compensate for the movement of the elongated channel 3222 so that, in effect, the target tissue is clamped between the anvil and the elongated channel without being otherwise moved. 61-63 depict another surgical tool embodiment 3201 that is substantially identical to surgical tool 3200 described above, except for the differences discussed below. In this embodiment, the threaded closure rod 3342 has variable pitched grooves. More specifically, as can be seen in 62, the closure rod 3342 has a distal groove section 3380 and a proximal groove section 3382. The distal and proximal groove sections 3380, 3382 are configured for engagement with a lug 3390 supported within the hollow threaded end portion 3341. As can be seen in 62, the distal groove section 3380 has a finer pitch than the groove section 3382. Thus, such variable pitch arrangement permits the elongated channel 3222 to be drawn into the shaft 3208 at a first speed or rate by virtue of the engagement between the lug 3390 and the proximal groove segment 3382. When the lug 3390 engages the distal groove segment, the channel 3222 will be drawn into the shaft 3208 at a second speed or rate. Because the proximal groove segment 3382 is coarser than the distal groove segment 3380, the first speed will be greater than the second speed. Such arrangement serves to speed up the initial closing of the end effector for tissue manipulation and then after the tissue has been properly positioned therein, generate the amount of closure forces to properly clamp the tissue for cutting and sealing. Thus, the anvil 3234 initially closes fast with a lower force and then applies a higher closing force as the anvil closes more slowly. The surgical end effector opening and closing motions are employed to enable the user to use the end effector to grasp and manipulate tissue prior to fully clamping it in the desired location for cutting and sealing. The user may, for example, open and close the surgical end effector numerous times during this process to orient the end effector in a proper position which enables the tissue to be held in a desired location. Thus, in at least some embodiments, to produce the high loading for firing, the fine thread may require as many as 5-10 full rotations to generate the necessary load. In some cases, for example, this action could take as long as 2-5 seconds. If it also took an equally long time to open and close the end effector each time during the positioning/tissue manipulation process, just positioning the end effector may take an undesirably long time. If that happens, it is possible that a user may abandon such use of the end effector for use of a conventional grasper device. Use of graspers, etc. may undesirably increase the costs associated with completing the surgical procedure. The above-described embodiments employ a battery or batteries to power the motors used to drive the end effector components. Activation of the motors is controlled by the robotic system 1000. In alternative embodiments, the power supply may comprise alternating current AC that is supplied to the motors by the robotic system 1000. That is, the AC power would be supplied from the system powering the robotic system 1000 through the tool holder and adapter. In still other embodiments, a power cord or tether may be attached to the tool mounting portion 3300 to supply the requisite power from a separate source of alternating or direct current. In use, the controller 1001 may apply an initial rotary motion to the closure shaft 3340 58 to draw the elongated channel 3222 axially inwardly into the elongated shaft assembly 3208 and move the anvil from a first position to an intermediate position at a first rate that corresponds with the point wherein the distal groove section 3380 transitions to the proximal groove section 3382. Further application of rotary motion to the closure shaft 3340 will cause the anvil to move from the intermediate position to the closed position relative to the surgical staple cartridge. When in the closed position, the tissue to be cut and stapled is properly clamped between the anvil and the surgical staple cartridge. 64-68 illustrate another surgical tool embodiment 3400 of the present invention. This embodiment includes an elongated shaft assembly 3408 that extends from a tool mounting portion 3500. The elongated shaft assembly 3408 includes a rotatable proximal closure tube segment 3410 that is rotatably journaled on a proximal spine member 3420 that is rigidly coupled to a tool mounting plate 3502 of the tool mounting portion 3500. The proximal spine member 3420 has a distal end 3422 that is coupled to an elongated channel portion 3522 of a surgical end effector 3412. For example, in at least one embodiment, the elongated channel portion 3522 has a distal end portion 3523 that hookingly engages the distal end 3422 of the spine member 3420. The elongated channel 3522 is configured to support a surgical staple cartridge 3534 therein. This embodiment may employ one of the various cutting instrument embodiments disclosed herein to sever tissue that is clamped in the surgical end effector 3412 and fire the staples in the staple cartridge 3534 into the severed tissue. Surgical end effector 3412 has an anvil 3524 that is pivotally coupled to the elongated channel 3522 by a pair of trunnions 3525 that are received in corresponding openings 3529 in the elongated channel 3522. The anvil 3524 is moved between the open 64 and closed positions 65-67 by a distal closure tube segment 3430. A distal end portion 3432 of the distal closure tube segment 3430 includes an opening 3445 into which a tab 3527 on the anvil 3524 is inserted in order to open and close the anvil 3524 as the distal closure tube segment 3430 moves axially relative thereto. In various embodiments, the opening 3445 is shaped such that as the closure tube segment 3430 is moved in the proximal direction, the closure tube segment 3430 causes the anvil 3524 to pivot to an open position. In addition or in the alternative, a spring not shown may be employed to bias the anvil 3524 to the open position. As can be seen in 64-67, the distal closure tube segment 3430 includes a lug 3442 that extends from its distal end 3440 into threaded engagement with a variable pitch groove/thread 3414 formed in the distal end 3412 of the rotatable proximal closure tube segment 3410. The variable pitch groove/thread 3414 has a distal section 3416 and a proximal section 3418. The pitch of the distal groove/thread section 3416 is finer than the pitch of the proximal groove/thread section 3418. As can also be seen in 64-67, the distal closure tube segment 3430 is constrained for axial movement relative to the spine member 3420 by an axial retainer pin 3450 that is received in an axial slot 3424 in the distal end of the spine member 3420. As indicated above, the anvil 2524 is open and closed by rotating the proximal closure tube segment 3410. The variable pitch thread arrangement permits the distal closure tube segment 3430 to be driven in the distal direction DD at a first speed or rate by virtue of the engagement between the lug 3442 and the proximal groove/thread section 3418. When the lug 3442 engages the distal groove/thread section 3416, the distal closure tube segment 3430 will be driven in the distal direction at a second speed or rate. Because the proximal groove/thread section 3418 is coarser than the distal groove/thread segment 3416, the first speed will be greater than the second speed. In at least one embodiment, the tool mounting portion 3500 is configured to receive a corresponding first rotary motion from the robotic controller 1001 and convert that first rotary motion to a primary rotary motion for rotating the rotatable proximal closure tube segment 3410 about a longitudinal tool axis LT-LT. As can be seen in 68, a proximal end 3460 of the proximal closure tube segment 3410 is rotatably supported within a cradle arrangement 3504 attached to a tool mounting plate 3502 of the tool mounting portion 3500. A rotation gear 3462 is formed on or attached to the proximal end 3460 of the closure tube segment 3410 for meshing engagement with a rotation drive assembly 3470 that is operably supported on the tool mounting plate 3502. In at least one embodiment, a rotation drive gear 3472 is coupled to a corresponding first one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 3502 when the tool mounting portion 3500 is coupled to the tool holder 1270. See 26 and 68. The rotation drive assembly 3470 further comprises a rotary driven gear 3474 that is rotatably supported on the tool mounting plate 3502 in meshing engagement with the rotation gear 3462 and the rotation drive gear 3472. Application of a first rotary control motion from the robotic controller 1001 through the tool holder 1270 and the adapter 1240 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 3472 by virtue of being operably coupled thereto. Rotation of the rotation drive gear 3472 ultimately results in the rotation of the closure tube segment 3410 to open and close the anvil 3524 as described above. As indicated above, the surgical end effector 3412 employs a cutting instrument of the type and constructions described above. 68 illustrates one form of knife drive assembly 3480 for axially advancing a knife bar 3492 that is attached to such cutting instrument. One form of the knife drive assembly 3480 comprises a rotary drive gear 3482 that is coupled to a corresponding third one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 3502 when the tool drive portion 3500 is coupled to the tool holder 1270. See 26 and 68. The knife drive assembly 3480 further comprises a first rotary driven gear assembly 3484 that is rotatably supported on the tool mounting plate 5200. The first rotary driven gear assembly 3484 is in meshing engagement with a third rotary driven gear assembly 3486 that is rotatably supported on the tool mounting plate 3502 and which is in meshing engagement with a fourth rotary driven gear assembly 3488 that is in meshing engagement with a threaded portion 3494 of drive shaft assembly 3490 that is coupled to the knife bar 3492. Rotation of the rotary drive gear 3482 in a second rotary direction will result in the axial advancement of the drive shaft assembly 3490 and knife bar 3492 in the distal direction DD. Conversely, rotation of the rotary drive gear 3482 in a secondary rotary direction opposite to the second rotary direction will cause the drive shaft assembly 3490 and the knife bar 3492 to move in the proximal direction. 69-78 illustrate another surgical tool 3600 embodiment of the present invention that may be employed in connection with a robotic system 1000. As can be seen in 69, the tool 3600 includes an end effector in the form of a disposable loading unit 3612. Various forms of disposable loading units that may be employed in connection with tool 3600 are disclosed, for example, in Patent Application Publication 2009/0206131, entitled END EFFECTOR ARRANGEMENTS FOR A SURGICAL CUTTING AND STAPLING INSTRUMENT, the disclosure of which is herein incorporated by reference in its entirety. In at least one form, the disposable loading unit 3612 includes an anvil assembly 3620 that is supported for pivotal travel relative to a carrier 3630 that operably supports a staple cartridge 3640 therein. A mounting assembly 3650 is pivotally coupled to the cartridge carrier 3630 to enable the carrier 3630 to pivot about an articulation axis AA-AA relative to a longitudinal tool axis LT-LT. Referring to 74, mounting assembly 3650 includes upper and lower mounting portions 3652 and 3654. Each mounting portion includes a threaded bore 3656 on each side thereof dimensioned to receive threaded bolts not shown for securing the proximal end of carrier 3630 thereto. A pair of centrally located pivot members 3658 extends between upper and lower mounting portions via a pair of coupling members 3660 which engage a distal end of a housing portion 3662. Coupling members 3660 each include an interlocking proximal portion 3664 configured to be received in grooves 3666 formed in the proximal end of housing portion 3662 to retain mounting assembly 3650 and housing portion 3662 in a longitudinally fixed position in relation thereto. In various forms, housing portion 3662 of disposable loading unit 3614 includes an upper housing half 3670 and a lower housing half 3672 contained within an outer casing 3674. The proximal end of housing half 3670 includes engagement nubs 3676 for releasably engaging an elongated shaft 3700 and an insertion tip 3678. Nubs 3676 form a bayonet-type coupling with the distal end of the elongated shaft 3700 which will be discussed in further detail below. Housing halves 3670, 3672 define a channel 3674 for slidably receiving axial drive assembly 3680. A second articulation link 3690 is dimensioned to be slidably positioned within a slot 3679 formed between housing halves 3670, 3672. A pair of blow out plates 3691 are positioned adjacent the distal end of housing portion 3662 adjacent the distal end of axial drive assembly 3680 to prevent outward bulging of drive assembly 3680 during articulation of carrier 3630. In various embodiments, the second articulation link 3690 includes at least one elongated metallic plate. Preferably, two or more metallic plates are stacked to form link 3690. The proximal end of articulation link 3690 includes a hook portion 3692 configured to engage first articulation link 3710 extending through the elongated shaft 3700. The distal end of the second articulation link 3690 includes a loop 3694 dimensioned to engage a projection formed on mounting assembly 3650. The projection is laterally offset from pivot pin 3658 such that linear movement of second articulation link 3690 causes mounting assembly 3650 to pivot about pivot pins 3658 to articulate the carrier 3630. In various forms, axial drive assembly 3680 includes an elongated drive beam 3682 including a distal working head 3684 and a proximal engagement section 3685. Drive beam 3682 may be constructed from a single sheet of material or, preferably, multiple stacked sheets. Engagement section 3685 includes a pair of engagement fingers which are dimensioned and configured to mountingly engage a pair of corresponding retention slots formed in drive member 3686. Drive member 3686 includes a proximal porthole 3687 configured to receive the distal end 3722 of control rod 2720 See 78 when the proximal end of disposable loading unit 3614 is engaged with elongated shaft 3700 of surgical tool 3600. Referring to 69 and 76-78, to use the surgical tool 3600, a disposable loading unit 3612 is first secured to the distal end of elongated shaft 3700. It will be appreciated that the surgical tool 3600 may include an articulating or a non-articulating disposable loading unit. To secure the disposable loading unit 3612 to the elongated shaft 3700, the distal end 3722 of control rod 3720 is inserted into insertion tip 3678 of disposable loading unit 3612, and insertion tip 3678 is slid longitudinally into the distal end of the elongated shaft 3700 in the direction indicated by arrow A in 76 such that hook portion 3692 of second articulation link 3690 slides within a channel 3702 in the elongated shaft 3700. Nubs 3676 will each be aligned in a respective channel not shown in elongated shaft 3700. When hook portion 3692 engages the proximal wall 3704 of channel 3702, disposable loading unit 3612 is rotated in the direction indicated by arrow B in 75 and 78 to move hook portion 3692 of second articulation link 3690 into engagement with finger 3712 of first articulation link 3710. Nubs 3676 also form a bayonet-type coupling within annular channel 3703 in the elongated shaft 3700. During rotation of loading unit 3612, nubs 3676 engage cam surface 3732 76 of block plate 3730 to initially move plate 3730 in the direction indicated by arrow C in 81 to lock engagement member 3734 in recess 3721 of control rod 3720 to prevent longitudinal movement of control rod 3720 during attachment of disposable loading unit 3612. During the final degree of rotation, nubs 3676 disengage from cam surface 3732 to allow blocking plate 3730 to move in the direction indicated by arrow D in 75 and 78 from behind engagement member 3734 to once again permit longitudinal movement of control rod 3720. While the above-described attachment method reflects that the disposable loading unit 3612 is manipulated relative to the elongated shaft 3700, the person of ordinary skill in the art will appreciate that the disposable loading unit 3612 may be supported in a stationary position and the robotic system 1000 may manipulate the elongated shaft portion 3700 relative to the disposable loading unit 3612 to accomplish the above-described coupling procedure. 79 illustrates another disposable loading unit 3612 that is attachable in a bayonet-type arrangement with the elongated shaft 3700 that is substantially identical to shaft 3700 except for the differences discussed below. As can be seen in 79, the elongated shaft 3700 has slots 3705 that extend for at least a portion thereof and which are configured to receive nubs 3676 therein. In various embodiments, the disposable loading unit 3612 includes arms 3677 extending therefrom which, prior to the rotation of disposable loading unit 3612, can be aligned, or at least substantially aligned, with nubs 3676 extending from housing portion 3662. In at least one embodiment, arms 3677 and nubs 3676 can be inserted into slots 3705 in elongated shaft 3700, for example, when disposable loading unit 3612 is inserted into elongated shaft 3700. When disposable loading unit 3612 is rotated, arms 3677 can be sufficiently confined within slots 3705 such that slots 3705 can hold them in position, whereas nubs 3676 can be positioned such that they are not confined within slots 3705 and can be rotated relative to arms 3677. When rotated, the hook portion 3692 of the articulation link 3690 is engaged with the first articulation link 3710 extending through the elongated shaft 3700. Other methods of coupling the disposable loading units to the end of the elongated shaft may be employed. For example, as shown in 80 and 81, disposable loading unit 3612 can include connector portion 3613 which can be configured to be engaged with connector portion 3740 of the elongated shaft 3700. In at least one embodiment, connector portion 3613 can include at least one projection and/or groove which can be mated with at least one projection and/or groove of connector portion 3740. In at least one such embodiment, the connector portions can include co-operating dovetail portions. In various embodiments, the connector portions can be configured to interlock with one another and prevent, or at least inhibit, distal and/or proximal movement of disposable loading unit 3612 along axis 3741. In at least one embodiment, the distal end of the axial drive assembly 3680 can include aperture 3681 which can be configured to receive projection 3721 extending from control rod 3720. In various embodiments, such an arrangement can allow disposable loading unit 3612 to be assembled to elongated shaft 3700 in a direction which is not collinear with or parallel to axis 3741. Although not illustrated, axial drive assembly 3680 and control rod 3720 can include any other suitable arrangement of projections and apertures to operably connect them to each other. Also in this embodiment, the first articulation link 3710 which can be operably engaged with second articulation link 3690. As can be seen in 69 and 82, the surgical tool 3600 includes a tool mounting portion 3750. The tool mounting portion 3750 includes a tool mounting plate 3751 that is configured for attachment to the tool drive assembly 1010. The tool mounting portion operably supported a transmission arrangement 3752 thereon. In use, it may be desirable to rotate the disposable loading unit 3612 about the longitudinal tool axis defined by the elongated shaft 3700. In at least one embodiment, the transmission arrangement 3752 includes a rotational transmission assembly 3753 that is configured to receive a corresponding rotary output motion from the tool drive assembly 1010 of the robotic system 1000 and convert that rotary output motion to a rotary control motion for rotating the elongated shaft 3700 and the disposable loading unit 3612 about the longitudinal tool axis LT-LT. As can be seen in 82, a proximal end 3701 of the elongated shaft 3700 is rotatably supported within a cradle arrangement 3754 that is attached to the tool mounting plate 3751 of the tool mounting portion 3750. A rotation gear 3755 is formed on or attached to the proximal end 3701 of the elongated shaft 3700 for meshing engagement with a rotation gear assembly 3756 operably supported on the tool mounting plate 3751. In at least one embodiment, a rotation drive gear 3757 drivingly coupled to a corresponding first one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 3751 when the tool mounting portion 3750 is coupled to the tool drive assembly 1010. The rotation transmission assembly 3753 further comprises a rotary driven gear 3758 that is rotatably supported on the tool mounting plate 3751 in meshing engagement with the rotation gear 3755 and the rotation drive gear 3757. Application of a first rotary output motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 3757 by virtue of being operably coupled thereto. Rotation of the rotation drive gear 3757 ultimately results in the rotation of the elongated shaft 3700 and the disposable loading unit 3612 about the longitudinal tool axis LT-LT primary rotary motion. As can be seen in 82, a drive shaft assembly 3760 is coupled to a proximal end of the control rod 2720. In various embodiments, the control rod 2720 is axially advanced in the distal and proximal directions by a knife/closure drive transmission 3762. One form of the knife/closure drive assembly 3762 comprises a rotary drive gear 3763 that is coupled to a corresponding second one of the driven rotatable body portions, discs or elements 1304 on the adapter side of the tool mounting plate 3751 when the tool mounting portion 3750 is coupled to the tool holder 1270. The rotary driven gear 3763 is in meshing driving engagement with a gear train, generally depicted as 3764. In at least one form, the gear train 3764 further comprises a first rotary driven gear assembly 3765 that is rotatably supported on the tool mounting plate 3751. The first rotary driven gear assembly 3765 is in meshing engagement with a second rotary driven gear assembly 3766 that is rotatably supported on the tool mounting plate 3751 and which is in meshing engagement with a third rotary driven gear assembly 3767 that is in meshing engagement with a threaded portion 3768 of the drive shaft assembly 3760. Rotation of the rotary drive gear 3763 in a second rotary direction will result in the axial advancement of the drive shaft assembly 3760 and control rod 2720 in the distal direction DD. Conversely, rotation of the rotary drive gear 3763 in a secondary rotary direction which is opposite to the second rotary direction will cause the drive shaft assembly 3760 and the control rod 2720 to move in the proximal direction. When the control rod 2720 moves in the distal direction, it drives the drive beam 3682 and the working head 3684 thereof distally through the surgical staple cartridge 3640. As the working head 3684 is driven distally, it operably engages the anvil 3620 to pivot it to a closed position. The cartridge carrier 3630 may be selectively articulated about articulation axis AA-AA by applying axial articulation control motions to the first and second articulation links 3710 and 3690. In various embodiments, the transmission arrangement 3752 further includes an articulation drive 3770 that is operably supported on the tool mounting plate 3751. More specifically and with reference to 82, it can be seen that a proximal end portion 3772 of an articulation drive shaft 3771 configured to operably engage with the first articulation link 3710 extends through the rotation gear 3755 and is rotatably coupled to a shifter rack gear 3774 that is slidably affixed to the tool mounting plate 3751 through slots 3775. The articulation drive 3770 further comprises a shifter drive gear 3776 that is coupled to a corresponding third one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 3751 when the tool mounting portion 3750 is coupled to the tool holder 1270. The articulation drive assembly 3770 further comprises a shifter driven gear 3778 that is rotatably supported on the tool mounting plate 3751 in meshing engagement with the shifter drive gear 3776 and the shifter rack gear 3774. Application of a third rotary output motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding driven element 1304 will thereby cause rotation of the shifter drive gear 3776 by virtue of being operably coupled thereto. Rotation of the shifter drive gear 3776 ultimately results in the axial movement of the shifter gear rack 3774 and the articulation drive shaft 3771. The direction of axial travel of the articulation drive shaft 3771 depends upon the direction in which the shifter drive gear 3776 is rotated by the robotic system 1000. Thus, rotation of the shifter drive gear 3776 in a first rotary direction will result in the axial movement of the articulation drive shaft 3771 in the proximal direction PD and cause the cartridge carrier 3630 to pivot in a first direction about articulation axis AA-AA. Conversely, rotation of the shifter drive gear 3776 in a second rotary direction opposite to the first rotary direction will result in the axial movement of the articulation drive shaft 3771 in the distal direction DD to thereby cause the cartridge carrier 3630 to pivot about articulation axis AA-AA in an opposite direction. 83 illustrates yet another surgical tool 3800 embodiment of the present invention that may be employed with a robotic system 1000. As can be seen in 83, the surgical tool 3800 includes a surgical end effector 3812 in the form of an endocutter 3814 that employs various cable-driven components. Various forms of cable driven endocutters are disclosed, for example, in 7,726,537, entitled SURGICAL STAPLER WITH UNIVERSAL ARTICULATION AND TISSUE PRE-CLAMP and Patent Application Publication 2008/0308603, entitled CABLE DRIVEN SURGICAL STAPLING AND CUTTING INSTRUMENT WITH IMPROVED CABLE ATTACHMENT ARRANGEMENTS, the disclosures of each are herein incorporated by reference in their respective entireties. Such endocutters 3814 may be referred to as a disposable loading unit because they are designed to be disposed of after a single use. However, the various unique and novel arrangements of various embodiments of the present invention may also be employed in connection with cable driven end effectors that are reusable. As can be seen in 83, in at least one form, the endocutter 3814 includes an elongated channel 3822 that operably supports a surgical staple cartridge 3834 therein. An anvil 3824 is pivotally supported for movement relative to the surgical staple cartridge 3834. The anvil 3824 has a cam surface 3825 that is configured for interaction with a preclamping collar 3840 that is supported for axial movement relative thereto. The end effector 3814 is coupled to an elongated shaft assembly 3808 that is attached to a tool mounting portion 3900. In various embodiments, a closure cable 3850 is employed to move pre-clamping collar 3840 distally onto and over cam surface 3825 to close the anvil 3824 relative to the surgical staple cartridge 3834 and compress the tissue therebetween. Preferably, closure cable 3850 attaches to the pre-clamping collar 3840 at or near point 3841 and is fed through a passageway in anvil 3824 or under a proximal portion of anvil 3824 and fed proximally through shaft 3808. Actuation of closure cable 3850 in the proximal direction PD forces pre-clamping collar 3840 distally against cam surface 3825 to close anvil 3824 relative to staple cartridge assembly 3834. A return mechanism, , a spring, cable system or the like, may be employed to return pre-clamping collar 3840 to a pre-clamping orientation which re-opens the anvil 3824. The elongated shaft assembly 3808 may be cylindrical in shape and define a channel 3811 which may be dimensioned to receive a tube adapter 3870. See 84. In various embodiments, the tube adapter 3870 may be slidingly received in friction-fit engagement with the internal channel of elongated shaft 3808. The outer surface of the tube adapter 3870 may further include at least one mechanical interface, , a cutout or notch 3871, oriented to mate with a corresponding mechanical interface, , a radially inwardly extending protrusion or detent not shown, disposed on the inner periphery of internal channel 3811 to lock the tube adapter 3870 to the elongated shaft 3808. In various embodiments, the distal end of tube adapter 3870 may include a pair of opposing flanges 3872a and 3872b which define a cavity for pivotably receiving a pivot block 3873 therein. Each flange 3872a and 3872b may include an aperture 3874a and 3874b that is oriented to receive a pivot pin 3875 that extends through an aperture in pivot block 3873 to allow pivotable movement of pivot block 3873 about an axis that is perpendicular to longitudinal tool axis LT-LT. The channel 3822 may be formed with two upwardly extending flanges 3823a, 3823b that have apertures therein, which are dimensioned to receive a pivot pin 3827. In turn, pivot pin 3875 mounts through apertures in pivot block 3873 to permit rotation of the surgical end effector 3814 about the Y axis as needed during a given surgical procedure. Rotation of pivot block 3873 about pin 3875 along Z axis rotates the surgical end effector 3814 about the Z axis. See 84. Other methods of fastening the elongated channel 3822 to the pivot block 3873 may be effectively employed without departing from the spirit and scope of the present invention. The surgical staple cartridge 3834 can be assembled and mounted within the elongated channel 3822 during the manufacturing or assembly process and sold as part of the surgical end effector 3812, or the surgical staple cartridge 3834 may be designed for selective mounting within the elongated channel 3822 as needed and sold separately, , as a single use replacement, replaceable or disposable staple cartridge assembly. It is within the scope of this disclosure that the surgical end effector 3812 may be pivotally, operatively, or integrally attached, for example, to distal end 3809 of the elongated shaft assembly 3808 of a disposable surgical stapler. As is known, a used or spent disposable loading unit 3814 can be removed from the elongated shaft assembly 3808 and replaced with an unused disposable unit. The endocutter 3814 may also preferably include an actuator, preferably a dynamic clamping member 3860, a sled 3862, as well as staple pushers not shown and staples not shown once an unspent or unused cartridge 3834 is mounted in the elongated channel 3822. See 84. In various embodiments, the dynamic clamping member 3860 is associated with, , mounted on and rides on, or with or is connected to or integral with and/or rides behind sled 3862. It is envisioned that dynamic clamping member 3860 can have cam wedges or cam surfaces attached or integrally formed or be pushed by a leading distal surface thereof. In various embodiments, dynamic clamping member 3860 may include an upper portion 3863 having a transverse aperture 3864 with a pin 3865 mountable or mounted therein, a central support or upward extension 3866 and substantially T-shaped bottom flange 3867 which cooperate to slidingly retain dynamic clamping member 3860 along an ideal cutting path during longitudinal, distal movement of sled 3862. The leading cutting edge 3868, here, knife blade 3869, is dimensioned to ride within slot 3835 of staple cartridge assembly 3834 and separate tissue once stapled. As used herein, the term knife assembly may include the aforementioned dynamic clamping member 3860, knife 3869, and sled 3862 or other knife/beam/sled drive arrangements and cutting instrument arrangements. In addition, the various embodiments of the present invention may be employed with knife assembly/cutting instrument arrangements that may be entirely supported in the staple cartridge 3834 or partially supported in the staple cartridge 3834 and elongated channel 3822 or entirely supported within the elongated channel 3822. In various embodiments, the dynamic clamping member 3860 may be driven in the proximal and distal directions by a cable drive assembly 3870. In one non-limiting form, the cable drive assembly comprises a pair of advance cables 3880, 3882 and a firing cable 3884. 85 and 86 illustrate the cables 3880, 3882, 3884 in diagrammatic form. As can be seen in those Figures, a first advance cable 3880 is operably supported on a first distal cable transition support 3885 which may comprise, for example, a pulley, rod, capstan, etc. that is attached to the distal end of the elongated channel 3822 and a first proximal cable transition support 3886 which may comprise, for example, a pulley, rod, capstan, etc. that is operably supported by the elongated channel 3822. A distal end 3881 of the first advance cable 3880 is affixed to the dynamic clamping assembly 3860. The second advance cable 3882 is operably supported on a second distal cable transition support 3887 which may, for example, comprise a pulley, rod, capstan etc. that is mounted to the distal end of the elongated channel 3822 and a second proximal cable transition support 3888 which may, for example, comprise a pulley, rod, capstan, etc. mounted to the proximal end of the elongated channel 3822. The proximal end 3883 of the second advance cable 3882 may be attached to the dynamic clamping assembly 3860. Also in these embodiments, an endless firing cable 3884 is employed and journaled on a support 3889 that may comprise a pulley, rod, capstan, etc. mounted within the elongated shaft 3808. In one embodiment, the retract cable 3884 may be formed in a loop and coupled to a connector 3889 that is fixedly attached to the first and second advance cables 3880, 3882. Various non-limiting embodiments of the present invention include a cable drive transmission 3920 that is operably supported on a tool mounting plate 3902 of the tool mounting portion 3900. The tool mounting portion 3900 has an array of electrical connecting pins 3904 which are configured to interface with the slots 1258 25 in the adapter 1240. Such arrangement permits the robotic system 1000 to provide control signals to a control circuit 3910 of the tool 3800. While the interface is described herein with reference to mechanical, electrical, and magnetic coupling elements, it should be understood that a wide variety of telemetry modalities might be used, including infrared, inductive coupling, or the like. Control circuit 3910 is shown in schematic form in 83. In one form or embodiment, the control circuit 3910 includes a power supply in the form of a battery 3912 that is coupled to an on-off solenoid powered switch 3914. In other embodiments, however, the power supply may comprise a source of alternating current. Control circuit 3910 further includes an on/off solenoid 3916 that is coupled to a double pole switch 3918 for controlling motor rotation direction. Thus, when the robotic system 1000 supplies an appropriate control signal, switch 3914 will permit battery 3912 to supply power to the double pole switch 3918. The robotic system 1000 will also supply an appropriate signal to the double pole switch 3918 to supply power to a shifter motor 3922. Turning to 87-92, at least one embodiment of the cable drive transmission 3920 comprises a drive pulley 3930 that is operably mounted to a drive shaft 3932 that is attached to a driven element 1304 of the type and construction described above that is designed to interface with a corresponding drive element 1250 of the adapter 1240. See 25 and 90. Thus, when the tool mounting portion 3900 is operably coupled to the tool holder 1270, the robot system 1000 can apply rotary motion to the drive pulley 3930 in a desired direction. A first drive member or belt 3934 drivingly engages the drive pulley 3930 and a second drive shaft 3936 that is rotatably supported on a shifter yoke 3940. The shifter yoke 3940 is operably coupled to the shifter motor 3922 such that rotation of the shaft 3923 of the shifter motor 3922 in a first direction will shift the shifter yoke in a first direction FD and rotation of the shifter motor shaft 3923 in a second direction will shift the shifter yoke 3940 in a second direction SD. Other embodiments of the present invention may employ a shifter solenoid arrangement for shifting the shifter yoke in said first and second directions. As can be seen in 87-90, a closure drive gear 3950 mounted to a second drive shaft 3936 and is configured to selectively mesh with a closure drive assembly, generally designated as 3951. Likewise a firing drive gear 3960 is also mounted to the second drive shaft 3936 and is configured to selectively mesh with a firing drive assembly generally designated as 3961. Rotation of the second drive shaft 3936 causes the closure drive gear 3950 and the firing drive gear 3960 to rotate. In one non-limiting embodiment, the closure drive assembly 3951 comprises a closure driven gear 3952 that is coupled to a first closure pulley 3954 that is rotatably supported on a third drive shaft 3956. The closure cable 3850 is drivingly received on the first closure pulley 3954 such that rotation of the closure driven gear 3952 will drive the closure cable 3850. Likewise, the firing drive assembly 3961 comprises a firing driven gear 3962 that is coupled to a first firing pulley 3964 that is rotatably supported on the third drive shaft 3956. The first and second driving pulleys 3954 and 3964 are independently rotatable on the third drive shaft 3956. The firing cable 3884 is drivingly received on the first firing pulley 3964 such that rotation of the firing driven gear 3962 will drive the firing cable 3884. Also in various embodiments, the cable drive transmission 3920 further includes a braking assembly 3970. In at least one embodiment, for example, the braking assembly 3970 includes a closure brake 3972 that comprises a spring arm 3973 that is attached to a portion of the transmission housing 3971. The closure brake 3972 has a gear lug 3974 that is sized to engage the teeth of the closure driven gear 3952 as will be discussed in further detail below. The braking assembly 3970 further includes a firing brake 3976 that comprises a spring arm 3977 that is attached to another portion of the transmission housing 3971. The firing brake 3976 has a gear lug 3978 that is sized to engage the teeth of the firing driven gear 3962. At least one embodiment of the surgical tool 3800 may be used as follows. The tool mounting portion 3900 is operably coupled to the interface 1240 of the robotic system 1000. The controller or control unit of the robotic system is operated to locate the tissue to be cut and stapled between the open anvil 3824 and the staple cartridge 3834. When in that initial position, the braking assembly 3970 has locked the closure driven gear 3952 and the firing driven gear 3962 such that they cannot rotate. That is, as shown in 88, the gear lug 3974 is in locking engagement with the closure driven gear 3952 and the gear lug 3978 is in locking engagement with the firing driven gear 3962. Once the surgical end effector 3814 has been properly located, the controller 1001 of the robotic system 1000 will provide a control signal to the shifter motor 3922 or shifter solenoid to move the shifter yoke 3940 in the first direction. As the shifter yoke 3940 is moved in the first direction, the closure drive gear 3950 moves the gear lug 3974 out of engagement with the closure driven gear 3952 as it moves into meshing engagement with the closure driven gear 3952. As can be seen in 87, when in that position, the gear lug 3978 remains in locking engagement with the firing driven gear 3962 to prevent actuation of the firing system. Thereafter, the robotic controller 1001 provides a first rotary actuation motion to the drive pulley 3930 through the interface between the driven element 1304 and the corresponding components of the tool holder 1240. As the drive pulley 3930 is rotated in the first direction, the closure cable 3850 is rotated to drive the preclamping collar 3840 into closing engagement with the cam surface 3825 of the anvil 3824 to move it to the closed position thereby clamping the target tissue between the anvil 3824 and the staple cartridge 3834. See 83. Once the anvil 3824 has been moved to the closed position, the robotic controller 1001 stops the application of the first rotary motion to the drive pulley 3930. Thereafter, the robotic controller 1001 may commence the firing process by sending another control signal to the shifter motor 3922 or shifter solenoid to cause the shifter yoke to move in the second direction SD as shown in 94. As the shifter yoke 3940 is moved in the second direction, the firing drive gear 3960 moves the gear lug 3978 out of engagement with the firing driven gear 3962 as it moves into meshing engagement with the firing driven gear 3962. As can be seen in 89, when in that position, the gear lug 3974 remains in locking engagement with the closure driven gear 3952 to prevent actuation of the closure system. Thereafter, the robotic controller 1001 is activated to provide the first rotary actuation motion to the drive pulley 3930 through the interface between the driven element 1304 and the corresponding components of the tool holder 1240. As the drive pulley 3930 is rotated in the first direction, the firing cable 3884 is rotated to drive the dynamic clamping member 3860 in the distal direction DD thereby firing the stapes and cutting the tissue clamped in the end effector 3814. Once the robotic system 1000 determines that the dynamic clamping member 3860 has reached its distal most positioneither through sensors or through monitoring the amount of rotary input applied to the drive pulley 3930, the controller 1001 may then apply a second rotary motion to the drive pulley 3930 to rotate the closure cable 3850 in an opposite direction to cause the dynamic clamping member 3860 to be retracted in the proximal direction PD. Once the dynamic clamping member has been retracted to the starting position, the application of the second rotary motion to the drive pulley 3930 is discontinued. Thereafter, the shifter motor 3922 or shifter solenoid is powered to move the shifter yoke 3940 to the closure position 92. . Once the closure drive gear 3950 is in meshing engagement with the closure driven gear 3952, the robotic controller 1001 may once again apply the second rotary motion to the drive pulley 3930. Rotation of the drive pulley 3930 in the second direction causes the closure cable 3850 to retract the preclamping collar 3840 out of engagement with the cam surface 3825 of the anvil 3824 to permit the anvil 3824 to move to an open position by a spring or other means to release the stapled tissue from the surgical end effector 3814. 93 illustrates a surgical tool 4000 that employs a gear driven firing bar 4092 as shown in 94-96. This embodiment includes an elongated shaft assembly 4008 that extends from a tool mounting portion 4100. The tool mounting portion 4100 includes a tool mounting plate 4102 that operable supports a transmission arrangement 4103 thereon. The elongated shaft assembly 4008 includes a rotatable proximal closure tube 4010 that is rotatably journaled on a proximal spine member 4020 that is rigidly coupled to the tool mounting plate 4102. The proximal spine member 4020 has a distal end that is coupled to an elongated channel portion 4022 of a surgical end effector 4012. The surgical effector 4012 may be substantially similar to surgical end effector 3412 described above. In addition, the anvil 4024 of the surgical end effector 4012 may be opened and closed by a distal closure tube 4030 that operably interfaces with the proximal closure tube 4010. Distal closure tube 4030 is identical to distal closure tube 3430 described above. Similarly, proximal closure tube 4010 is identical to proximal closure tube segment 3410 described above. Anvil 4024 is opened and closed by rotating the proximal closure tube 4010 in manner described above with respect to distal closure tube 3410. In at least one embodiment, the transmission arrangement comprises a closure transmission, generally designated as 4011. As will be further discussed below, the closure transmission 4011 is configured to receive a corresponding first rotary motion from the robotic system 1000 and convert that first rotary motion to a primary rotary motion for rotating the rotatable proximal closure tube 4010 about the longitudinal tool axis LT-LT. As can be seen in 96, a proximal end 4060 of the proximal closure tube 4010 is rotatably supported within a cradle arrangement 4104 that is attached to a tool mounting plate 4102 of the tool mounting portion 4100. A rotation gear 4062 is formed on or attached to the proximal end 4060 of the closure tube segment 4010 for meshing engagement with a rotation drive assembly 4070 that is operably supported on the tool mounting plate 4102. In at least one embodiment, a rotation drive gear 4072 is coupled to a corresponding first one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 4102 when the tool mounting portion 4100 is coupled to the tool holder 1270. See 26 and 96. The rotation drive assembly 4070 further comprises a rotary driven gear 4074 that is rotatably supported on the tool mounting plate 4102 in meshing engagement with the rotation gear 4062 and the rotation drive gear 4072. Application of a first rotary control motion from the robotic system 1000 through the tool holder 1270 and the adapter 1240 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 4072 by virtue of being operably coupled thereto. Rotation of the rotation drive gear 4072 ultimately results in the rotation of the closure tube segment 4010 to open and close the anvil 4024 as described above. As indicated above, the end effector 4012 employs a cutting element 3860 as shown in 94 and 95. In at least one non-limiting embodiment, the transmission arrangement 4103 further comprises a knife drive transmission that includes a knife drive assembly 4080. 96 illustrates one form of knife drive assembly 4080 for axially advancing the knife bar 4092 that is attached to such cutting element using cables as described above with respect to surgical tool 3800. In particular, the knife bar 4092 replaces the firing cable 3884 employed in an embodiment of surgical tool 3800. One form of the knife drive assembly 4080 comprises a rotary drive gear 4082 that is coupled to a corresponding second one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 4102 when the tool mounting portion 4100 is coupled to the tool holder 1270. See 26 and 96. The knife drive assembly 4080 further comprises a first rotary driven gear assembly 4084 that is rotatably supported on the tool mounting plate 4102. The first rotary driven gear assembly 4084 is in meshing engagement with a third rotary driven gear assembly 4086 that is rotatably supported on the tool mounting plate 4102 and which is in meshing engagement with a fourth rotary driven gear assembly 4088 that is in meshing engagement with a threaded portion 4094 of drive shaft assembly 4090 that is coupled to the knife bar 4092. Rotation of the rotary drive gear 4082 in a second rotary direction will result in the axial advancement of the drive shaft assembly 4090 and knife bar 4092 in the distal direction DD. Conversely, rotation of the rotary drive gear 4082 in a secondary rotary direction opposite to the second rotary direction will cause the drive shaft assembly 4090 and the knife bar 4092 to move in the proximal direction. Movement of the firing bar 4092 in the proximal direction PD will drive the cutting element 3860 in the distal direction DD. Conversely, movement of the firing bar 4092 in the distal direction DD will result in the movement of the cutting element 3860 in the proximal direction PD. 97-103 illustrate yet another surgical tool 5000 that may be effectively employed in connection with a robotic system 1000. In various forms, the surgical tool 5000 includes a surgical end effector 5012 in the form of a surgical stapling instrument that includes an elongated channel 5020 and a pivotally translatable clamping member, such as an anvil 5070, which are maintained at a spacing that assures effective stapling and severing of tissue clamped in the surgical end effector 5012. As can be seen in 99, the elongated channel 5020 may be substantially U-shaped in cross-section and be fabricated from, for example, titanium, 203 stainless steel, 304 stainless steel, 416 stainless steel, 17-4 stainless steel, 17-7 stainless steel, 6061 or 7075 aluminum, chromium steel, ceramic, etc. A substantially U-shaped metal channel pan 5022 may be supported in the bottom of the elongated channel 5020 as shown. Various embodiments include an actuation member in the form of a sled assembly 5030 that is operably supported within the surgical end effector 5012 and axially movable therein between a starting position and an ending position in response to control motions applied thereto. In some forms, the metal channel pan 5022 has a centrally-disposed slot 5024 therein to movably accommodate a base portion 5032 of the sled assembly 5030. The base portion 5032 includes a foot portion 5034 that is sized to be slidably received in a slot 5021 in the elongated channel 5020. See 104. As can be seen in 98, 99, 102, and 103, the base portion 5032 of sled assembly 5030 includes an axially extending threaded bore 5036 that is configured to be threadedly received on a threaded drive shaft 5130 as will be discussed in further detail below. In addition, the sled assembly 5030 includes an upstanding support portion 5038 that supports a tissue cutting blade or tissue cutting instrument 5040. The upstanding support portion 5038 terminates in a top portion 5042 that has a pair of laterally extending retaining fins 5044 protruding therefrom. As shown in 99, the fins 5044 are positioned to be received within corresponding slots 5072 in anvil 5070. The fins 5044 and the foot 5034 serve to retain the anvil 5070 in a desired spaced closed position as the sled assembly 5030 is driven distally through the tissue clamped within the surgical end effector 5014. As can also be seen in 101 and 103, the sled assembly 5030 further includes a reciprocatably or sequentially activatable drive assembly 5050 for driving staple pushers toward the closed anvil 5070. More specifically and with reference to 99 and 100, the elongated channel 5020 is configured to operably support a surgical staple cartridge 5080 therein. In at least one form, the surgical staple cartridge 5080 comprises a body portion 5082 that may be fabricated from, for example, Vectra, Nylon 6/6 or 6/12 and include a centrally disposed slot 5084 for accommodating the upstanding support portion 5038 of the sled assembly 5030. See 99. These materials could also be filled with glass, carbon, or mineral fill of 10%-40%. The surgical staple cartridge 5080 further includes a plurality of cavities 5086 for movably supporting lines or rows of staple-supporting pushers 5088 therein. The cavities 5086 may be arranged in spaced longitudinally extending lines or rows 5090, 5092, 5094, 5096. For example, the rows 5090 may be referred to herein as first outboard rows. The rows 5092 may be referred to herein as first inboard rows. The rows 5094 may be referred to as second inboard rows and the rows 5096 may be referred to as second outboard rows. The first inboard row 5090 and the first outboard row 5092 are located on a first lateral side of the longitudinal slot 5084 and the second inboard row 5094 and the second outboard row 5096 are located on a second lateral side of the longitudinal slot 5084. The first staple pushers 5088 in the first inboard row 5092 are staggered in relationship to the first staple pushers 5088 in the first outboard row 5090. Similarly, the second staple pushers 5088 in the second outboard row 5096 are staggered in relationship to the second pushers 5088 in the second inboard row 5094. Each pusher 5088 operably supports a surgical staple 5098 thereon. In various embodiments, the sequentially-activatable or reciprocatably-activatable drive assembly 5050 includes a pair of outboard drivers 5052 and a pair of inboard drivers 5054 that are each attached to a common shaft 5056 that is rotatably mounted within the base 5032 of the sled assembly 5030. The outboard drivers 5052 are oriented to sequentially or reciprocatingly engage a corresponding plurality of outboard activation cavities 5026 provided in the channel pan 5022. Likewise, the inboard drivers 5054 are oriented to sequentially or reciprocatingly engage a corresponding plurality of inboard activation cavities 5028 provided in the channel pan 5022. The inboard activation cavities 5028 are arranged in a staggered relationship relative to the adjacent outboard activation cavities 5026. See 100. As can also be seen in 100 and 102, in at least one embodiment, the sled assembly 5030 further includes distal wedge segments 5060 and intermediate wedge segments 5062 located on each side of the bore 5036 to engage the pushers 5088 as the sled assembly 5030 is driven distally in the distal direction DD. As indicated above, the sled assembly 5030 is threadedly received on a threaded portion 5132 of a drive shaft 5130 that is rotatably supported within the end effector 5012. In various embodiments, for example, the drive shaft 5130 has a distal end 5134 that is supported in a distal bearing 5136 mounted in the surgical end effector 5012. See 99 and 100. In various embodiments, the surgical end effector 5012 is coupled to a tool mounting portion 5200 by an elongated shaft assembly 5108. In at least one embodiment, the tool mounting portion 5200 operably supports a transmission arrangement generally designated as 5204 that is configured to receive rotary output motions from the robotic system. The elongated shaft assembly 5108 includes an outer closure tube 5110 that is rotatable and axially movable on a spine member 5120 that is rigidly coupled to a tool mounting plate 5201 of the tool mounting portion 5200. The spine member 5120 also has a distal end 5122 that is coupled to the elongated channel portion 5020 of the surgical end effector 5012. In use, it may be desirable to rotate the surgical end effector 5012 about a longitudinal tool axis LT-LT defined by the elongated shaft assembly 5008. In various embodiments, the outer closure tube 5110 has a proximal end 5112 that is rotatably supported on the tool mounting plate 5201 of the tool drive portion 5200 by a forward support cradle 5203. The proximal end 5112 of the outer closure tube 5110 is configured to operably interface with a rotation transmission portion 5206 of the transmission arrangement 5204. In various embodiments, the proximal end 5112 of the outer closure tube 5110 is also supported on a closure sled 5140 that is also movably supported on the tool mounting plate 5201. A closure tube gear segment 5114 is formed on the proximal end 5112 of the outer closure tube 5110 for meshing engagement with a rotation drive assembly 5150 of the rotation transmission 5206. As can be seen in 97, the rotation drive assembly 5150, in at least one embodiment, comprises a rotation drive gear 5152 that is coupled to a corresponding first one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 5201 when the tool drive portion 5200 is coupled to the tool holder 1270. The rotation drive assembly 5150 further comprises a rotary driven gear 5154 that is rotatably supported on the tool mounting plate 5201 in meshing engagement with the closure tube gear segment 5114 and the rotation drive gear 5152. Application of a first rotary control motion from the robotic system 1000 through the tool holder 1270 and the adapter 1240 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 5152. Rotation of the rotation drive gear 5152 ultimately results in the rotation of the elongated shaft assembly 5108 and the end effector 5012 about the longitudinal tool axis LT-LT represented by arrow R in 97. Closure of the anvil 5070 relative to the surgical staple cartridge 5080 is accomplished by axially moving the outer closure tube 5110 in the distal direction DD. Such axial movement of the outer closure tube 5110 may be accomplished by a closure transmission portion 5144 of the transmission arrangement 5204. As indicated above, in various embodiments, the proximal end 5112 of the outer closure tube 5110 is supported by the closure sled 5140 which enables the proximal end 5112 to rotate relative thereto, yet travel axially with the closure sled 5140. In particular, as can be seen in 97, the closure sled 5140 has an upstanding tab 5141 that extends into a radial groove 5115 in the proximal end portion 5112 of the outer closure tube 5110. In addition, as was described above, the closure sled 5140 is slidably mounted to the tool mounting plate 5201. In various embodiments, the closure sled 5140 has an upstanding portion 5142 that has a closure rack gear 5143 formed thereon. The closure rack gear 5143 is configured for driving engagement with the closure transmission 5144. In various forms, the closure transmission 5144 includes a closure spur gear 5145 that is coupled to a corresponding second one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 5201. Thus, application of a second rotary control motion from the robotic system 1000 through the tool holder 1270 and the adapter 1240 to the corresponding second driven element 1304 will cause rotation of the closure spur gear 5145 when the interface 1230 is coupled to the tool mounting portion 5200. The closure transmission 5144 further includes a driven closure gear set 5146 that is supported in meshing engagement with the closure spur gear 5145 and the closure rack gear 5143. Thus, application of a second rotary control motion from the robotic system 1000 through the tool holder 1270 and the adapter 1240 to the corresponding second driven element 1304 will cause rotation of the closure spur gear 5145 and ultimately drive the closure sled 5140 and the outer closure tube 5110 axially. The axial direction in which the closure tube 5110 moves ultimately depends upon the direction in which the second driven element 1304 is rotated. For example, in response to one rotary closure motion received from the robotic system 1000, the closure sled 5140 will be driven in the distal direction DD and ultimately the outer closure tube 5110 will be driven in the distal direction as well. The outer closure tube 5110 has an opening 5117 in the distal end 5116 that is configured for engagement with a tab 5071 on the anvil 5070 in the manners described above. As the outer closure tube 5110 is driven distally, the proximal end 5116 of the closure tube 5110 will contact the anvil 5070 and pivot it closed. Upon application of an opening rotary motion from the robotic system 1000, the closure sled 5140 and outer closure tube 5110 will be driven in the proximal direction PD and pivot the anvil 5070 to the open position in the manners described above. In at least one embodiment, the drive shaft 5130 has a proximal end 5137 that has a proximal shaft gear 5138 attached thereto. The proximal shaft gear 5138 is supported in meshing engagement with a distal drive gear 5162 attached to a rotary drive bar 5160 that is rotatably supported with spine member 5120. Rotation of the rotary drive bar 5160 and ultimately rotary drive shaft 5130 is controlled by a rotary knife transmission 5207 which comprises a portion of the transmission arrangement 5204 supported on the tool mounting plate 5210. In various embodiments, the rotary knife transmission 5207 comprises a rotary knife drive system 5170 that is operably supported on the tool mounting plate 5201. In various embodiments, the knife drive system 5170 includes a rotary drive gear 5172 that is coupled to a corresponding third one of the driven discs or elements 1304 on the adapter side of the tool mounting plate 5201 when the tool drive portion 5200 is coupled to the tool holder 1270. The knife drive system 5170 further comprises a first rotary driven gear 5174 that is rotatably supported on the tool mounting plate 5201 in meshing engagement with a second rotary driven gear 5176 and the rotary drive gear 5172. The second rotary driven gear 5176 is coupled to a proximal end portion 5164 of the rotary drive bar 5160. Rotation of the rotary drive gear 5172 in a first rotary direction will result in the rotation of the rotary drive bar 5160 and rotary drive shaft 5130 in a first direction. Conversely, rotation of the rotary drive gear 5172 in a second rotary direction opposite to the first rotary direction will cause the rotary drive bar 5160 and rotary drive shaft 5130 to rotate in a second direction. 2400. Thus, rotation of the drive shaft 2440 results in rotation of the drive sleeve 2400. One method of operating the surgical tool 5000 will now be described. The tool drive 5200 is operably coupled to the interface 1240 of the robotic system 1000. The controller 1001 of the robotic system 1000 is operated to locate the tissue to be cut and stapled between the open anvil 5070 and the surgical staple cartridge 5080. Once the surgical end effector 5012 has been positioned by the robot system 1000 such that the target tissue is located between the anvil 5070 and the surgical staple cartridge 5080, the controller 1001 of the robotic system 1000 may be activated to apply the second rotary output motion to the second driven element 1304 coupled to the closure spur gear 5145 to drive the closure sled 5140 and the outer closure tube 5110 axially in the distal direction to pivot the anvil 5070 closed in the manner described above. Once the robotic controller 1001 determines that the anvil 5070 has been closed by, for example, sensors in the surgical end effector 5012 and/or the tool drive portion 5200, the robotic controller 1001 system may provide the surgeon with an indication that signifies the closure of the anvil. Such indication may be, for example, in the form of a light and/or audible sound, tactile feedback on the control members, etc. Then the surgeon may initiate the firing process. In alternative embodiments, however, the robotic controller 1001 may automatically commence the firing process. To commence the firing process, the robotic controller applies a third rotary output motion to the third driven disc or element 1304 coupled to the rotary drive gear 5172. Rotation of the rotary drive gear 5172 results in the rotation of the rotary drive bar 5160 and rotary drive shaft 5130 in the manner described above. Firing and formation of the surgical staples 5098 can be best understood from reference to 98, 100, and 101. As the sled assembly 5030 is driven in the distal direction DD through the surgical staple cartridge 5080, the distal wedge segments 5060 first contact the staple pushers 5088 and start to move them toward the closed anvil 5070. As the sled assembly 5030 continues to move distally, the outboard drivers 5052 will drop into the corresponding activation cavity 5026 in the channel pan 5022. The opposite end of each outboard driver 5052 will then contact the corresponding outboard pusher 5088 that has moved up the distal and intermediate wedge segments 5060, 5062. Further distal movement of the sled assembly 5030 causes the outboard drivers 5052 to rotate and drive the corresponding pushers 5088 toward the anvil 5070 to cause the staples 5098 supported thereon to be formed as they are driven into the anvil 5070. It will be understood that as the sled assembly 5030 moves distally, the knife blade 5040 cuts through the tissue that is clamped between the anvil and the staple cartridge. Because the inboard drivers 5054 and outboard drivers 5052 are attached to the same shaft 5056 and the inboard drivers 5054 are radially offset from the outboard drivers 5052 on the shaft 5056, as the outboard drivers 5052 are driving their corresponding pushers 5088 toward the anvil 5070, the inboard drivers 5054 drop into their next corresponding activation cavity 5028 to cause them to rotatably or reciprocatingly drive the corresponding inboard pushers 5088 towards the closed anvil 5070 in the same manner. Thus, the laterally corresponding outboard staples 5098 on each side of the centrally disposed slot 5084 are simultaneously formed together and the laterally corresponding inboard staples 5098 on each side of the slot 5084 are simultaneously formed together as the sled assembly 5030 is driven distally. Once the robotic controller 1001 determines that the sled assembly 5030 has reached its distal most positioneither through sensors or through monitoring the amount of rotary input applied to the drive shaft 5130 and/or the rotary drive bar 5160, the controller 1001 may then apply a third rotary output motion to the drive shaft 5130 to rotate the drive shaft 5130 in an opposite direction to retract the sled assembly 5030 back to its starting position. Once the sled assembly 5030 has been retracted to the starting position as signaled by sensors in the end effector 5012 and/or the tool drive portion 5200, the application of the second rotary motion to the drive shaft 5130 is discontinued. Thereafter, the surgeon may manually activate the anvil opening process or it may be automatically commenced by the robotic controller 1001. To open the anvil 5070, the second rotary output motion is applied to the closure spur gear 5145 to drive the closure sled 5140 and the outer closure tube 5110 axially in the proximal direction. As the closure tube 5110 moves proximally, the opening 5117 in the distal end 5116 of the closure tube 5110 contacts the tab 5071 on the anvil 5070 to pivot the anvil 5070 to the open position. A spring may also be employed to bias the anvil 5070 to the open position when the closure tube 5116 has been returned to the starting position. Again, sensors in the surgical end effector 5012 and/or the tool mounting portion 5200 may provide the robotic controller 1001 with a signal indicating that the anvil 5070 is now open. Thereafter, the surgical end effector 5012 may be withdrawn from the surgical site. 104-109 diagrammatically depict the sequential firing of staples in a surgical tool assembly 5000 that is substantially similar to the surgical tool assembly 5000 described above. In this embodiment, the inboard and outboard drivers 5052, 5054 have a cam-like shape with a cam surface 5053 and an actuator protrusion 5055 as shown in 104-110. The drivers 5052, 5054 are journaled on the same shaft 5056 that is rotatably supported by the sled assembly 5030. In this embodiment, the sled assembly 5030 has distal wedge segments 5060 for engaging the pushers 5088. 104 illustrates an initial position of two inboard or outboard drivers 5052, 5054 as the sled assembly 5030 is driven in the distal direction DD. As can be seen in that Figure, the pusher 5088a has advanced up the wedge segment 5060 and has contacted the driver 5052, 5054. Further travel of the sled assembly 5030 in the distal direction causes the driver 5052, 5054 to pivot in the P direction 105 until the actuator portion 5055 contacts the end wall 5029a of the activation cavity 5026, 5028 as shown in 111. Continued advancement of the sled assembly 5030 in the distal direction DD causes the driver 5052, 5054 to rotate in the D direction as shown in 107. As the driver 5052, 5054 rotates, the pusher 5088a rides up the cam surface 5053 to the final vertical position shown in 108. When the pusher 5088a reaches the final vertical position shown in 108 and 109, the staple not shown supported thereon has been driven into the staple forming surface of the anvil to form the staple. 111-116 illustrate a surgical end effector 5312 that may be employed for example, in connection with the tool mounting portion 1300 and shaft 2008 described in detail above. In various forms, the surgical end effector 5312 includes an elongated channel 5322 that is constructed as described above for supporting a surgical staple cartridge 5330 therein. The surgical staple cartridge 5330 comprises a body portion 5332 that includes a centrally disposed slot 5334 for accommodating an upstanding support portion 5386 of a sled assembly 5380. See 111-113. The surgical staple cartridge body portion 5332 further includes a plurality of cavities 5336 for movably supporting staple-supporting pushers 5350 therein. The cavities 5336 may be arranged in spaced longitudinally extending rows 5340, 5342, 5344, 5346. The rows 5340, 5342 are located on one lateral side of the longitudinal slot 5334 and the rows 5344, 5346 are located on the other side of longitudinal slot 5334. In at least one embodiment, the pushers 5350 are configured to support two surgical staples 5352 thereon. In particular, each pusher 5350 located on one side of the elongated slot 5334 supports one staple 5352 in row 5340 and one staple 5352 in row 5342 in a staggered orientation. Likewise, each pusher 5350 located on the other side of the elongated slot 5334 supports one surgical staple 5352 in row 5344 and another surgical staple 5352 in row 5346 in a staggered orientation. Thus, every pusher 5350 supports two surgical staples 5352. As can be further seen in 111, 112, the surgical staple cartridge 5330 includes a plurality of rotary drivers 5360. More particularly, the rotary drivers 5360 on one side of the elongated slot 5334 are arranged in a single line 5370 and correspond to the pushers 5350 in lines 5340, 5342. In addition, the rotary drivers 5360 on the other side of the elongated slot 5334 are arranged in a single line 5372 and correspond to the pushers 5350 in lines 5344, 5346. As can be seen in 116, each rotary driver 5360 is rotatably supported within the staple cartridge body 5332. More particularly, each rotary driver 5360 is rotatably received on a corresponding driver shaft 5362. Each driver 5360 has an arcuate ramp portion 5364 formed thereon that is configured to engage an arcuate lower surface 5354 formed on each pusher 5350. See 116. In addition, each driver 5360 has a lower support portion 5366 extend therefrom to slidably support the pusher 5360 on the channel 5322. Each driver 5360 has a downwardly extending actuation rod 5368 that is configured for engagement with a sled assembly 5380. As can be seen in 113, in at least one embodiment, the sled assembly 5380 includes a base portion 5382 that has a foot portion 5384 that is sized to be slidably received in a slot 5333 in the channel 5322. See 111. The sled assembly 5380 includes an upstanding support portion 5386 that supports a tissue cutting blade or tissue cutting instrument 5388. The upstanding support portion 5386 terminates in a top portion 5390 that has a pair of laterally extending retaining fins 5392 protruding therefrom. The fins 5392 are positioned to be received within corresponding slots not shown in the anvil not shown. As with the above-described embodiments, the fins 5392 and the foot portion 5384 serve to retain the anvil not shown in a desired spaced closed position as the sled assembly 5380 is driven distally through the tissue clamped within the surgical end effector 5312. The upstanding support portion 5386 is configured for attachment to a knife bar 2200 32. The sled assembly 5380 further has a horizontally-extending actuator plate 5394 that is shaped for actuating engagement with each of the actuation rods 5368 on the pushers 5360. Operation of the surgical end effector 5312 will now be explained with reference to 111 and 112. As the sled assembly 5380 is driven in the distal direction DD through the staple cartridge 5330, the actuator plate 5394 sequentially contacts the actuation rods 5368 on the pushers 5360. As the sled assembly 5380 continues to move distally, the actuator plate 5394 sequentially contacts the actuator rods 5368 of the drivers 5360 on each side of the elongated slot 5334. Such action causes the drivers 5360 to rotate from a first unactuated position to an actuated portion wherein the pushers 5350 are driven towards the closed anvil. As the pushers 5350 are driven toward the anvil, the surgical staples 5352 thereon are driven into forming contact with the underside of the anvil. Once the robotic system 1000 determines that the sled assembly 5080 has reached its distal most position through sensors or other means, the control system of the robotic system 1000 may then retract the knife bar and sled assembly 5380 back to the starting position. Thereafter, the robotic control system may then activate the procedure for returning the anvil to the open position to release the stapled tissue. 117-121 depict one form of an automated reloading system embodiment of the present invention, generally designated as 5500. In one form, the automated reloading system 5500 is configured to replace a spent surgical end effector component in a manipulatable surgical tool portion of a robotic surgical system with a new surgical end effector component. As used herein, the term surgical end effector component may comprise, for example, a surgical staple cartridge, a disposable loading unit or other end effector components that, when used, are spent and must be replaced with a new component. Furthermore, the term spent means that the end effector component has been activated and is no longer useable for its intended purpose in its present state. For example, in the context of a surgical staple cartridge or disposable loading unit, the term spent means that at least some of the unformed staples that were previously supported therein have been fired therefrom. As used herein, the term new surgical end effector component refers to an end effector component that is in condition for its intended use. In the context of a surgical staple cartridge or disposable loading unit, for example, the term new refers to such a component that has unformed staples therein and which is otherwise ready for use. In various embodiments, the automated reloading system 5500 includes a base portion 5502 that may be strategically located within a work envelope 1109 of a robotic arm cart 1100 18 of a robotic system 1000. As used herein, the term manipulatable surgical tool portion collectively refers to a surgical tool of the various types disclosed herein and other forms of surgical robotically-actuated tools that are operably attached to, for example, a robotic arm cart 1100 or similar device that is configured to automatically manipulate and actuate the surgical tool. The term work envelope as used herein refers to the range of movement of the manipulatable surgical tool portion of the robotic system. 18 generally depicts an area that may comprise a work envelope of the robotic arm cart 1100. Those of ordinary skill in the art will understand that the shape and size of the work envelope depicted therein is merely illustrative. The ultimate size, shape and location of a work envelope will ultimately depend upon the construction, range of travel limitations, and location of the manipulatable surgical tool portion. Thus, the term work envelope as used herein is intended to cover a variety of different sizes and shapes of work envelopes and should not be limited to the specific size and shape of the sample work envelope depicted in 18. As can be seen in 117, the base portion 5502 includes a new component support section or arrangement 5510 that is configured to operably support at least one new surgical end effector component in a loading orientation. As used herein, the term loading orientation means that the new end effector component is supported in such away so as to permit the corresponding component support portion of the manipulatable surgical tool portion to be brought into loading engagement with i. e. , operably seated or operably attached to the new end effector component or the new end effector component to be brought into loading engagement with the corresponding component support portion of the manipulatable surgical tool portion without human intervention beyond that which may be necessary to actuate the robotic system. As will be further appreciated as the present Detailed Description proceeds, in at least one embodiment, the preparation nurse will load the new component support section before the surgery with the appropriate length and color cartridges some surgical staple cartridges may support certain sizes of staples the size of which may be indicated by the color of the cartridge body required for completing the surgical procedure. However, no direct human interaction is necessary during the surgery to reload the robotic endocutter. In one form, the surgical end effector component comprises a staple cartridge 2034 that is configured to be operably seated within a component support portion elongated channel of any of the various other end effector arrangements described above. For explanation purposes, new unused cartridges will be designated as 2034a and spent cartridges will be designated as 2034b. The Figures depict cartridges 2034a, 2034b designed for use with a surgical end effector 2012 that includes a channel 2022 and an anvil 2024, the construction and operation of which were discussed in detail above. Cartridges 2034a, 2034b are identical to cartridges 2034 described above. In various embodiments, the cartridges 2034a, 2034b are configured to be snappingly retained i. e. , loading engagement within the channel 2022 of a surgical end effector 2012. As the present Detailed Description proceeds, however, those of ordinary skill in the art will appreciate that the unique and novel features of the automated cartridge reloading system 5500 may be effectively employed in connection with the automated removal and installation of other cartridge arrangements without departing from the spirit and scope of the present invention. In the depicted embodiment, the term loading orientation means that the distal tip portion 2035a of the a new surgical staple cartridge 2034a is inserted into a corresponding support cavity 5512 in the new cartridge support section 5510 such that the proximal end portion 2037a of the new surgical staple cartridge 2034a is located in a convenient orientation for enabling the arm cart 1100 to manipulate the surgical end effector 2012 into a position wherein the new cartridge 2034a may be automatically loaded into the channel 2022 of the surgical end effector 2012. In various embodiments, the base 5502 includes at least one sensor 5504 which communicates with the control system 1003 of the robotic controller 1001 to provide the control system 1003 with the location of the base 5502 and/or the reload length and color doe each staged or new cartridge 2034a. As can also be seen in the Figures, the base 5502 further includes a collection receptacle 5520 that is configured to collect spent cartridges 2034b that have been removed or disengaged from the surgical end effector 2012 that is operably attached to the robotic system 1000. In addition, in one form, the automated reloading system 5500 includes an extraction system 5530 for automatically removing the spent end effector component from the corresponding support portion of the end effector or manipulatable surgical tool portion without specific human intervention beyond that which may be necessary to activate the robotic system. In various embodiments, the extraction system 5530 includes an extraction hook member 5532. In one form, for example, the extraction hook member 5532 is rigidly supported on the base portion 5502. In one embodiment, the extraction hook member has at least one hook 5534 formed thereon that is configured to hookingly engage the distal end 2035 of a spent cartridge 2034b when it is supported in the elongated channel 2022 of the surgical end effector 2012. In various forms, the extraction hook member 5532 is conveniently located within a portion of the collection receptacle 5520 such that when the spent end effector component cartridge 2034b is brought into extractive engagement with the extraction hook member 5532, the spent end effector component cartridge 2034b is dislodged from the corresponding component support portion elongated channel 2022, and falls into the collection receptacle 5020. Thus, to use this embodiment, the manipulatable surgical tool portion manipulates the end effector attached thereto to bring the distal end 2035 of the spent cartridge 2034b therein into hooking engagement with the hook 5534 and then moves the end effector in such a way to dislodge the spent cartridge 2034b from the elongated channel 2022. In other arrangements, the extraction hook member 5532 comprises a rotatable wheel configuration that has a pair of diametrically-opposed hooks 5334 protruding therefrom. See 122 and 125. The extraction hook member 5532 is rotatably supported within the collection receptacle 5520 and is coupled to an extraction motor 5540 that is controlled by the controller 1001 of the robotic system. This form of the automated reloading system 5500 may be used as follows. 119 illustrates the introduction of the surgical end effector 2012 that is operably attached to the manipulatable surgical tool portion 1200. As can be seen in that Figure, the arm cart 1100 of the robotic system 1000 locates the surgical end effector 2012 in the shown position wherein the hook end 5534 of the extraction member 5532 hookingly engages the distal end 2035 of the spent cartridge 2034b in the surgical end effector 2012. The anvil 2024 of the surgical end effector 2012 is in the open position. After the distal end 2035 of the spent cartridge 2034b is engaged with the hook end 5532, the extraction motor 5540 is actuated to rotate the extraction wheel 5532 to disengage the spent cartridge 2034b from the channel 2022. To assist with the disengagement of the spent cartridge 2034b from the channel 2022 or if the extraction member 5530 is stationary, the robotic system 1000 may move the surgical end effector 2012 in an upward direction arrow U in 120. As the spent cartridge 2034b is dislodged from the channel 2022, the spent cartridge 2034b falls into the collection receptacle 5520. Once the spent cartridge 2034b has been removed from the surgical end effector 2012, the robotic system 1000 moves the surgical end effector 2012 to the position shown in 121. In various embodiments, a sensor arrangement 5533 is located adjacent to the extraction member 5532 that is in communication with the controller 1001 of the robotic system 1000. The sensor arrangement 5533 may comprise a sensor that is configured to sense the presence of the surgical end effector 2012 and, more particularly the tip 2035b of the spent surgical staple cartridge 2034b thereof as the distal tip portion 2035b is brought into engagement with the extraction member 5532. In some embodiments, the sensor arrangement 5533 may comprise, for example, a light curtain arrangement. However, other forms of proximity sensors may be employed. In such arrangement, when the surgical end effector 2012 with the spent surgical staple cartridge 2034b is brought into extractive engagement with the extraction member 5532, the sensor senses the distal tip 2035b of the surgical staple cartridge 2034b , the light curtain is broken. When the extraction member 5532 spins and pops the surgical staple cartridge 2034b loose and it falls into the collection receptacle 5520, the light curtain is again unbroken. Because the surgical end effector 2012 was not moved during this procedure, the robotic controller 1001 is assured that the spent surgical staple cartridge 2034b has been removed therefrom. Other sensor arrangements may also be successfully employed to provide the robotic controller 1001 with an indication that the spent surgical staple cartridge 2034b has been removed from the surgical end effector 2012. As can be seen in 121, the surgical end effector 2012 is positioned to grasp a new surgical staple cartridge 2034a between the channel 2022 and the anvil 2024. More specifically, as shown in 118 and 121, each cavity 5512 has a corresponding upstanding pressure pad 5514 associated with it. The surgical end effector 2012 is located such that the pressure pad 5514 is located between the new cartridge 2034a and the anvil 2024. Once in that position, the robotic system 1000 closes the anvil 2024 onto the pressure pad 5514 which serves to push the new cartridge 2034a into snapping engagement with the channel 2022 of the surgical end effector 2012. Once the new cartridge 2034a has been snapped into position within the elongated channel 2022, the robotic system 1000 then withdraws the surgical end effector 2012 from the automated cartridge reloading system 5500 for use in connection with performing another surgical procedure. 122-126 depict another automated reloading system 5600 that may be used to remove a spent disposable loading unit 3612 from a manipulatable surgical tool arrangement 3600 69-82 that is operably attached to an arm cart 1100 or other portion of a robotic system 1000 and reload a new disposable loading unit 3612 therein. As can be seen in 122 and 123, one form of the automated reloading system 5600 includes a housing 5610 that has a movable support assembly in the form of a rotary carrousel top plate 5620 supported thereon which cooperates with the housing 5610 to form a hollow enclosed area 5612. The automated reloading system 5600 is configured to be operably supported within the work envelop of the manipulatable surgical tool portion of a robotic system as was described above. In various embodiments, the rotary carrousel plate 5620 has a plurality of holes 5622 for supporting a plurality of orientation tubes 5660 therein. As can be seen in 123 and 124, the rotary carrousel plate 5620 is affixed to a spindle shaft 5624. The spindle shaft 5624 is centrally disposed within the enclosed area 5612 and has a spindle gear 5626 attached thereto. The spindle gear 5626 is in meshing engagement with a carrousel drive gear 5628 that is coupled to a carrousel drive motor 5630 that is in operative communication with the robotic controller 1001 of the robotic system 1000. Various embodiments of the automated reloading system 5600 may also include a carrousel locking assembly, generally designated as 5640. In various forms, the carrousel locking assembly 5640 includes a cam disc 5642 that is affixed to the spindle shaft 5624. The spindle gear 5626 may be attached to the underside of the cam disc 5642 and the cam disc 5642 may be keyed onto the spindle shaft 5624. In alternative arrangements, the spindle gear 5626 and the cam disc 5642 may be independently non-rotatably affixed to the spindle shaft 5624. As can be seen in 123 and 124, a plurality of notches 5644 are spaced around the perimeter of the cam disc 5642. A locking arm 5648 is pivotally mounted within the housing 5610 and is biased into engagement with the perimeter of the cam disc 5642 by a locking spring 5649. As can be seen in 122, the outer perimeter of the cam disc 5642 is rounded to facilitate rotation of the cam disc 5642 relative to the locking arm 5648. The edges of each notch 5644 are also rounded such that when the cam disc 5642 is rotated, the locking arm 5648 is cammed out of engagement with the notches 5644 by the perimeter of the cam disc 5642. Various forms of the automated reloading system 5600 are configured to support a portable/replaceable tray assembly 5650 that is configured to support a plurality of disposable loading units 3612 in individual orientation tubes 5660. More specifically and with reference to 123 and 124, the replaceable tray assembly 5650 comprises a tray 5652 that has a centrally-disposed locator spindle 5654 protruding from the underside thereof. The locator spindle 5654 is sized to be received within a hollow end 5625 of spindle shaft 5624. The tray 5652 has a plurality of holes 5656 therein that are configured to support an orientation tube 5660 therein. Each orientation tube 5660 is oriented within a corresponding hole 5656 in the replaceable tray assembly 5650 in a desired orientation by a locating fin 5666 on the orientation tube 5660 that is designed to be received within a corresponding locating slot 5658 in the tray assembly 5650. In at least one embodiment, the locating fin 5666 has a substantially V-shaped cross-sectional shape that is sized to fit within a V-shaped locating slot 5658. Such arrangement serves to orient the orientation tube 5660 in a desired starting position while enabling it to rotate within the hole 5656 when a rotary motion is applied thereto. That is, when a rotary motion is applied to the orientation tube 5660 the V-shaped locating fin 5666 will pop out of its corresponding locating slot enabling the tube 5660 to rotate relative to the tray 5652 as will be discussed in further detail below. As can also be seen in 122-124, the replaceable tray 5652 may be provided with one or more handle portions 5653 to facilitate transport of the tray assembly 5652 when loaded with orientation tubes 5660. As can be seen in 126, each orientation tube 5660 comprises a body portion 5662 that has a flanged open end 5664. The body portion 5662 defines a cavity 5668 that is sized to receive a portion of a disposable loading unit 3612 therein. To properly orient the disposable loading unit 3612 within the orientation tube 5660, the cavity 5668 has a flat locating surface 5670 formed therein. As can be seen in 126, the flat locating surface 5670 is configured to facilitate the insertion of the disposable loading unit into the cavity 5668 in a desired or predetermined non-rotatable orientation. In addition, the end 5669 of the cavity 5668 may include a foam or cushion material 5672 that is designed to cushion the distal end of the disposable loading unit 3612 within the cavity 5668. Also, the length of the locating surface may cooperate with a sliding support member 3689 of the axial drive assembly 3680 of the disposable loading unit 3612 to further locate the disposable loading unit 3612 at a desired position within the orientation tube 5660. The orientation tubes 5660 may be fabricated from Nylon, polycarbonate, polyethylene, liquid crystal polymer, 6061 or 7075 aluminum, titanium, 300 or 400 series stainless steel, coated or painted steel, plated steel, etc. and, when loaded in the replaceable tray 5662 and the locator spindle 5654 is inserted into the hollow end 5625 of spindle shaft 5624, the orientation tubes 5660 extend through corresponding holes 5662 in the carrousel top plate 5620. Each replaceable tray 5662 is equipped with a location sensor 5663 that communicates with the control system 1003 of the controller 1001 of the robotic system 1000. The sensor 5663 serves to identify the location of the reload system, and the number, length, color and fired status of each reload housed in the tray. In addition, an optical sensor or sensors 5665 that communicate with the robotic controller 1001 may be employed to sense the type/size/length of disposable loading units that are loaded within the tray 5662. Various embodiments of the automated reloading system 5600 further include a drive assembly 5680 for applying a rotary motion to the orientation tube 5660 holding the disposable loading unit 3612 to be attached to the shaft 3700 of the surgical tool 3600 collectively the manipulatable surgical tool portion that is operably coupled to the robotic system. The drive assembly 5680 includes a support yoke 5682 that is attached to the locking arm 5648. Thus, the support yoke 5682 pivots with the locking arm 5648. The support yoke 5682 rotatably supports a tube idler wheel 5684 and a tube drive wheel 5686 that is driven by a tube motor 5688 attached thereto. Tube motor 5688 communicates with the control system 1003 and is controlled thereby. The tube idler wheel 5684 and tube drive wheel 5686 are fabricated from, for example, natural rubber, sanoprene, isoplast, etc. such that the outer surfaces thereof create sufficient amount of friction to result in the rotation of an orientation tube 5660 in contact therewith upon activation of the tube motor 5688. The idler wheel 5684 and tube drive wheel 5686 are oriented relative to each other to create a cradle area 5687 therebetween for receiving an orientation tube 5060 in driving engagement therein. In use, one or more of the orientation tubes 5660 loaded in the automated reloading system 5600 are left empty, while the other orientation tubes 5660 may operably support a corresponding new disposable loading unit 3612 therein. As will be discussed in further detail below, the empty orientation tubes 5660 are employed to receive a spent disposable loading unit 3612 therein. The automated reloading system 5600 may be employed as follows after the system 5600 is located within the work envelope of the manipulatable surgical tool portion of a robotic system. If the manipulatable surgical tool portion has a spent disposable loading unit 3612 operably coupled thereto, one of the orientation tubes 5660 that are supported on the replaceable tray 5662 is left empty to receive the spent disposable loading unit 3612 therein. If, however, the manipulatable surgical tool portion does not have a disposable loading unit 3612 operably coupled thereto, each of the orientation tubes 5660 may be provided with a properly oriented new disposable loading unit 3612. As described hereinabove, the disposable loading unit 3612 employs a rotary bayonet-type coupling arrangement for operably coupling the disposable loading unit 3612 to a corresponding portion of the manipulatable surgical tool portion. That is, to attach a disposable loading unit 3612 to the corresponding portion of the manipulatable surgical tool portion 3700see 75, 76, a rotary installation motion must be applied to the disposable loading unit 3612 and/or the corresponding portion of the manipulatable surgical tool portion when those components have been moved into loading engagement with each other. Such installation motions are collectively referred to herein as loading motions. Likewise, to decouple a spent disposable loading unit 3612 from the corresponding portion of the manipulatable surgical tool, a rotary decoupling motion must be applied to the spent disposable loading unit 3612 and/or the corresponding portion of the manipulatable surgical tool portion while simultaneously moving the spent disposable loading unit and the corresponding portion of the manipulatable surgical tool away from each other. Such decoupling motions are collectively referred to herein as extraction motions. To commence the loading process, the robotic system 1000 is activated to manipulate the manipulatable surgical tool portion and/or the automated reloading system 5600 to bring the manipulatable surgical tool portion into loading engagement with the new disposable loading unit 3612 that is supported in the orientation tube 5660 that is in driving engagement with the drive assembly 5680. Once the robotic controller 1001 17 of the robotic control system 1000 has located the manipulatable surgical tool portion in loading engagement with the new disposable loading unit 3612, the robotic controller 1001 activates the drive assembly 5680 to apply a rotary loading motion to the orientation tube 5660 in which the new disposable loading unit 3612 is supported and/or applies another rotary loading motion to the corresponding portion of the manipulatable surgical tool portion. Upon application of such rotary loading motionss, the robotic controller 1001 also causes the corresponding portion of the manipulatable surgical tool portion to be moved towards the new disposable loading unit 3612 into loading engagement therewith. Once the disposable loading unit 3612 is in loading engagement with the corresponding portion of the manipulatable tool portion, the loading motions are discontinued and the manipulatable surgical tool portion may be moved away from the automated reloading system 5600 carrying with it the new disposable loading unit 3612 that has been operably coupled thereto. To decouple a spent disposable loading unit 3612 from a corresponding manipulatable surgical tool portion, the robotic controller 1001 of the robotic system manipulates the manipulatable surgical tool portion so as to insert the distal end of the spent disposable loading unit 3612 into the empty orientation tube 5660 that remains in driving engagement with the drive assembly 5680. Thereafter, the robotic controller 1001 activates the drive assembly 5680 to apply a rotary extraction motion to the orientation tube 5660 in which the spent disposable loading unit 3612 is supported and/or applies a rotary extraction motion to the corresponding portion of the manipulatable surgical tool portion. The robotic controller 1001 also causes the manipulatable surgical tool portion to withdraw away from the spent rotary disposable loading unit 3612. Thereafter the rotary extraction motions are discontinued. After the spent disposable loading unit 3612 has been removed from the manipulatable surgical tool portion, the robotic controller 1001 may activate the carrousel drive motor 5630 to index the carrousel top plate 5620 to bring another orientation tube 5660 that supports a new disposable loading unit 3612 therein into driving engagement with the drive assembly 5680. Thereafter, the loading process may be repeated to attach the new disposable loading unit 3612 therein to the portion of the manipulatable surgical tool portion. The robotic controller 1001 may record the number of disposable loading units that have been used from a particular replaceable tray 5652. Once the controller 1001 determines that all of the new disposable loading units 3612 have been used from that tray, the controller 1001 may provide the surgeon with a signal visual and/or audible indicating that the tray 5652 supporting all of the spent disposable loading units 3612 must be replaced with a new tray 5652 containing new disposable loading units 3612. 127-132 depict another non-limiting embodiment of a surgical tool 6000 of the present invention that is well-adapted for use with a robotic system 1000 that has a tool drive assembly 1010 22 that is operatively coupled to a master controller 1001 that is operable by inputs from an operator i. e. , a surgeon. As can be seen in 127, the surgical tool 6000 includes a surgical end effector 6012 that comprises an endocutter. In at least one form, the surgical tool 6000 generally includes an elongated shaft assembly 6008 that has a proximal closure tube 6040 and a distal closure tube 6042 that are coupled together by an articulation joint 6100. The surgical tool 6000 is operably coupled to the manipulator by a tool mounting portion, generally designated as 6200. The surgical tool 6000 further includes an interface 6030 which may mechanically and electrically couple the tool mounting portion 6200 to the manipulator in the various manners described in detail above. In at least one embodiment, the surgical tool 6000 includes a surgical end effector 6012 that comprises, among other things, at least one component 6024 that is selectively movable between first and second positions relative to at least one other component 6022 in response to various control motions applied to component 6024 as will be discussed in further detail below to perform a surgical procedure. In various embodiments, component 6022 comprises an elongated channel 6022 configured to operably support a surgical staple cartridge 6034 therein and component 6024 comprises a pivotally translatable clamping member, such as an anvil 6024. Various embodiments of the surgical end effector 6012 are configured to maintain the anvil 6024 and elongated channel 6022 at a spacing that assures effective stapling and severing of tissue clamped in the surgical end effector 6012. Unless otherwise stated, the end effector 6012 is similar to the surgical end effector 2012 described above and includes a cutting instrument not shown and a sled not shown. The anvil 6024 may include a tab 6027 at its proximal end that interacts with a component of the mechanical closure system described further below to facilitate the opening of the anvil 6024. The elongated channel 6022 and the anvil 6024 may be made of an electrically conductive material such as metal so that they may serve as part of an antenna that communicates with sensors in the end effector, as described above. The surgical staple cartridge 6034 could be made of a nonconductive material such as plastic and the sensor may be connected to or disposed in the surgical staple cartridge 6034, as was also described above. As can be seen in 127, the surgical end effector 6012 is attached to the tool mounting portion 6200 by the elongated shaft assembly 6008 according to various embodiments. As shown in the illustrated embodiment, the elongated shaft assembly 6008 includes an articulation joint generally designated as 6100 that enables the surgical end effector 6012 to be selectively articulated about a first tool articulation axis AA1-AA1 that is substantially transverse to a longitudinal tool axis LT-LT and a second tool articulation axis AA2-AA2 that is substantially transverse to the longitudinal tool axis LT-LT as well as the first articulation axis AA1-AA1. See 128. In various embodiments, the elongated shaft assembly 6008 includes a closure tube assembly 6009 that comprises a proximal closure tube 6040 and a distal closure tube 6042 that are pivotably linked by a pivot links 6044 and 6046. The closure tube assembly 6009 is movably supported on a spine assembly generally designated as 6102. As can be seen in 129, the proximal closure tube 6040 is pivotally linked to an intermediate closure tube joint 6043 by an upper pivot link 6044U and a lower pivot link 6044L such that the intermediate closure tube joint 6043 is pivotable relative to the proximal closure tube 6040 about a first closure axis CA1-CA1 and a second closure axis CA2-CA2. In various embodiments, the first closure axis CA1-CA1 is substantially parallel to the second closure axis CA2-CA2 and both closure axes CA1-CA1, CA2-CA2 are substantially transverse to the longitudinal tool axis LT-LT. As can be further seen in 129, the intermediate closure tube joint 6043 is pivotally linked to the distal closure tube 6042 by a left pivot link 6046L and a right pivot link 6046R such that the intermediate closure tube joint 6043 is pivotable relative to the distal closure tube 6042 about a third closure axis CA3-CA3 and a fourth closure axis CA4-CA4. In various embodiments, the third closure axis CA3-CA3 is substantially parallel to the fourth closure axis CA4-CA4 and both closure axes CA3-CA3, CA4-CA4 are substantially transverse to the first and second closure axes CA1-CA1, CA2-CA2 as well as to longitudinal tool axis LT-LT. The closure tube assembly 6009 is configured to axially slide on the spine assembly 6102 in response to actuation motions applied thereto. The distal closure tube 6042 includes an opening 6045 which interfaces with the tab 6027 on the anvil 6024 to facilitate opening of the anvil 6024 as the distal closure tube 6042 is moved axially in the proximal direction PD. The closure tubes 6040, 6042 may be made of electrically conductive material such as metal so that they may serve as part of the antenna, as described above. Components of the spine assembly 6102 may be made of a nonconductive material such as plastic. As indicated above, the surgical tool 6000 includes a tool mounting portion 6200 that is configured for operable attachment to the tool mounting assembly 1010 of the robotic system 1000 in the various manners described in detail above. As can be seen in 131, the tool mounting portion 6200 comprises a tool mounting plate 6202 that operably supports a transmission arrangement 6204 thereon. In various embodiments, the transmission arrangement 6204 includes an articulation transmission 6142 that comprises a portion of an articulation system 6140 for articulating the surgical end effector 6012 about a first tool articulation axis TA1-TA1 and a second tool articulation axis TA2-TA2. The first tool articulation axis TA1-TA1 is substantially transverse to the second tool articulation axis TA2-TA2 and both of the first and second tool articulation axes are substantially transverse to the longitudinal tool axis LT-LT. See 128. To facilitate selective articulation of the surgical end effector 6012 about the first and second tool articulation axes TA1-TA1, TA2-TA2, the spine assembly 6102 comprises a proximal spine portion 6110 that is pivotally coupled to a distal spine portion 6120 by pivot pins 6122 for selective pivotal travel about TA1-TA1. Similarly, the distal spine portion 6120 is pivotally attached to the elongated channel 6022 of the surgical end effector 6012 by pivot pins 6124 to enable the surgical end effector 6012 to selectively pivot about the second tool axis TA2-TA2 relative to the distal spine portion 6120. In various embodiments, the articulation system 6140 further includes a plurality of articulation elements that operably interface with the surgical end effector 6012 and an articulation control arrangement 6160 that is operably supported in the tool mounting member 6200 as will described in further detail below. In at least one embodiment, the articulation elements comprise a first pair of first articulation cables 6144 and 6146. The first articulation cables are located on a first or right side of the longitudinal tool axis. Thus, the first articulation cables are referred to herein as a right upper cable 6144 and a right lower cable 6146. The right upper cable 6144 and the right lower cable 6146 extend through corresponding passages 6147, 6148, respectively along the right side of the proximal spine portion 6110. See 132. The articulation system 6140 further includes a second pair of second articulation cables 6150, 6152. The second articulation cables are located on a second or left side of the longitudinal tool axis. Thus, the second articulation cables are referred to herein as a left upper articulation cable 6150 and a left articulation cable 6152. The left upper articulation cable 6150 and the left lower articulation cable 6152 extend through passages 6153, 6154, respectively in the proximal spine portion 6110. As can be seen in 128, the right upper cable 6144 extends around an upper pivot joint 6123 and is attached to a left upper side of the elongated channel 6022 at a left pivot joint 6125. The right lower cable 6146 extends around a lower pivot joint 6126 and is attached to a left lower side of the elongated channel 6022 at left pivot joint 6125. The left upper cable 6150 extends around the upper pivot joint 6123 and is attached to a right upper side of the elongated channel 6022 at a right pivot joint 6127. The left lower cable 6152 extends around the lower pivot joint 6126 and is attached to a right lower side of the elongated channel 6022 at right pivot joint 6127. Thus, to pivot the surgical end effector 6012 about the first tool articulation axis TA1-TA1 to the left arrow L, the right upper cable 6144 and the right lower cable 6146 must be pulled in the proximal direction PD. To articulate the surgical end effector 6012 to the right arrow R about the first tool articulation axis TA1-TA1, the left upper cable 6150 and the left lower cable 6152 must be pulled in the proximal direction PD. To articulate the surgical end effector 6012 about the second tool articulation axis TA2-TA2, in an upward direction arrow U, the right upper cable 6144 and the left upper cable 6150 must be pulled in the proximal direction PD. To articulate the surgical end effector 6012 in the downward direction arrow DW about the second tool articulation axis TA2-TA2, the right lower cable 6146 and the left lower cable 6152 must be pulled in the proximal direction PD. The proximal ends of the articulation cables 6144, 6146, 6150, 6152 are coupled to the articulation control arrangement 6160 which comprises a ball joint assembly that is a part of the articulation transmission 6142. More specifically and with reference to 132, the ball joint assembly 6160 includes a ball-shaped member 6162 that is formed on a proximal portion of the proximal spine 6110. Movably supported on the ball-shaped member 6162 is an articulation control ring 6164. As can be further seen in 132, the proximal ends of the articulation cables 6144, 6146, 6150, 6152 are coupled to the articulation control ring 6164 by corresponding ball joint arrangements 6166. The articulation control ring 6164 is controlled by an articulation drive assembly 6170. As can be most particularly seen in 132, the proximal ends of the first articulation cables 6144, 6146 are attached to the articulation control ring 6164 at corresponding spaced first points 6149, 6151 that are located on plane 6159. Likewise, the proximal ends of the second articulation cables 6150, 6152 are attached to the articulation control ring 6164 at corresponding spaced second points 6153, 6155 that are also located along plane 6159. As the present Detailed Description proceeds, those of ordinary skill in the art will appreciate that such cable attachment configuration on the articulation control ring 6164 facilitates the desired range of articulation motions as the articulation control ring 6164 is manipulated by the articulation drive assembly 6170. In various forms, the articulation drive assembly 6170 comprises a horizontal articulation assembly generally designated as 6171. In at least one form, the horizontal articulation assembly 6171 comprises a horizontal push cable 6172 that is attached to a horizontal gear arrangement 6180. The articulation drive assembly 6170 further comprises a vertically articulation assembly generally designated as 6173. In at least one form, the vertical articulation assembly 6173 comprises a vertical push cable 6174 that is attached to a vertical gear arrangement 6190. As can be seen in 131 and 132, the horizontal push cable 6172 extends through a support plate 6167 that is attached to the proximal spine portion 6110. The distal end of the horizontal push cable 6174 is attached to the articulation control ring 6164 by a corresponding ball/pivot joint 6168. The vertical push cable 6174 extends through the support plate 6167 and the distal end thereof is attached to the articulation control ring 6164 by a corresponding ball/pivot joint 6169. The horizontal gear arrangement 6180 includes a horizontal driven gear 6182 that is pivotally mounted on a horizontal shaft 6181 that is attached to a proximal portion of the proximal spine portion 6110. The proximal end of the horizontal push cable 6172 is pivotally attached to the horizontal driven gear 6182 such that, as the horizontal driven gear 6172 is rotated about horizontal pivot axis HA, the horizontal push cable 6172 applies a first pivot motion to the articulation control ring 6164. Likewise, the vertical gear arrangement 6190 includes a vertical driven gear 6192 that is pivotally supported on a vertical shaft 6191 attached to the proximal portion of the proximal spine portion 6110 for pivotal travel about a vertical pivot axis VA. The proximal end of the vertical push cable 6174 is pivotally attached to the vertical driven gear 6192 such that as the vertical driven gear 6192 is rotated about vertical pivot axis VA, the vertical push cable 6174 applies a second pivot motion to the articulation control ring 6164. The horizontal driven gear 6182 and the vertical driven gear 6192 are driven by an articulation gear train 6300 that operably interfaces with an articulation shifter assembly 6320. In at least one form, the articulation shifter assembly comprises an articulation drive gear 6322 that is coupled to a corresponding one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 6202. See 26. Thus, application of a rotary input motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding driven element 1304 will cause rotation of the articulation drive gear 6322 when the interface 1230 is coupled to the tool holder 1270. An articulation driven gear 6324 is attached to a splined shifter shaft 6330 that is rotatably supported on the tool mounting plate 6202. The articulation driven gear 6324 is in meshing engagement with the articulation drive gear 6322 as shown. Thus, rotation of the articulation drive gear 6322 will result in the rotation of the shaft 6330. In various forms, a shifter driven gear assembly 6340 is movably supported on the splined portion 6332 of the shifter shaft 6330. In various embodiments, the shifter driven gear assembly 6340 includes a driven shifter gear 6342 that is attached to a shifter plate 6344. The shifter plate 6344 operably interfaces with a shifter solenoid assembly 6350. The shifter solenoid assembly 6350 is coupled to corresponding pins 6352 by conductors 6352. See 131. Pins 6352 are oriented to electrically communicate with slots 1258 25 on the tool side 1244 of the adaptor 1240. Such arrangement serves to electrically couple the shifter solenoid assembly 6350 to the robotic controller 1001. Thus, activation of the shifter solenoid 6350 will shift the shifter driven gear assembly 6340 on the splined portion 6332 of the shifter shaft 6330 as represented by arrow S in 131 and 132. Various embodiments of the articulation gear train 6300 further include a horizontal gear assembly 6360 that includes a first horizontal drive gear 6362 that is mounted on a shaft 6361 that is rotatably attached to the tool mounting plate 6202. The first horizontal drive gear 6362 is supported in meshing engagement with a second horizontal drive gear 6364. As can be seen in 137, the horizontal driven gear 6182 is in meshing engagement with the distal face portion 6365 of the second horizontal driven gear 6364. Various embodiments of the articulation gear train 6300 further include a vertical gear assembly 6370 that includes a first vertical drive gear 6372 that is mounted on a shaft 6371 that is rotatably supported on the tool mounting plate 6202. The first vertical drive gear 6372 is supported in meshing engagement with a second vertical drive gear 6374 that is concentrically supported with the second horizontal drive gear 6364. The second vertical drive gear 6374 is rotatably supported on the proximal spine portion 6110 for travel therearound. The second horizontal drive gear 6364 is rotatably supported on a portion of said second vertical drive gear 6374 for independent rotatable travel thereon. As can be seen in 132, the vertical driven gear 6192 is in meshing engagement with the distal face portion 6375 of the second vertical driven gear 6374. In various forms, the first horizontal drive gear 6362 has a first diameter and the first vertical drive gear 6372 has a second diameter. As can be seen in 131 and 132, the shaft 6361 is not on a common axis with shaft 6371. That is, the first horizontal driven gear 6362 and the first vertical driven gear 6372 do not rotate about a common axis. Thus, when the shifter gear 6342 is positioned in a center locking position such that the shifter gear 6342 is in meshing engagement with both the first horizontal driven gear 6362 and the first vertical drive gear 6372, the components of the articulation system 6140 are locked in position. Thus, the shiftable shifter gear 6342 and the arrangement of first horizontal and vertical drive gears 6362, 6372 as well as the articulation shifter assembly 6320 collectively may be referred to as an articulation locking system, generally designated as 6380. In use, the robotic controller 1001 of the robotic system 1000 may control the articulation system 6140 as follows. To articulate the end effector 6012 to the left about the first tool articulation axis TA1-TA1, the robotic controller 1001 activates the shifter solenoid assembly 6350 to bring the shifter gear 6342 into meshing engagement with the first horizontal drive gear 6362. Thereafter, the controller 1001 causes a first rotary output motion to be applied to the articulation drive gear 6322 to drive the shifter gear in a first direction to ultimately drive the horizontal driven gear 6182 in another first direction. The horizontal driven gear 6182 is driven to pivot the articulation ring 6164 on the ball-shaped portion 6162 to thereby pull right upper cable 6144 and the right lower cable 6146 in the proximal direction PD. To articulate the end effector 6012 to the right about the first tool articulation axis TA1-TA1, the robotic controller 1001 activates the shifter solenoid assembly 6350 to bring the shifter gear 6342 into meshing engagement with the first horizontal drive gear 6362. Thereafter, the controller 1001 causes the first rotary output motion in an opposite direction to be applied to the articulation drive gear 6322 to drive the shifter gear 6342 in a second direction to ultimately drive the horizontal driven gear 6182 in another second direction. Such actions result in the articulation control ring 6164 moving in such a manner as to pull the left upper cable 6150 and the left lower cable 6152 in the proximal direction PD. In various embodiments the gear ratios and frictional forces generated between the gears of the vertical gear assembly 6370 serve to prevent rotation of the vertical driven gear 6192 as the horizontal gear assembly 6360 is actuated. To articulate the end effector 6012 in the upper direction about the second tool articulation axis TA2-TA2, the robotic controller 1001 activates the shifter solenoid assembly 6350 to bring the shifter gear 6342 into meshing engagement with the first vertical drive gear 6372. Thereafter, the controller 1001 causes the first rotary output motion to be applied to the articulation drive gear 6322 to drive the shifter gear 6342 in a first direction to ultimately drive the vertical driven gear 6192 in another first direction. The vertical driven gear 6192 is driven to pivot the articulation ring 6164 on the ball-shaped portion 6162 of the proximal spine portion 6110 to thereby pull right upper cable 6144 and the left upper cable 6150 in the proximal direction PD. To articulate the end effector 6012 in the downward direction about the second tool articulation axis TA2-TA2, the robotic controller 1001 activates the shifter solenoid assembly 6350 to bring the shifter gear 6342 into meshing engagement with the first vertical drive gear 6372. Thereafter, the controller 1001 causes the first rotary output motion to be applied in an opposite direction to the articulation drive gear 6322 to drive the shifter gear 6342 in a second direction to ultimately drive the vertical driven gear 6192 in another second direction. Such actions thereby cause the articulation control ring 6164 to pull the right lower cable 6146 and the left lower cable 6152 in the proximal direction PD. In various embodiments, the gear ratios and frictional forces generated between the gears of the horizontal gear assembly 6360 serve to prevent rotation of the horizontal driven gear 6182 as the vertical gear assembly 6370 is actuated. In various embodiments, a variety of sensors may communicate with the robotic controller 1001 to determine the articulated position of the end effector 6012. Such sensors may interface with, for example, the articulation joint 6100 or be located within the tool mounting portion 6200. For example, sensors may be employed to detect the position of the articulation control ring 6164 on the ball-shaped portion 6162 of the proximal spine portion 6110. Such feedback from the sensors to the controller 1001 permits the controller 1001 to adjust the amount of rotation and the direction of the rotary output to the articulation drive gear 6322. Further, as indicated above, when the shifter drive gear 6342 is centrally positioned in meshing engagement with the first horizontal drive gear 6362 and the first vertical drive gear 6372, the end effector 6012 is locked in the articulated position. Thus, after the desired amount of articulation has been attained, the controller 1001 may activate the shifter solenoid assembly 6350 to bring the shifter gear 6342 into meshing engagement with the first horizontal drive gear 6362 and the first vertical drive gear 6372. In alternative embodiments, the shifter solenoid assembly 6350 may be spring activated to the central locked position. In use, it may be desirable to rotate the surgical end effector 6012 about the longitudinal tool axis LT-LT. In at least one embodiment, the transmission arrangement 6204 on the tool mounting portion includes a rotational transmission assembly 6400 that is configured to receive a corresponding rotary output motion from the tool drive assembly 1010 of the robotic system 1000 and convert that rotary output motion to a rotary control motion for rotating the elongated shaft assembly 6008 and surgical end effector 6012 about the longitudinal tool axis LT-LT. In various embodiments, for example, a proximal end portion 6041 of the proximal closure tube 6040 is rotatably supported on the tool mounting plate 6202 of the tool mounting portion 6200 by a forward support cradle 6205 and a closure sled 6510 that is also movably supported on the tool mounting plate 6202. In at least one form, the rotational transmission assembly 6400 includes a tube gear segment 6402 that is formed on or attached to the proximal end 6041 of the proximal closure tube 6040 for operable engagement by a rotational gear assembly 6410 that is operably supported on the tool mounting plate 6202. As can be seen in 131, the rotational gear assembly 6410, in at least one embodiment, comprises a rotation drive gear 6412 that is coupled to a corresponding second one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 6202 when the tool mounting portion 6200 is coupled to the tool drive assembly 1010. See 26. The rotational gear assembly 6410 further comprises a first rotary driven gear 6414 that is rotatably supported on the tool mounting plate 6202 in meshing engagement with the rotation drive gear 6412. The first rotary driven gear 6414 is attached to a drive shaft 6416 that is rotatably supported on the tool mounting plate 6202. A second rotary driven gear 6418 is attached to the drive shaft 6416 and is in meshing engagement with tube gear segment 6402 on the proximal closure tube 6040. Application of a second rotary output motion from the tool drive assembly 1010 of the robotic system 1000 to the corresponding driven element 1304 will thereby cause rotation of the rotation drive gear 6412. Rotation of the rotation drive gear 6412 ultimately results in the rotation of the elongated shaft assembly 6008 and the surgical end effector 6012 about the longitudinal tool axis LT-LT. It will be appreciated that the application of a rotary output motion from the tool drive assembly 1010 in one direction will result in the rotation of the elongated shaft assembly 6008 and surgical end effector 6012 about the longitudinal tool axis LT-LT in a first direction and an application of the rotary output motion in an opposite direction will result in the rotation of the elongated shaft assembly 6008 and surgical end effector 6012 in a second direction that is opposite to the first direction. In at least one embodiment, the closure of the anvil 2024 relative to the staple cartridge 2034 is accomplished by axially moving a closure portion of the elongated shaft assembly 2008 in the distal direction DD on the spine assembly 2049. As indicated above, in various embodiments, the proximal end portion 6041 of the proximal closure tube 6040 is supported by the closure sled 6510 which comprises a portion of a closure transmission, generally depicted as 6512. As can be seen in 131, the proximal end portion 6041 of the proximal closure tube portion 6040 has a collar 6048 formed thereon. The closure sled 6510 is coupled to the collar 6048 by a yoke 6514 that engages an annular groove 6049 in the collar 6048. Such arrangement serves to enable the collar 6048 to rotate about the longitudinal tool axis LT-LT while still being coupled to the closure transmission 6512. In various embodiments, the closure sled 6510 has an upstanding portion 6516 that has a closure rack gear 6518 formed thereon. The closure rack gear 6518 is configured for driving engagement with a closure gear assembly 6520. See 131. In various forms, the closure gear assembly 6520 includes a closure spur gear 6522 that is coupled to a corresponding second one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 6202. See 26. Thus, application of a third rotary output motion from the tool drive assembly 1010 of the robotic system 1000 to the corresponding second driven element 1304 will cause rotation of the closure spur gear 6522 when the tool mounting portion 6202 is coupled to the tool drive assembly 1010. The closure gear assembly 6520 further includes a closure reduction gear set 6524 that is supported in meshing engagement with the closure spur gear 6522 and the closure rack gear 2106. Thus, application of a third rotary output motion from the tool drive assembly 1010 of the robotic system 1000 to the corresponding second driven element 1304 will cause rotation of the closure spur gear 6522 and the closure transmission 6512 and ultimately drive the closure sled 6510 and the proximal closure tube 6040 axially on the proximal spine portion 6110. The axial direction in which the proximal closure tube 6040 moves ultimately depends upon the direction in which the third driven element 1304 is rotated. For example, in response to one rotary output motion received from the tool drive assembly 1010 of the robotic system 1000, the closure sled 6510 will be driven in the distal direction DD and ultimately drive the proximal closure tube 6040 in the distal direction DD. As the proximal closure tube 6040 is driven distally, the distal closure tube 6042 is also driven distally by virtue of it connection with the proximal closure tube 6040. As the distal closure tube 6042 is driven distally, the end of the closure tube 6042 will engage a portion of the anvil 6024 and cause the anvil 6024 to pivot to a closed position. Upon application of an opening out put motion from the tool drive assembly 1010 of the robotic system 1000, the closure sled 6510 and the proximal closure tube 6040 will be driven in the proximal direction PD on the proximal spine portion 6110. As the proximal closure tube 6040 is driven in the proximal direction PD, the distal closure tube 6042 will also be driven in the proximal direction PD. As the distal closure tube 6042 is driven in the proximal direction PD, the opening 6045 therein interacts with the tab 6027 on the anvil 6024 to facilitate the opening thereof. In various embodiments, a spring not shown may be employed to bias the anvil 6024 to the open position when the distal closure tube 6042 has been moved to its starting position. In various embodiments, the various gears of the closure gear assembly 6520 are sized to generate the necessary closure forces needed to satisfactorily close the anvil 6024 onto the tissue to be cut and stapled by the surgical end effector 6012. For example, the gears of the closure transmission 6520 may be sized to generate approximately 70-120 pounds of closure forces. In various embodiments, the cutting instrument is driven through the surgical end effector 6012 by a knife bar 6530. See 131. In at least one form, the knife bar 6530 is fabricated with a joint arrangement not shown and/or is fabricated from material that can accommodate the articulation of the surgical end effector 6102 about the first and second tool articulation axes while remaining sufficiently rigid so as to push the cutting instrument through tissue clamped in the surgical end effector 6012. The knife bar 6530 extends through a hollow passage 6532 in the proximal spine portion 6110. In various embodiments, a proximal end 6534 of the knife bar 6530 is rotatably affixed to a knife rack gear 6540 such that the knife bar 6530 is free to rotate relative to the knife rack gear 6540. The distal end of the knife bar 6530 is attached to the cutting instrument in the various manners described above. As can be seen in 131, the knife rack gear 6540 is slidably supported within a rack housing 6542 that is attached to the tool mounting plate 6202 such that the knife rack gear 6540 is retained in meshing engagement with a knife drive transmission portion 6550 of the transmission arrangement 6204. In various embodiments, the knife drive transmission portion 6550 comprises a knife gear assembly 6560. More specifically and with reference to 131, in at least one embodiment, the knife gear assembly 6560 includes a knife spur gear 6562 that is coupled to a corresponding fourth one of the driven discs or elements 1304 on the adapter side 1307 of the tool mounting plate 6202. See 26. Thus, application of another rotary output motion from the robotic system 1000 through the tool drive assembly 1010 to the corresponding fourth driven element 1304 will cause rotation of the knife spur gear 6562. The knife gear assembly 6560 further includes a knife gear reduction set 6564 that includes a first knife driven gear 6566 and a second knife drive gear 6568. The knife gear reduction set 6564 is rotatably mounted to the tool mounting plate 6202 such that the first knife driven gear 6566 is in meshing engagement with the knife spur gear 6562. Likewise, the second knife drive gear 6568 is in meshing engagement with a third knife drive gear assembly 6570. As shown in 131, the second knife driven gear 6568 is in meshing engagement with a fourth knife driven gear 6572 of the third knife drive gear assembly 6570. The fourth knife driven gear 6572 is in meshing engagement with a fifth knife driven gear assembly 6574 that is in meshing engagement with the knife rack gear 6540. In various embodiments, the gears of the knife gear assembly 6560 are sized to generate the forces needed to drive the cutting instrument through the tissue clamped in the surgical end effector 6012 and actuate the staples therein. For example, the gears of the knife gear assembly 6560 may be sized to generate approximately 40 to 100 pounds of driving force. It will be appreciated that the application of a rotary output motion from the tool drive assembly 1010 in one direction will result in the axial movement of the cutting instrument in a distal direction and application of the rotary output motion in an opposite direction will result in the axial travel of the cutting instrument in a proximal direction. As can be appreciated from the foregoing description, the surgical tool 6000 represents a vast improvement over prior robotic tool arrangements. The unique and novel transmission arrangement employed by the surgical tool 6000 enables the tool to be operably coupled to a tool holder portion 1010 of a robotic system that only has four rotary output bodies, yet obtain the rotary output motions therefrom to: i articulate the end effector about two different articulation axes that are substantially transverse to each other as well as the longitudinal tool axis; ii rotate the end effector 6012 about the longitudinal tool axis; iii close the anvil 6024 relative to the surgical staple cartridge 6034 to varying degrees to enable the end effector 6012 to be used to manipulate tissue and then clamp it into position for cutting and stapling; and iv firing the cutting instrument to cut through the tissue clamped within the end effector 6012. The unique and novel shifter arrangements of various embodiments of the present invention described above enable two different articulation actions to be powered from a single rotatable body portion of the robotic system. The various embodiments of the present invention have been described above in connection with cutting-type surgical instruments. It should be noted, however, that in other embodiments, the inventive surgical instrument disclosed herein need not be a cutting-type surgical instrument, but rather could be used in any type of surgical instrument including remote sensor transponders. For example, it could be a non-cutting endoscopic instrument, a grasper, a stapler, a clip applier, an access device, a drug/gene therapy delivery device, an energy device using ultrasound, RF, laser, etc. In addition, the present invention may be in laparoscopic instruments, for example. The present invention also has application in conventional endoscopic and open surgical instrumentation as well as robotic-assisted surgery. 133 depicts use of various aspects of certain embodiments of the present invention in connection with a surgical tool 7000 that has an ultrasonically powered end effector 7012. The end effector 7012 is operably attached to a tool mounting portion 7100 by an elongated shaft assembly 7008. The tool mounting portion 7100 may be substantially similar to the various tool mounting portions described hereinabove. In one embodiment, the end effector 7012 includes an ultrasonically powered jaw portion 7014 that is powered by alternating current or direct current in a known manner. Such ultrasonically-powered devices are disclosed, for example, in 6,783,524, entitled ROBOTIC SURGICAL TOOL WITH ULTRASOUND CAUTERIZING AND CUTTING INSTRUMENT, the entire disclosure of which is herein incorporated by reference. In the illustrated embodiment, a separate power cord 7020 is shown. It will be understood, however, that the power may be supplied thereto from the robotic controller 1001 through the tool mounting portion 7100. The surgical end effector 7012 further includes a movable jaw 7016 that may be used to clamp tissue onto the ultrasonic jaw portion 7014. The movable jaw portion 7016 may be selectively actuated by the robotic controller 1001 through the tool mounting portion 7100 in anyone of the various manners herein described. 134 illustrates use of various aspects of certain embodiments of the present invention in connection with a surgical tool 8000 that has an end effector 8012 that comprises a linear stapling device. The end effector 8012 is operably attached to a tool mounting portion 8100 by an elongated shaft assembly 3700 of the type and construction describe above. However, the end effector 8012 may be attached to the tool mounting portion 8100 by a variety of other elongated shaft assemblies described herein. In one embodiment, the tool mounting portion 8100 may be substantially similar to tool mounting portion 3750. However, various other tool mounting portions and their respective transmission arrangements describe in detail herein may also be employed. Such linear stapling head portions are also disclosed, for example, in 7,673,781, entitled SURGICAL STAPLING DEVICE WITH STAPLE DRIVER THAT SUPPORTS MULTIPLE WIRE DIAMETER STAPLES, the entire disclosure of which is herein incorporated by reference. Various sensor embodiments described in 8,167,185, the disclosure of which is herein incorporated by reference in its entirety, may be employed with many of the surgical tool embodiments disclosed herein. As was indicated above, the master controller 1001 generally includes master controllers generally represented by 1003 which are grasped by the surgeon and manipulated in space while the surgeon views the procedure via a stereo display 1002. See 17. The master controllers 1001 are manual input devices which preferably move with multiple degrees of freedom, and which often further have an actuatable handle for actuating the surgical tools. Some of the surgical tool embodiments disclosed herein employ a motor or motors in their tool drive portion to supply various control motions to the tool's end effector. Such embodiments may also obtain additional control motions from the motor arrangement employed in the robotic system components. Other embodiments disclosed herein obtain all of the control motions from motor arrangements within the robotic system. Such motor powered arrangements may employ various sensor arrangements that are disclosed in the published US patent application cited above to provide the surgeon with a variety of forms of feedback without departing from the spirit and scope of the present invention. For example, those master controller arrangements 1003 that employ a manually actuatable firing trigger can employ run motor sensors to provide the surgeon with feedback relating to the amount of force applied to or being experienced by the cutting member. The run motor sensors may be configured for communication with the firing trigger portion to detect when the firing trigger portion has been actuated to commence the cutting/stapling operation by the end effector. The run motor sensor may be a proportional sensor such as, for example, a rheostat or variable resistor. When the firing trigger is drawn in, the sensor detects the movement, and sends an electrical signal indicative of the voltage or power to be supplied to the corresponding motor. When the sensor is a variable resistor or the like, the rotation of the motor may be generally proportional to the amount of movement of the firing trigger. That is, if the operator only draws or closes the firing trigger in a small amount, the rotation of the motor is relatively low. When the firing trigger is fully drawn in or in the fully closed position, the rotation of the motor is at its maximum. In other words, the harder the surgeon pulls on the firing trigger, the more voltage is applied to the motor causing greater rates of rotation. Other arrangements may provide the surgeon with a feed back meter 1005 that may be viewed through the display 1002 and provide the surgeon with a visual indication of the amount of force being applied to the cutting instrument or dynamic clamping member. Other sensor arrangements may be employed to provide the master controller 1001 with an indication as to whether a staple cartridge has been loaded into the end effector, whether the anvil has been moved to a closed position prior to firing, etc. In alternative embodiments, a motor-controlled interface may be employed in connection with the controller 1001 that limit the maximum trigger pull based on the amount of loading , clamping force, cutting force, experienced by the surgical end effector. For example, the harder it is to drive the cutting instrument through the tissue clamped within the end effector, the harder it would be to pull/actuate the activation trigger. In still other embodiments, the trigger on the controller 1001 is arranged such that the trigger pull location is proportionate to the end effector-location/condition. For example, the trigger is only fully depressed when the end effector is fully fired. The devices disclosed herein can be designed to be disposed of after a single use, or they can be designed to be used multiple times. In either case, however, the device can be reconditioned for reuse after at least one use. Reconditioning can include any combination of the steps of disassembly of the device, followed by cleaning or replacement of particular pieces, and subsequent reassembly. In particular, the device can be disassembled, and any number of the particular pieces or parts of the device can be selectively replaced or removed in any combination. Upon cleaning and/or replacement of particular parts, the device can be reassembled for subsequent use either at a reconditioning facility, or by a surgical team immediately prior to a surgical procedure. Those skilled in the art will appreciate that reconditioning of a device can utilize a variety of techniques for disassembly, cleaning/replacement, and reassembly. Use of such techniques, and the resulting reconditioned device, are all within the scope of the present application. Although the present invention has been described herein in connection with certain disclosed embodiments, many modifications and variations to those embodiments may be implemented. For example, different types of end effectors may be employed. Also, where materials are disclosed for certain components, other materials may be used. The foregoing description and following claims are intended to cover all such modification and variations. Any patent, publication, or other disclosure material, in whole or in part, that is said to be incorporated by reference herein is incorporated herein only to the extent that the incorporated materials does not conflict with existing definitions, statements, or other disclosure material set forth in this disclosure. As such, and to the extent necessary, the disclosure as explicitly set forth herein supersedes any conflicting material incorporated herein by reference. Any material, or portion thereof, that is said to be incorporated by reference herein, but which conflicts with existing definitions, statements, or other disclosure material set forth herein will only be incorporated to the extent that no conflict arises between that incorporated material and the existing disclosure material.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/70/394/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9759693741798401,
                    "F_SPEC_PARAMS": [
                        "post-operative recovery time",
                        "complications"
                    ],
                    "S_SPEC_PARAMS": [
                        "assure proper operation",
                        "user-feedback effects are not suitably realizable"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "<s> Endoscopic surgical instruments are often preferred over traditional open surgical devices since the use of a natural orifice tends to reduce the post-operative recovery time and complications."
                    ],
                    "S_SENTS": [
                        "However, surgeons typically prefer to experience feedback from the working end to assure proper operation of the end effector.",
                        "The user-feedback effects are not suitably realizable in present motor-driven devices."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Side Effects"
                    ],
                    "F_SIM_SCORE": 0.637123167514801,
                    "S_TRIZ_PARAMS": [
                        "Level of Automation"
                    ],
                    "S_SIM_SCORE": 0.5205965638160706,
                    "GLOBAL_SCORE": 1.7548292398452758
                },
                "sort": [
                    1.7548293
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10913156-20210209",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10913156-20210209",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-09-24",
                    "PUBLICATION_DATE": "2021-02-09",
                    "INVENTORS": [
                        "Michael Nahum",
                        "Casey Edward Emtman"
                    ],
                    "APPLICANTS": [
                        "Mitutoyo Corporation    ( Kanagawa-ken , JP )"
                    ],
                    "INVENTION_TITLE": "Robot system with end tool metrology position coordinates determination system",
                    "DOMAIN": "B25J 91664",
                    "ABSTRACT": "An end tool metrology position coordinates determination system is provided for use with a robot. A first accuracy level defined as a robot accuracy , for controlling and sensing an end tool position of an end tool that is mounted proximate to a distal end of a movable arm configuration of the robot is based on using position sensors , encoders included in the robot. The system includes the end tool, an imaging configuration, XY scale, image triggering portion and processing portion. One of the XY scale or imaging configuration is coupled to the end tool and the other is coupled to a stationary element , a frame located above the robot. The imaging configuration acquires an image of the XY scale, which is utilized to determine a relative position that is indicative of the end tool position, with an accuracy level that is better than the robot accuracy.",
                    "CLAIMS": "1. An end tool metrology position coordinates determination system for use in conjunction with a robot, the robot comprising: a movable arm configuration, wherein the movable arm configuration comprises an end tool mounting configuration that is located proximate to a distal end of the movable arm configuration, and the robot is configured to move the movable arm configuration so as to move at least a portion of an end tool that is mounted to the end tool mounting configuration along at least two dimensions in an end tool working volume; and a motion control system configured to control an end tool position with a level of accuracy defined as a robot accuracy, based at least in part on sensing and controlling a position of the movable arm configuration using at least one position sensor included in the robot, the end tool metrology position coordinates determination system comprising: an end tool configured to mount to the end tool mounting configuration proximate to the distal end of the movable arm configuration; a first imaging configuration comprising a first camera, the first imaging configuration having an optical axis; an XY scale comprising a nominally planar substrate and a plurality of respective imageable features distributed on the planar substrate, wherein the respective imageable features are located at respective known XY scale coordinates on the XY scale; an image triggering portion configured to input at least one input signal that is related to the end tool position and determine a timing of a first imaging trigger signal based on the at least one input signal and output the first imaging trigger signal to the first imaging configuration, wherein the first imaging configuration is configured to acquire a digital image of the XY scale at an image acquisition time in response to receiving the first imaging trigger signal, and a metrology position coordinate processing portion configured to input the acquired image and identify at least one respective imageable feature included in the acquired image of the XY scale, and the related respective known XY scale coordinate location, wherein: the end tool metrology position coordinates determination system is configured with a movable one of the XY scale or the first imaging configuration coupled to the end tool and the other configured to be coupled to a stationary element proximate to the robot and stationary, with the stationary one of the XY scale or the first imaging configuration defining a first reference position; in an operational configuration of the end tool metrology position coordinates determination system, a scale plane is defined to nominally coincide with the planar substrate of the XY scale, and a direction normal to the scale plane is defined as a scale imaging axis direction, and at least one of the XY scale or the first imaging configuration is arranged with the optical axis of the first imaging configuration parallel to a direction of the scale imaging axis direction and with the scale plane located within a range of focus of the first imaging configuration along the scale imaging axis direction; the end tool metrology position coordinates determination system is configured such that when the moveable one of the XY scale or the first imaging configuration and the stationary one of the XY scale or the first imaging configuration are arranged in the operational configuration, and the movable arm configuration is positioned with the XY scale in a field of view of the first imaging configuration, then the metrology position coordinate processing portion is operable to determine a relative position between the movable one of the XY scale or the first imaging configuration and the first reference position with an accuracy level that is better than the robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image; and the determined relative position is indicative of the metrology position coordinates of the end tool position at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. 2. The end tool metrology position coordinates determination system of claim 1, wherein: the robot is configured to move the end tool and the movable one of the XY scale or the first imaging configuration in a plane parallel to the scale plane, while the end tool metrology position coordinates determination system is in the operational configuration. 3. The end tool metrology position coordinates determination system of claim 2, wherein: the robot comprises at least one respective rotary joint that provides at least one respective rotational degree of freedom for the end tool; and the robot is configured to translate and rotate the movable one of the XY scale or the first imaging configuration relative to the stationary one of the XY scale or the first imaging configuration, including rotating the end tool corresponding to the at least one respective rotational degree of freedom, to at least nominally provide the operational configuration. 4. The end tool metrology position coordinates determination system of claim 1, further comprising a bracket that couples the movable one of the XY scale or the first imaging configuration to the end tool. 5. The end tool metrology position coordinates determination system of claim 1, wherein the XY scale is coupled to the end tool and the first imaging configuration is coupled to the stationary element. 6. The end tool metrology position coordinates determination system of claim 5, wherein the end tool comprises a stylus with a contact point for contacting a surface of a workpiece, the XY scale is located primarily on one side of the end tool or the stylus, and the end tool is configured to be rotated so that the contact point may be moved into contact with the workpiece without the XY scale interfering. 7. The end tool metrology position coordinates determination system of claim 5, wherein the end tool comprises a stylus with a contact point for contacting a surface of a workpiece, and the XY scale at least partially surrounds at least one of the end tool or the stylus in the scale plane. 8. The end tool metrology position coordinates determination system of claim 5, wherein the stationary element comprises a frame arranged above at least a portion of the end tool working volume, and the first imaging configuration is fixed to the frame above a portion of the end tool working volume. 9. The end tool metrology position coordinates determination system of claim 1, wherein when the end tool is a touch probe that is used for measuring a workpiece and that outputs a touch signal when it touches the workpiece, and the image triggering portion is configured to input the touch signal or a signal derived therefrom as its at least one input signal. 10. The end tool metrology position coordinates determination system of claim 9, wherein a central axis of the touch probe is at least nominally parallel to the scale imaging axis direction. 11. The end tool metrology position coordinates determination system of claim 1, wherein when the end tool is a scanning probe that is used for measuring a workpiece and that provides respective workpiece measurement sample data corresponding to a respective sample timing signal, the image triggering portion is configured to input that respective sample timing signal or a signal derived therefrom as its at least one input signal. 12. The end tool metrology position coordinates determination system of claim 1, wherein when the end tool is a camera that is used to provide a respective workpiece measurement image corresponding to a respective workpiece image acquisition signal, the image triggering portion is configured to input that workpiece image acquisition signal or a signal derived therefrom as its at least one input signal. 13. The end tool metrology position coordinates determination system of claim 1, wherein the end tool metrology position coordinates determination system is configured to determine the metrology position coordinates of the end tool position at the image acquisition time, based on the determined relative position and a known coordinate position offset between the end tool position and the movable one of the XY scale or the first imaging configuration. 14. The end tool metrology position coordinates determination system of claim 1, wherein: the respective imageable features of the XY scale comprise a set of imageable features having unique identifiable patterns, wherein that set of imageable features are distributed on the planar substrate such that they are spaced apart by less than a distance corresponding to a distance across the field of view of the first imaging configuration; and the metrology position coordinate processing portion is configured to identify at least one respective imageable feature included in the acquired image of the XY scale based on its unique identifiable pattern. 15. The end tool metrology position coordinates determination system of claim 1, wherein the metrology position coordinate processing portion is configured to identify at least one respective imageable feature included in the acquired image of the XY scale based its image position in the acquired image and based on robot position data derived from the motion control system corresponding to the image acquisition time. 16. The end tool metrology position coordinates determination system of claim 15, wherein the respective imageable features of the XY scale comprise a set of similar imageable features that are distributed on the planar substrate such that they are spaced apart from one another by a distance that is more than a maximum position error that is allowed within the robot accuracy. 17. A robot system, comprising: a robot, comprising: a movable arm configuration, wherein the movable arm configuration comprises an end tool mounting configuration that is located proximate to a distal end of the movable arm configuration, and the robot is configured to move the movable arm configuration so as to move at least a portion of an end tool that is mounted to the end tool mounting configuration along at least two dimensions in an end tool working volume; and a motion control system configured to control an end tool position with a level of accuracy defined as a robot accuracy, based at least in part on sensing and controlling the position of the movable arm configuration using at least one position sensor included in the robot; and an end tool metrology position coordinates determination system, comprising: an end tool configured to mount to the end tool mounting configuration proximate to the distal end of the movable arm configuration; a first imaging configuration comprising a first camera, the first imaging configuration having an optical axis; an XY scale comprising a nominally planar substrate and a plurality of respective imageable features distributed on the planar substrate, wherein the respective imageable features are located at respective known XY scale coordinates on the XY scale; an image triggering portion configured to input at least one input signal that is related to the end tool position and determine the timing of a first imaging trigger signal based on the at least one input signal and output the first imaging trigger signal to the first imaging configuration, wherein the first imaging configuration is configured to acquire a digital image of the XY scale at an image acquisition time in response to receiving the first imaging trigger signal, and a metrology position coordinate processing portion configured to input the acquired image and identify at least one respective imageable feature included in the acquired image of the XY scale, and the related respective known XY scale coordinate location, wherein: the end tool metrology position coordinates determination system is configured with a movable one of the XY scale or the first imaging configuration coupled to the end tool and the other is coupled to a stationary element, with the stationary one of the XY scale or the first imaging configuration defining a first reference position; in an operational configuration of the end tool metrology position coordinates determination system, a scale plane is defined to nominally coincide with the planar substrate of the XY scale, and a direction normal to the scale plane is defined as a scale imaging axis direction, and at least one of the XY scale or the first imaging configuration is arranged with the optical axis of the first imaging configuration parallel to the direction of the scale imaging axis direction and with the scale plane located within the range of focus of the first imaging configuration along the scale imaging axis direction; the end tool metrology position coordinates determination system is configured such that when the moveable one of the XY scale or the first imaging configuration and the stationary one of the XY scale or the first imaging configuration are arranged in the operational configuration, and the movable arm configuration is positioned with the XY scale in a field of view of the first imaging configuration, then the metrology position coordinate processing portion is operable to determine a relative position between the movable one of the XY scale or the first imaging configuration and the first reference position with an accuracy level that is better than the robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image; and the determined relative position is indicative of the metrology position coordinates of the end tool position at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. 18. The robot system of claim 17, wherein the robot is a SCARA type robot and the movable arm configuration comprises a terminal arm portion and the distal end of the terminal arm portion corresponds to the distal end of the movable arm configuration, and the end tool mounting configuration is located proximate to the distal end of the terminal arm portion. 19. The robot system of claim 18, wherein the movable arm configuration further comprises: a first arm portion mounted to a first rotary joint at a proximal end of the first arm portion, the first rotary joint having a rotary axis nominally aligned along a z axis direction such that the first arm portion moves about the first rotary joint nominally in an x-y plane that is perpendicular to the z axis, wherein the first arm portion has a second rotary joint located at a distal end of the first arm portion, the second rotary joint having its rotary axis nominally aligned along the z axis direction; a second arm portion mounted to the second rotary joint at a proximal end of the second arm portion, such that the second arm portion moves about the second rotary joint nominally in an x-y plane that is perpendicular to the z axis; and wherein: the terminal arm portion is a third portion that is coupled proximate to the distal end of the second arm portion and is configured to provide at least one of motion nominally along the z axis direction or rotation of the end tool nominally around the z axis direction; and the scale plane is configured to be nominally perpendicular to the z axis direction and the scale imaging axis direction nominally corresponds to the z axis direction. 20. The robot system of claim 19, wherein the XY scale is coupled to the end tool and the first imaging configuration is coupled to the stationary element. 21. A method for operating an end tool metrology position coordinates determination system that is utilized with a robot, the robot comprising: a movable arm configuration, wherein the movable arm configuration comprises an end tool mounting configuration that is located proximate to a distal end of the movable arm configuration, and the robot is configured to move the movable arm configuration so as to move at least a portion of an end tool that is mounted to the end tool mounting configuration along at least two dimensions in an end tool working volume; and a motion control system configured to control an end tool position with a level of accuracy defined as a robot accuracy, based at least in part on sensing and controlling the position of the movable arm configuration using at least one position sensor included in the robot, the end tool metrology position coordinates determination system comprising: an end tool configured to mount to the end tool mounting configuration proximate to the distal end of the movable arm configuration; a first imaging configuration comprising a first camera, the first imaging configuration having an optical axis; an XY scale comprising a nominally planar substrate and a plurality of respective imageable features distributed on the planar substrate, wherein the respective imageable features are located at respective known XY scale coordinates on the XY scale; an image triggering portion; and a metrology position coordinate processing portion; wherein: the end tool metrology position coordinates determination system is configured with a movable one of the XY scale or the first imaging configuration coupled to the end tool and the other configured to be coupled to a stationary element proximate to the robot, with the stationary one of the XY scale or the first imaging configuration defining a first reference position; in an operational configuration of the end tool metrology position coordinates determination system, a scale plane is defined to nominally coincide with the planar substrate of the XY scale, and a direction normal to the scale plane is defined as a scale imaging axis direction, and at least one of the XY scale or the first imaging configuration is arranged with the optical axis of the first imaging configuration parallel to the direction of the scale imaging axis direction and with the scale plane located within the range of focus of the first imaging configuration along the scale imaging axis direction; and the end tool metrology position coordinates determination system is configured such that when the moveable one of the XY scale or the first imaging configuration and the stationary one of the XY scale or the first imaging configuration are arranged in the operational configuration, and the movable arm configuration is positioned with the XY scale in a field of view of the first imaging configuration, then the metrology position coordinate processing portion is operable to determine a relative position between the movable one of the XY scale or the first imaging configuration and the first reference position with an accuracy level that is better than the robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image; the method comprising: receiving at the image triggering portion at least one input signal that is related to the end tool position and determining the timing of a first imaging trigger signal based on the at least one input signal and outputting the first imaging trigger signal to the first imaging configuration, wherein the first imaging configuration acquires a digital image of the XY scale at an image acquisition time in response to receiving the first imaging trigger signal; receiving at the metrology position coordinate processing portion the acquired image and identifying at least one respective imageable feature included in the acquired image of the XY scale, and the related respective known XY scale coordinate location; and determining a relative position between the movable one of the XY scale or the first imaging configuration and the first reference position with an accuracy level that is better than the robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image, wherein the determined relative position is indicative of the metrology position coordinates of the end tool position at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. 22. The method of claim 21, further comprising utilizing the determined relative position for at least one of measuring a workpiece or determining a position of the movable arm configuration.",
                    "FIELD_OF_INVENTION": "This disclosure relates to robot systems, and more particularly to systems for determining coordinates of an end tool position of a robot.",
                    "STATE_OF_THE_ART": "Robotic systems are increasingly utilized for manufacturing and other processes. Various types of robots that may be utilized include articulated robots, selective compliance articulated robot arm SCARA robots, cartesian robots, cylindrical robots, spherical robots, etc. As one example of components that may be included in a robot, a SCARA robot system may typically have a base, with a first arm portion rotationally coupled to the base, and a second arm portion rotationally coupled to an end of the first arm portion. In various configurations, an end tool may be coupled to an end of the second arm portion , for performing certain work and/or inspection operations. Such systems may include position sensors , rotary encoders utilized for determining/controlling the positioning of the arm portions and correspondingly the positioning of the end tool. In various implementations, such systems may have a positioning accuracy of approximately 100 microns, as limited by certain factors , the rotary encoder performance in combination with the mechanical stability of the robot system, . 4,725,965, which is hereby incorporated herein by reference in its entirety, discloses certain calibration techniques for improving the accuracy of a SCARA system. As described in the '965 patent, a technique is provided for calibrating a SCARA type robot comprising a first rotatable arm portion and a second rotatable arm portion which carries an end tool. The calibration technique is in relation to the fact that the SCARA robot may be controlled using a kinematic model, which, when accurate, allows the arm portions to be placed in both a first and second angular configuration at which the end tool carried by the second arm portion remains at the same position. To calibrate the kinematic model, the arm portions are placed in a first configuration to locate the end tool above a fixed datum point. Then, the arm portions are placed in a second angular configuration to nominally locate the end tool again in registration with the datum point. The error in the kinematic model is computed from the shift in the position of the end tool from the datum point when the arm portions are switched from the first to the second angular configuration. The kinematic model is then compensated in accordance with the computed error. The steps are repeated until the error reaches zero, at which time the kinematic model of the SCARA robot is considered to be calibrated. As further described in the '965 patent, the calibration technique may include the use of certain cameras. For example, in one implementation, the datum point may be the center of the viewing area of a stationary television camera i. e. , located on the ground below the end tool, and the output signal of the camera may be processed to determine the shift in the position of the end tool from the center of the viewing area of the camera when the links are switched from the first to the second configuration. In another implementation, the second arm portion may carry a camera, and the technique may begin by placing the arm portions in a first angular configuration, at which a second predetermined interior angle is measured between the arm portions, to center the camera carried by the second arm portion directly above a fixed datum point. The arm portions are then placed in a second angular configuration, at which an interior angle, equal to the second predetermined interior angle, is measured between the arm portions, to nominally center the camera again above the datum point. The output signal of the camera is then processed to determine the shift in the position of the datum point, as seen by the camera, upon switching the arm portions from the first to the second angular configuration. The error in the known position of the camera is then determined in accordance with the shift in the position of the datum point as seen by the camera. The steps are then repeated as part of the calibration process until the error approaches zero. While techniques such as those described in the '965 patent may be utilized for calibrating a robot system, in certain applications it may be less desirable to utilize such techniques , which may require significant time and/or may not provide a desired level of accuracy for all possible orientations of a robot during certain operations, . A robot system that can provide improvements with regard to such issues , for increasing the reliability, repeatability, speed, etc. , of the position determination during workpiece measurements and other processes would be desirable.",
                    "SUMMARY": [
                        "This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter. An end tool metrology position coordinates determination system is provided for use in conjunction with a robot as part of a robot system. The robot , an articulated robot, a SCARA robot, a cartesian robot, a cylindrical robot, a spherical robot, includes a movable arm configuration and a motion control system. The movable arm configuration includes an end tool mounting configuration that is located proximate to a distal end of the movable arm configuration. The robot is configured to move the movable arm configuration so as to move at least a portion of an end tool that is mounted to the end tool mounting configuration along at least two dimensions in an end tool working volume. The motion control system is configured to control an end tool position with a level of accuracy defined as a robot accuracy, based at least in part on sensing and controlling the position of the movable arm configuration using at least one position sensor , a rotary encoder, a linear encoder, included in the robot. The end tool metrology position coordinates determination system includes an end tool, a first imaging configuration, an XY scale, an image triggering portion and a metrology position coordinate processing portion. The end tool is configured to mount to the end tool mounting configuration proximate to the distal end of the movable arm configuration. The first imaging configuration includes a first camera and has an optical axis. The XY scale includes a nominally planar substrate and a plurality of respective imageable features distributed on the substrate, wherein the respective imageable features are located at respective known XY scale coordinates on the XY scale. The image triggering portion is configured to input at least one input signal that is related to the end tool position and determine the timing of a first imaging trigger signal based on the at least one input signal and to output the first imaging trigger signal to the first imaging configuration. The first imaging configuration is configured to acquire a digital image of the XY scale at an image acquisition time in response to receiving the first imaging trigger signal. The metrology position coordinate processing portion is configured to input the acquired image and identify at least one respective imageable feature included in the acquired image of the XY scale and the related respective known XY scale coordinate location. In various implementations, the XY scale may be an incremental scale or an absolute scale. The end tool metrology position coordinates determination system is configured with a movable one of the XY scale or the first imaging configuration coupled to the end tool, and the other configured to be coupled to a stationary element proximate to the robot. The stationary one of the XY scale or the first imaging configuration defines a first reference position. In an operational configuration of the end tool metrology position coordinates determination system, a scale plane is defined to nominally coincide with the planar substrate of the XY scale, and a direction normal to the scale plane is defined as a scale imaging axis direction, and at least one of the XY scale or the first imaging configuration is arranged with the optical axis of the first imaging configuration parallel to the direction of the scale imaging axis direction and with the scale plane located within the range of focus of the first imaging configuration along the scale imaging axis direction. The end tool metrology position coordinates determination system is configured such that when the moveable one of the XY scale or the first imaging configuration and the stationary one of the XY scale or the first imaging configuration are arranged in the operational configuration, and the movable arm configuration is positioned with the XY scale in a field of view of the first imaging configuration, then the metrology position coordinate processing portion is operable to determine a relative position between the movable one of the XY scale or the first imaging configuration and the first reference position with an accuracy level that is better than the robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image. The determined relative position is indicative of the metrology position coordinates of the end tool position at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. In various implementations, the end tool metrology position coordinates determination system is configured to determine the metrology position coordinates of the end tool position at the image acquisition time, based on the determined relative position and a known coordinate position offset between the end tool position and the movable one of the XY scale or the first imaging configuration. In various implementations, the robot is configured to move the end tool and the movable one of the XY scale or the first imaging configuration in a plane parallel to the scale plane, while the end tool metrology position coordinates determination system is in the operational configuration. In various implementations, such movement in a plane parallel to the scale plane may be made in two dimensions , x and y dimensions, such as by a SCARA robot, articulated robot, etc. , or alternatively such movement may be primarily made in one dimension , an x or y dimension, such as by a linear robot, etc. In various implementations, the robot may include at least one respective rotary joint that provides at least one respective rotational degree of freedom for the end tool, and the robot may be configured to translate and rotate the movable one of the XY scale or the first imaging configuration relative to the stationary one of the XY scale or the first imaging configuration, including rotating the end tool corresponding to the at least one respective rotational degree of freedom, to provide the operational configuration. In various implementations, the XY scale may be coupled to the end tool and the first imaging configuration may be coupled to the stationary element. In various implementations, the stationary element may comprise a frame arranged above at least a portion of an end tool working volume. The first imaging configuration may be fixed to the frame above a portion of the end tool working volume. In various implementations, the robot system may be operated in either a robot position coordinates mode or an end tool metrology position coordinates mode. The robot position coordinates mode may correspond to an independent and/or standard mode of operation for the robot , a mode in which the robot is operated independently, such as when an end tool metrology position coordinates determination system is not active or is otherwise not provided. In the robot position coordinates mode, the robot movements and corresponding end tool position are controlled and determined with the level of accuracy defined as the robot accuracy i. e. , utilizing the position sensors included in the robot. Conversely, in the end tool metrology position coordinates mode, a relative position may be determined by the end tool metrology position coordinates determination system that is indicative of the metrology position coordinates of the end tool position at an image acquisition time, with an accuracy level that is better than the robot accuracy , better than the accuracy of the position sensors included in the robot, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. In various implementations, the determined position information , the determined relative position, the determined metrology position coordinates of the end tool position and/or other related determined position information may then be utilized for performing a designated function , as part of workpiece measurements, positioning control of the robot, . As an example with respect to positioning control of the robot, during the end tool metrology position coordinates mode, rather than utilizing the position sensors included in the robot for controlling a positioning of the end tool, an output from the end tool metrology position coordinates determination system may be fed back to the motion control system or otherwise utilized to control the positioning of the end tool. In various implementations, a hybrid operation may be implemented, wherein the position sensors included in the robot may be utilized during a first portion of a robot movement timing , for determining/controlling the positions of the arm portions and corresponding end tool position as part of initial/fast/coarse movement positioning. Then, during a second portion of the robot movement timing, rather than utilizing the position sensors of the robot, an output of the end tool metrology position coordinates determination system , the determined relative position or related position information may be fed back to the motion control system or otherwise utilized for controlling the positioning , for determining/controlling the positions of the arm portions and the corresponding end tool position as part of a more accurate final/slower/fine movement positioning.",
                        "1 is a block diagram of a first exemplary implementation of a robot system including an articulated robot and an end tool metrology position coordinates determination system; 2 is an isometric diagram of a portion of a second exemplary implementation of a robot system similar to the robot system of 1 including an articulated robot, in which a first imaging configuration is coupled to a stationary element; 3 is an isometric diagram of a portion of a third exemplary implementation of a robot system including a SCARA robot, in which a first imaging configuration is coupled to a stationary element; 4 is an isometric diagram of a portion of a fourth exemplary implementation of a robot system including a SCARA robot, in which an XY scale is coupled to a stationary element; 5 is an isometric diagram of an exemplary implementation of an incremental XY scale; 6 is an isometric diagram of an exemplary implementation of an absolute XY scale; 7A and 7B are flow diagrams illustrating exemplary implementations of routines for operating a robot system including a robot and an end tool metrology position coordinates determination system; and 8 is a flow diagram illustrating one exemplary implementation of a routine for determining an end tool position in which position sensors may be utilized during a first portion of a movement timing and a determined relative position of an end tool metrology position coordinates determination system may be utilized during a second portion of a movement timing."
                    ],
                    "DESCRIPTION": "1 is a block diagram of a first exemplary implementation of a robot system 100 including a robot 110 and an end tool metrology position coordinates determination system 150. The robot 110 , an articulated robot includes a movable arm configuration MAC and a robot motion control and processing system 140. The end tool metrology position coordinates determination system 150 includes an end tool ETL, a first imaging configuration 160-1, an XY scale 170, an image triggering portion 181 and a metrology position coordinate processing portion 190. In the configuration of 1, the XY scale 170 is coupled to the end tool ETL. As will be described in more detail below, the first imaging configuration 160-1 has a first optical axis OA1 that may be parallel to a scale imaging axis direction SIA when in an operational configuration. In the example of 1, the movable arm configuration MAC includes a lower base portion BSE, arm portions 121-125, motion mechanisms 131-135, position sensors SEN1-SEN5, and an end tool mounting configuration ETMC. As will be described in more detail below and as further illustrated in 2, each of the arm portions 121-125 may have respective proximal ends PE1-PE5 and respective distal ends DE1-DE5. In various implementations, some or all of the arm portions 121-125 may be mounted to respective motion mechanisms 131-135 at respective proximal ends PE1-PE5 of the respective arm portions 121-125. In the example of 1, some or all of the motion mechanisms 131-135 , rotary joints with corresponding motors may enable motion , rotation of the respective arm portions 121-125 , about respective rotary axes RA1-RA5. In various implementations, the position sensors SEN1-SEN5 , rotary encoders may be utilized for determining the positions , angular orientations of the respective arm portions 121-125. In various implementations, the movable arm configuration MAC may have a portion that is designated as a terminal portion , the fifth arm portion 125. In the example configuration of 1, the end tool mounting configuration ETMC is located proximate to , located at the distal end DE5 of the fifth arm portion 125 , designated as the terminal portion, which corresponds to a distal end of the movable arm configuration MAC. In various alternative implementations, a terminal portion of a movable arm configuration may be an element , a rotatable element, that is not an arm portion but for which at least part of the terminal portion corresponds to a distal end of the movable arm configuration where the end tool mounting configuration ETMC is located. In various implementations, the end tool mounting configuration ETMC may include various elements for coupling and maintaining the end tool ETL proximate to the distal end of the movable arm configuration MAC. For example, in various implementations, the end tool mounting configuration ETMC may include an autojoint connection, a magnetic coupling portion and/or other coupling elements as are known in the art for mounting an end tool ETL to a corresponding element. The end tool mounting configuration ETMC may also include electrical connections , a power connection, one or more signal lines, for providing power to and/or sending signals to and from at least part of the end tool ETL , to and from the end tool sensing portion ETSN. In various implementations, the end tool ETL may include the end tool sensing portion ETSN and an end tool stylus ETST with a contact point CP , for contacting a surface of a workpiece WP. The fifth motion mechanism 135 is located proximate to the distal end DE4 of the fourth arm portion 124. In various implementations, the fifth motion mechanism 135 , a rotary joint with a corresponding motor may be configured to rotate the fifth arm portion 125 about a rotary axis RA5 , which in some orientations may be parallel to the optical axis OA1 and/or scale imaging axis direction SIA, such as when so oriented by the rotation of the fourth arm portion 124 by the fourth motion mechanism 134 to be in the operational configuration, . In some implementations, the fifth motion mechanism 135 may also or alternatively include a different type of motion mechanism , a linear actuator that is configured to move the fifth arm portion 125 linearly , up and down in the scale imaging axis direction SIA when so oriented in the operational configuration. In any case, the end tool ETL is mounted to , coupled to the end tool mounting configuration ETMC, and has a corresponding end tool position ETP with corresponding coordinates , x, y and z coordinates. In various implementations, the end tool position ETP may correspond to or be proximate to the position of the end tool mounting configuration ETMC , at or proximate to the distal end DE5 of the fifth arm portion 125, which may correspond to the distal end of the movable arm configuration MAC. The motion control system 140 is configured to control the end tool position ETP of the end tool ETL with a level of accuracy defined as a robot accuracy. More specifically, the motion control system 140 is generally configured to control the coordinates , x, y and z coordinates of the end tool position ETP with the robot accuracy based at least in part on utilizing the motion mechanisms 131-135 and position sensors SEN1-SEN5 for sensing and controlling the positions of the arm portions 121-125. In various implementations, the motion control and processing system 140 may include motion mechanism control and sensing portions 141-145 that may respectively receive signals from the respective position sensors SEN1-SEN5, for sensing the positions , angular positions, linear positions, of the respective arm portions 121-125, and/or may provide control signals to the respective motion mechanisms 131-135 , including motors, linear actuators, for moving the respective arm portions 121-125. The motion control and processing system 140 may also receive signals from the end tool sensing portion ETSN. In various implementations, the end tool sensing portion ETSN may include circuitry and/or configurations related to the operations of the end tool ETL for sensing a workpiece WP. As will be described in more detail below, in various implementations the end tool ETL , a touch probe, a scanning probe, a camera, may utilized for contacting or otherwise sensing surface locations/positions/points on a workpiece WP, for which various corresponding signals may be received, determined and/or processed by the end tool sensing portion ETSN, which may provide corresponding signals to the motion control and processing system 140. In various implementations, the motion control and processing system 140 may include an end tool control and sensing portion 146 that may provide control signals to and/or receive sensing signals from the end tool sensing portion ETSN. In various implementations, the end tool control and sensing portion 146 and the end tool sensing portion ETSN may be merged and/or indistinguishable. In various implementations, the motion mechanism control and sensing portions 141-145 and the end tool control and sensing portion 146 may all provide outputs to and/or receive control signals from a robot position processing portion 147 which may control and/or determine the overall positioning of the movable arm configuration MAC of the robot 110 and corresponding end tool position ETP as part of the robot motion control and processing system 140. In various implementations, the end tool metrology position coordinates determination system 150 may be included with or otherwise added to a robot 110 , as part of a retrofit configuration for being added to an existing robot 110, . In general, the end tool metrology position coordinates determination system 150 may be utilized to provide an improved level of accuracy for the determination of the end tool position ETP. More specifically, as will be described in more detail below, the end tool metrology position coordinates determination system 150 may be utilized to determine a relative position that is indicative of the metrology position coordinates of the end tool position ETP, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. In various implementations , where the scale imaging axis direction SIA and the end tool stylus ETST are parallel to the z axis, this may correspond to the accuracy level being better than the robot accuracy, at least for x and y metrology position coordinates in an x-y plane that is perpendicular to the z axis. As illustrated in 1, the first imaging configuration 160-1 is coupled to a stationary element STE proximate to the robot 110. In various implementations, the stationary element STE may comprise a frame arranged above at least a portion of an end tool working volume ETWV, and for which the first imaging configuration 160-1 is fixed to the frame above a portion of the end tool working volume ETWV. In various implementations, the stationary element STE may include one or more structural support elements SSP , extending from a floor, ceiling, for maintaining the stationary element STE in a fixed location , with a fixed position and/or orientation relative to the robot 110. In various implementations, the end tool working volume ETWV consists of a volume in which at least a portion of at least one of the end tool ETL and/or the XY scale 170 may be moved. In the example of 1, the end tool working volume ETWV is illustrated as including a volume in which the contact point CP of the end tool ETL may be moved when inspecting a workpiece. As one alternative example, an end tool working volume may alternatively include a volume in which the XY scale 170 may move when the end tool ETL is moved for inspecting a workpiece. In various implementations, the robot 110 is configured to move the movable arm configuration MAC so as to move at least a portion of an end tool ETL , the contact point CP that is mounted to the end tool mounting configuration ETMC along at least two dimensions , x and y dimensions in the end tool working volume ETWV. In the example of 1, the portion of the end tool ETL , the contact point CP is movable by the robot 110 along three dimensions , x, y and z dimensions. The first imaging configuration 160-1 includes a first camera CAM1 and has an optical axis OA1. In an operational configuration of the end tool metrology position coordinates determination system 150, the optical axis OA1 of the first imaging configuration 160-1 is parallel to the direction of the scale imaging axis direction SIA. The first imaging configuration 160-1 has an effective focus range REFP along its optical axis OA1. In various implementations, the range REFP may be bound by first and second effective focus positions EFP1 and EFP2, as will be described in more detail below. At a given time, the first imaging configuration 160-1 has an effective focus position EFP that falls within the range REFP. In an implementation where a variable focal length VFL lens is used, the range REFP may correspond to the range of focus of the VFL lens. In various implementations, a VFL lens that is utilized may be a tunable acoustic gradient index of refraction TAG lens. With respect to the general operations of such a TAG lens, in various implementations a lens controller , as included in a first imaging configuration control and processing portion 180 may rapidly adjust or modulate the optical power of the TAG lens periodically, to achieve a high-speed TAG lens capable of a periodic modulation i. e. , at a TAG lens resonant frequency of 250 kHz, or 70 kHz, or 30 kHz, or the like. In such a configuration, the effective focus position EFP of the first imaging configuration 160-1 may be , rapidly moved within the range REFP , an autofocus search range. The effective focus position EFP1 or EFPmax may correspond to a maximum optical power of the TAG lens, and the effective focus position EFP2 or EFPmin may correspond to a maximum negative optical power of the TAG lens. In various implementations, the middle of the range REFP may be designated as EFPnom, and may correspond to zero optical power of the TAG lens. In various implementations, such a VFL lens , a TAG lens and/or a corresponding range REFP may be advantageously chosen such that the configuration limits or eliminates the need for macroscopic mechanical adjustments of the first imaging configuration 160-1 and/or adjustment of distances between components in order to change the effective focus position EFP. For example, in an implementation where an unknown amount of tilt or sag at the distal end DE5 of the fifth arm portion 125 , corresponding to the distal end of the movable arm configuration MAC may occur , due to the weight and/or specific orientations of the arm portions 121-125, , the precise focus distance from the first imaging configuration 160-1 to the XY scale 170 may be unknown and/or may vary with different orientations of the arm portions, etc. It will also be appreciated that in the example configuration of 1, the distance between the XY scale 170 and the first imaging configuration 160-1 may generally change in accordance with the general operations of the movable arm configuration MAC, which may move the end tool position ETP to different locations/distances from the first imaging configuration 160-1 along the scale imaging axis direction SIA , as part of the operations for scanning a surface of a workpiece WP, . In such configurations, it may be desirable for a VFL lens to be utilized that can scan or otherwise adjust the effective focus position EFP to determine and accurately focus at the XY scale 170. In various implementations, such techniques utilizing a VFL lens may be utilized in combination with other focus adjusting techniques , utilized in combination with changeable objective lenses that may also be included in the first imaging configuration 160-1, . In various implementations, the XY scale 170 comprises a nominally planar substrate SUB , arranged nominally perpendicular to the scale imaging axis direction SIA when in the operational configuration and a plurality of respective imageable features that are distributed on the substrate SUB. The respective imageable features are located at respective known scale coordinates , x and y scale coordinates on the XY scale 170. In various implementations, the XY scale 170 may be an incremental or absolute scale, as will be described in more detail below with respect to 5 and 6. In various implementations, the scale plane is defined to nominally coincide with the planar substrate SUB of the XY scale 170, and a direction normal to the scale plane is defined as the scale imaging axis direction SIA. In the operational configuration of the end tool metrology position coordinates determination system 150, the movable XY scale 170 is arranged so that the direction of the scale imaging axis direction SIA is parallel to the optical axis OA1 of the first imaging configuration 160-1 and the scale plane is located within the range of focus REFP of the first imaging configuration 160-1 along the scale imaging axis direction SIA. It will be appreciated that in order to place the end tool metrology position coordinates determination system 150 at least nominally in the operational configuration with the above noted characteristics, various adjustments may be made to the positions/orientations of the arm portions 121-125 of the movable arm configuration MAC. As used herein, the term nominally encompasses variations of one or more parameters that fall within acceptable tolerances. As an example, in one implementation two elements may be defined herein as being nominally parallel when an angle between the two is less than 5 degrees. In one implementation, the optical axis OA1 of the first imaging configuration 160-1 may be correspondingly defined as being nominally parallel to the direction of the scale imaging axis direction SIA when an angle between the two is less than 5 degrees. In accordance with this definition, the end tool metrology position coordinates determination system 150 may be correspondingly defined as at least nominally being in the operational configuration , being at least one of in or nominally in the operational configuration when the components are arranged so that the optical axis OA1 of the first imaging configuration 160-1 is at least one of parallel or nominally parallel to the direction of the scale imaging axis direction SIA, and the scale plane is located within the range of focus of the first imaging configuration 160-1 along the scale imaging axis direction SIA. In various implementations, the end tool metrology position coordinates determination system 150 may be configured such that when the moveable one of the XY scale 170 or the first imaging configuration 160-1 and the stationary one of the XY scale 170 or the first imaging configuration 160-1 are at least nominally arranged in the operational configuration, and the movable arm configuration MAC is positioned with the XY scale 170 in a field of view FOV of the first imaging configuration 160-1, then the metrology position coordinate processing portion 190 is operable to determine a relative position between the movable one of the XY scale 170 or the first imaging configuration 160-1 and the first reference position REF1 with an accuracy level that is better than the robot accuracy. In various implementations, at least part of a robot 110 , the movable arm configuration MAC may include at least one respective rotary joint that provides at least one respective rotational degree of freedom for the end tool ETL, and the robot 110 may be configured to translate and rotate the movable one of the XY scale or the first imaging configuration relative to the stationary one of the XY scale or the first imaging configuration, including rotating the end tool ETL corresponding to the at least one respective rotational degree of freedom, to at least nominally provide the operational configuration. With respect to the example configurations of 1 and 2, the at least one respective rotary joint may correspond to at least the fourth motion mechanism 134, and the at least one respective rotational degree of freedom may correspond to at least the rotation around the rotary axis RA4. In accordance with this configuration, the fourth motion mechanism 134 may be operated to rotate the fourth arm portion 124 so as to cause the scale imaging axis direction SIA to be at least one of parallel or nominally parallel to the optical axis OA1 , to at least nominally be in the operational configuration. In certain implementations, such adjustments may be made automatically , a circuit, routine, etc. , may be utilized to continually monitor the orientation of the fourth arm portion 124, such as by using the position sensor SEN4 or other sensor, and to utilize the fourth motion mechanism 134 to continually adjust the orientation to cause the XY scale 170 and scale plane to be level/parallel to an x-y plane or otherwise have the scale imaging axis direction SIA be parallel to the optical axis OA1. In various implementations, such operations may be continually performed to maintain the end tool metrology position coordinates determination system 150 at least nominally in the operational configuration. In various implementations, a robot 110 may also be configured to move the end tool ETL and the movable one of the XY scale or the first imaging configuration in a plane parallel to the scale plane, while the end tool metrology position coordinates determination system 150 is at least nominally in the operational configuration. In various implementations, such movement in a plane parallel to the scale plane may be made in two dimensions , x and y dimensions, such as by a SCARA robot, an articulated robot, etc. , or such movement may be primarily made in one dimension , an x or y dimension, such as by a linear robot, etc. In the example configurations of 1 and 2, the articulated robot 110 may perform such movement in a plane parallel to the scale plane, for example, by utilizing the first motion mechanism 131 to rotate the first arm portion 121 about the rotary axis RA1, thus producing movement , at the distal end of the movable arm configuration MAC of the end tool ETL and attached XY scale 170 in two dimensions , x and y dimensions in a plane parallel to the scale plane , which may allow the end tool metrology position coordinates determination system 150 to at least nominally remain in the operational configuration during such movements. In various implementations, the image triggering portion 181 and/or the metrology position coordinate processing portion 190 may be included as part of an external control system ECS , as part of an external computer, . The image triggering portion 181 may be included as part of the first imaging configuration control and processing portion 180. In various implementations, the image triggering portion 181 is configured to input at least one input signal that is related to the end tool position ETP and to determine the timing of a first imaging trigger signal based on the at least one input signal, and to output the first imaging trigger signal to the first imaging configuration 160-1. In various implementations, the first imaging configuration 160-1 is configured to acquire a digital image of the XY scale 170 at an image acquisition time in response to receiving the first imaging trigger signal. In various implementations, the metrology position coordinate processing portion 190 is configured to input the acquired image and to identify at least one respective imageable feature included in the acquired image of the XY scale 170 and the related respective known XY scale coordinate location. In various implementations, the external control system ECS may also include a standard robot position coordinates mode portion 149 and an end tool metrology position coordinates mode portion 192, for implementing corresponding modes, as will be described in more detail below. In various implementations, the first imaging configuration 160-1 may include a component , a subcircuit, routine, that activates an image integration of the camera CAM1 periodically , at a set timing interval, for which the first imaging trigger signal from the image triggering portion 181 may activate a strobe light timing or other mechanism to effectively freeze motion and correspondingly determine an exposure within the integration period. In such implementations, if no first imaging trigger signal is received during the integration period, a resulting image may be discarded, wherein if a first imaging trigger signal is received during the integration period, the resulting image may be saved and/or otherwise processed/analyzed to determine a relative position, as will be described in more detail below. In various implementations, different types of end tools ETL may provide different types of outputs that may be utilized with respect to the image triggering portion 181. For example, in an implementation where the end tool ETL is a touch probe that is used for measuring a workpiece and that outputs a touch signal when it touches the workpiece , when the contact point CP contacts the workpiece, the image triggering portion 181 may be configured to input that touch signal or a signal derived therefrom as the at least one input signal that the timing of a first imaging trigger signal is determined based on. In various implementations where the end tool ETL is a touch probe, a central axis of the touch probe may be oriented along the scale imaging axis direction SIA , with the central axis of the touch probe corresponding to the end tool axis EA. As another example, in an implementation where the end tool ETL is a scanning probe that is used for measuring a workpiece and that provides respective workpiece measurement sample data corresponding to a respective sample timing signal, the image triggering portion 181 may be configured to input that respective sample timing signal or a signal derived therefrom as the at least one input signal. As another example, in an implementation where the end tool ETL is a camera that is used to provide a respective workpiece measurement image corresponding to a respective workpiece image acquisition signal, the image triggering portion 181 may be configured to input that workpiece image acquisition signal or a signal derived therefrom as the at least one input signal. In the example implementation of 1, the end tool metrology position coordinates determination system 150 is configured with the XY scale 170 coupled to the end tool ETL. In addition, the first imaging configuration 160-1 is coupled to a stationary element STE , a frame arranged above and proximate to the robot 110 and defines a first reference position REF1. In an alternative implementation , as will be described in more detail below with respect to 4, an end tool metrology position coordinates determination system may be configured with the first imaging configuration 160-1 coupled to a movable arm configuration MAC proximate to the distal end of the movable arm configuration MAC, and the XY scale 170 coupled to a stationary element STE and defining a first reference position REF1. In either case, as will be described in more detail below, the end tool metrology position coordinates determination system 150 may be configured such that when the moveable one of the XY scale 170 or the first imaging configuration 160-1 and the stationary one of the XY scale 170 or the first imaging configuration 160-1 are arranged in the operational configuration, and the movable arm configuration MAC is positioned with the XY scale 170 in a field of view FOV of the first imaging configuration 160-1, then the metrology position coordinate processing portion 190 is operable to determine a relative position between the movable one of the XY scale 170 or the first imaging configuration 160-1 and the first reference position REF1 with an accuracy level that is better than the robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image. The determined relative position is indicative of the metrology position coordinates of the end tool position ETP at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction SIA. In various implementations, the end tool metrology position coordinates determination system 150 may be configured to determine the metrology position coordinates of the end tool position ETP at the image acquisition time, based on the determined relative position and a known coordinate position offset between the end tool position ETP and the movable one of the XY scale 170 or the first imaging configuration 160-1. It will be appreciated that such a system may have certain advantages over various alternative systems. For example, in various implementations a system such as that disclosed herein may be smaller and/or less expensive than alternative systems utilizing technologies such as laser trackers or photogrammetry for tracking robot movement/positions, and may also have higher accuracy in some implementations. The disclosed system also does not take up or obscure any part of the end tool working volume ETWV, such as alternative systems that may include a scale or fiducial on the ground or stage, or otherwise in the same area , in the end tool working volume ETWV where workpieces may otherwise be worked on and/or inspected, etc. 2 is an isometric diagram of a portion of a second exemplary implementation of a robot system 200 similar to the robot system 100 of 1 in which the first imaging configuration 160-1 is coupled to a stationary element STE , the stationary element STE of 1. It will be appreciated that certain numbered components , 1XX, 1XX or 2XX of 2 may correspond to and/or have similar operations as identically or similarly numbered counterpart components , 1XX of 1, and may be understood to be similar or identical thereto and may otherwise be understood by analogy thereto and as otherwise described below. This numbering scheme to indicate elements having analogous and/or identical design and/or function is also applied to the following 3-6. In the configuration of 2 i. e. , similar to the configuration of 1, the stationary element STE that the first imaging configuration 160-1 is coupled to may comprise a frame arranged above the robot 110. The movable arm configuration MAC includes the arm portions 121-125 and the XY scale 170 is coupled to the end tool ETL. In various implementations, a bracket BRKT is utilized for coupling the XY scale 170 to the end tool ETL. In other configurations, other coupling configurations may be utilized for coupling the XY scale 170 to the end tool ETL. In various implementations , as illustrated in 1 and 2 the XY scale 170 may be located primarily on one side of the end tool ETL or the stylus ETST. In various alternative implementations, the XY scale 170 may at least partially surround at least one of the end tool ETL or the stylus ETST in the scale plane. In various implementations, the XY scale 170 may be coupled as close as practical to the contact point CP, such as with the scale plane located as close as possible to the contact point CP , so as to reduce the magnitude of certain types of position errors that may occur under certain circumstances. For example, if the scale imaging axis direction SIA is not aligned to be parallel with the optical axis OA1, there may be a difference between the position of the contact point CP as compared to the position indicated by the XY scale 170 , in an x-y plane, for which such differences may be reduced by coupling the XY scale 170 to the end tool ETL as close as practical to the contact point CP. As illustrated in 2, the first arm portion 121 , an upper base portion is mounted to the first motion mechanism 131 , including a rotary joint at a proximal end PE1 of the first arm portion 121. The first motion mechanism 131 is located at an upper end of the lower supporting base portion BSE and has a rotary axis RA1 aligned along the scale imaging axis direction SIA such that the first arm portion 121 rotates in a plane that is perpendicular to the scale imaging axis direction SIA. In an implementation where the optical axis OA1 of the first imaging configuration 160-1 , and correspondingly the scale imaging axis direction SIA in the operational configuration is parallel to the z axis, the first arm portion 121 may correspondingly rotate in an x-y plane that is perpendicular to the z axis. In various implementations, the position sensor SEN1 , a rotary encoder may be utilized for determining the angular position , the angular orientation of the first arm portion 121. The second motion mechanism 132 , including a rotary joint is located proximate to a distal end DE1 of the first arm portion 121. The second motion mechanism 132 has a rotary axis RA2 , which may be nominally aligned along a direction that is perpendicular to the optical axis OA1 and/or the scale imaging axis direction SIA. The second arm portion 122 is mounted to the second motion mechanism 132 at a proximal end PE2 of the second arm portion 122, such that the second arm portion 122 moves about the second motion mechanism 132 , in a plane that may be nominally parallel to the optical axis OA1 and/or the scale imaging axis direction SIA. In various implementations, the position sensor SEN2 , a rotary encoder may be utilized for determining the angular position , in a plane that may be parallel to the optical axis OA1 and/or the scale imaging axis direction SIA of the second arm portion 122. The third motion mechanism 133 , including a rotary joint is located at a distal end DE2 of the second arm portion 122. The third motion mechanism 133 has a rotary axis RA3 , which may be nominally aligned along a direction that is perpendicular to the optical axis OA1 and/or the scale imaging axis direction SIA. The third arm portion 123 is mounted to the third motion mechanism 133 at a proximal end PE3 of the third arm portion 123, such that the third arm portion 123 moves about the third motion mechanism 133 , in a plane that may be nominally parallel to the optical axis OA1 and/or scale imaging axis direction SIA. In various implementations, the position sensor SEN3 , a rotary encoder may be utilized for determining the angular position , in a plane that may be parallel to the optical axis OA1 and/or scale imaging axis direction SIA of the third arm portion 123. The fourth motion mechanism 134 , including a rotary joint is located at a distal end DE3 of the third arm portion 123. The fourth motion mechanism 134 has a rotary axis RA4 , which may be nominally aligned along a direction that is perpendicular to the optical axis OA1 and/or the scale imaging axis direction SIA. The fourth arm portion 124 is mounted to the fourth motion mechanism 134 at a proximal end PE4 of the fourth arm portion 124, such that the fourth arm portion 124 rotates , in a plane that may be nominally parallel to the optical axis OA1 and/or scale imaging axis direction SIA. In various implementations, the position sensor SEN4 , a rotary encoder may be utilized for determining the angular position , in a plane that may be parallel to the optical axis OA1 and/or the scale imaging axis direction SIA of the fourth arm portion 124. The fifth motion mechanism 135 is located at a distal end DE4 of the fourth arm portion 124. As noted above, in some implementations, the fifth motion mechanism 135 , including a rotary joint may be configured to rotate the fifth arm portion 125 about a rotary axis RA5 , which may be parallel to the scale imaging axis direction SAI and in some orientations may be parallel to the optical axis OA1, such as when in the operational configuration as so oriented by the rotation of the fourth arm portion 124 by the fourth motion mechanism 134, . In such configurations, the fifth arm portion 125 may be mounted to the fifth motion mechanism 135 at a proximal end PE5 of the fifth arm portion 125. In some implementations, the fifth motion mechanism 135 may also or alternatively include a different type of motion mechanism , a linear actuator that is configured to move the fifth arm portion 125 linearly , up and down in the scale imaging axis direction SIA when so oriented in the operational configuration. In various implementations, the fifth arm portion 125 may be designated as a terminal portion of the movable arm configuration MAC, wherein the distal end of the movable arm configuration MAC corresponds to the distal end DE5 of the fifth arm portion 125, where the end tool mounting configuration ETMC may be located. In an implementation where the fifth motion mechanism 135 includes a rotary joint and the scale imaging axis direction SIA is parallel to the z axis in the operational configuration, the XY scale 170 as coupled to the end tool ETL may correspondingly rotate in an x-y plane that is perpendicular to the z axis. In various implementations, as noted above, the XY scale 170 may be located primarily on one side of the stylus or the end tool ETL , as illustrated in 2, and the end tool ETL may be configured to be rotated , by the fifth motion mechanism 135 so that the contact point CP of the end tool ETL may be moved into contact with a workpiece WP without the XY scale 170 interfering. More specifically, in a configuration where the XY scale 170 is located primarily on one side, when the contact point CP is moved toward a workpiece, in certain instances and for certain orientations the XY scale may inadvertently come into physical contact with the workpiece WP before the contact point CP can be moved into contact with the workpiece WP. In order to prevent such occurrences, the end tool ETL or a portion thereof may be rotated , by the motion mechanism 135 so as to rotate the XY scale 170 away from the workpiece WP so as to allow the contact point CP to come into contact with the workpiece WP without the XY scale 170 getting in the way or otherwise interfering. In various implementations, different reference axes and lines may be designated for referencing certain movements, coordinates and angles of the components of the movable arm configuration MAC. As some specific examples, as illustrated in 2 the second and third arm portions 122 and 123 may each have designated center lines CL2 and CL3, respectively, passing down the centers of the respective arm portions. An angle A2 , which may correspond to an amount of rotation of the second motion mechanism 132 may be designated as occurring between the center line CL2 of the second arm portion 122 and plane , parallel to the scale plane in the operational configuration, which may be in an x-y plane when the optical axis OA1 is parallel to the z axis. An angle A3 may be designated as occurring between the center line CL2 of the second arm portion 122 and the center line CL3 of the third arm portion 123 , in accordance with an amount of rotation of the third motion mechanism 133 about the third rotary axis RA3. It will be appreciated that the other arm portions 121, 124 and 125 may similarly have corresponding reference lines and/or axes, etc. , for referencing certain movements, coordinates and angles of the components of the movable arm configuration MAC. In various implementations, the end tool ETL may be mounted , coupled to the end tool mounting configuration ETMC proximate to the distal end DE5 of the fifth arm portion 125. The end tool ETL may be designated as having an end tool axis EA , passing through the middle and/or central axis of the stylus ETST, which may coincide with the fifth rotary axis RA5 of the fifth motion mechanism 135 and which may intersect with an extended line of the fourth rotary axis RA4 of the fourth motion mechanism 134. In various implementations, the end tool axis EA passes through the end tool position ETP, and has a known coordinate position offset from the XY scale 170, and in the operational configuration is parallel to the scale imaging axis direction SIA , such that the end tool ETL with the stylus ETST is oriented parallel to the scale imaging axis direction SIA. Correspondingly, there may be a known coordinate position offset between the end tool position ETP and the XY scale 170. For example, the XY scale 170 may have a designated reference point , at a center or edge of the XY scale 170 which has a known coordinate position offset , a known distance in a plane that is parallel to the scale plane or otherwise from the end tool axis EA , and correspondingly from the end tool position ETP. In various implementations, such a known coordinate position offset may be expressed in terms of known offset components , a known x offset and a known y offset and/or a known distance combined with an angular orientation of the fifth arm portion 125 about the rotary axis RA5, such as may be determined from the position sensor SEN5. In various implementations, the known coordinate position offset between the end tool position ETP and the XY scale 170 may be utilized as part of the process for determining the metrology position coordinates of the end tool position ETP. More specifically, as noted above, the end tool metrology position coordinates determination system 150 may be configured such that the metrology position coordinate processing portion 190 operates to determine a relative position between the XY scale 170 and the first reference position REF1 i. e. , as defined by the stationary first imaging configuration 160-1, based on determining an image position of the identified at least one respective imageable feature i. e. , of the XY scale 170 in the acquired image. The end tool metrology position coordinates determination system 150 may further be configured to determine the metrology position coordinates of the end tool position ETP, based on the determined relative position and a known coordinate position offset between the end tool position ETP and the movable XY scale 170. In one specific example implementation, the known coordinate position offset , expressed in terms of known offset components, such as a known x offset and a known y offset and/or a known distance combined with an angular orientation of the fifth arm portion 125 about the rotary axis RA5, such as may be determined from the position sensor SEN5 may be added to or otherwise combined with the determined relative position in order to determine the metrology position coordinates of the end tool position ETP. As one specific example position coordinate configuration, in an implementation where in the operational configuration the scale imaging axis direction SIA is parallel to the z axis, the XY scale 170 may be designated as having a reference position , an origin location at X0, Y0, Z0 , which, for an origin location, may have values of 0,0,0. In such a configuration, the reference location REF1 i. e. , as defined by the stationary first imaging configuration 160-1 may be at relative coordinates of X1, Y1, Z1, and a center of a corresponding field of view FOV , corresponding to an acquired image may be at relative coordinates of X1, Y1, Z0. A location of the end tool axis EA in an x-y plane extending from the XY scale 170 may be designated as having relative coordinates of X2, Y2, Z0. The end tool position ETP may be designated as having coordinates of X2, Y2, Z2. In various implementations, the end tool ETL may have a contact point CP , at the end of an end tool stylus ETST for contacting a workpiece which may be designated as having coordinates X3, Y3, Z3. In an implementation where the contact point CP of the end tool ETL does not vary in the x or y directions relative to the rest of the end tool, the X3 and Y3 coordinates may be equal to the X2 and Y2 coordinates, respectively. In one specific example implementation, an acquired image may be analyzed by the metrology position coordinate processing portion 190 to determine a relative position , to determine the X1, Y1 coordinates corresponding to the center of the field of view FOV of the stationary first imaging configuration 160-1. Such a determination may be made in accordance with standard camera/scale image processing techniques , for determining a location of camera relative to a scale. Various examples of such techniques are described in 6,781,694; 6,937,349; 5,798,947; 6,222,940 and 6,640,008, each of which is hereby incorporated herein by reference in its entirety. In various implementations, such techniques may be utilized to determine the location of a field of view , as corresponding to a position of a camera within a scale range , within the XY scale 170, as will be described in more detail below with respect to 5 and 6. In various implementations, such a determination may include identifying at least one respective imageable feature included in the acquired image of the XY scale 170 and the related respective known XY scale coordinate location. Such a determination may correspond to determining a relative position between the XY scale 170 and the first reference position REF1 i. e. , as defined by the stationary first imaging configuration 160-1. The relative X2, Y2 coordinates i. e. , of the end tool position ETP may then be determined according to the known coordinate position offset between the end tool position ETP and the XY scale 170 , adding known x and y position offset values to X1 and Y1 in order to determine X2 and Y2. As noted above, in various implementations the determined relative position , corresponding to the determination of the X1, Y1 coordinates relative to the X0, Y0 coordinates is indicative of the metrology position coordinates of the end tool position , the X2, Y2 coordinates at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. In the above example, in a configuration with the scale imaging axis direction SIA being parallel to the z axis in the operational configuration, this may correspond to the accuracy level being better than the robot accuracy, at least for x and y metrology position coordinates in an x-y plane that is perpendicular to the z axis or more generally, better than the robot accuracy for metrology position coordinates in a coordinate plane analogous to the x-y plane, but that is transverse to the z axis and not necessarily perfectly perpendicular to the z axis, in various implementations. More specifically, in such a configuration, a vector component of the x and y metrology position coordinates that is perpendicular to the scale imaging axis direction SIA may correspond to a vector represented by the component pair X1-X0, Y1-Y0, or where X0 and Y0 equal 0, simply X1, Y1. In various implementations, the end tool metrology position coordinates determination system 150 may further include one or more additional imaging configurations. For example, as illustrated in 2, the end tool metrology position coordinates determination system 150 may include a second imaging configuration 160-2 having a second camera CAM2 and a second optical axis OA2 that is parallel to the direction of the scale imaging axis direction SIA when in the operational configuration. The second imaging configuration 160-2 may define a second reference position REF2 , having relative coordinates of X1, Y1 and Z1. The second imaging configuration 160-2 may have an effective focus range REFP along its optical axis OA2. In such a configuration, the image triggering portion 181 may be further configured to input at least one input signal that is related to the end tool position ETP and determine the timing of a second imaging trigger signal based on the at least one input signal and output the second imaging trigger signal to the second imaging configuration 160-2. In various implementations, the second imaging configuration 160-2 may be configured to acquire a digital image of the XY scale 170 at an image acquisition time in response to receiving the second imaging trigger signal. The metrology position coordinate processing portion 190 may be further configured to input the acquired image and identify at least one respective imageable feature included in the acquired image of the XY scale 170 and the related respective known XY scale coordinate location. In various implementations, the metrology position coordinate processing portion 190 may be operable to determine a relative position between the XY scale 170 and the second reference position REF2 with an accuracy level that is better than the robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image. In such an implementation, the determined relative position is indicative of the metrology position coordinates of the end tool position ETP at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. In various implementations, the at least one input signal that is input to the image triggering portion 181 includes one or more signals derived from the motion control system 140. In such configurations, the image triggering portion 181 may be configured to determine whether the XY scale 170 is aligned with the first or second imaging configuration 160-1 or 160-2 based on the one or more signals derived from the motion control system 140. If the XY scale 170 is determined to be aligned with the first imaging configuration 160-1 , such that a sufficient portion of the XY scale 170 is imaged by the first imaging configuration 160-1, the image triggering portion 181 is configured to output the first imaging trigger signal. Conversely, if the XY scale 170 is determined to be aligned with the second imaging configuration 160-2 , such that a sufficient portion of the XY scale 170 is imaged by the second imaging configuration 160-2, the image triggering portion 181 is configured to output the second imaging trigger signal. It will be appreciated that in such an implementation, the XY scale 170 may be in the operational configuration with respect to at least one of the first imaging configuration 160-1 or the second imaging configuration 160-2. 3 is an isometric diagram of a portion of a third exemplary implementation of a robot system 300 including a robot 110 , a SCARA robot, in which the first imaging configuration 160-1 is coupled to the stationary element STE proximate to the robot 110. In the example of 3, the movable arm configuration MAC includes a lower base portion BSE, arm portions 121-123, motion mechanisms 131-133, position sensors SEN1-SEN3, and an end tool mounting configuration ETMC. Each of the arm portions 121-123 have respective proximal ends PE1-PE3 and respective distal ends DE1-DE3. In various implementations, some or all of the arm portions 121-123 may be mounted to respective motion mechanisms 131-133 at respective proximal ends PE1-PE3 of the respective arm portions 121-123. In the example of 3, some or all of the motion mechanisms 131-133 , including rotary joints with corresponding rotary motors, linear motion mechanisms with linear actuators, may enable motion , rotary motion, linear motion, of the respective arm portions 121-123 , about and/or along respective axes RA1-RA3, . In various implementations, the position sensors SEN1-SEN3 , rotary encoders, linear encoders, may be utilized for determining the positions , angular orientations, linear positions, of the respective arm portions 121-123. In various implementations, the movable arm configuration MAC may have a portion that is designated as a terminal portion , the third arm portion 123. In the example configuration of 3, the end tool mounting configuration ETMC is located proximate to , located at the distal end DE3 of the third arm portion 123 , designated as the terminal portion of the movable arm configuration MAC, which corresponds to a distal end of the movable arm configuration MAC. In various alternative implementations, a terminal portion of a movable arm configuration may be an element , a rotatable element, that is not an arm portion but for which at least part of the terminal portion corresponds to a distal end of the movable arm configuration, which an end tool mounting configuration ETMC may be located proximate to. As noted above, in various implementations the end tool ETL may include the end tool sensing portion ETSN and the end tool stylus ETST with the contact point CP , for contacting a surface of a workpiece WP. The third motion mechanism 133 is located proximate to the distal end DE2 of the second arm portion 122. As noted above, in some implementations, the third motion mechanism 133 , a rotary joint with a corresponding motor may be configured to rotate the third arm portion 123 about a rotary axis RA3 , which may be parallel to the scale imaging axis direction SIA and/or the optical axis OA1 when in the operational configuration. In some implementations, the third motion mechanism 133 may also or alternatively include a different type of motion mechanism , a linear actuator that is configured to move the third arm portion 123 linearly , up and down in the scale imaging axis direction SIA, such as for causing the contact point CP to contact a surface point on a workpiece WP. In any case, the end tool ETL is mounted to , coupled to the end tool mounting configuration ETMC, and has a corresponding end tool position ETP with corresponding coordinates , x, y and z coordinates. In various implementations, the end tool position ETP may correspond to or be proximate to the position of the end tool mounting configuration ETMC , at or proximate to the distal end DE3 of the third arm portion 123 which corresponds to the distal end of the movable arm configuration MAC. As noted above, in various implementations the robot 110 is configured to move the movable arm configuration MAC so as to move at least a portion of the end tool ETL , the contact point CP that is mounted to the end tool mounting configuration ETMC along at least two dimensions , x and y dimensions in an end tool working volume. In the example of 3, the portion of the end tool ETL , the contact point CP is movable by the robot 110 along at least two dimensions , the x and y dimensions by movement of the first and second arm portions 121 and 122, and may further be movable along a third dimension , the z dimension by movement of the third arm portion 123 , utilizing a linear actuator as part of the third motion mechanism 133, . As noted above, the motion control system 140 of 1 is configured to control the end tool position ETP of the end tool ETL with a level of accuracy defined as a robot accuracy. More specifically, the motion control system 140 is generally configured to control the coordinates , x, y and z coordinates of the end tool position ETP with the robot accuracy based at least in part on utilizing the motion mechanisms 131-133 and position sensors SEN1-SEN3 for sensing and controlling the positions of the arm portion 121-123. In various implementations, the motion control and processing system 140 may include motion mechanism control and sensing portions , motion mechanism control and sensing portions 141-143 that may respectively receive signals from the respective position sensors SEN1-SEN3, for sensing the positions , angular positions, linear positions, of the respective arm portions 121-123, and/or may provide control signals to the respective motion mechanisms 131-133 , including motors, rotary joints, linear actuators, for moving the respective arm portions 121-123. In the example implementation of 3, the movable arm configuration MAC of the robot 110 may be configured such that the scale imaging axis direction SIA is generally parallel to a z axis direction. In such a configuration, the first motion mechanism 131 , located at an upper end of the supporting base portion BSE may have its rotary axis RA1 aligned along the z axis direction , parallel to the scale imaging axis direction SIA such that the first arm portion 121 moves about the first motion mechanism 131 in an x-y plane , parallel to the scale plane that is perpendicular to the z axis. The first arm portion 121 is mounted to the first motion mechanism 131 at the proximal end PE1 of the first arm portion 121. The second motion mechanism 132 is located at the distal end DE1 of the first arm portion 121. The second motion mechanism 132 has its rotary axis RA2 that may be nominally aligned along the z axis direction , parallel to the scale imaging axis direction SIA. The second arm portion 122 is mounted to the second motion mechanism 132 at the proximal end PE2 of the second arm portion 122, such that the second arm portion 122 may move about the second motion mechanism 132 in an x-y plane that is nominally perpendicular to the z axis. The third motion mechanism 133 , including a rotary joint is located at the distal end DE2 of the second arm portion 122. The third motion mechanism 133 has the rotary axis RA3 that may be nominally aligned along the z axis direction , parallel to the scale imaging axis direction SIA. The third arm portion 123 may be mounted to the third motion mechanism 133 at the proximal end PE3 of the third arm portion 123. In various implementations, the position sensor SEN3 , including a rotary encoder may be utilized for determining the angular position , the angular orientation of the third arm portion 123. As noted above, in some implementations, the third motion mechanism 133 may also or alternatively include a different type of motion mechanism , a linear actuator that may be configured to move the third arm portion 123 up and down , in the z axis direction. The position sensor SEN3 may also or alternatively include a linear encoder for sensing the linear position , along the z axis direction of the third arm portion 123. As noted above, the motion control system 140 is configured to control the end tool position ETP of the end tool ETL with a level of accuracy defined as a robot accuracy. More specifically, in the example configuration of 3, the motion control system 140 is generally configured to control the x and y coordinates of the end tool position ETP with the robot accuracy based at least in part on sensing and controlling the angular positions , in an x-y plane of the first and second arm portions 121 and 122 about the first and second motion mechanisms 131 and 132, respectively, using the position sensors SEN1 and SEN2. In various implementations, the motion control and processing system 140 may include motion mechanism control and sensing portions , first and second motion mechanism control and sensing portions 141 and 142 that may receive signals from the position sensors SEN1 and SEN2, respectively, for sensing the angular positions of the first and second arm portions 121 and 122, and/or may provide control signals , to motors, in the first and second motion mechanisms 131 and 132 for rotating the first and second arm portions 121 and 122. In addition, the motion control and processing system 140 may generally be configured to control the z coordinate of the end tool position ETP with the robot accuracy based at least in part on sensing and controlling the linear position , along the z axis of the third arm portion 123 using the third motion mechanism 133 , including a linear actuator and the position sensor SEN3 , including a linear encoder. In various implementations, the motion control and processing system 140 may include a motion mechanism control and sensing portion , motion mechanism control and sensing portion 143 that may receive signals from the position sensor SEN3 , for sensing the linear position of the third arm portion 123, and/or may provide control signals to the third motion mechanism 133 , including a linear actuator to control the position , the z position of the third arm portion 123. As noted above, the motion control and processing system 140 may also receive signals from the end tool sensing portion ETSN. As noted above, the end tool metrology position coordinates determination system 150 may be utilized to determine a relative position that is indicative of the metrology position coordinates of the end tool position ETP, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. In various implementations , in the configuration of 3 in which the scale imaging axis direction SIA is parallel to the z axis, this may correspond to the accuracy level being better than the robot accuracy, at least for x and y metrology position coordinates in an x-y plane that is perpendicular to the z axis. The first imaging configuration 160-1 and the XY scale 170 generally operate as described above with respect to 1 and 2. In the example configuration of 3, the first imaging configuration 160-1 includes the first camera CAM1 and has the optical axis OA1 that may be parallel to the scale imaging axis direction SIA , parallel to the z axis when in the operational configuration. In various implementations, a VFL lens , a TAG lens and a corresponding range REFP of the first imaging configuration 160-1 may be advantageously chosen such that the configuration limits or eliminates the need for macroscopic mechanical adjustments of the first imaging configuration 160-1 and/or adjustment of distances between components in order to change the effective focus position EFP. For example, in an implementation where an unknown amount of tilt or sag at the distal end DE2 of the second arm portion 122 may occur , due to the weight and/or specific orientations of the first and second arm portions 121 and 122, , the precise focus distance from the first imaging configuration 160-1 to the XY scale 170 may be unknown and/or may vary with different orientations of the arms, etc. In such a configuration, it may be desirable for a VFL lens to be utilized that can scan or otherwise adjust the effective focus position EFP to determine the correct focus distance and accurately focus at the XY scale 170. In the configuration of 3, different reference axes and lines may be designated for referencing certain movements, coordinates and angles of the components of the movable arm configuration MAC. For example, the first and second arm portions 121 and 122 may each have designated horizontal center lines CL1 and CL2, respectively, passing down the centers of the respective arm portions. An angle A1 may be designated as occurring between the center line CL1 of the first arm portion 121 and a plane , an x-z plane in accordance with an amount of rotation of the first motion mechanism 131 about the first rotary axis RA1. An angle A2 may be designated as occurring between the horizontal center line CL1 of the first arm portion 121 and the horizontal center line CL2 of the second arm portion 122, in accordance with an amount of rotation of the second motion mechanism 132 about the second rotary axis RA2. In various implementations, an end tool axis EA , passing through the center of the third arm portion 123, the end tool position ETP, and/or the center of the end tool stylus ETST may nominally intersect the center line CL2 of the second arm portion 122, and for which the end tool axis EA may generally be assumed to nominally coincide with the rotary axis RA2 , parallel to the z axis direction. In various implementations, the end tool axis EA passes through the end tool position ETP, and has a known coordinate position offset , for x and y coordinates from the XY scale 170. Correspondingly, there may be a known coordinate position offset between the end tool position ETP and the XY scale 170. For example, the XY scale 170 may have a designated reference point , at a center or edge of the XY scale 170 which has a known coordinate position offset , a known distance in an x-y plane from the end tool axis EA and correspondingly from the end tool position ETP. In various implementations, such a known coordinate position offset may be expressed in terms of a known x offset and a known y offset and/or a known distance combined with an angular orientation of the third arm portion 123 about the rotary axis RA3, such as may be determined from the position sensor SEN3. As one specific example position coordinate configuration, similar to the configuration described above with respect to 2, the XY scale 170 may be designated as having a reference position , an origin location at X0, Y0, Z0 , which, for an origin location, may have values of 0,0,0. In such a configuration, the reference location REF1 i. e. , as defined by the stationary first imaging configuration 160-1 may be at relative coordinates of X1, Y1, Z1, and a center of a corresponding field of view FOV , corresponding to an acquired image may be at relative coordinates of X1, Y1, Z0. A location of the end tool axis EA in an x-y plane extending from the XY scale 170 may be designated as having relative coordinates of X2, Y2, Z0. The end tool position ETP may be designated as having coordinates of X2, Y2, Z2. In various implementations, the contact point CP , at the end of an end tool stylus ETST for contacting a workpiece may be designated as having coordinates X3, Y3, Z3. In an implementation where the contact point CP of the end tool ETL does not vary in the x or y directions relative to the rest of the end tool, the X3 and Y3 coordinates may be equal to the X2 and Y2 coordinates, respectively. Similar to the examples described above with respect to 2, in various implementations the determined relative position , corresponding to the determination of the X1, Y1 coordinates relative to the X0, Y0 coordinates is indicative of the metrology position coordinates of the end tool position , the X2, Y2 coordinates at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. In various implementations, the end tool metrology position coordinates determination system 150 may further include one or more additional imaging configurations , the imaging configuration 160-2 defining a second reference position REF2 as described above with respect to 2, which may similarly be utilized for determining a relative position , between the XY scale 170 and the second reference position REF2. As described above, in various implementations the robot 110 and the end tool metrology position coordinates determination system 150 may be arranged to at least nominally provide an operational configuration of the end tool metrology position coordinates determination system 150. The scale plane is defined to nominally coincide with the planar substrate of the XY scale 170, and a direction normal to the scale plane is defined as the scale imaging axis direction SIA. At least one of the XY scale 170 or the first imaging configuration 160-1 may be arranged with the optical axis OA1 of the first imaging configuration 160-1 at least one of parallel or nominally parallel to the direction of the scale imaging axis direction SIA, and with the scale plane located within the range of focus of the first imaging configuration 160-1 along the scale imaging axis direction SIA. As described above, the robot 110 i. e. , including the movable arm configuration MAC is configured to move the end tool ETL and the XY scale 170 i. e. , as coupled to the end tool in a plane parallel to the scale plane, while the end tool metrology position coordinates determination system 150 may be at least nominally in the operational configuration. More specifically, in the example configurations of 3 and 4, the SCARA robot 110 may perform such movement in a plane that is at least one of parallel or nominally parallel to the scale plane, for example, by utilizing one or both of the first or second motion mechanisms 131 or 132 to rotate the respective arm portion 121 or 122 about the respective rotary axis RA1 or RA2. Such rotations of either or both arm portions 121 and 122 produce corresponding movement , at the distal end of the movable arm configuration MAC of the end tool ETL and either the attached XY scale 170 3 or the attached first imaging configuration 160-1 4 in two dimensions , x and y dimensions in a plane parallel to the scale plane , which allows the end tool metrology position coordinates determination system 150 to at least nominally remain in the operational configuration during such movements. It will appreciated that in the example implementation of 3 as described above , for the general operations and orientations of the movable arm configuration MAC, in various implementations, few or no adjustments may be required to at least nominally achieve the operational configuration. More specifically, as described above, the general movements of the movable arm configuration MAC may be such that the scale imaging axis direction SIA is at least one of parallel or nominally parallel to the optical axis OA1 , and may remain so in accordance with the rotation of the first and second arm portions 121 and 122 at least one of in or nominally in an x-y plane. Such a configuration may reduce or eliminate the need for certain types of adjustments , reducing or eliminating the need for certain types of adjustments to the orientation of the XY scale 170, such as in the example implementations of 1 and 2 where the fourth motion mechanism 134 may be utilized for adjusting the orientation of the XY scale 170, . In various implementations, such features , including the scale imaging axis direction SIA remaining at least one of parallel or nominally parallel to the optical axis OA during movement of the movable arm configuration MAC and thus requiring fewer adjustments to at least nominally maintain the operational configuration may be considered as an advantage of the use of the movable arm configuration MAC in conjunction with the end tool metrology position coordinates determination system 150. 4 is an isometric diagram of a portion of a fourth exemplary implementation of a robot system 400 including the robot 110 , a SCARA robot and an end tool metrology position coordinates determination system 150 in which an XY scale 170 is coupled to the stationary element STE proximate to the robot 110 and defines a first reference position REF1. In the configuration of 4, the first imaging configuration 160-1 is coupled to the end tool ETL , by a bracket BRKT or other coupling mechanism. In various implementations, the first imaging configuration 160-1 may be coupled to the end tool ETL such that its optical axis OA1 is as close as practical to being aligned with the contact point CP along the Z axis direction , so as to reduce the magnitude of certain types of position errors that may occur due to sag or tilt of the end tool ETL under certain circumstances. In any case, in an operational configuration of the end tool metrology position coordinates determination system 150, the first imaging configuration 160-1 is arranged with the optical axis OA1 of the first imaging configuration 160-1 parallel to the direction of the scale imaging axis direction SIA and with the scale plane located within the range of focus of the first imaging configuration 160-1 along the scale imaging axis direction SIA. In various implementations, the XY scale 170 may be relatively large , covering an entire area above an end tool working volume, . It will be appreciated that in some instances the XY scale 170 may be relatively larger than the XY scale 170 of 1, 2 and 3, in that the XY scale 170 is not attached to the movable arm configuration MAC and so does not have certain corresponding size or weight restrictions , the size of the XY scale 170 will not interfere with movements of the movable arm configuration MAC in limited spatial areas and/or near a workpiece, . In various implementations, the end tool axis EA has a known coordinate position offset , for x and y coordinates from the first imaging configuration 160-1. Correspondingly, there may be a known coordinate position offset between the end tool position ETP and the first imaging configuration 160-1. For example, the first imaging configuration 160-1 may have a designated reference point , at a center of the first imaging configuration 160-1 which has a known coordinate position offset , a known distance in an x-y plane from the end tool axis EA and correspondingly from the end tool position ETP. In various implementations, such a known coordinate position offset may be expressed in terms of a known x offset and a known y offset and/or a known distance combined with an angular orientation of the third arm portion 123 about the rotary axis RA3, such as may be determined from the position sensor SEN3. In various implementations, the known coordinate position offset between the end tool position ETP and the first imaging configuration 160-1 may be utilized as part of the process for determining the metrology position coordinates of the end tool position ETP. More specifically, as noted above, the end tool metrology position coordinates determination system 150 may be configured such that the metrology position coordinate processing portion 190 operates to determine a relative position between the first imaging configuration 160-1 and the first reference position REF1 i. e. , as defined by the stationary XY scale 170, based on determining an image position of an identified at least one respective imageable feature i. e. , of the XY scale 170 in the acquired image. The end tool metrology position coordinates determination system 150 may further be configured to determine the metrology position coordinates of the end tool position ETP, based on the determined relative position and a known coordinate position offset between the end tool position ETP and the movable first imaging configuration 160-1. In one specific example implementation, the known coordinate position offset , expressed in terms of a known x offset and a known y offset and/or a known distance combined with an angular orientation of the third arm portion 123 about the rotary axis RA3, such as may be determined from the position sensor SEN3 may be added to or otherwise combined with the determined relative position in order to determine the metrology position coordinates of the end tool position ETP. As one specific example position coordinate configuration, the XY scale 170 may be designated as having a reference location REF1 , an origin location at X0, Y0, Z0 , which may have values of 0,0,0. The first imaging configuration 160-1 may be at a location with relative coordinates of X1, Y1, Z1, and a center of a corresponding field of view FOV , as captured in an acquired image may be at relative coordinates of X1, Y1, Z0. A location of the end tool axis EA in an x-y plane extending from the first imaging configuration 160-1 may be designated as having relative coordinates of X2, Y2, Z1. The end tool position ETP may be designated as having coordinates of X2, Y2, Z2. In various implementations, the contact point CP , at the end of the end tool stylus ETST for contacting a workpiece may be designated as having coordinates X3, Y3, Z3. In an implementation where the contact point CP of the end tool ETL does not vary in direction , in the x or y directions relative to the rest of the end tool, the X3 and Y3 coordinates may be equal to the X2 and Y2 coordinates, respectively. It will be understood that some implementations may include additional respective XY scales analogous to the XY scale 170, which may be designated as having a having respective reference locations , analogous to but different from the reference location REF1. In such implementations, the XY scales in addition to the first XY scale 170 , second XY scale, third XY scale, and so on may be associated with respective high accuracy end tool metrology position coordinates determination volumes at respective locations within the overall working volume of a robot. In various implementations, the robot system 400 of 4 may have certain different design considerations and aspects as compared to the robot system 300 of 3 , related to a possible vertical displacement or sag at the distal ends DE1 and DE2 of the first and second arm portions 121 and 122, respectively. In an implementation where such displacement or sag may occur , due to the weight and/or different orientations of the arm portions, imaging configuration 160-1, , a particularly undesirable effect may be experienced in the robot system 400 of 4 with respect to the field of view FOV of the first imaging configuration 160-1 being correspondingly shifted. More specifically, such vertical displacement or sag may cause a relatively significant shift/change in the location of the field of view FOV on the XY scale 170 , corresponding to a shift of the orientation of the optical axis OA1 away from parallel to the scale imaging axis direction SIA, and thus farther away from the operational configuration, which may result in a relatively significant error in the determined relative position and the corresponding metrology position coordinates of the end tool position ETP. Due to such issues, in certain implementations the configuration of the robot system 300 of 3 may be considered to have corresponding advantages over the robot system 400 of 4. 5 is an isometric diagram of an exemplary implementation of an incremental XY scale 170A , utilizable as the XY scale 170 or 170 as described above. As noted above, in various implementations, the XY scale 170A comprises a nominally planar substrate SUB , arranged nominally perpendicular to the scale imaging axis direction SIA when in the operational configuration and a plurality of respective imageable features IIF that are distributed on the substrate SUB, wherein the respective imageable features IIF are located at respective known XY scale coordinates , x and y coordinates on the XY scale 170A i. e. , thus each corresponding to a known XY scale coordinate location. In the example implementation of 5, the array of incremental imageable features IIF are evenly spaced. In various implementations, the incremental XY scale 170A may have a periodicity that is smaller than 100 microns , for which periodic spacings Xsp1 and Ysp1 between the incremental imageable features IIF, such as along respective x and y axes, may each be less than 100 microns. In various implementations, the position information that is determined utilizing the incremental XY scale 170A may have an accuracy of at least 10 microns. In contrast to a robot accuracy that may be approximately 100 microns in certain implementations, the accuracy determined utilizing such an XY scale 170A may be at least 10 that of the robot accuracy. In one specific example implementation, the incremental XY scale 170A may have an even higher periodicity of approximately 10 microns, for which, if the magnification of the first imaging configuration 160-1 is approximately 1, and interpolation is performed by a factor of 10, then an approximately 1 micron accuracy may be achieved. Such a configuration would have an approximately 100 improvement in accuracy over a robot accuracy of approximately 100 microns. In various implementations, a location of a field of view FOV of the first imaging configuration 160-1 within the incremental XY scale 170A may provide an indication of a relative position between the XY scale 170A and the first reference position REF1. In various implementations, the first imaging configuration 160-1 may be utilized in combination with the incremental XY scale 170A as part of a camera/scale image processing configuration. For example, the metrology position coordinate processing portion 190 1 may determine a relative incremental position between the XY scale 170A and the first reference position REF1 based on the location of the field of view FOV within the incremental XY scale 170A, as indicated by the portion of the XY scale 170A in the acquired image, and as is known in the art for camera/scale image processing techniques , as described in the previously incorporated references. In various implementations, the incremental XY scale 170A may be of various sizes relative to the field of view FOV , the incremental XY scale 170A may be at least 4, 10, 20, etc. , larger than the field of view FOV. In various implementations, the incremental position indicated by the XY scale 170A may be combined with position information from the robot 110 or 110 to determine a relatively precise and/or absolute position. For example, the position sensors SEN1 and SEN 2 or SEN1 and SEN2 , rotary encoders of the robot 110 or 110 may indicate the end tool position ETP with the robot accuracy, for which the incremental position indicated by the XY scale 170A may be utilized to further refine the determined end tool position ETP to have an accuracy that is better than the robot accuracy. In one such configuration, the metrology position coordinate processing portion 190 may be configured to identify one or more respective imageable features IIF included in the acquired image of the XY scale 170A based on the image positions of the one or more imageable features IIF in the acquired image and based on robot position data derived from the motion control system 140 corresponding to the image acquisition time. In such configurations, the respective imageable features IIF of the XY scale 170A may comprise a set of similar imageable features IIF that are distributed on the substrate such that they are spaced apart from one another at regular intervals by a distance that is more than a maximum position error that is allowed within the robot accuracy. As illustrated in 5, the imageable features IIF are spaced apart , at spacings Xsp1 and Ysp1 by more than a maximum position error MPE as represented by a circle surrounding a representative imageable feature IIF. It will be appreciated that in such a configuration, the robot accuracy for the position determination is sufficient to determine the location with an accuracy that is greater than the spacing between the imageable features IIF. More specifically, in various implementations, a single imageable feature IIF on the XY scale 170A , wherein the imageeable features are all at known metrology position coordinates, such as known x and y metrology position coordinates, on the XY scale 170A according to the even spacings across the scale may thus be identified by the robot position data with sufficient accuracy so that no two imageable features IIF may be confused with one another. In such a configuration, the location of a single imageable feature IIF in the acquired image may then be utilized to further refine the end tool position ETP to have an accuracy that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction , at least for x and y metrology position coordinates of the end tool position ETP in an x-y plane that is perpendicular to the z axis. As described above with respect to 2 and 3, in certain specific example implementations, the XY scale 170A may be designated as having a reference position , an origin location at X0, Y0, Z0 , which, for an origin location, may have values of 0,0,0. In such a configuration, the reference location REF1 i. e. , as defined by the stationary first imaging configuration 160-1 may be at relative coordinates of X1, Y1, Z1, and a center of a corresponding field of view FOV , as captured in an acquired image may be at relative coordinates of X1, Y1, Z0. A location of the end tool axis EA , in an x-y plane extending from the XY scale 170 may be designated as having relative coordinates of X2, Y2, Z0. The end tool position ETP may be designated as having coordinates of X2, Y2, Z2. In operation, an acquired image may be analyzed by the metrology position coordinate processing portion 190 to determine the X1, Y1 coordinates corresponding to the center of the field of view FOV of the stationary first imaging configuration 160-1. In various implementations, such a determination may be made in accordance with standard camera/scale image processing techniques, for determining a location of a field of view , corresponding to a location of a camera within a scale range , within the XY scale 170A. It will be appreciated that in accordance with standard camera/scale image processing techniques, the reference position/origin location X0, Y0, Z0 is not required to be in the field of view FOV for such a determination to be made i. e. , the relative position may be determined from the scale information at any location along the XY scale 170A, as provided in part by the scale elements comprising the evenly spaced incremental imageable features IIF. In various implementations, such a determination may include identifying at least one respective imageable feature included in the acquired image of the XY scale 170 and the related respective known XY scale coordinate location. Such a determination may correspond to determining a relative position between the XY scale 170 and the first reference position REF1 i. e. , as defined by the stationary first imaging configuration 160-1. The relative X2, Y2 coordinates i. e. , of the end tool position ETP may then be determined according to the known coordinate position offset between the end tool position ETP and the XY scale 170 , adding the x and y position offset values to X1 and Y1 in order to determine X2 and Y2. A specific illustrative example of combining the position information from the robot 110 or 110 with the incremental position information indicated by the XY scale 170A to determine a relatively precise and/or absolute position is as follows. As illustrated in 5, the acquired image may indicate that the center of the field of view FOV is in the middle of four incremental imageable features IIF, but may not indicate which specific four incremental imageable features IIF of the XY scale 170 are included in the image. The position information from the robot 110 or 110 may be accurate enough to provide such information, for which the specific four incremental imageable features IIF of the XY scale 170A may be identified , based in part on the principles noted above by which the imageable features IIF are spaced apart by more than a maximum position error as represented by a representative circular area MPE so that each imageable feature IIF may be uniquely identified. The acquired image may then be analyzed by the metrology position coordinate processing portion 190 to determine precisely where the center of the field of view i. e. , at the coordinates X1, Y1, Z0 occurs within that section of the XY scale i. e. , which includes the specific four incremental imageable features IIF. The process may then continue as indicated above , for correspondingly determining the X2 and Y2 coordinates of the end tool position ETP, . 6 is an isometric diagram of an exemplary implementation of an absolute XY scale 170B , utilizable as the XY scale 170 or 170 as described above. In the example of 6, similar to the incremental XY scale 170A, the absolute XY scale 170B includes an array of evenly spaced incremental imageable features IIF, and also includes a set of absolute imageable features AIF having unique identifiable patterns , a 16-bit pattern. In various implementations, the imageable features IIF and AIF are all located at respective known XY scale coordinates , x and y coordinates on the XY scale i. e. , thus each corresponding to a known XY scale coordinate location. In operation, a location of a field of view FOV of the first imaging configuration 160-1 within the absolute XY scale 170B i. e. , as included in a captured image provides an indication of an absolute position between the XY scale 170B and the first reference position REF1. In the implementation of 6, the set of absolute imageable features AIF are distributed on the substrate SUB such that they are spaced apart , at spacings Xsp2 and Ysp2 by less than a distance corresponding to a distance across a field of view FOV of the first imaging configuration 160-1 i. e. , so that at least one absolute imageable feature AIF will always be included in a field of view. In operation, the metrology position coordinate processing portion 190 is configured to identify at least one respective absolute imageable feature AIF included in the acquired image of the XY scale 170B based on the unique identifiable pattern of the respective absolute imageable feature AIF. It will be appreciated that such implementations are able to independently determine an absolute position that is indicative of the end tool position ETP with an accuracy that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction , at least for x and y metrology position coordinates of the end tool position ETP in an x-y plane that is perpendicular to the z axis, and which in contrast to the incremental XY scale 170B may not require combining with position information from the robot 110 or 110 to determine the absolute position. A specific illustrative example of utilizing the absolute imageable features AIF to determine a relatively precise and absolute position is as follows. As illustrated in 6, the acquired image may indicate that the center of the field of view FOV is in the middle of a number of incremental imageable features IIF. The position information from the included two absolute imageable features AIF indicates which section of the XY scale 170B the image includes, for which the included incremental imageable features IIF of the XY scale 170 may also be identified. The acquired image may accordingly be analyzed by the metrology position coordinate processing portion 190 to determine precisely where the center of the field of view i. e. , at the coordinates X1, Y1, Z0 occurs within that section of the XY scale i. e. , which includes the two absolute imageable features and the incremental imageable features IIF. The process may then continue as indicated above , for correspondingly determining the X2 and Y2 coordinates of the end tool position ETP, . 7A and 7B are flow diagrams illustrating exemplary implementations of routines 700A and 700B for operating a robot system including a robot and an end tool metrology position coordinates determination system. As shown in 7A, at a decision block 710, a determination is made as to whether the robot system is to be operated in an end tool metrology position coordinates mode. In various implementations, a selection and/or activation of an end tool metrology position coordinates mode or a standard robot position coordinates mode may be made by a user and/or may be automatically made by the system in response to certain operations and/or instructions. For example, in one implementation an end tool metrology position coordinates mode may be entered , automatically or in accordance with a selection by a user when the robot moves into a particular position , moves an end tool from a general area where assembly or other operations are performed to a more specific area where workpiece inspection operations are typically performed, and/or where the end tool metrology position coordinates mode would otherwise be utilized. In various implementations, such modes may be implemented by an external control system ECS , such as the external control system ECS of 1 utilizing a standard robot position coordinates mode portion 149 and an end tool metrology position coordinates mode portion 192. In various implementations, a hybrid mode may be operated either independently or as part of an end tool metrology position coordinates mode and/or may be implemented as a switching between the modes, as will be described in more detail below with respect to 8. If at the decision block 710 it is determined that the robot system is not to be operated in an end tool metrology position coordinates mode, the routine proceeds to a block 715, where the robot system is operated in a standard robot position coordinates mode. As part of the standard robot position coordinates mode, the position sensors , rotary encoders, linear encoders, of the robot are utilized to control and determine the robot movements and corresponding end tool position with the robot accuracy , which is based at least in part on the accuracy of the position sensors of the robot. As noted above with respect to 1-6, the position sensors of the robot may indicate the position of the movable arm configuration MAC or MAC , the positions of the arm portions with a lower degree of accuracy than the position information that is determined utilizing the XY scale. In general, the robot position coordinates mode may correspond to an independent and/or standard mode of operation for the robot , a mode in which the robot is operated independently, such as when an end tool metrology position coordinates determination system is not active or is otherwise not provided. If the robot system is to be operated in an end tool metrology position coordinates mode, the routine proceeds to a block 720, where the robot and the end tool metrology position coordinates determination system are arranged to at least nominally provide an operational configuration of the end tool metrology position coordinates determination system. A scale plane is defined to nominally coincide with the planar substrate of the XY scale, and a direction normal to the scale plane is defined as a scale imaging axis direction. At least one of the XY scale or the first imaging configuration is arranged with the optical axis of the first imaging configuration at least one of parallel or nominally parallel to the direction of the scale imaging axis direction and with the scale plane located within the range of focus of the first imaging configuration along the scale imaging axis direction. As described above, in various implementations, this process for at least nominally achieving the operational configuration may include making various adjustments , to the positions of the arm portions of the movable arm configuration MAC, . As one specific example, in the implementations of 1 and 2, the fourth motion mechanism 134 may be operated to rotate the fourth arm portion 124 so as to rotate the XY scale 170 to cause the scale imaging axis direction SIA to be at least one of parallel or nominally parallel to the optical axis OA1. In certain implementations, such adjustments may be made automatically , a circuit, routine, etc. , may be utilized to continually monitor the orientation of the fourth arm portion 124 and to utilize the fourth motion mechanism 134 to continually adjust the orientation to cause the XY scale 170 to be approximately level or otherwise have the scale imaging axis direction SIA be at least one of parallel or nominally parallel to the optical axis OA1. In various implementations, various adjustments may be made to the first imaging configuration 160-1 , the magnification and/or range of focus may be adjusted, so as to cause the scale plane to be located within the range of focus of the first imaging configuration 160-1 along the scale imaging axis direction SIA. In certain other implementations , in the example configurations of 3 and 4, fewer or no adjustments may sometimes be required to achieve the operational configuration , the configuration of the movable arm configuration MAC may be such that the scale imaging axis direction SIA is at least nominally parallel to the optical axis OA1, in accordance with the rotation of the first and second arm portions 121 and 122 nominally in an x-y plane in certain implementations. At a block 730, at least one input signal is received , at an image triggering portion that is related to an end tool position of the robot. A timing is determined of a first imaging trigger signal based on the at least one input signal and the first imaging trigger signal is output to a first imaging configuration. The first imaging configuration acquires a digital image of an XY scale at an image acquisition time in response to receiving the first imaging trigger signal. In various implementations, different types of end tools may provide different types of outputs that may be utilized with respect to the at least one input signal. For example, in an implementation where the end tool is a touch probe that is used for measuring a workpiece and that outputs a touch signal when it touches the workpiece, that touch signal or a signal derived therefrom may be input as the at least one input signal that the timing of a first imaging trigger signal is determined based on. As another example, in an implementation where the end tool is a scanning probe that is used for measuring a workpiece and that provides respective workpiece measurement sample data corresponding to a respective sample timing signal, that respective sample timing signal or a signal derived therefrom may be input as the at least one input signal. As another example, in an implementation where the end tool is a camera that is used to provide a respective workpiece measurement image corresponding to a respective workpiece image acquisition signal, that workpiece image acquisition signal or a signal derived therefrom may be input as the at least one input signal. At a block 740, the acquired image is received , at a metrology position coordinate processing portion, and at least one respective imageable feature included in the acquired image of the XY scale and the related respective known XY scale coordinate location are identified. At a block 750, a relative position between a movable one of the XY scale or the first imaging configuration and the first reference position is determined with an accuracy level that is better than a robot accuracy, based on determining an image position of the identified at least one respective imageable feature in the acquired image. The determined relative position is indicative of the metrology position coordinates of the end tool position at the image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. At a block 760, determined position information , the determined relative position, the determined metrology position coordinates of the end tool position and/or other related determined position information is utilized for a designated function , for workpiece measurement, positioning control of the movable arm configuration of the robot, . As part of such operations or otherwise, the routine may then proceed to a point A, where in various implementations the routine may end, or may otherwise continue as will be described in more detail below with respect to 7B. As indicated in 7B, the routine 700B may continue from the point A to a block 765. As will be described in more detail below, as part of the routine 700B, the determined position information , from the block 760 may correspond to or otherwise be utilized for determining a first surface location on a workpiece, and for which a second surface location on the workpiece may then be determined , as part of a workpiece measurement. At the block 765, the robot and the end tool metrology position coordinates determination system are arranged to at least nominally provide the operational configuration of the end tool metrology position coordinates determination system, wherein at least one of the XY scale or the first imaging configuration is arranged with the optical axis of the first imaging configuration at least one of parallel or nominally parallel to the direction of the scale imaging axis direction and with the scale plane located within the range of focus of the first imaging configuration along the scale imaging axis direction. For example, in the implementation of 1 and 2, this may correspond to the movable arm configuration MAC moving the end tool ETL , and contact point CP proximate to , above a second surface location on a workpiece, for which an adjustment may be made , by the fourth motion mechanism 134 for adjusting the orientation of the XY scale 170 in order to at least nominally achieve the operational configuration proximate to the second surface location on the workpiece. At a block 770, at least one second input signal is received , at the image triggering portion that is related to the end tool position, and the timing of a second imaging trigger signal is determined based on the at least one second input signal. The second imaging trigger signal is output to the first imaging configuration, wherein the first imaging configuration acquires a second digital image of the XY scale at a second image acquisition time in response to receiving the second imaging trigger signal. At a block 780, the acquired image is received , at the metrology position coordinate processing portion, and at least one second respective imageable feature included in the second acquired image of the XY scale and a related respective second known XY scale coordinate location are identified. At a block 790, a second relative position between the movable one of the XY scale or the first imaging configuration and the second reference position is determined with an accuracy level that is better than the robot accuracy, based on determining a second image position of the identified at least one second respective imageable feature in the second acquired image. The determined second relative position is indicative of the metrology position coordinates of the end tool position at the second image acquisition time, with an accuracy level that is better than the robot accuracy, at least for a vector component of the x and y metrology position coordinates that is at least one of transverse or perpendicular to the scale imaging axis direction. The second relative position is different than the first relative position, and corresponds to a second surface location on the workpiece that is different than the first surface location , and for which the first respective imageable feature may not be included in the second acquired image and/or the second respective imageable feature may not be included in the first acquired image. Such techniques are noted to be distinct from techniques utilizing fiducials or other reference marks , for which the same fiducial or reference mark is required to be in each image, as compared to an XY scale 170 for which position information may be determined across the entire range of the XY scale 170, and correspondingly for any portion of the XY scale 170 that is included in an image corresponding to a field of view FOV of an imaging configuration 160. At a block 795, the first and second relative positions and/or related position information is utilized to determine a dimension of the workpiece that corresponds to a distance between the first and second surface locations on the workpiece that correspond to the respective end tool positions , as indicating the contact point positions when contacting the respective first and second surface locations on the workpiece, at the first and second image acquisition times. It will be appreciated that rather than using the position sensors , rotary encoders, linear encoders, of the robot to determine the first and second surface locations on the workpiece with the robot accuracy, more accurate position information may be determined utilizing the techniques as described above. More specifically, the determination of the first and second surface locations i. e. , as corresponding to first and second locations on the XY scale for which a precise distance between such locations may be determined utilizing the techniques as described above in accordance with the accuracy of the XY scale allows the corresponding dimension on the workpiece between the first and second surface locations to be determined with a high degree of accuracy. 8 is a flow diagram illustrating one exemplary implementation of a routine 800 for determining an end tool position in which different techniques may be utilized during different portions of a movement timing. In general, during the movement timing, one or more arm portions of the robot are moved from first positions to second positions , which may include rotating one or more arm portions around motion mechanisms from first rotary orientations to second rotary orientations. As shown in 8, at a decision block 810, a determination is made as to whether a hybrid mode will be utilized for determining the end tool position during the movement timing. In various implementations, a hybrid mode may also be representative of a process which includes switching between the end tool metrology position coordinates mode and the standard robot position coordinates mode, as described above with respect to 7A. If the hybrid mode is not to be utilized, the routine continues to a block 820, where the position sensors , rotary encoders, linear encoders, of the robot , of the movable arm configuration are solely utilized for determining the end tool position during the movement timing. If the hybrid mode is to be utilized, the routine proceeds to a block 830, for which, during a first portion of a movement timing, the position sensors included in the robot , included in the movable arm configuration of the robot are utilized for determining the end tool position. During such operations, a relative position of an end tool metrology position coordinates determination system may not be determined and/or is otherwise not utilized to determine the end tool position. At a block 840, during a second portion of the movement timing that occurs after the first portion of the movement timing, a determined relative position of the end tool metrology position coordinates determination system is utilized to determine the end tool position. It will be appreciated that such operations enable the system to perform initial/fast/coarse movement of the end tool position during the first portion of the movement timing, and to perform more accurate final/slower/fine movement of the end tool position during the second portion of the movement timing. It will be understood that, although the element name XY scale has been used in this disclosure with reference to the elements 170, 170, 170A, 170B and the like, this element name is exemplary only, and not limiting. It is referred to as an XY scale with reference to a cartesian coordinate system, and it is described as comprising a nominally planar substrate , arranged nominally perpendicular to a scale imaging axis direction, which may be parallel to a z axis in certain implementations. However, more generally, the element name XY scale should be understood to refer to any reference scale comprising a plurality of features or markings that correspond to known two-dimensional coordinates on that reference scale , accurate and/or accurately calibrated locations in two dimensions, provided that the scale is able to operate as disclosed herein. For example, such scale features may be expressed and/or marked to be in a cartesian coordinate system on that reference scale, or in a polar coordinate system, or any other convenient coordinate system. Furthermore, such features may comprise features distributed evenly or unevenly throughout an operational scale area, and may comprise graduated or ungraduated scale markings, provided that such features correspond to known two-dimensional coordinates on the scale and are able to operate as disclosed herein. It will be understood that although the robot systems disclosed and illustrated herein are generally shown and described with reference to a certain number of arm portions , 3 arm portions, 5 arm portions, , such systems are not so limited. In various implementations, provided that it includes arm portions such as those described and/or claimed herein, the robot system may include fewer or more arm portions if desired. It will be understood that the XY scale or reference scale and a camera that is used to image the scale may undergo rotation relative to one another, depending on the motion and/or position of the robot system. It will be appreciated that methods known in the art , as disclosed in the incorporated references may be used to accurately determine any such relative rotation and/or perform any required coordinate transformations, and/or analyze the relative position of the camera and the scale according to principles disclosed herein, despite such relative rotations. It will be understood that the metrology position coordinates referred to herein take into account any such relative rotation. Furthermore, it will be understood that in some implementations the metrology position coordinates referred to herein may comprise a set of coordinates that include a precise determination and/or indication of any such relative rotation, if desired. While preferred implementations of the present disclosure have been illustrated and described, numerous variations in the illustrated and described arrangements of features and sequences of operations will be apparent to one skilled in the art based on this disclosure. Various alternative forms may be used to implement the principles disclosed herein. In addition, the various implementations described above can be combined to provide further implementations. All of the patents and patent applications referred to in this specification are incorporated herein by reference, in their entirety. Aspects of the implementations can be modified, if necessary to employ concepts of the various patents and applications to provide yet further implementations. These and other changes can be made to the implementations in light of the above-detailed description. In general, in the following claims, the terms used should not be construed to limit the claims to the specific implementations disclosed in the specification and the claims, but should be construed to include all possible implementations along with the full scope of equivalents to which such claims are entitled.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/56/131/109/0.pdf",
                    "CONTRADICTION_SCORE": 0.9427707195281982,
                    "F_SPEC_PARAMS": [
                        "positioning accuracy",
                        "rotary encoder performance",
                        "mechanical stability",
                        "accuracy"
                    ],
                    "S_SPEC_PARAMS": [
                        "time",
                        "accuracy"
                    ],
                    "A_PARAMS": [
                        "calibration techniques"
                    ],
                    "F_SENTS": [
                        "In various implementations, such systems may have a positioning accuracy of approximately 100 microns, as limited by certain factors , the rotary encoder performance in combination with the mechanical stability of the robot system, .",
                        "4,725,965, which is hereby incorporated herein by reference in its entirety, discloses certain calibration techniques for improving the accuracy of a SCARA system."
                    ],
                    "S_SENTS": [
                        "While techniques such as those described in the '965 patent may be utilized for calibrating a robot system, in certain applications it may be less desirable to utilize such techniques , which may require significant time and/or may not provide a desired level of accuracy for all possible orientations of a robot during certain operations, ."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Stability of Object"
                    ],
                    "F_SIM_SCORE": 0.7756151258945465,
                    "S_TRIZ_PARAMS": [
                        "Speed",
                        "Accuracy of Measurement"
                    ],
                    "S_SIM_SCORE": 0.6403828263282776,
                    "GLOBAL_SCORE": 1.7507696956396104
                },
                "sort": [
                    1.7507697
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11383940-20220712",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11383940-20220712",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2017-04-28",
                    "PUBLICATION_DATE": "2022-07-12",
                    "INVENTORS": [
                        "Qixin Xu",
                        "Rong Du",
                        "Lingyu Li"
                    ],
                    "APPLICANTS": [
                        "SHANGHAI MICRO ELECTRONICS EQUIPMENT (GROUP) CO., LTD.    ( Shanghai , CN )"
                    ],
                    "INVENTION_TITLE": "Robot arm for holding cassette and automatic cassette transfer device",
                    "DOMAIN": "B65G 47905",
                    "ABSTRACT": "A robotic manipulator for handling a cassette and an automated cassette transport device are disclosed, the device including a mechanical arm 1, an end actuator 2 at an end of the mechanical arm 1 and a vision-based locating assembly 3 on the end actuator 2. On the one hand, the vision-based locating assembly 3 can determine the position of a flange of the cassette, thereby identifying the cassette. In other words, the cassette can be identified based on a feature of the cassette itself. This dispense with a separate locating marker while improving cassette measuring and handling accuracy. On the other hand, transportation of the cassette is allowed by complementary retention of its flange on a recessed surface of the end actuator 2.",
                    "CLAIMS": "1. A robotic manipulator for handling a cassette, comprising a mechanical arm, an end actuator at an end of the mechanical arm and a vision-based locating assembly on the end actuator, wherein the vision-based locating assembly is configured to obtain a position of a flange on a top of the cassette, and wherein the end actuator comprises a recessed surface complementary to the flange on the top of the cassette, and the end actuator is configured to hold the flange on the top of the cassette by the recessed surface based on the position of the flange, wherein the end actuator is a C-shaped semi-closed structure with support plates arranged on two lateral sides of the semi-closed structure. 2. The robotic manipulator for handling a cassette of claim 1, wherein the vision-based locating assembly comprises a horizontal locating member and a vertical locating member, the horizontal locating member being configured for capturing a horizontal feature of a surface of the cassette, the vertical locating member being configured for capturing a vertical feature of the surface of the cassette, and wherein the position of the flange is determined based on the horizontal and vertical features of the surface of the cassette. 3. The robotic manipulator for handling a cassette of claim 2, wherein the horizontal locating member comprise a first camera facing toward either lateral side of the end actuator. 4. The robotic manipulator for handling a cassette of claim 2, wherein the vertical locating member comprise a second camera and a reflector, both disposed on a lateral side of the end actuator. 5. The robotic manipulator for handling a cassette of claim 1, wherein the end actuator further comprises shoulders that are provided along a front side of the recessed surface to prevent accidental drop-off of the cassette. 6. The robotic manipulator for handling a cassette of claim 1, wherein at least one presence sensor is provided on the recessed surface to sense whether the cassette is held by the recessed surface. 7. The robotic manipulator for handling a cassette of claim 1, further comprising a sonar sensor that is disposed on the end actuator to ensure safety during transportation of the cassette. 8. The robotic manipulator for handling a cassette of claim 1, wherein at least two locating pins are arranged on the recessed surface, each being configured to mate with a corresponding one of notches of the flange. 9. The robotic manipulator for handling a cassette of claim 1, wherein the end actuator is capable of forming an inverted-L shape together with the end of the mechanical arm. 10. The robotic manipulator for handling a cassette of claim 1, wherein the end actuator is 360-degree rotatable at the end of the mechanical arm. 11. The robotic manipulator for handling a cassette of claim 1, wherein the mechanical arm is movable in six degrees of freedom. 12. An automated cassette transport device, comprising an automatic guided vehicle; a carrier frame mounted on the automatic guided vehicle; and the robotic manipulator for handling a cassette of claim 1, which is disposed on the carrier frame. 13. The automated cassette transport device of claim 12, further comprising cassette carriers disposed on the carrier frame, wherein the robotic manipulator is located at a corner of the carrier frame relative to the cassette carriers.",
                    "FIELD_OF_INVENTION": "The present invention relates to the field of mechanics and, in particular, to a robotic manipulator for handling cassette and an automated cassette transport device.",
                    "STATE_OF_THE_ART": "Since the advent of the 21st century, automated logistics systems have been widely used in various industrial applications. In the integrated circuit IC industry, automated material handling systems AMHSs, including stocker, overhead hoist transfer OHT or automatic guided vehicle AGV, have found extensive use in production lines in front-end semiconductor fabrication plants fabs, in particular in 12-inch lines. In such an AMHS, for example, wafer cassettes are directly transported, as scheduled by a manufacturing execution system MES system, from the stocker to load ports of various pieces of process equipment for automated production. This fades out the previous low-efficiency manual handling, greatly improves the automation of advanced IC production lines, significantly saves labor costs and increases product output and customer value. At present, OHT systems are being used mainly for material handling in 12 production lines for front-end processes globally but not in back-end packaging lines despite their similar needs for automated material handling. With the increasing prevalence of 12 products, back-end fab workers are increasingly frequently faced with the need for handling 12 cassettes which weigh about 7. 5 kg when fully loaded. This job is strenuous and somehow risky. This, coupled with the predicted trend of increasing labor shortage and cost, gives rise to a growing demand for cassette transport robots. However, due to limitations in production scale and profit, most packaging manufacturers could not afford the investment in OHT systems as the front-end semiconductor manufacturers do. Therefore, there is an urgent need in the art to develop a convenient, cost-efficient automatic cassette transport solution and equipment. Globally, some electronics and semiconductor plants have replaced manual handling with AGVs in their production lines. Although these transport robots can save a lot of manual labor, most of them can handle only 8 cassettes but not 12 ones. Existing 12 cassette transport devices are low in efficiency because they can carry only one cassette per trip. Moreover, the cassette pickup operation of the cassette transport devices is usually not adaptive to ordinary stock shelves. This is because such a 12 cassette transport device has a long robotic arm which requires a large operating space. On the contrary, in order to achieve a maximum storage capacity and intra-fab free space, most stock shelves installed in current fabs are designed as tri-layered structures with an interlayer spacing not exceeding 450 mm. For an automated 12 cassette transport device, it leaves a narrow space for the long robotic arm to fetch cassettes therefrom. Further, cassette location identification plays a key role in ensuring correct cassette handling of the aforesaid automated cassette transport devices. Currently, such identification is essentially accomplished in either of the following two approaches. In the first approach, an OHT system is employed, in which an OHT shuttle is loaded with a cassette from the stocker and then follows a track to approach a load port of the target process equipment. When it is located over the port, the cassette is lowered thereon by a rope. This process is so established that the location of the cassette in the stocker relative to other locations therein is taught by a robotic arm of the stocker and the location thereof in the OHT shuttle can be known from the track and the involved locating mechanical elements. In addition, the discrimination of the target load port, on which the cassette is to be placed by the OHT shuttle, from other load ports is enabled also by the track as well as by the process equipment layout. In the second approach, photo-detectors for reading markers attached to the shelves are used to identify a target cassette. This solution, however, suffers from the following two drawbacks:1. Locating accuracy of the markers degrades over time, which may finally lead to incorrect cassette pickups. 2. The markers must be deployed relative to cassette trays with sufficient locating accuracy, which increases the cost of manufacturing the shelves.",
                    "SUMMARY": [
                        "The present invention seeks to overcome the drawbacks of the conventional automated cassette transport devices by proposing a cassette handling robotic manipulator and a novel automated cassette transport device. To this end, the proposed cassette handling robotic manipulator includes a mechanical arm, an end actuator at an end of the mechanical arm and a vision-based locating assembly on the end actuator,wherein the vision-based locating assembly is configured to obtain a position of a flange on a top of the cassette, andwherein the end actuator includes a recessed surface complementary to the flange on the top of the cassette, and the end actuator is configured to hold the flange on the top of the cassette by the recessed surface based on the position of the flange. Optionally, in the cassette handling robotic manipulator, the vision-based locating assembly may include: a horizontal locating member and a vertical locating member, the horizontal locating member being configured for capturing a horizontal feature of a surface of the cassette, the vertical locating member being configured for capturing a vertical feature of the surface of the cassette, and wherein the position of the flange is determined based on the horizontal and vertical features of the surface of the cassette. Optionally, in the cassette handling robotic manipulator, the horizontal locating member may include a first camera facing toward either lateral side of the end actuator. Optionally, in the cassette handling robotic manipulator, the vertical locating member may include a second camera and a reflector, both disposed on a lateral side of the end actuator. Optionally, in the cassette handling robotic manipulator, the end actuator may be a C-shaped semi-closed structure with support plates arranged on two lateral sides of the semi-closed structure. Optionally, in the cassette handling robotic manipulator, the end actuator may further include shoulders that are provided along a front side of the recessed surface to prevent accidental drop-off of the cassette. Optionally, in the cassette handling robotic manipulator, at least one presence sensor may be provided on the recessed surface to sense whether the cassette is held by the recessed surface. Optionally, the cassette handling robotic manipulator may further include a sonar sensor that is disposed on the end actuator to ensure safety during transportation of the cassette. Optionally, in the cassette handling robotic manipulator, at least two locating pins may be arranged on the recessed surface, each being configured to mate with a corresponding one of notches of the flange. Optionally, in the cassette handling robotic manipulator, the end actuator may define an inverted-L shape together with the end portion of the mechanical arm. Optionally, in the cassette handling robotic manipulator, the end actuator may be 360-degree rotatable at the end of the mechanical arm. Optionally, in the cassette handling robotic manipulator, the mechanical arm may be movable in six degrees of freedom. The above object is also attained by the proposed automated cassette transport device which includes: an automatic guided vehicle AGV; a carrier frame mounted on the AGV; and the cassette handling robotic manipulator as defined above, which is disposed on the carrier frame. Optionally, the automated cassette transport device may further include cassette carriers disposed on the carrier frame, wherein the robotic manipulator is located at a corner of the carrier frame relative to the cassette carriers. In the cassette handling robotic manipulator of the present invention including the mechanical arm, the end actuator at an end of the mechanical arm and the vision-based locating assembly on the end actuator, the vision-based locating assembly identifies a cassette by locating a top flange thereof, i. e. , identify the cassette based on a feature of its own, without needing a separate locating marker. This leads to improvements in both cassette measuring and handling accuracy. Moreover, transportation of the cassette is enabled by complementary retention of its top flange on a recessed surface of the end actuator. Further, the retention of the top flange on the recessed surface can concur with mating between locating pins on the recessed surface and notches in the top flange, which allows self-centering of the cassette, thereby achieving an additional improvement in locating accuracy.",
                        "1 is a schematic view of an automated cassette transport device according to one embodiment of the present invention. 2 is a schematic view of an end actuator according to one embodiment of the present invention. 3 is a schematic view illustrating the retention of a flange of a cassette on a recessed surface of the end actuator according to one embodiment of the present invention. 4 is a schematic view of a presence sensor according to one embodiment of the present invention. In these figures, 1mechanical arm; 2end actuator; 20support plate; 21recessed surface; 22shoulder; 23presence sensor; 24sonar sensor; 25locating pin; 3vision-based locating assembly; 30horizontal locating member; 300first camera; 31vertical locating member; 310second camera; 311reflector; 40flange; 5carrier frame."
                    ],
                    "DESCRIPTION": "The cassette handling robotic manipulator and automated cassette transport device proposed in the present invention will be further described with reference to the following detailed description of several particular embodiments thereof, which is to be read in connection with the accompanying drawings. Features and advantages of the invention will be more apparent from the following detailed description, and from the appended claims. It is noted that the figures are provided in a very simplified form and not necessarily drawn to scale, with the only intention to facilitate convenience and clarity in explaining the embodiments. Reference is now made to 1, a schematic view of an automated cassette transport device constructed in accordance with the present invention. As shown in 1, the automated cassette transport device includes: an automatic guided vehicle AGV; a carrier frame 5 mounted on the AGV; a robotic manipulator disposed on the carrier frame 5 for handling the cassettes; and cassette carriers disposed on the carrier frame 5. The robotic manipulator may be arranged at a corner of the carrier frame 5 relative to the cassette carriers. Preferably, the number of the cassette carriers is two, which are positioned one above the other. Referring to 2 and 3, according to one embodiment, the robotic manipulator essentially includes: a mechanical arm 1; an end actuator 2 at one end of the mechanical arm 1; and a vision-based locating assembly disposed on the end actuator 2. The vision-based locating assembly is used for obtaining a position of a flange 40 on the top of a cassette the flange being a feature of the cassette itself. The end actuator 2 includes a recessed surface 21 complementary to the flange 40 on the top of the cassette. The end actuator 2 is configured to hold the flange 40 on the top of the cassette by means of the recessed surface 21 based on the obtained position of the flange 40 of the cassette. The position of the flange 40 of the cassette obtained by the vision-based locating assembly accomplishes identification of the cassette. Since each cassette has a fixed size, the location where the cassette is retained can be known once the position of the flange 40 of the cassette is obtained. This dispenses with the need for a separate locating marker while improving measuring and handling accuracy. Transportation of the cassette may follow by the end actuator 2, in particular, by the retention of the flange 40 of the cassette on the recessed surface 21 of the end actuator 2. Therefore, the flange 40 of the cassette is not only capable of identification but also can assist the end actuator 2 in transporting the cassette. With reference to 1 and 3, in a particular process of transporting the cassette, the robotic manipulator may hold the cassette by inserting the end actuator 2 into a space between the flange 40 on the top of the cassette and the main body of the cassette to a proper position and then vertically lift the cassette. After the cassette is put in place in the automated cassette transport device, the end actuator 2 may be lowered to detach itself from the cassette and then be withdrawn. Preferably, the mechanical arm 1 in the robotic manipulator is movable in six degrees of freedom, and the end actuator 2 is 360-degree rotatable at the end of the mechanical arm 1. In operation, the end actuator 2 may be oriented to define an inverted-L shape together with the end section of the mechanical arm 1. Specifically, with the end actuator 2 being inserted in the stock shelf, an adjacent end section of the mechanical arm 1 may be located under the end actuator 2. In this way, loss of free space in the room where the cassette is contained , the shelf, the cassette carrier, resulting from the insertion of the end actuator 2 can be minimized, and the end of the mechanical arm 1 will experience a reduced torque. According to one embodiment, at least two locating pins 25 are provided on the recessed surface 21. The locating pins 25 can mate with corresponding notches in the flange 40. The notches of the flange 40 may be triangular or circular, and the locating pins 25 are complementary to the notches of the flange 40 on the top of the cassette in terms of both position and shape. While the locating pins 25 have been described above as being cross-sectionally triangular or circular, the present invention is not so limited because the locating pins 25 may have any cross-sectional shape as long as they can be received in the notches of the flange 40, with the geometric centers of the locating pins 25 coinciding with those of the notches. When the notches of the flange 40 mate with the locating pins 25 on the recessed surface 21, the retained cassette will be self-centered, resulting in an additional improvement in locating accuracy of the cassette. Holding the cassette on the end actuator 2 with precisely controlled positional accuracy is helpful in correct placement of the cassette onto the transport vehicle. As shown in 3, the end actuator works in a passive mode. The end actuator 2 may be a C-shaped semi-closed structure with support plates 20 on both sides. The C-shaped semi-closed structure can provide a function like the cables in a cable-stayed bridge. As shown in 2, the vision-based locating assembly may include a horizontal locating member 30 and a vertical locating member 31. The horizontal locating member 30 is used for capturing a horizontal feature of the surface of the cassette; and the vertical locating member is used for capturing a vertical feature of the surface of the cassette. The position of the flange 40 of the cassette, i. e. , the exact spatial position of the flange 40, can be determined based on the horizontal and vertical features to serve a basis for subsequent accurate cassette retention and transportation by the end actuator 2. The horizontal locating member 30 may include a first camera 300. The first camera 300 is disposed at a center of the end actuator 2. The first camera 300 has a lens facing toward either one of the support plates 20 of the end actuator 2. The vertical locating member 31 may include a second camera 310 and a reflector 311. The second camera 310 and the reflector 311 are both disposed on the other one of the support plates 20 than the one toward which the lens of the first camera 300 faces, as well as on a proximal lateral side of the end actuator 2. With this arrangement, the distance between the second camera 310 and the flange 40 of a target cassette can be minimized to facilitate a measurement of the flange 40 of the target cassette, with the end actuator 2 being inserted within the room where the cassette is retained. The end actuator 2 may further include shoulders 22 and at least one presence sensor 23. The shoulders 22 are provided along a front side of the recessed surface 21 in order to avoid drop-off of the cassette upon sudden power-off of the robotic manipulator. The at least one presence sensor 23 is arranged on the recessed surface 21, preferably in an area coming into contact with the flange 40, in order to sense whether the cassette is held on the recessed surface 21. Preferably, the presence sensor 23 may include a mechanical switching element, as shown in 4, which will be triggered to, once the cassette is self-centered, raise a signal indicating the presence of the cassette. In a preferred embodiment, the robotic manipulator further includes a sonar sensor 24 disposed on either one of the support plates 20 of the end actuator 2. Once any object is detected to be within a threshold detection range of the sonar sensor during cassette transportation, the mechanical arm 1 may be immediately deactivated so as to ensure safety. Vertical laser sensors may be further arranged on sides of the carrier frame 5 to enable vertical obstacle detection for the automated cassette transport device. A process of fetching a cassette from a storage shelf by the automated cassette transport device as defined above may essentially include:determining parking positions for various layers of the shelf; performing a horizontal measurement of the notches of the flange 40 using the first camera 300; obtaining positions of the cassette in the X-, Y-, Z- and Rx-directions from horizontal features of the notches of the flange 40; bringing the second camera 310 above the notches of the flange 40 and obtaining X-, Y- and Rz-directional positions of the cassette from vertical features of the notches; and holding the cassette by the end actuator from under the flange 40 on the top of the cassette. The present invention also provides a cassette transport system incorporating the automated cassette transport device as defined above. The cassette transport system can perform the following operations:1 Remote Scheduling: A controller of the cassette transport system issues a cassette transfer task and provides associated information indicating the ID of the cassette to be transferred, start and destination points, etc. A scheduling server performs optimal scheduling based on the positions and usage states of all automated cassette transport devices in the fab to select the most suitable one for the task and informs its AGV of all the information necessary for fulfilling the task, including the cassette ID and the start and destination points. 2 Movement to Target Positions: The AGV of the automated cassette transport device calculates an optimal path based on the coordinates of its current and target positions and follows the path to reach the start point where the target cassette is accessible. During the movement, it can autonomously avoid any obstacle in the path. Upon the arrival, the AGV adjusts its own orientation to allow the mechanical arm to hold the cassette from the shelf or the process equipment so as to get ready for transportation. 3 Cassette Transportation: the task may include either removal of the cassette from shelf/process equipment or placement thereof onto the shelf/process equipment. The removal may be accomplished by the following steps:a The automated transport device moves to a parking position in the vicinity of the load port of the process equipment/shelf. Assuming the shelf is a tri-layered structure having three layers of different heights, hereinafter referred to as a top layer, a middle layer and a bottom layer, the parking position may vary with the layer where the cassette is stored, in order to ensure operational reliability of the automated transport device. For example, in case of the cassette being stored on the top layer higher than the robotic manipulator, leaving a large space for the operation thereof, the parking position may be close to the shelf, , about 100 mm away therefrom, in order to minimize a stretched length of the robotic manipulator to reduce the possibility of tip over of the transport device due to a torque generated when the cassette is load. In addition, motion of the robotic manipulator should be designed to avoid collisions or inadvertent harassment. If the cassette is stored on the bottom layer that is nearly as high as the robotic manipulator, the parking position may be farther from the shelf, , about 400 mm away therefrom. In this case, as the operating space for the robotic manipulator is reduced, motion of the robotic manipulator should be properly designed to avoid collisions or inadvertent harassment. For the case in which the cassette is located on the middle layer, the parking position may be between those for the two cases discussed above, and the avoidance of collisions or inadvertent harassment should also be taken into account. b The first camera 300, and hence the sonar sensor 24, is turned laterally, by the end actuator to face toward the target cassette. The first camera 300 measures the X-directional distance X between the flange 40 of the cassette and the first camera 300. Specifically, assuming the lens of the first camera 300 has a magnification M = Image Size Object Size and a focal length f, X can be obtained as X = f * 1 M + 1 . c Image processing may then be carried out to determine the positions of the notches of flange 40, from which a Y-directional distance from the flange 40 to the first camera 300 can be obtained. The notches may be identified using a contour or gray scale based library matching algorithm, matching based on corner points and descriptors, a linear detection based algorithm or the like. d Based on the X- and Y-directional distances from the flange 40 to the first camera 300, a Z-directional distance and Rx-directional rotation between the flange 40 and the first camera 300 may be calculated using a Hough transform or a similar linear detection based algorithm. e Based on the measured X-, Y-, Z- and Rx-directional positions of the flange 40 of the cassette relative to the first camera 300, the robotic manipulator brings the second camera 310 above the notches of the flange 40 on the top of the cassette. f Image processing may be performed to determine the positions of more than two notches of the flange 40, from which, along with the nominal dimensions of the flange 40 itself, X-, Y- and Rz-directional positions of the flange 40 relative to the second camera 310 can be calculated. g Based on the X-, Y- and Rz-directional positions of the flange 40 relative to the second camera 310, the end actuator 2 is inserted under the flange to a proper position allowing notches therein to mate with the locating pins 25 on the end actuator 2. The cassette is then lifted by the end actuator 2 and self-centered, triggering the mechanical switching element in the cassette presence sensor to inform the controller of the automated cassette transport device of the presence of the cassette. h The controller of the automated cassette transport device dictates the mechanical arm to follow a certain motion trajectory to put the cassette held by the end actuator in place onto a cassette carrier in the device itself. The trajectory should not go beyond a widthwise projected space of the AGV. The placement may result from slow lowering of the robotic manipulator. i After the cassette is put in place, an RFID sensor may read information of the cassette and record data about the whole cassette transport process, followed by feedback of the data to the cassette transport system. j The automated cassette transport device waits for the next task in a predefined sequence of tasks. 4 Cassette Shelve: This operation may be accomplished by the following steps:a The automated transport device moves to a target parking position. This step is similar to the above step a and will not be further detailed for the sake of simplicity. b The vision-based locating assembly of the end actuator is laterally turned to face toward the shelf in particular, to a facial datum plane for the cassette. c The mechanical arm brings the vision-based locating assembly above a cassette tray on the load port of the process equipment/shelf, onto which the cassette is to be placed. Features of the cassette tray , locating pins on the load port of standard process equipment are then captured to determine its center. A target position at which the cassette is to be placed is then determined based on the center of the features as well as a center of the cassette held by the automated cassette transport device or a center of the locating pins thereon that lock the cassette. d From positions of the aforesaid features relative to the vision-based locating assembly, X-, Y-, Z- and Rz-directional offsets from the vision-based locating assembly to the center of the features are obtained, based on which the distance of the end actuator of the mechanical arm from the target position is in turn obtained. e Based on the determined target position for the cassette, the end actuator is inserted below the flange 40 on the top of the cassette and lifts the cassette, thus detaching it from the locating pins consequently, the cassette can be moved in the X or Y direction. f Based on the obtained distance, the mechanical arm follows a certain motion trajectory to put in place the cassette onto the cassette tray on the load port of the process equipment/shelf. The trajectory should be within a projected not go beyond a projected space of a base. g During the movement of the mechanical arm, lateral sonars equipped on both sides of the base are activated to sense whether there is an approaching person. If this happens, the mechanical arm is slowed down and gives out an alert to prompt the person to step away from the operation site. h The placement may result from slow lowering of the robotic manipulator. i After the cassette is put in place, data about the whole process are recorded. j The recorded data are fed back to the cassette transport system. k The automated cassette transport device waits for the next task in a predefined sequence of tasks. In summary, in the cassette handling robotic manipulator of the present invention including the mechanical arm, the end actuator at an end of the mechanical arm and the vision-based locating assembly on the end actuator, the vision-based locating assembly identifies a cassette by locating a flange thereof, i. e. , identify the cassette based on a feature of its own, without needing a separate locating marker. This leads to improvements in both cassette measuring and handling accuracy. Moreover, transportation of the cassette is enabled by complementary retention of its flange on a recessed surface of the end actuator. Further, the retention of the flange on the recessed surface can concur with mating between locating pins on the recessed surface and notches in the top flange, which allows self-centering of the cassette, thereby achieving an additional improvement in locating accuracy. The description presented above is merely that of a few preferred embodiments of the present invention and does not limit the scope thereof in any sense. Any and all changes and modifications made by those of ordinary skill in the art based on the above teachings fall within the scope as defined in the appended claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/40/839/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9713745713233948,
                    "F_SPEC_PARAMS": [
                        "low-efficiency manual handling,",
                        "saves labor costs",
                        "increases product output and customer value"
                    ],
                    "S_SPEC_PARAMS": [
                        "efficiency",
                        "carry only one cassette per trip",
                        "adaptive to ordinary stock shelves",
                        "incorrect cassette pickups"
                    ],
                    "A_PARAMS": [
                        "Locating accuracy of the markers"
                    ],
                    "F_SENTS": [
                        "This fades out the previous low-efficiency manual handling, greatly improves the automation of advanced IC production lines, significantly saves labor costs and increases product output and customer value."
                    ],
                    "S_SENTS": [
                        "Existing 12 cassette transport devices are low in efficiency because they can carry only one cassette per trip.",
                        "Moreover, the cassette pickup operation of the cassette transport devices is usually not adaptive to ordinary stock shelves.",
                        "This solution, however, suffers from the following two drawbacks:1.",
                        "Locating accuracy of the markers degrades over time, which may finally lead to incorrect cassette pickups."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Level of Automation",
                        "Productivity"
                    ],
                    "F_SIM_SCORE": 0.5196327716112137,
                    "S_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.7680668234825134,
                    "GLOBAL_SCORE": 1.7485577022035916
                },
                "sort": [
                    1.7485577
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11266421-20220308",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11266421-20220308",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-10-01",
                    "PUBLICATION_DATE": "2022-03-08",
                    "INVENTORS": [
                        "Riddhit Mitra",
                        "Samuel Dumpe",
                        "Branislav Jaramaz",
                        "David Davidson"
                    ],
                    "APPLICANTS": [
                        "Smith & Nephew, Inc.    ( Memphis , US )"
                    ],
                    "INVENTION_TITLE": "Robotically assisted bone preparation for positioning of cutting block",
                    "DOMAIN": "A61B 171604",
                    "ABSTRACT": "Surgical punch tools and methods of using the same are disclosed herein. A surgical punch tool may include a stationary base component having a planar bottom side. The planar bottom side may include one or more posts extending therefrom. Each post may include an associated sharp pin. A movable actuation portion may be configured to move each sharp pin from a non-actuated position within the corresponding post to an actuated position extending through the base component and from the corresponding post.",
                    "CLAIMS": "1. A surgical punch tool, comprising: a stationary base component having a planar bottom side and an upper housing comprising one or more channels; first and second posts spaced apart from each other and extending from the planar bottom side such that the first and second posts are configured to sit against medial and lateral condyles of a femur, respectively, when placed against a resected distal surface of the femur; a first sharp pin and a second sharp pin separate from the first sharp pin; an actuation portion received by and moveable within the channels and comprising an impact face; and first and second springs coupled to the actuation portion and the stationary base component and configured to bias the actuation portion relative to the stationary base component, wherein the actuation portion is configured to simultaneously move the first and second sharp pins from a non-actuated position within the first and second posts, respectively, to an actuated position extending through the stationary base component and from the first and second posts, respectively, in response to impaction of the impact face, to thereby engage the medial and lateral condyles. 2. The surgical punch tool of claim 1, wherein the first and second springs are configured to provide a biasing force opposing movement of the actuation portion towards the stationary base component. 3. The surgical punch tool of claim 1, wherein the first and second springs are configured to provide a biasing force promoting movement of the actuation portion towards the stationary base component. 4. The surgical punch tool of claim 1, wherein each of the first and second posts extends from the stationary base component by about 2. 5 mm to about 5 mm. 5. The surgical punch tool of claim 1, wherein each of the first and second posts has a diameter of about 5 mm to about 7. 5 mm. 6. The surgical punch tool of claim 1, wherein each of the first and second sharp pins extends from the stationary base component by about 15 mm to about 20 mm in the actuated position. 7. The surgical punch tool of claim 1, wherein each of the first and second sharp pins has a diameter of about 2. 5 mm to about 4 mm. 8. The surgical punch tool of claim 1, wherein a center of the first sharp pin is about 1. 25 inches to about 1. 75 inches away from a center of the second sharp pin.",
                    "FIELD_OF_INVENTION": "The present disclosure generally relates to the use of robotic systems in planning and performing orthopedic implant surgical procedures. More specifically, the present disclosure relates to using robotic systems to guide bone resection for more efficient placement of a cutting block.",
                    "STATE_OF_THE_ART": "The use of computers, robotics, and imaging to aid orthopedic surgery is well known in the art. There has been a great deal of study and development of computer-aided navigation and robotics systems used to guide surgical procedures. For example, a precision freehand sculptor PFS employs a robotic surgery system to assist the surgeon in accurately cutting a bone into a desired shape. In interventions such as total knee replacement, computer-aided surgery techniques have been used to improve the accuracy and reliability of the surgery. A cut guide can be used in an orthopedic surgery to assist a surgeon in cutting or modifying some portions of a target bone. For example, in joint replacement surgeries, such as total hip replacement THR or total knee replacement TKR, a cut guide can be temporarily attached to the target bone such as a femur or a tibia. An orthopedic surgical cutting tool can be used together with the cut guide to allow the surgeon to selectively cut portions of the ends of the target bone and replace them with endoprosthetic implants. Positioning a cut guide for use in preparing the target bone can be a time-consuming and complicated process, which is critical to positive outcomes for the patient.",
                    "SUMMARY": [
                        "There is provided a surgical punch tool that includes a stationary base component having a planar bottom side, one or more posts extending from the planar bottom side, one or more sharp pins each associated with a corresponding post, and a movable actuation portion configured to move each sharp pin from a non-actuated position within the corresponding post to an actuated position extending through the base component and from the corresponding post. In some embodiments, the base component further includes a plurality of channels configured to receive and guide the actuation portion. In some embodiments, the surgical punch tool further comprises one or more devices configured to provide a biasing force opposing movement of the actuation portion towards the base component. In some embodiments, the surgical punch tool further comprises one or more devices configured to provide a biasing force promoting movement of the actuation portion towards the base component. In some embodiments, the one or more devices comprise at least one spring. In some embodiments, the surgical punch tool further includes a catch configured to secure the sharp pins in the non-actuated position, and a release mechanism configured to release the catch from securing the sharp pins in the non-actuated position. In some embodiments, the actuation portion comprises an impact face configured to receive an impact force that causes the sharp pins to move from the non-actuated position to the actuated position. In some embodiments, each of the one or more posts extends from the base by about 2. 5 mm to about 5 mm. In some embodiments, each of the one or more posts has a diameter of about 5 mm to about 7. 5 mm. In some embodiments, each of the one or more sharp pins extends from the base by about 15 mm to about 20 mm in the actuated position. In some embodiments, each of the one or more sharp pins has a diameter of about 2. 5 mm to about 4 mm. In some embodiments, the one or more sharp pins comprise a first sharp pin and a second sharp pin, and a center of the first sharp pin is about 1. 25 inches to about 1. 75 inches away from a center of the second sharp pin. There is also provided a method of preparing a target bone to receive a cut guide. The method includes selecting a punch tool corresponding to the cut guide. The punch tool comprises a plurality of sharp pins and a movable actuation portion configured to move each sharp pin from a recessed position to an extended position. The method further includes resecting the target bone, positioning the punch tool with respect to the resected target bone, and actuating the actuation portion of the punch tool to move the plurality of sharp pins from the recessed position to the extend position, whereby each of plurality of sharp pins forms a hole in the target bone. In some embodiments, the cut guide comprises a first mounting pin and a second mounting pin, the first mounting pin is spaced apart from the second mounting pin by a distance, the plurality of sharp pins comprises a first sharp pin and a second sharp pin, and the first sharp pin is spaced apart from the second sharp pin by the distance. In some embodiments, resecting the target bone comprises forming, using a robotically assisted cutting tool, a distal plane in the target bone, and forming, using a robotically assisted cutting tool, a plurality of divots in the distal plane, wherein the plurality of divots are configured to receive the punch tool. In some embodiments, each of the plurality of divots has a recessed portion, and each recessed portion is in a shape of one of a cylinder, a cube, a rectangular prism, a triangular prism, a pyramid, and a cone. In some embodiments, positioning the punch tool comprises positioning the punch tool such that each of the plurality of sharp pins is vertically aligned with one of the plurality of divots. In some embodiments, resecting the target bone comprises forming, using a robotically assisted cutting tool, a distal plane in the target bone, and positioning the punch tool comprises tracking a position of the punch tool with a surgical tracking system. In some embodiments, actuating the actuation portion of the punch tool comprises applying an impact force to the actuation portion of the punch tool. In some embodiments, actuating the actuation portion comprises releasing a catch configured to retain the actuation portion in a position such that the plurality of sharp pins are in the recessed position. There is also provided a method of preparing a target bone to receive a cut guide. The method includes receiving, by a surgical system, an indication of the cut guide to use with the target bone and determining, by the surgical system, a punch tool associated with the cut guide. The punch tool comprises a plurality of sharp pins and a movable actuation portion configured to move each sharp pin from a recessed position to an extended position. The method further includes determining, by the surgical system, a portion of the target bone to resect based on the cut guide, controlling, by the surgical system, the operation of a cutting device based on the determined amount and location of the target bone to resect, directing, by the surgical system, placement of the punch tool within the resected target bone, and directing, by the surgical system, actuation of the actuation portion of the punch tool. In some embodiments, controlling the operation of the cutting device comprises tracking, by the surgical system, the location and orientation of the cutting device, and activating, by the surgical system, the cutting tool when the cutting tool is in proximity to the portion of the target bone to resect. The example embodiments as described above can provide various advantages over prior techniques. For example, the techniques as taught herein can more quickly and precisely position a cut guide during a surgical implant procedure. As such, the techniques taught herein provide better patient outcomes resulting from reduced operation times and better placed implants. Further features and advantages of at least some of the embodiments of the present disclosure, as well as the structure and operation of various embodiments of the present disclosure, are described in detail below with reference to the accompanying drawings.",
                        "The accompanying drawings, which are incorporated in and form a part of the specification, illustrate the embodiments of the present disclosure and together with the written description serve to explain the principles, characteristics, and features of the present disclosure. In the drawings: 1 depicts a diagram illustrating an environment for operating a system for planning and performing robotic-assisted implant surgery in accordance with an embodiment. 2 depicts a block diagram depicting a system for planning and performing implant surgery in accordance with an embodiment. 3A and 3B depict a sample bone to be operated on and a surgical guide for use in operating on the sample bone in accordance with an embodiment. 4A and 4B illustrate preparation of a target bone for temporary mounting of a cut guide in accordance with an embodiment. 5A and 5B depict multiple views of a punch guide for aiding in cut guide attachment in accordance with an embodiment. 6A-6D depict a visual workflow of preparing a bone, attaching the punch tool, activating the punch tool, and attaching a cut guide to the bone in accordance with an embodiment. 7 depicts a process workflow for a surgeon performing the surgical process as shown in 6A-6D in accordance with an embodiment. 8 depicts a process workflow for a computer system assisting in the surgical process as shown in 6A-6D in accordance with an embodiment. 9 depicts a computer system for use with the robotically assisted surgical system as described herein."
                    ],
                    "DESCRIPTION": "This disclosure is not limited to the particular systems, devices and methods described, as these may vary. The terminology used in the description is for the purpose of describing the particular versions or embodiments only, and is not intended to limit the scope. As used in this document, the singular forms a, an, and the include plural references unless the context clearly dictates otherwise. Unless defined otherwise, all technical and scientific terms used herein have the same meanings as commonly understood by one of ordinary skill in the art. Nothing in this disclosure is to be construed as an admission that the embodiments described in this disclosure are not entitled to antedate such disclosure by virtue of prior invention. As used in this document, the term comprising means including, but not limited to. The embodiments of the present teachings described below are not intended to be exhaustive or to limit the teachings to the precise forms disclosed in the following detailed description. Rather, the embodiments are chosen and described so that others skilled in the art may appreciate and understand the principles and practices of the present teachings. Robotically assisted surgeries, such as Total Knee Arthroplasty TKA, provide a surgeon with the advantage of planning the procedure and viewing the projected outcome of the procedure prior to performing bone resection. One of the challenges to providing a robotically assisted option is optimizing the workflow efficiency by maintaining continuity with conventional instrumentation. For example, during the manual portion of existing robotically assisted TKA procedures, a set of post holes , four post holes are drilled into the patient's femur. A distal cut guide is then fastened to the femur using these post holes to cut the initial distal cut into the femur. A drill guide can then be fastened to the distal cut guide, and two or more additional holes can be drilled. The drill guide and distal cut guide can then be removed, and a multi-cut guide block, such as a 5-in-1 cut block, can be attached using the two additional drill holes. This disclosure describes an improved workflow for positioning and attaching the multi-cut guide block onto the patient's femur. To achieve the improved workflow, this disclosure teaches using a robotically assisted cutting tool to do the initial distal cut in, for example, a femur, eliminating the distal cutting guide. After the distal cut is complete, a punch tool that can be used to produce holes for mounting a multi-cut guide to eliminate the drill guide, thereby reducing the surgical workflow and instrumentation required for the procedure. In the proposed workflow, the distal cut in the femur is prepared by burring or otherwise cutting the bone under robotic assistance. After preparation of the distal cut using the robotically controlled cutting device, the surgeon can prepare additional divots located proximate to the position of attachment pins on the multi-cut guide block. For example, the surgeon can prepare two additional divots on the distal cut, a first divot on the medial condyle and a second divot on the lateral condyle. A punch tool can be positioned on the distal cut surface, the punch tool including two reference posts configured to sit within the prepared divots. The surgeon can impact the punch tool to create, for example, two holes positioned to receive the mounting pins on the multi-cut guide block and perform additional bone resection using the mounted multi-cut guide block. 1 is an illustration of a system 100 for performing a surgical procedure using a robotic system. The system 100 can include a surgical cutting tool 150 with an associated optical tracking frame 155 also referred to as tracking array 155, a display device 130, an optical tracking system 140, and patient tracking frames 120 also referred to as tracking arrays 120. 1 further illustrates an incision 110 through which a knee replacement surgery can be performed. In certain implementations, the illustrated robotic surgical system 100 can include a hand-held, computer-controlled surgical robotic system, such as the NAVIO Surgical System from Blue Belt Technologies, Inc. of Pittsburgh, Pa. NAVIO is a registered trademark of Blue Belt Technologies, Inc. The illustrated robotic system uses an optical tracking system coupled to the robotic controller to track and control a hand-held surgical instrument. For example, the optical tracking system 140 tracks the tracking array 155 coupled to the surgical tool 150 and tracking arrays 120 coupled to the patient to track a location of the instrument relative to the target bone , femur and tibia for knee procedures. 2 is a block diagram depicting an example system 200 for performing a robotically assisted surgical procedure. In certain implementations, the system 200 can include a control system 210, a tracking system 140, and a surgical instrument 150. Optionally, the system 200 also can include a display device 130 and a database 230. In some examples, these components can be combined to provide navigation and control of the surgical instrument 150, which can include navigation and control of a cutting tool or a point probe, among other things, used during an orthopedic prosthetic implant surgery or similar surgery. The control system 210 can further include one or more computing devices configured to coordinate information received from the tracking system 140 and provide control to the surgical instrument 150. In an example, the control system 210 can include a planning module 212, a navigation module 214, a control module 216, and a communication interface 218. In certain examples, the planning module 212 can provide pre-operative planning services that allow clinicians the ability to virtually plan a procedure prior to reshaping the target joint during the surgical procedure on the patient. A portion of the planning process performed within the planning module can include operations similar to those discussed in 6,205,411 titled Computer-assisted Surgery Planner and Intra-Operative Guidance System, to Digioia et al. , which discusses yet another approach to pre-operative planning. 6,205,411 is hereby incorporated by reference in its entirety. In some examples, such as a TKA, the planning module 212 can be used to manipulate a virtual model of the implant in reference to a virtual implant host model. The virtual model of the implant host illustrating the joint to be replaced can be created through use of a point probe or similar instrument tracked by the optical tracking system 140. In certain implementations, the planning module 212 can collect data from surfaces of the target joint to recreate a virtual model of the patient's actual anatomical structure. Particularly in a joint replacement surgery, this method can increase the accuracy of the planning process by using data collected after the joint has been exposed and without intra-operative imaging. Collecting surface data from the target bones also can allow for iterative reshaping of the target bone to ensure proper fit of the prosthetic implants and optimization of anatomical alignment. In certain implementations, the navigation module 214 can coordinate tracking the location and orientation of the implant, the implant host, and the surgical instrument 150 during the surgical procedure. In certain examples, the navigation module 214 also may coordinate tracking of the virtual models used during pre-operative or intra-operative planning within the planning module 212. Tracking the virtual models can include operations such as alignment of the virtual models with the implant host through data obtained via the tracking system 140. In these examples, the navigation module 214 can receive input from the tracking system 140 regarding the physical location and orientation of the surgical instrument 150 and an implant host. Tracking of the implant host can include tracking multiple individual bone structures, such as with tracking arrays 120. For example, during a total knee replacement procedure, the tracking system 140 can individually track the femur and the tibia using tracking devices anchored to the individual bones as illustrated in 1. In some examples, the control module 216 can process information provided by the navigation module 214 to generate control signals for controlling the surgical instrument 150. The control module 216 also can work with the navigation module 214 to produce visual animations to assist the surgeon during an operative procedure. Visual animations can be displayed via a display device, such as display device 130. In certain implementations, the visual animations can include real-time 3-D representations of the implant, the implant host, and the surgical instrument 150, among other things. In some examples, the visual animations are color-coded to further assist the surgeon with positioning and orienting the implant. In certain implementations, the communication interface 218 can facilitate communication between the control system 210 and external systems and devices. The communication interface 218 can include both wired and wireless communication interfaces, such as Ethernet, IEEE 802. 11 wireless, or Bluetooth, among others. As illustrated in 1, the primary external systems connected via the communication interface 218 can include the tracking system 140 and the surgical instrument 150. Although not shown, the database 230 and the display device 130, among other devices, also can be connected to the control system 210 via the communication interface 218. In some examples, the communication interface 218 can communicate over an internal bus to other modules and hardware systems within the control system 210. In some examples, the tracking system 140 can provide location and orientation information for surgical devices and parts of an implant host's anatomy to assist in navigation and control of semi-active robotic surgical devices. The tracking system 140 can include a tracker , tracking array 120 that can include or otherwise provide tracking data based on at least three positions and at least three angles. The tracker can include one or more first tracking markers associated with the implant host and one or more second markers associated with the surgical device , surgical instrument 150. The markers or some of the markers can be one or more of infrared sources, light emitting sources, radio frequency RF sources, ultrasound sources, and/or transmitters. As examples, the tracking system 140 can be an infrared tracking system, an optical tracking system, an ultrasound tracking system, an inertial tracking system, a wired system, and/or an RF tracking system. One illustrative tracking system can be the OPTOTRAK 3-D motion and position measurement and tracking system from Northern Digital, Inc. of Ontario, Canada, although those of ordinary skill in the art will recognize that other tracking systems of other accuracies and/or resolutions can be used. 6,757,582, titled Methods and Systems to Control a Shaping Tool, to Brisson et al. , provides additional detail regarding the use of tracking systems, such as tracking system 140, within a surgical environment. 6,757,582 is hereby incorporated by reference in its entirely. 3A and 3B illustrate a sample target bone 300 , a patient's femur and a multi-cut guide block 305. In certain implementations, the multi-cut guide block can be a 5-in-1 cut guide for use in TKA as manufactured by Smith &amp; Nephew, Inc. of Memphis, Tenn. However, it should be noted that the 5-in-1 cut guide is provided and described by way of example only, and other types of multi-cut guides can be used with the techniques described herein. As shown in 3B, the guide block 305 includes mounting pins 310. To reduce rotation of the guide block 305 once mounted on the target bone 300, multiple mounting pins 310 can be included. However, as guide block 305 is shown in 3B in a side-profile view, a second mounting pin is positioned behind and hidden by the visible mounting pin 310. The guide block 305 also includes a flat bottom plane 315 configured to interface with a distal cut surface on the target bone 300. In certain implementations, the position of the mounting pins 310 prevent rotational movement of the guide block 305 when inserted into mounting holes, and the interface between the bottom plane 315 and the cut bone prevents lateral movement of the guide block 305. Referring back to 3A, dashed lines 320 and 325 indicate areas of bone to be removed to accommodate the guide block 305. For example, line 320 illustrates a hole configured to receive at least one of the mounting pins 310, and line 325 illustrates the distal cut plane configured to interface with the bottom plane 315 of the guide block 305. 4A and 4B illustrate before and after diagrams of a target bone 300 being prepared for a surgical procedure. In this example, the distal surface of the target bone 300 is resected. Using, for example, a surface preparation tool 420, the surgeon can resect the target bone 300 to remove area 405, thereby defining the distal cut plane , corresponding to line 325 in 3A through the cortical portion of the target bone. However, rather than drill the entire hole corresponding to line 320 in 3A, the surgeon can use the surface preparation tool to create one or more divots 410 into the cancellous portion of the target bone 300. The size and shape of a divot 410 can be determined based on, for example, the size and shape of the protruding posts of the punch tool described in more detail in the discussion of 5A and 5B below. A recessed portion of a divot 410 can be in a shape of a cylinder, a cube, a rectangular prism, a triangular prism, a pyramid, a cone, or other three-dimensional shapes. The size and shape of a divot 410 also can be determined based on the location of a landing site for the punch tool and/or guide block, or based on the anatomical, mechanical, and physical properties of the bone and soft tissue at the landing site. The location of the distal cut plane and the divots 410 can be defined according to the surgical plan as determined by, for example, the planning module 212 as described above. In order to achieve proper bone resection, thereby accurately forming the distal cut plane and the divots 410, the surface preparation tool 420 can be monitored by the tracking system 140 as described above, and operation of a cutting burr or other cutting interface of the surface preparation tool can be controlled by the control module 216 in response to the currently tracked position of the surface preparation tool. It should be noted that a spherical cutting burr is shown in 4B by way of example only, and other shapes, such as a cylindrical or conical cutting burr, can be used for the resection of target bone 300. In certain implementations, surface preparation tool 420 can include a rotary device including a cutting burr, a surgical drill, a surgical mill, a surgical saw, or other surgical equipment capable of creating the recessed portion on the target bone 300. The surface preparation tool 420 can be operated semi-manually by an operator, such as a surgeon, while it is connected to and monitored by an automated computer-controlled system, such as a precision freehand sculptor PFS or other robotic surgical system. 5A and 5B illustrate the punch tool 500 as described herein. As shown in 5A, the punch tool 500 is in a non-actuated position. The punch tool 500 can include a stationary base component 505. In certain implementations, the base 505 can include a planar bottom 510 configured to sit against the distal cut plane of the target bone , target bone 300 as described above as well as posts 515. In some examples, the posts 515 can be configured to extend from the base 505 by about 4. 0 mm. In other examples, the posts 515 can be configured to extend about 2. 5 mm to about 5. 0 mm from the base 505. Similarly, in some implementations, the posts 515 can have a diameter of about 6. 5 mm. In some examples, the posts 515 can have a diameter of about 5 mm to about 7. 5 mm. Based upon the size of the posts 515, the divots , divots 410 as described above can be sized to accept the posts 515. For example, if the posts 515 are extend form the base 505 by about 4. 0 mm, and the posts are about 6. 5 mm in diameter, the divots can be sized about 4. 5 mm deep and 6. 5 mm in diameter. The punch tool 500 can further include an actuation portion 520. The actuation portion 520 can be sized to fit within channels 525 on the base 505, the channels configured to guide the movement of the actuation portion. In certain implementations, the punch tool 500 can further include springs 530 or another device configured to provide a biasing force opposing the movement of the actuation portion 520. It should be noted that springs 530 are shown as coil springs by way of example only. The actuation portion 520 can include an impact face 535. To actuate the punch tool 500, a surgeon can apply an impact force , via a slap hammer to the impact face 535. Such an applied force can move the actuation portion 520 into an actuated position, as shown by 5B. As shown in 5B, when actuated, two sharp pins 540 are exposed from the punch tool 500. The sharp pins 540 are configured to exit the base 505 of the punch tool 500 through the posts 515, thereby penetrating the target bone adjacent to the divots prepared in the target bone. The sharp pins 540 can then penetrate the cancellous bone exposed as a result of the distal cut, thereby forming two holes in the cancellous bone. The sharp pins 540 can be sized based upon the size of the mounting pins on a corresponding multi-cut guide block , guide block 305 as described above. For example, the sharp pins 540 can be about 3 mm in diameter and protrude about 18 mm from the base 505 of the punch tool 500. In some implementations, the diameter of the sharp pins 540 can be from about 2. 5 mm to about 4 mm. Similarly, in some examples, the sharp pins can protrude about 15 mm to about 20 mm from the base 505 of the punch tool 500. Additionally, the spacing between the sharp pins 540 can be determined based upon the corresponding multi-cut guide block being used. For example, a multi-cut guide block can have its mounting pins spaced 1. 5 inches apart on center. A corresponding punch tool 500 can have similarly spaced sharp pins 540, i. e. , 1. 5 inches apart. In some implementations, a punch tool 500 can have the sharp pins 540 spaced about 1. 25 inches apart to about 1. 75 inches apart. Depending upon the manufacturer of the cut guide, in some examples, each cut guide size in a family or similar product line can have the same sized mounting pins. In such an example, a single punch tool can be used with each size cut guide in the family. For example, each cutting guide in the JOURNEY II family as manufactured by Smith &amp; Nephew, Inc. of Memphis, Tenn. , has the same set of mounting pins. JOURNEY is a registered trademark of Smith &amp; Nephew, Inc. Thus, a single sized punch tool can be used with each size of cut guide in the JOURNEY II family. In other implementations, each specific cut guide can have a dedicated and appropriately sized punch tool. Additionally, the punch tool 500 can include a connection point 545 for attaching a handle to the punch tool for easier manipulation and positioning. The punch tool 500 as described above can be manufactured from a material or set of materials that facilitate easy cleaning and sterilization through, for example, a steam sterilization process. In certain implementations, the punch tool 500 can be manufactured from titanium, stainless steel, and other similar materials that are commonly used to manufacture tools and guides for use during surgery. In some examples, the punch tool 500 can be designed to have a unibody or single-piece design that is not able to be disassembled. In such a design, similar to that as shown in 5A and 5B, an open central space can be included in the design to facilitate cleaning of, for example, the sharp pins 540 when in the retracted position as shown in 5A. In other examples, the punch tool 500 can a multi-piece design configured to be taken apart for cleaning and sterilization. It should be noted that the springs 530 as shown above are provided as a way to keep the punch tool 500 in a retracted position, thereby protecting a user from the sharp pins 540. However, additional retraction mechanisms, such as a ball and detent, can be used to keep the punch tool 500 in a retracted position. Keeping the punch tool 500 in a retracted position also acts to protect the sharp pins 540, thereby reducing the chance of misalignment or damage to the posts. In an alternative design, the punch tool 500 can be designed such that the springs 530 are configured to promote movement of the actuation portion 520 toward the target bone. For example, the punch tool 500 can include a catch that secures the punch tool in the non-actuated position. Upon activation of a release mechanism, the springs can pull or otherwise apply a force to the actuation portion 520, thereby causing the sharp pins 540 to penetrate the cancellous portion of the target bone. Such a design removes the need for a surgeon to apply a force to the punch tool 500 for actuation of the sharp pins 540. 6A-6D illustrate a visual representation of a target bone being prepared to receive a multi-cut guide using a punch tool according to the techniques described herein. As shown in 6A, the target bone 600 has been prepared using, for example, a rotary cutting tool or another similar surgical cutting tool according to the process as described above in regard to 4A and 4B. As such, target bone 600 has been prepared to include a distal plane cut 605 and divots 610. Referring to 6B, a punch tool 620 is positioned on the target bone 600. A bottom surface of the punch tool , bottom 510 as described above can be positioned such that it interfaces with the distal cut 605. Additionally, the posts , posts 515 of the punch tool 620 can be positioned within divots 610 of the target bone 600. As shown in 6B, the punch tool 620 can include an attached handle 625 to aid in positioning and holding the punch tool in position. Referring to 6C, the punch tool 620 has been activated, thereby resulting in sharp posts 630 extending into the target bone 600, creating two holes in the target bone corresponding to, for example, line 320 as described above in regard to 3A. Referring to 6D, the punch tool 620 can be removed from the target bone 600. A multi-cut guide 640 can be positioned on the target bone 600 such that mounting pins 645 fit within the holes created by the sharp posts 630, thereby ensuring that the multi-cut guide is securely and properly positioned on the target bone. The surgeon can further secure the cut guide 640 to the target bone 600 using, for example, inch holding pins. The target bone 600 can be further resected using a cutting tool such as an oscillating saw and the multi-cut guide 640. 7 illustrates a sample workflow that may be performed by a surgeon performing the surgical process using the punch tool techniques described herein. It is assumed that various initial steps for the surgical procedure have already been performed prior to actual bone resection, such as selecting an implant for implanting into the patient, initial incisions and soft tissue removal, bone surface mapping and imaging, and other similar steps covered, for example, in 6,205,411 as described above. Based upon the patient's anatomy , the size and shape of the patient's femur and the type of surgery being performed , a TKA, the surgeon can select 705 a cut guide for use during the procedure. As noted above, based upon the size and shape of the cut guide, each cut guide can have an associated punch tool, or a single punch tool can be adapted to work with a series of cut guides. The surgeon can provide information related to the selected 705 cut guide to the surgical system , through the communication interface 218 as described above and, based upon this information, the surgical system can update the surgery plan. It should be noted that the workflow process for the surgical system is described in greater detail in the following discussion of 8. Referring again to 7, the surgeon can then resect 710 the target bone per the robotic-assisted cutting tool instructions. For example, the surgical system can determine an amount of bone to remove during the resection, and provide control signals to a robotically assisted cutting tool being operated by the surgeon. Depending upon the procedure being performed, the surgeon can be instructed to remove various portions of the target bone. For example, during a TKA, the surgeon can use the robotically assisted cutting tool to create 712 a distal plane. Similarly, the surgeon can create 714 the divots for receiving the punch tool. Following bone resection, the surgeon can attach 715 the punch tool to the target bone. The surgeon can check the position of the punch tool and, upon verifying the punch tool is properly positioned, can actuate 720 the punch tool. The surgeon can remove 725 the punch tool, thereby exposing the holes made by the punch tool. The surgeon can attach 730 the selected cut guide to the holes created by the punch tool, and continue 735 the procedure. As noted above, in order to assist the surgeon during the procedure described herein as outlined in 7, a surgical system, such as the control system described above in reference to 2, can perform various functions as well. For example, 8 illustrates a sample workflow for a surgical system during a procedure using techniques and processes described herein. Similar to 7, the description of 8 assumes that various initial steps for the surgical procedure have already been performed by the surgical system prior to actual bone resection, such as receiving a selection of an implant for implanting into the patient, determining an initial surgical plan for the initial incisions and soft tissue removal, receiving information related to the bone surface mapping and imaging, determining a surgical plan for bone resection, and other similar steps covered, for example, in 6,205,411 as described above. Referring to 8, the surgical system can receive 805 an indication of a cut guide that the surgeon has selected. Based upon the cut guide to be used, the surgical system can optionally determine 807 a punch tool associated with the selected cut guide. Based upon the selected cut guide and optionally the punch tool, the surgical system can update 810 the surgical plan to determine an amount and location of bone to be removed for the target bone to properly receive the cut guide. Based upon the updated surgical plan, the surgical system can provide the surgeon with a visual indication of the update , provide a display of the bone to be removed to accommodate the selected cut guide and can control 815 the operation of a cutting device, ensuring that the surgeon is removing bone according to the surgical plan. As noted above, during control 815 of the cutting device, the surgical system also can track the movement and position of the cutting device to accurately monitor 820 the progress of the bone resection. The surgical system can continually map the amount of bone removed to determine 825 whether the cutting is complete. If the cutting is not complete, the surgical system can continue to control 815 operation of the cutting tool and monitor 820 the process of the bone resection. However, if the surgical system does determine that the cutting is complete, the surgical system can wait while the surgeon performs various manual steps , placing, actuating and removing the punch tool and placing the cut guide. Upon receiving 830 and indication that the surgeon has completed the manual steps, the surgical system can continue to monitor and assist with the surgical procedure. It should be noted that, depending upon the capabilities of the tracking system associated with the surgical system, and the design of the punch tool and cut guide, the surgical system can continue to monitor the procedure during the manual steps. For example, if the punch tool and cut guide are configured to include trackable components such as a visual tracking sensor array, the surgical system can monitor the placement of the punch tool and/or the cut guide during the manual steps of the procedure as described herein. It should be also noted that the above description is generally directed to positioning a femur cutting block by way of example only. The punch tool and associated processes for incorporating the punch tool into a surgical workflow can be applied to any surgical procedure where a bone surface is prepared or otherwise resected to a specific geometry for, for example, accepting an implant component. Additionally, various alterations to the workflow as described herein are possible. For example, the handle or another component of the punch tool as described herein can be trackable using the tracking system. In such an embodiment, the surgeon can merely prepare the distal cut and, rather than use the divot and post combination as described herein, use the tracking system to determine proper placement of the punch tool prior to actuation. 9 is a block diagram that illustrates an example of a machine in the form of a computer system 900 within which instructions, for causing the computer system to perform any one or more of the methods discussed herein, may be executed. In various embodiments, the machine can operate as a standalone device or may be connected , networked to other machines. In a networked deployment, the machine may operate in the capacity of a server or a client machine in server-client network environment, or as a peer machine in a peer-to-peer or distributed network environment. The machine may be a personal computer PC, a tablet PC, a set-top box STB, a PDA, a cellular telephone, a web appliance, a network router, switch or bridge, or any machine capable of executing instructions sequential or otherwise that specify actions to be taken by that machine. Further, while only a single machine is illustrated, the term machine shall also be taken to include any collection of machines that individually or jointly execute a set or multiple sets of instructions to perform any one or more of the methodologies and/or processes discussed herein. The example computer system 900 includes a processor 902 such as a central processing unit CPU, a graphics processing unit GPU, or both, a main memory 904 and a static memory 906, which communicate with each other via a bus 908. The computer system 900 may further include a video display unit 910 such as a liquid crystal display LCD or a cathode ray tube CRT, an alpha-numeric input device 912 such as a keyboard, a user interface UI navigation device or cursor control device 914 such as a mouse, a disk drive unit 916, a signal generation device 918 , a speaker and a network interface device 920. The disk drive unit 916 includes a machine-readable storage medium 922 on which is stored one or more sets of instructions and data structures , software 924 embodying or used by any one or more of the methods or functions described herein. The instructions 924 may also reside, completely or at least partially, within the main memory 904, static memory 906, and/or within the processor 902 during execution thereof by the computer system 900, the main memory 904 and the processor 902 also constituting machine-readable media. In an example, the instructions 924 stored in the machine-readable storage medium 922 include instructions causing the computer system 900 to receive a target bone representation including a data set representing the anatomic structure of the target bone. The instructions 924 can also store the instructions 924 that cause the computer system 900 to generate a cut guide positioning plan for positioning the cut guide onto or conforming to the target bone. The machine-readable storage medium 922 may further store the instructions 924 that cause the computer system 900 to produce the two or more divots sized, shaped or otherwise configured to receive and position the punch tool. While the machine-readable storage medium 922 is shown in an example embodiment to be a single medium, the term machine-readable storage medium may include a single medium or multiple media , a centralized or distributed database, and/or associated caches and servers that store the one or more instructions or data structures. The term machine-readable storage medium shall also be taken to include any tangible medium that is capable of storing, encoding or carrying instructions for execution by the machine and that cause the machine to perform any one or more of the methods of the present invention, or that is capable of storing, encoding or carrying data structures used by or associated with such instructions. The term machine-readable storage medium shall accordingly be taken to include, but not be limited to, solid-state memories, and optical and magnetic media. Specific examples of machine-readable media include non-volatile memory, including by way of example, semiconductor memory devices , erasable programmable read-only memory EPROM, electrically erasable programmable read-only memory EEPROM and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. A machine-readable storage medium shall also include devices that may be interpreted as transitory, such as register memory, processor cache, and RAM, among others. The definition provided herein of machine-readable storage medium is applicable even if the machine-readable storage medium is further characterized as being non-transitory. For example, any addition of non-transitory, such as non-transitory machine-readable storage medium, is intended to continue to encompass register memory, processor cache and RAM, among other memory devices. In various examples, the instructions 924 may further be transmitted or received over a communications network 926 using a transmission medium. The instructions 924 may be transmitted using the network interface device 920 and any one of a number of well-known transfer protocols , HTTP. Examples of communication networks include a LAN, a WAN, the Internet, mobile telephone networks, plain old telephone POTS networks, and wireless data networks , Wi-Fi and WiMAX networks. The term transmission medium shall be taken to include any intangible medium that is capable of storing, encoding or carrying instructions for execution by the machine, and includes digital or analog communications signals or other intangible media to facilitate communication of such software. In the above detailed description, reference is made to the accompanying drawings, which form a part hereof. In the drawings, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, drawings, and claims are not meant to be limiting. Other embodiments may be used, and other changes may be made, without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that various features of the present disclosure, as generally described herein, and illustrated in the Figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein. The present disclosure is not to be limited in terms of the particular embodiments described in this application, which are intended as illustrations of various features. Many modifications and variations can be made without departing from its spirit and scope, as will be apparent to those skilled in the art. Functionally equivalent methods and apparatuses within the scope of the disclosure, in addition to those enumerated herein, will be apparent to those skilled in the art from the foregoing descriptions. Such modifications and variations are intended to fall within the scope of the appended claims. The present disclosure is to be limited only by the terms of the appended claims, along with the full scope of equivalents to which such claims are entitled. It is to be understood that this disclosure is not limited to particular methods, reagents, compounds, compositions or biological systems, which can, of course, vary. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only, and is not intended to be limiting. With respect to the use of substantially any plural and/or singular terms herein, those having skill in the art can translate from the plural to the singular and/or from the singular to the plural as is appropriate to the context and/or application. The various singular/plural permutations may be expressly set forth herein for sake of clarity. It will be understood by those within the art that, in general, terms used herein, and especially in the appended claims for example, bodies of the appended claims are generally intended as open terms for example, the term including should be interpreted as including but not limited to, the term having should be interpreted as having at least, the term includes should be interpreted as includes but is not limited to, et cetera. While various compositions, methods, and devices are described in terms of comprising various components or steps interpreted as meaning including, but not limited to, the compositions, methods, and devices can also consist essentially of or consist of the various components and steps, and such terminology should be interpreted as defining essentially closed-member groups. It will be further understood by those within the art that if a specific number of an introduced claim recitation is intended, such an intent will be explicitly recited in the claim, and in the absence of such recitation no such intent is present. For example, as an aid to understanding, the following appended claims may contain usage of the introductory phrases at least one and one or more to introduce claim recitations. However, the use of such phrases should not be construed to imply that the introduction of a claim recitation by the indefinite articles a or an limits any particular claim containing such introduced claim recitation to embodiments containing only one such recitation, even when the same claim includes the introductory phrases one or more or at least one and indefinite articles such as a or an for example, a and/or an should be interpreted to mean at least one or one or more; the same holds true for the use of definite articles used to introduce claim recitations. In addition, even if a specific number of an introduced claim recitation is explicitly recited, those skilled in the art will recognize that such recitation should be interpreted to mean at least the recited number for example, the bare recitation of two recitations, without other modifiers, means at least two recitations, or two or more recitations. Furthermore, in those instances where a convention analogous to at least one of A, B, and C, et cetera is used, in general such a construction is intended in the sense one having skill in the art would understand the convention for example, a system having at least one of A, B, and C would include but not be limited to systems that have A alone, B alone, C alone, A and B together, A and C together, B and C together, and/or A, B, and C together, et cetera. In those instances where a convention analogous to at least one of A, B, or C, et cetera is used, in general such a construction is intended in the sense one having skill in the art would understand the convention for example, a system having at least one of A, B, or C would include but not be limited to systems that have A alone, B alone, C alone, A and B together, A and C together, B and C together, and/or A, B, and C together, et cetera. It will be further understood by those within the art that virtually any disjunctive word and/or phrase presenting two or more alternative terms, whether in the description, claims, or drawings, should be understood to contemplate the possibilities of including one of the terms, either of the terms, or both terms. For example, the phrase A or B will be understood to include the possibilities of A or B or A and B. In addition, where features of the disclosure are described in terms of Markush groups, those skilled in the art will recognize that the disclosure is also thereby described in terms of any individual member or subgroup of members of the Markush group. As will be understood by one skilled in the art, for any and all purposes, such as in terms of providing a written description, all ranges disclosed herein also encompass any and all possible subranges and combinations of subranges thereof. Any listed range can be easily recognized as sufficiently describing and enabling the same range being broken down into at least equal halves, thirds, quarters, fifths, tenths, et cetera. As a non-limiting example, each range discussed herein can be readily broken down into a lower third, middle third and upper third, et cetera. As will also be understood by one skilled in the art all language such as up to, at least, and the like include the number recited and refer to ranges that can be subsequently broken down into subranges as discussed above. Finally, as will be understood by one skilled in the art, a range includes each individual member. Thus, for example, a group having 1-3 cells refers to groups having 1, 2, or 3 cells. Similarly, a group having 1-5 cells refers to groups having 1, 2, 3, 4, or 5 cells, and so forth. The term about, as used herein, refers to variations in a numerical quantity that can occur, for example, through measuring or handling procedures in the real world; through inadvertent error in these procedures; through differences in the manufacture, source, or purity of compositions or reagents; and the like. Typically, the term about as used herein means greater or lesser than the value or range of values stated by 1/10 of the stated values, , 10%. The term about also refers to variations that would be recognized by one skilled in the art as being equivalent so long as such variations do not encompass known values practiced by the prior art. Each value or range of values preceded by the term about is also intended to encompass the embodiment of the stated absolute value or range of values. Whether or not modified by the term about, quantitative values recited in the claims include equivalents to the recited values, , variations in the numerical quantity of such values that can occur, but would be recognized to be equivalents by a person skilled in the art. Various of the above-disclosed and other features and functions, or alternatives thereof, may be combined into many other different systems or applications. Various presently unforeseen or unanticipated alternatives, modifications, variations or improvements therein may be subsequently made by those skilled in the art, each of which is also intended to be encompassed by the disclosed embodiments.",
                    "IMAGE": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAEBAMgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKxP7P1NNQ1KVZlkhuJFeBTOymMeWqMBwcchmGO5/GqraRrjeFp9La+Q3eQIbsSMrEb8/NgcHaMZHB9BVeTSfFsLtFZavCbaEKLc3B3SPiNgTIdnJ3FT+H4VbsbHxKmrpLfanBLZI7HykUKWUhgAfl9dp6+tdFWPqEXiD7cZdOuLM2+BiG4U9cHPIGcdPyp6prf2m1d5LUxCJluETIy5xhlJBwBzxVS2tvEyyR/ab22dAy7tgAOMrnPycnAccY69u3QViXUXiQai72dxYNaE/LHOrbgOO469D+ftU9umueXG1xLZ+aN5dYw20/KNo554bOfaq7QeJGtkxeWSXBZi+EJQDK4A4z0DdfX8tyisb7JrX2wOL2PyRdFypGd0Jx8vTgjnnP/wBaKe117ZcrHcxv5iyLEfM2GMl2KtnYeilB0PSqwtPF4jula/09mkiZYHCsPKfJIYjHPGBj6e+bIstf+y3i/bY1lkhcQEtu8ty7FTnYOApUd+n41a0W31S3ilGqXKTyHbtZWyOFAJxtGMnJxzWpRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRVW+vDZrCRC0vmzLF8v8Oe9WqKKKKKKKKKKKKKKKKKKKKKKKo3chkv7S1QKTkzSEgHaq8d+hJI/DNXqKKKKhuLmK1EZlJCySLGDjgMeBn6nA/EUs1xFbhTK20M20HHenxyJLGHQ5U96dRRRRRRRRRRRRRRRVDTSbh7i+PSZ9sf8A1zXIH5nc3/AqxPFvi+Tw9d2FjaWMN3eXgkZRNdrboiptzlm6n5hgDk1p6PqEmt6TFeJIYHbKuiqDsYdR8wz+dW2trzChb8jHUmFTmmNbagemoqP+3cf41G1lqpzjWAPTFqv+NVp9J1i4iMT63EVOCQ1ipzjn19aZPo+tXUTRT61bSRk5Ktp4/wDi6dHpmuxqVXXLYAkn5dPA5PP9+nrp2vD72vxEf9eIH/s1SpY6yuN2sxt6/wChgf8As1XIIbtGzNdrIPQRbf61Zoooooooooooqlqcji08mJiJrhhChHUZ6n8FyfwrM1nWodMt3srSVIDbxgz3DLmOzj7E+rEfdTqTjtXNXOhXvjDRVumsbcW4ZTYwXy7pGUkb5nJB+cjkDp09K07Sy1zQorK0s7e0ayjVjLaQnErjAGVY4GQTkgkZ9atf2jbXEywDUpbO6f7sN0vkufoHXB/DNMvdN1OC3muZtRdIY0Mjs0qYVQMk/wCq9K5+Ya+ZI/KsdRKyjcuZYAcf3iPLyo/3sVQguNeuS/lpvC/fSG+t5ZU+qLFn8sn2q/babqt9aw3NrqKTxylgoilTJZThhzCOQQeM9qWTRtahXzLq6a2TO0NPLHt591jOPxq1D4Z1q9hjnh1KAIw4wwyPUf6vrVl9E8Q2USu+vRRLkKC7qBk9v9XXW2dq8NrEs8hlnCgO+4/MfXsP0q0BgYooooooooooqvf3aWGnXN5J9yCJpW+ign+lcvqy2FpHbX2r6omn3Jhxb3LSBX85uWODwQMAbTxgkVBpukyeJmgm1BYk0q1k3C1Qk/argH5pZCeSueinr1PYV24GBgVk6472/wDZ10gyY72JG/3ZD5Z/9DB/CtKaCG5iaKeJJY26o6hgfwNY174Wtp7Ka1tbm6s4ZkKPFFITGQe2052/8B21Elvf6daR2tzYw3tlEFXdafI5CjjdG3BHsG/DtUlyLHWIla0gEk0JwcqEKf7Lq2Dj2I9xg1hbZNHvtQtbe3ljcxrq8MZcyFpFbbMgPfcu3H++a6nUbWHX9Alhil/d3UIaKVT0JGVYfoawPBWrz3Jktbxds7BmcHqsyELKPxOx/wDgZ9K6LVUhlhghmVmMkyhFU8k9c/gAT+FX6KKKKKKKKKKKyfE6h/DV/G3SWPy+mfvEL/WvO/GvgvU/GXiO4v7B4WFpmyEV1HvjC7VJKjIGcu2cjt+Fd54TsE0jSjpEe7bYeXAC77mOIkOScDnmt6sjxLxo27ulzbsPqJkNa9FFUtQsNPvE/wBOggcAcNIBkfQ9q5ZRpGm6pA0GsWDLDkKiyqsiZxkEA7WHHoD7mt7Rbmxigayt7y3dInPkqkqk+WeQMDoBnH0ArDvtLm0vxXJqVojNHLi5CqOCwBDr/wACUsf94r6V0luwvdSkuQcw26+VH6Fjgsf/AEEf99VoUUUUUUUUUUUVjeKlLeHZwO0kR/ASKTUuijnUeP8Al9k/pSaVzqetn/p8Uf8AkCKtWsjxAGmtrS0UZNxeQqf91WEjfohrXorHupbzUdQksLOY21vBj7TcqAXLEZEaZ4BwQSx6ZGOTkPj8NaMrB5NPhuJf+etyPOc/Vnya0o4o4UCRRpGg6KowBTLiztrtNlzbwzL6SIGH61mXmm2On2jzWdsIJuFiEBKZc8KMDg8kdQax9H1K80/U4tGkjnFsYWEU93aNGWmByVDL8rZG5u3Stz7NqN1KSdVkgQfww2qoT+L7quW9q8By13cTn/pqV/oBVmiiiiiiiiisrxMSvhfU3XrHbPIP+AjP9KNEbLakPS9f9Qp/rUehTLNea4V6rqBQn3EUVbNZcarqGs/atwMNlvijHrKcBm/AfL+LVqVDdXKWltJM5ACqW5OM4BP9KbaWy20JGQzyMZHbH3mP+cfQCrFFFZ7f6Zq6p1isxuPHWVhx+Skn/gQq1c2yXKIGOGRw6MOqkH/I+hNTUUUUUUUUUUUVDeWyXljcWsn3Jo2jb6EY/rXncPj3S/CNtK+uSPFJdSIwAXO1lijifIGSAHRhnFdj4ajhOnz3sDq6X11LchlOVIJwCD3GFBpdVOvi4zpqWj220Z3OVkzzkAEEHtjke9RDUodP02C1sbaWS8f5Y7ST5ZN2fmaT0GSSW5BzxnIzNFpN5L8+o6tcyOeTHbYhjX2GPmP4sar6voLXN7p93A0kggLwzQPKSskMi7WPP8QyD74I71q6dA1rp8Fs7FjCvlhj1IHAz+GKtUVXmvIYrKS73hokUtlT1x2+uePrTNOt3t7NfOx58hMkpH988kfQdB7AVbooooooooooooormtb8C6Dr98l9fWMMtzGPkeRA4U5zkKePzyPat+0tYrK0itoF2xRrtUUtzMLa1mnYZWJGcj6DNQafAgiF0wBuJ0VpJO54yAPYZ4H+Jq5RWbDqG3VJLWVwFY/u93r3APcHr7EEelaVFc9qGkm71NYLK9ubM5FzcGMh1JB+T5XBAJIzx/drShh1G2U7rpLz/rqgjb814/Ssy/8AEOoQQxTafod1qO8skkEbxo8LKcENubB/D8+RVvR/EVjq+nW1ysqwySxq7QynayEjOMHGcdMiteiiiiiiiiiiiiiuc+33d9qNzY5Ro/MaJo+FGw7h1wSThWqx4fvdsX9jXZK6jYIqMrcedGOFlX1BHX0OQa2JZo4ImlmkWONRlmY4AH1rhNS8a3+o3wsdAs5BEQSbqRDukH/TNew9XbAHscVqabYWgU2WqPnUZQVSQjGMcnyn7sDyT1JGcAYAtwahq1ozxzwxanHE2x5LX5Jl9C0bcHPqp+grQTV7UwSTSie3SMbnNxA8e0fUjFZ+m63pfkNdSahbefdP5nliUM4HRV2jnIUDI9c1uRSpPGJI23I3Q+tYi2Ftd+J9QaQP8kEPCyMvzHfk8EdQEH4Co5LOLStWjSwmW0jumy8LR74i/wDeAyCrHIBweSQSM81vxCQIBKyM/cqu0flk0+isyzfW3u2+2Q2MdsJGA8t2Z2X+E+nPGR9a06KKKKKKKKp6Y101s5vHVpPMbBAxhc8CuKgeM+M72K5QPb/bNzArkY8qU/p1rX1mGy063had40tvMH2KaJ/LlgcjJ2sM7hgZxjkAghhUx1GyvbUCe8jvgnDxwpsk5HXYTkn6YPpUt6NPtPCl/fabHEqNavKskQ5f5Tgk9z9ayLvVRNb39pJpt5cRR38nmyrGQqIp5dX7MvUd+Kuot41z5XmqNbtI90Mx+VL+D0YfocfdY5HDYLtU1BtSs7QQl40kyZ7ZzsYlSAY2OO7ELgdd3pV4/wBpfabT7RbwrF52ZHikzkbTgEEZ+9t6E9K2ScDJ6Vi6fLt1a6mlwovpP3A77UXA/wC+sOw9hWR48WSCGzvI2dAJNjMvZsZU/gRn6qtdPpt6uo6ZbXiDAmjDFf7p7j8DkfhSymf7bbqjYiIYyDZnPTHParNcnaeIbq+1ewhL2W15XWS1jkbz4iBJneM9FAQHIwS3HQZ6yiiiiiiiiqM2nKZmuLSQ21w3LMoysn++vQ/Xg+9cHY3nkeO3W9URMb4qXHMbExSDgnp24PrxmtoM9n4t0zRm+aKCVri1PcRNFKCv/AWGB7FR2q6+vXzRvi0t7eRS4JmZ28vacZYBecjkYOD60anZrYeANQgWbzv9EmdpcY3s25iQOwyTxVNV1FtN1NLOWziilv545GuVdiNz7eNpHrW/eW8S6dG8sZd7UB0aM4ZSBjIJ9s/UcGsrTrddW19tXKfuEUGI9N5wQp98KS3/AG0H90Vd8ya4WYahHKIBjMccZAXBzy2ct25GBUF2LpIYY7fVDLFcFQoaJXYR9WbcOo255IPJHrUHi6R7XSIdbtGEgsHWZsc7owRu5+mcn0LVf8QxW2oeHbiN540WVA0TseC4+ZcevIHHeuX8Cate39hLYwEQlWMnmXCksMnDgJ67hu5Ixv6V2lvp8UEpnZpJrgjBllbJx6Dso9gBVuuP0B/M1ktc2mrwXJXJeT7R5LN824Hd8uAAMdueK7CiiiiiiiiiuIigiTxFqUs8fmR/aVcgLu4+YHI9OaTUdX0rVtUs7jTJbpdVsGYQslm0isrDDIw4yDx0IIwPxs3F54jv7ZoLjTmtbR1KzSwbTKy9woZhsz6nOK0dbZrjwpqdstjcRA2UqKrbePkIHRjSaXZi80rU7Z2ZPNvJwGXqp3cEe4PNLdahIqyafftFHLJDgvHnBB+Ut1+UZPHWtDSLOXT9NgtZZBI0SKgYdwAB6D0q5LIkMTyyMFjRSzMegA6msGyksYZVvZWFv5UbJHFJkOokbfjb+AAAz3FWnjuNQjmhithb2k6kSNcLlnBGDiPtkf3vyrlPDkuuwNNYWuhW9w2lyGy/tC9vipkCgYKgI2PlK5AwOaXTNMu/DevfarmRHe6uZZJURmYIsknQEjtmM/RDXoFFFFFFFFFFFFFc+vh+DUNSuLu+VZrV3/dW7D5G6fM46MM5wDxyT3GIvFOhaVeaaizW7rMCsNqLeQxkMThQMcYGSenABPauZvdS13wLiw1WZtW0O4XZHqBB861J4AkHO5Tk4Oe2M9BXVpd6vfJlbS1ubC4jysiSbNyMOvU9vate0hFtA5ZVjLu0jgNkAk5PNcrr2nwre2s0l602ycXE+5l3bI9xRQB28xh+QrfsNbgn0mK8uWSFmZoygySXVipCjqeR0rK1a41ae33SWm3Sw6tK+zM+wc58rkFeBnnOM/KaunTkWW11jTnF1Nt+eR33meFuflboD0IxgdRxnNbMciTRiRDlT0rnbK+Wx8UXtk3+rurs7T/00MCMB+SOab4v/wBGWzvm3eQrNDOVHIVxjd9RyB7sK2dHvhqOkWt1nLOmH9nHDD8wavUUUUUUUUUUU2RS8TqrbSykA+lMt0CWscYbcEUJn1xxWPLJ9g1WJ9UZng3FbS4PCRs3G1/Rj0DHg5xwTzcuLJryM/aohJsyuwhWSUFcHII4HzMPXFc7oaroxl0TTr1vsDTultPIm/yHyS0QJPOMjBPGTjk9ekntobXTJ1k8ycbSzmRizOf89hgVx9kh1kRMXkleeRBDhNgCRtl2Y9wSCPqRjmtG0gl0a01We3jjW7GZYlukYsp/iG7OGU8cqRyTxniumsrtLyzt51wpmiWUJnkAjNZBLeG7lnOP7FmcseP+PNyeT/1zJ/75J9D8u8oXGVxhucjvXLavbtE2rTiMGaCWDUoTnqEUBgPfCMP+BVu6laLqek3FsrL++j/dt1AbqrfgcGuX8A3oSCXT2yBuLxg9VYY3r9QCh+rN6V2tFFFFFFFFFFFY91NcaPdPciN59NlO6VY1LPbt3cAcsh6kDkHnkE4lvZ/tOnpc2sUeo2UifvIV2sJYz3XPBPseDVO2t5vshOh6iTAx2+Vc5fyPULn5lI/utkD0FOuk03QNOEk2fKhhZIoQNzufvNjHJYkAk+2axPEkuoB7BtWljgspXVfIjYnMhydrY+907/KM+2a6GLQ4GjgldSk6RKgwfuYXGBjtnJx3P0qKKCfVrPypZibMTSRvuA3yKpwO394H6j61yel2OpaFHeW39rGO6s5Y0jQqzJOhACu4ZjkYBGEKgEV1v9sz2eItdshbxsdn2uJt9u2f7xPKZ6fMMdsmmnSrvSx/xJrlxbE8WbosiR/7mWUqPbJA7AVaiIuLqA3IBlkt5EK7NuRuXcCMn27nvRoLkaYLR2LSWbtbMSME7fun8V2n8a5K6J0jxBcmQBGik+2RFe6BmLfiY3mGP+mQNegA5GR0oooooooooooorMXSTZTyz6bMYPNJd7cjMLMerY6qT3wcHqQTVWLVJppzDBpwbUOFmdD+7h/32OCceg5P0Oah1G3tbLFxfXDXU6r5rIx2htnO5v7sanBx0yOdxxU/2K31HRpriW6hvGuIyftCfNGF67Uwfu/jk96s2MTXXh6zWbZO7QoSZCcNwOTUGnRvobw6bIFNm+77PKDjaeWMbZ9s4PfBzz1r6xZRNqMGqPcJ5cDx74w331DYG7nou4sPcVp6dMt7p7RzBZGRnt5lb5slSVOfqOfxpsfl6OsNuSRZs4jiJ58on7qk+nYfgPSoNakazvNM1DOIY5/In9kk+UH8H2fhmpBm18SMMYjvYN2f+mkfH5lWH/fFY3jSDyDa6n5XmJH+7lHtncM/UB0/7a1seHp2k0oW8jF5bN2tnY9W2/db/gS7W/GtWiiiiiiiiiikd1jRndgqqMlicACs4TXGqf8AHuXt7PvNjDyc/wAIPQf7XX07GsyK6/srXp9Fsooy1xGlxApOFT+GQsep6KcdSW+pG5aWUdrvckyTy4MsrdXP9AOwHAqCfToYLZ/sg+zBUb93CAEb2K4x+I596m01gdOgXbtMaCNlP8JHBH6VR8RRrJa2YkRXi+2wrIG6YZtv/swqjP8AD/wxOzudMVZWBG9ZXBGf+BUnhqP+z71rVQywXVusyqxJ2yx4jlGTz2T9a3tQsotR0+4s5h+7mQoT3Ge49x1rndQvJNS8CrFJta9u9tkVI6z7tjcexDN9Fra1J1M1uscMk9zE/nJHGVBAwVJJYgYIJH/6qjuHkv7KW3urCS3SRcbpZEIB7fdY98VHoELxrcOFQQnYkbI+4SbRt3ZIHYKPfbmtmiiiiiiiiiismTF7rjWs8RaCCISAF8ozEjBK45x2ycVqSsUidlGWCkgeprGg0qLUtGgF3CYLnf54kiYiSOTn5wx5zj14xxjHFWIJdUs023kaXir0mtxtcj/aQnr/ALp/AVNBq1jcXDW63CrcKATDKCjgeu1sHHvV0c9Ko6mYWhjglJLPIjJGoyzFWDcD/gNZN3Bu1D+11bVLBwnlsww6ED+9EM5HvwR7VFazQzztPbXcFy9pcLPviOQY5BtcYycchmx7CuprIl0iys76TVILV5bxixjjMjbBIwALBTwpIAywHTPqcrH9n0HTrjUNVvYlYjzLq6kO1R6AZ6KM4A/mSaw7Sy1HxZqw1PU0ltNEi/48rBjte49ZZh2B7J1x16kV2IAUAAAAcAClooooooooooooooqvd2Fnfqq3lpBcKpyBNGHA+mabFpljboyQ2dvErDBCRhQR+FLb6dZWkrS29rDHIwwzqgDEemas1k39nbf2razyQxfv0e2d9uG5G4c/8BYfjVrS5ZZbFVnOZ4maKQ+pU4z+IAP407UdQttLsZby6fZFGuT6n2A7muR0Ox1DxbeLruvp5dijbtP03OUXHSR/7zenb045PcUUUUUUUUUUUVTfVLVNTj04OXunUuY0UtsUfxMeijoBnrnirlFFFFFFFUNV877PF5ESSP5yHDZ4wc5GO/FTXFjFcbsvNHu5YxSshJ/Aikj061SPYyNKMhv37tKcjofmJq0AAAAMAdBRRRRRRRRRRRRWXpksdzqGpzhGWRZhASy4+VBxj1GSx/GtSiiiiiiiiiiiiiiiiiiiiiiimpGke7YoXcdxwOp9adRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX//Z",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/21/664/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9621431827545166,
                    "F_SPEC_PARAMS": [
                        "accuracy",
                        "reliability"
                    ],
                    "S_SPEC_PARAMS": [
                        "time-consuming",
                        "complicated process,",
                        "positive outcomes"
                    ],
                    "A_PARAMS": [
                        "computer-aided"
                    ],
                    "F_SENTS": [
                        "In interventions such as total knee replacement, computer-aided surgery techniques have been used to improve the accuracy and reliability of the surgery."
                    ],
                    "S_SENTS": [
                        "Positioning a cut guide for use in preparing the target bone can be a time-consuming and complicated process, which is critical to positive outcomes for the patient."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Reliability"
                    ],
                    "F_SIM_SCORE": 0.8757559359073639,
                    "S_TRIZ_PARAMS": [
                        "Productivity",
                        "Waste of Substance"
                    ],
                    "S_SIM_SCORE": 0.4939844012260437,
                    "GLOBAL_SCORE": 1.7470133513212205
                },
                "sort": [
                    1.7470133
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10935986-20210302",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10935986-20210302",
                    "KIND_CODE": "B1",
                    "APPLICATION_DATE": "2020-10-14",
                    "PUBLICATION_DATE": "2021-03-02",
                    "INVENTORS": [
                        "Junzhi Yu",
                        "Zhengxing Wu",
                        "Jian Wang",
                        "Shuaizheng Yan",
                        "Min Tan"
                    ],
                    "APPLICANTS": [
                        "INSTITUTE OF AUTOMATION, CHINESE ACADEMY OF SCIENCES    ( Beijing , CN )"
                    ],
                    "INVENTION_TITLE": "Gliding depth control method, system and device for biomimetic gliding robotic dolphin",
                    "DOMAIN": "G05D 10875",
                    "ABSTRACT": "A gliding depth control method for a biomimetic gliding robotic dolphin includes: obtaining a preset gliding depth and a preset yaw angle; obtaining an estimated velocity by a sliding mode observer based on depth information and inertial navigation information, and obtaining a control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin by a yaw controller in combination with the preset yaw angle; obtaining a segmented diving velocity reference trajectory by constructing and segmenting a Bzier curve; obtaining a diving control quantity by a model predictive control method in combination with the estimated velocity; obtaining a target position of a piston through a buoyancy principle, and obtaining a control quantity of the piston according to a current position of the piston; and controlling the biomimetic gliding robotic dolphin to glide based on the control quantity of the piston and the control quantity of the pectoral fins.",
                    "CLAIMS": "1. A gliding depth control method for a biomimetic gliding robotic dolphin, comprising: step S100: obtaining a preset gliding depth and a preset yaw angle; step S200: obtaining an estimated velocity by a sliding mode observer based on depth information and inertial navigation information of the biomimetic gliding robotic dolphin at a current time, and obtaining a control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin by a yaw controller in combination with the preset yaw angle; step S300: obtaining a segmented diving velocity reference trajectory by constructing and segmenting a Bzier curve based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and obtaining a diving control quantity of the biomimetic gliding robotic dolphin by a model predictive control method in combination with the estimated velocity at the current time; step S400: obtaining a target position of a piston in a buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through a buoyancy principle according to the diving control quantity, and obtaining a control quantity of the piston according to a current position of the piston; and step S500: controlling the biomimetic gliding robotic dolphin to glide based on the control quantity of the piston and the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin. 2. The gliding depth control method of claim 1, wherein a method for obtaining the estimated velocity by the sliding mode observer in step S200 comprises: constructing a full-state three-dimensional dynamical model corresponding to the biomimetic gliding robotic dolphin based on a coordinate system of the biomimetic gliding robotic dolphin; simplifying the full-state three-dimensional dynamical model by ignoring a lateral motion to obtain a simplified dynamical model; and obtaining a velocity vector of the simplified dynamical model by the sliding mode observer based on the depth information and the inertial navigation information of the biomimetic gliding robotic dolphin at the current time, and using the velocity vector as the estimated velocity of the biomimetic gliding robotic dolphin. 3. The gliding depth control method of claim 2, wherein the simplified dynamical model is expressed as: M{dot over }=CD+, wherein, =[u, w, q]T denotes a forward velocity, a longitudinal velocity and a pitching angular velocity under a dolphin coordinate system, respectively; M=diag{m1, m2, m3} denotes an inertial mass matrix containing an added mass; m1, m2, m3 denote mass parameters; D=diag{d1, d2, d3} denotes a damping matrix simplified as a constant term; d1, d2, d3 denote damping parameters; =[0, uc, {right arrow over a}Gm sin] denotes an input matrix; uc denotes a real-time control quantity; {right arrow over a} denotes a position vector of a gravity center and a buoyancy center of the biomimetic gliding robotic dolphin; Gm denotes an acceleration of gravity; denotes a pitch angle; C denotes a Coriolis force and centripetal force matrix; and {dot over } denotes a velocity derivative. 4. The gliding depth control method of claim 1, wherein a method for obtaining the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin by the yaw controller in step S200 comprises: calculating the control quantity of the pectoral fins by the following formula: uf=kfkpe+kie+kd, wherein, uf denotes the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin, kf denotes a weight coefficient of the yaw controller, kp denotes a proportional factor, ki denotes an integral factor, kd denotes a differential factor, e denotes a yaw angle error, and denotes a derivative of the yaw angle error. 5. The gliding depth control method of claim 1, wherein a method for obtaining the segmented diving velocity reference trajectory by constructing and segmenting the Bzier curve in step S300 comprises: constructing a second-order Bzier curve trajectory based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and using the second-order Bzier curve trajectory as a depth reference trajectory of the biomimetic gliding robotic dolphin; obtaining a diving velocity reference trajectory by taking a derivative of the depth reference trajectory; and segmenting the diving velocity reference trajectory according to an absolute value of a difference between the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time and a value of a preset depth threshold, to obtain the segmented diving velocity reference trajectory. 6. The gliding depth control method of claim 5, wherein the segmented diving velocity reference trajectory is calculated as follows: Y ref = { c 1 V r e f , if d r - d &gt; d t h e s h o l d c 2 V r e f , otherwise wherein, Yref denotes the segmented diving velocity reference trajectory, c1 and c2 denote weight coefficients, dr denotes the preset gliding depth, d denotes the depth information, dtheshold denotes the preset depth threshold, and Vref denotes the diving velocity reference trajectory. 7. The gliding depth control method of claim 1, wherein a method for obtaining the target position of the piston in the buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through the buoyancy principle in step S400 comprises: calculating the the target position of the piston by the following formula: r sr = u c t g S , wherein, rsr denotes the target position of the piston, denotes a density of water; g denotes an acceleration of gravity, S denotes a bottom area of the buoyancy adjustment mechanism, and uct denotes the diving control quantity. 8. A storage device, wherein a plurality of programs are stored in the storage device; the plurality of programs are configured to be loaded and executed by a processor to implement the gliding depth control method of claim 1. 9. The storage device of claim 8, wherein a method for obtaining the estimated velocity by the sliding mode observer in step S200 comprises: constructing a full-state three-dimensional dynamical model corresponding to the biomimetic gliding robotic dolphin based on a coordinate system of the biomimetic gliding robotic dolphin; simplifying the full-state three-dimensional dynamical model by ignoring a lateral motion to obtain a simplified dynamical model; and obtaining a velocity vector of the simplified dynamical model by the sliding mode observer based on the depth information and the inertial navigation information of the biomimetic gliding robotic dolphin at the current time, and using the velocity vector as the estimated velocity of the biomimetic gliding robotic dolphin. 10. The storage device of claim 9, wherein the simplified dynamical model is expressed as: M{dot over }=CD+, wherein, =[u, w, q]T denotes a forward velocity, a longitudinal velocity and a pitching angular velocity under a dolphin coordinate system, respectively; M=diag{m1, m2, m3} denotes an inertial mass matrix containing an added mass; m1, m2, m3 denote mass parameters; D=diag{d1, d2, d3} denotes a damping matrix simplified as a constant term; d1, d2, d3 denote damping parameters; =[0, uc, {right arrow over a}Gm sin] denotes an input matrix; uc denotes a real-time control quantity; {right arrow over a} denotes a position vector of a gravity center and a buoyancy center of the biomimetic gliding robotic dolphin; Gm denotes an acceleration of gravity; denotes a pitch angle; C denotes a Coriolis force and centripetal force matrix; and {dot over } denotes a velocity derivative. 11. The storage device of claim 8, wherein a method for obtaining the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin by the yaw controller in step S200 comprises: calculating the control quantity of the pectoral fins by the following formula: uf=kfkpe+kie+kd, wherein, uf denotes the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin, kf denotes a weight coefficient of the yaw controller, kp denotes a proportional factor, ki denotes an integral factor, kd denotes a differential factor, e denotes a yaw angle error, and denotes a derivative of the yaw angle error. 12. The storage device of claim 8, wherein a method for obtaining the segmented diving velocity reference trajectory by constructing and segmenting the Bzier curve in step S300 comprises: constructing a second-order Bzier curve trajectory based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and using the second-order Bzier curve trajectory as a depth reference trajectory of the biomimetic gliding robotic dolphin; obtaining a diving velocity reference trajectory by taking a derivative of the depth reference trajectory; and segmenting the diving velocity reference trajectory according to an absolute value of a difference between the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time and a value of a preset depth threshold, to obtain the segmented diving velocity reference trajectory. 13. The storage device of claim 12, wherein the segmented diving velocity reference trajectory is calculated as follows: Y ref = { c 1 V r e f , if d r - d &gt; d t h e s h o l d c 2 V r e f , otherwise , wherein, Yref denotes the segmented diving velocity reference trajectory, c1 and c2 denote weight coefficients, dr denotes the preset gliding depth, d denotes the depth information, dtheshold denotes the preset depth threshold, and Vref denotes the diving velocity reference trajectory. 14. The storage device of claim 8, wherein a method for obtaining the target position of the piston in the buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through the buoyancy principle in step S400 comprises: calculating the the target position of the piston by the following formula: r sr = u c t g S , wherein, rsr denotes the target position of the piston, denotes a density of water; g denotes an acceleration of gravity, S denotes a bottom area of the buoyancy adjustment mechanism, and uct denotes the diving control quantity. 15. A processing device, comprising a processor and a storage device, wherein the processor is configured to execute a plurality of programs, and the storage device is configured to store the plurality of programs; the plurality of programs are configured to be loaded and executed by the processor to implement the gliding depth control method of claim 1. 16. The processing device of claim 15, wherein a method for obtaining the estimated velocity by the sliding mode observer in step S200 comprises: constructing a full-state three-dimensional dynamical model corresponding to the biomimetic gliding robotic dolphin based on a coordinate system of the biomimetic gliding robotic dolphin; simplifying the full-state three-dimensional dynamical model by ignoring a lateral motion to obtain a simplified dynamical model; and obtaining a velocity vector of the simplified dynamical model by the sliding mode observer based on the depth information and the inertial navigation information of the biomimetic gliding robotic dolphin at the current time, and using the velocity vector as the estimated velocity of the biomimetic gliding robotic dolphin. 17. The processing device of claim 16, wherein the simplified dynamical model is expressed as: M{dot over }=CD+, wherein, =[u, w, q]T denotes a forward velocity, a longitudinal velocity and a pitching angular velocity under a dolphin coordinate system, respectively; M=diag{m1, m2, m3} denotes an inertial mass matrix containing an added mass; m1, m2, m3 denote mass parameters; D=diag{d1, d2, d3} denotes a damping matrix simplified as a constant term; d1, d2, d3 denote damping parameters; =[0, uc, {right arrow over a}Gm sin] denotes an input matrix; uc denotes a real-time control quantity; {right arrow over a} denotes a position vector of a gravity center and a buoyancy center of the biomimetic gliding robotic dolphin; Gm denotes an acceleration of gravity; denotes a pitch angle; C denotes a Coriolis force and centripetal force matrix; and {dot over } denotes a velocity derivative. 18. The processing device of claim 15, wherein a method for obtaining the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin by the yaw controller in step S200 comprises: calculating the control quantity of the pectoral fins by the following formula: uf=kfkpe+kie+kd, wherein, uf denotes the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin, kf denotes a weight coefficient of the yaw controller, kp denotes a proportional factor, ki denotes an integral factor, kd denotes a differential factor, e denotes a yaw angle error, and denotes a derivative of the yaw angle error. 19. The processing device of claim 15, wherein a method for obtaining the segmented diving velocity reference trajectory by constructing and segmenting the Bzier curve in step S300 comprises: constructing a second-order Bzier curve trajectory based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and using the second-order Bzier curve trajectory as a depth reference trajectory of the biomimetic gliding robotic dolphin; obtaining a diving velocity reference trajectory by taking a derivative of the depth reference trajectory; and segmenting the diving velocity reference trajectory according to an absolute value of a difference between the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time and a value of a preset depth threshold, to obtain the segmented diving velocity reference trajectory. 20. A gliding depth control system for a biomimetic gliding robotic dolphin, comprising a preset value acquisition module, a pectoral fin control acquisition module, a diving control acquisition module, a piston control acquisition module, and a gliding control module; wherein the preset value acquisition module is configured to obtain a preset gliding depth and a preset yaw angle; the pectoral fin control acquisition module is configured to obtain an estimated velocity by a sliding mode observer based on depth information and inertial navigation information of the biomimetic gliding robotic dolphin at a current time, and obtain a control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin by a yaw controller in combination with the preset yaw angle; the diving control acquisition module is configured to obtain a segmented diving velocity reference trajectory by constructing and segmenting a Bzier curve based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and obtain a diving control quantity of the biomimetic gliding robotic dolphin by a model predictive control method in combination with the estimated velocity at the current time; the piston control acquisition module is configured to obtain a target position of a piston in a buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through a buoyancy principle according to the diving control quantity, and obtain a control quantity of the piston according to a current position of the piston; and the gliding control module is configured to control the biomimetic gliding robotic dolphin to glide based on the control quantity of the piston and the control quantity of the pectoral fins on the both sides of the biomimetic gliding robotic dolphin.",
                    "FIELD_OF_INVENTION": "The present invention relates to the technical field of underwater robot control, in particular, a gliding depth control method, system and device for a biomimetic gliding robotic dolphin.",
                    "STATE_OF_THE_ART": "In recent years, underwater biomimetic robots have attracted widespread attention, as they play an increasingly important role in aspects of underwater detection, search rescue, and facility maintenance. Biomimetic robotic dolphins achieve high maneuverability by imitating dolphins' dorsoventral oscillating motion, and successfully realize diving, rolling back and forth, and other difficult motions. However, the robotic dolphin consumes a substantial amount of energy when performing these motions. In order to improve the endurance capability of the robotic dolphin, researchers have introduced a buoyancy-driven mechanism of the underwater glider into the design of the biomimetic robotic dolphin, and developed a platform that is used for studying biomimetic gliding robotic dolphins. This design combines the gliding motion and the dolphin-like oscillating motion, and provides a high maneuverability and a strong endurance to the robotic dolphin, which significantly expands its application field and scope. The problem of depth control remains an area of concentration in underwater robot research. High-precision depth control lays a solid foundation for autonomous navigation and path planning of underwater robots, which is critical to the autonomous operation of underwater robots. According to different control methods that are known, depth control for the underwater robots is mainly divided into three types: a gravity center adjustment method, a buoyancy adjustment method, and a movable fin surface adjustment method. The gravity center adjustment method typically uses a movable slider in the underwater robot to change the gravity center distribution to obtain a pitching moment, thereby realizing depth control. The buoyancy adjustment method realizes depth control by changing the water displacement of the underwater robot itself to perform buoyancy adjustment. These two methods only require static sealing operation, and are considered safe and easy to implement. However, due to the speed limitation of slider movement and buoyancy adjustment, the systems that are operated under these two methods experience a relatively large delay and are prone to overshoot. The movable fin surface adjustment method realizes depth control by controlling the rotation angle of the fin surface, which has a fast response, a simple structure and wide applications. However, this method has high requirements for sealing, and is prone to strong coupling effects on the yaw motion. The aforementioned methods are widely used in researches on underwater biomimetic robots. Aiming at the depth control problem of biomimetic robotic dolphins, by means of the gravity center adjustment method, Shen designed a fuzzy proportional-integral-differential PID controller with a depth error of 5 cm to provide gravity center adjustment. See, Shen Fei, Modeling and Control for Biomimetic Robotic Dolphin and Its Application in Water Quality Monitoring, Ph. D. Dissertation, Beijing: Graduate School of Chinese Academy of Sciences, 2012. Aiming at the depth control problem of biomimetic robotic fish, by means of the buoyancy adjustment method, Makrodimitris provides the depth control with a depth error of 2 cm. See, M. Makrodimitris, I. Aliprantis, and E. Papadopoulos, Design and implementation of a low cost, pump-based, depth control of a small robotic fish, in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. , Chicago, USA, September 2014, pp. 1127-1132. Aiming at the depth control problem of biomimetic robotic dolphins, by means of the movable fin surface adjustment method, Yu proveds the depth control with a depth error of 0. 5 cm by controlling the rotation angle of the pectoral fins via an improved sliding mode fuzzy controller. See, J. Yu, J. Liu, Z. Wu, and H. Fang, Depth control of a bioinspired robotic dolphin based on sliding mode fuzzy control method, IEEE Trans. Ind. Electron. , vol. 65, no. 3, pp. 2429-2438, 2018. The depth control for the biomimetic gliding robotic dolphin includes gliding depth control and dolphin depth control. The accuracy problem of the gliding depth control has not been solved satisfactorily because of the large delay and the low accuracy of the system. Aiming at the problem of gliding depth control of the biomimetic gliding robotic dolphin, the present invention designs a depth controller to control the buoyancy to further the goal of inproving depth control.",
                    "SUMMARY": [
                        "To solve the problem of low accuracy of gliding depth control for existing biomimetic gliding robotic dolphins, as stated above, according to a first aspect of the present invention, a gliding depth control method for a biomimetic gliding robotic dolphin includes:step S100: obtaining a preset gliding depth and a preset yaw angle;step S200: obtaining an estimated velocity by a sliding mode observer based on depth information and inertial navigation information of the biomimetic gliding robotic dolphin at a current time; obtaining a control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin by a yaw controller in combination with the preset yaw angle;step S300: obtaining a segmented diving velocity reference trajectory by constructing and segmenting a Bzier curve based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and obtaining a diving control quantity of the biomimetic gliding robotic dolphin by a model predictive control method in combination with the estimated velocity at the current time;step S400: obtaining a target position of a piston in a buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through a buoyancy principle according to the diving control quantity; obtaining a control quantity of the piston according to a current position of the piston; and step S500: controlling the biomimetic gliding robotic dolphin to glide based on the control quantity of the piston and the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin. In some preferred embodiments, a method for obtaining the estimated velocity by the sliding mode observer in step S200 includes:constructing a full-state three-dimensional dynamical model corresponding to the biomimetic gliding robotic dolphin based on a coordinate system of the biomimetic gliding robotic dolphin;simplifying the full-state three-dimensional dynamical model by ignoring lateral motion to obtain a simplified dynamical model; andobtaining a velocity vector of the simplified dynamical model by the sliding mode observer based on the depth information and the inertial navigation information of the biomimetic gliding robotic dolphin at the current time, and using the velocity vector as the estimated velocity of the biomimetic gliding robotic dolphin. In some preferred embodiments, the simplified dynamical model is expressed as follows: M{dot over }=CD+, wherein, =[u, w, q]T denotes a forward velocity, a longitudinal velocity and a pitching angular velocity under a dolphin coordinate system, respectively; M=diag{m1, m2, m3} denotes an inertial mass matrix containing an added mass; m1, m2, m3 denote mass parameters; D=diag{d1, d2, d3} denotes a damping matrix simplified as a constant term; d1, d2, d3 denote damping parameters; =[0, uc, {right arrow over a}Gm sin] denotes an input matrix; uc denotes a real-time control quantity; {right arrow over a} denotes a position vector of a gravity center and a buoyancy center of the robot; Gm denotes an acceleration of gravity; denotes a pitch angle; C denotes a Coriolis force and centripetal force matrix; and {dot over } denotes a velocity derivative. In some preferred embodiments, a method for obtaining the control quantity of the pectoral fins on both sides of the biometric gliding robotic dolphin by the yaw controller in step S200 is: uf=kfkpe+kie+kd, wherein, uf denotes the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin, kf denotes a weight coefficient of the yaw controller, kp denotes a proportional factor, ki denotes an integral factor, kd denotes a differential factor, e denotes a yaw angle error, and denotes a derivative of the yaw angle error. In some preferred embodiments, a method for obtaining the segmented diving velocity reference trajectory by constructing and segmenting the Bzier curve in step S300 includes:constructing a second-order Bzier curve trajectory based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and using the second-order Bzier curve trajectory as a depth reference trajectory of the biomimetic gliding robotic dolphin;obtaining a diving velocity reference trajectory by taking a derivative of the depth reference trajectory;segmenting the diving velocity reference trajectory according to an absolute value of a difference between the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time and a value of a preset depth threshold, to obtain the segmented diving velocity reference trajectory. In some preferred embodiments, the segmented diving velocity reference trajectory is calculated as follows: Y ref { c 1 V ref , if d r - d &gt; d theshold c 2 V ref , otherwise , wherein, Yref denotes the segmented diving velocity reference trajectory, c1 and c2 denote weight coefficients, dr denotes a target depth, d denotes a measured value of a depth sensor, dtheshold denotes the preset depth threshold, and Vref denotes the diving velocity reference trajectory. In some preferred embodiments, a method for obtaining the target position of the piston in the buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through the buoyancy principle in step S400 is: r sr = u c t gS ,wherein, rsr denotes the target position of the piston, denotes the density of water; g denotes the acceleration of gravity, S denotes a bottom area of the buoyancy adjustment mechanism, and uct denotes the diving control quantity. According to a second aspect of the present invention, a gliding depth control system for a biomimetic gliding robotic dolphin includes a preset value acquisition module, a pectoral fin control acquisition module, a diving control acquisition module, a piston control acquisition module, and a gliding control module. The preset value acquisition module is configured to obtain a preset gliding depth and a preset yaw angle. The pectoral fin control acquisition module is configured to obtain an estimated velocity by a sliding mode observer based on depth information and inertial navigation information of the biomimetic gliding robotic dolphin at a current time, and obtain a control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin by a yaw controller in combination with the preset yaw angle. The diving control acquisition module is configured to obtain a segmented diving velocity reference trajectory by constructing and segmenting a Bzier curve based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and obtain a diving control quantity of the biomimetic gliding robotic dolphin by a model predictive control method in combination with the estimated velocity at the current time. The piston control acquisition module is configured to obtain a target position of a piston in a buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through a buoyancy principle according to the diving control quantity, and obtain a control quantity of the piston according to a current position of the piston. And the gliding control module is configured to control the biomimetic gliding robotic dolphin to glide based on the control quantity of the piston and the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin. The third aspect of the present invention provides a storage device, wherein a plurality of programs are stored in the storage device. The programs are configured to be loaded and executed by a processor to implement the gliding depth control method for the biomimetic gliding robotic dolphin described above. According to a fourth aspect of the present invention, a processing device includes a processor and a storage device. The processor is configured to execute a plurality of programs, and the storage device is configured to store the plurality of programs. The programs are configured to be loaded and executed by the processor to implement the gliding depth control method for the biomimetic gliding robotic dolphin described above. The present invention has the following advantages. The present invention improves the accuracy of the gliding depth control for the biomimetic gliding robotic dolphin. According to the present invention, a full-state dynamical model of the biomimetic gliding robotic dolphin is constructed, and the full-state dynamical model is simplified by ignoring the lateral motion to obtain a simplified dynamic model. An estimated velocity of the biomimetic gliding robotic dolphin is obtained by a sliding mode observer using depth, attitude and other information. In consideration of the large delay of the system of the biomimetic gliding robotic dolphin, a segmented Bzier reference trajectory is designed based on the estimated velocity, and piston motion in the buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin is adjusted by a depth controller based on model prediction to control the buoyancy, so as to achieve high-accuracy gliding depth control. Additionally, the control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin is obtained by an improved PID yaw controller, and the gliding attitude is adjusted by the differential motion of the movable pectoral fins. In this way, during the gliding depth control for the biomimetic gliding robotic dolphin, the yaw disturbance is avoided, and the heading of the biomimetic gliding robotic dolphin is maintained unchanged.",
                        "Other features, objectives and advantages of the present invention will be obvious by the detailed descriptions of the non-restrictive embodiments with reference to the drawings. 1 is a flow chart of the gliding depth control method for the biomimetic gliding robotic dolphin according to an embodiment of the present invention; 2 is a frame diagram of the gliding depth control system for the biomimetic gliding robotic dolphin according to an embodiment of the present invention; 3 is a schematic diagram of the coordinate system of the system of the biomimetic gliding robotic dolphin according to an embodiment of the present invention; 4 is a schematic diagram of an electromechanical structure of the biomimetic gliding robotic dolphin according to an embodiment of the present invention; 5 is a schematic diagram of a gliding depth control structure of the biomimetic gliding robotic dolphin according to an embodiment of the present invention; and 6A-6I are schematic diagrams showing a gliding depth control experiment of the biomimetic gliding robotic dolphin according to an embodiment of the present invention."
                    ],
                    "DESCRIPTION": "In order to make the objectives, technical solutions and advantages of the present invention clearer, the technical solutions in the embodiments of the present invention are described below with reference to the drawings. Obviously, the described embodiments are a part of embodiments of the present invention rather than all the embodiments. Based on the embodiments of the present invention, all other embodiments obtained by those having ordinary skill in the art without creative efforts shall fall within the scope of protection of the present invention. The present invention is further described in detail below in conjunction with the drawings and embodiments. It should be understood that the specific embodiments described herein are used only for explaining the present invention rather than limiting the present invention. Furthermore, it should be noted that, for the convenience of description, only parts relating to the present invention are shown in the drawings. It should be noted that the embodiments and the features in the embodiments of the present invention can be combined with each other when no in conflict. As shown in 1, a gliding depth control method for a biomimetic gliding robotic dolphin of the present invention includes the following steps. Step S100: a preset gliding depth and a preset yaw angle are obtained. Step S200: an estimated velocity is obtained by a sliding mode observer based on depth information and inertial navigation information of the biomimetic gliding robotic dolphin at a current time, and a control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin is obtained by a yaw controller in combination with the preset yaw angle. Step S300: a segmented diving velocity reference trajectory is obtained by constructing and segmenting a Bzier curve based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and a diving control quantity of the biomimetic gliding robotic dolphin is obtained by a model predictive control method in combination with the estimated velocity at the current time. Step S400: a target position of a piston in a buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin is obtained through a buoyancy principle according to the diving control quantity, and a control quantity of the piston is obtained according to a current position of the piston. Step S500: the biomimetic gliding robotic dolphin is controlled to glide based on the control quantity of the piston and the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin. In order to clearly illustrate the gliding depth control method for the biomimetic gliding robotic dolphin of the present invention, each step in an embodiment of the method of the present invention is described in detail below with reference to the drawings. Step S100: a preset gliding depth and a preset yaw angle are obtained. In the present embodiment, the preset gliding depth and the preset yaw angle for controlling the biomimetic gliding robotic dolphin to glide are first obtained. Specifically, the preset gliding depth is set manually, and the preset yaw angle is set to the initial direction, namely the heading direction remains unchanged. Step S200: an estimated velocity is obtained by a sliding mode observer based on depth information and inertial navigation information of the biomimetic gliding robotic dolphin at the current time, and the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin is obtained by a yaw controller in combination with the preset yaw angle. 3 schematically shows the coordinate system of the system of the gliding robotic dolphin, wherein cg=ogxg ygzg denotes a global inertial coordinate system, and cb=obxbybzb denotes a dolphin coordinate system. The x-axis direction of the dolphin coordinate system is the positive direction of the head of the dolphin, and the z-axis direction of the dolphin coordinate system is along the positive gravity direction. Besides, body-fixed coordinate systems are established at the corresponding movable joints and include cw=owxwywzw, ct=otxtytzt, cl=olxlylzl and cr=orxryrzr which denote a waist coordinate system, a tail coordinate system, a left pectoral fin coordinate system and a right pectoral fin coordinate system, respectively. 4 is a schematic diagram of the electromechanical structure of the biomimetic gliding robotic dolphin, which mainly includes three cabins: a pectoral fin cabin, a battery cabin and a waist-tail cabin. 4 shows the structure of the pectoral fin cabinand the structure of the battery cabin. The pectoral fin cabin includes a buoyancy adjustment mechanism, a piston motor and a pectoral fin steering gear. The battery cabin includes a movable slider, a slider motor and a lithium-ion battery pack. Each of the waist joint, the tail joint, the buoyancy adjustment mechanism and the gravity center adjustment mechanism is driven by a motor, and the pectoral fins on both sides of the biomimetic gliding robotic dolphin are driven by the steering gear. The biomimetic gliding robotic dolphin further includes various sensors and a power switch. 4 shows an inertial navigation sensor and a depth sensor. The full-state three-dimensional dynamical model of the biomimetic gliding robotic dolphin is constructed based on the coordinate systems in 3, which has both a gliding mode and a dolphin mode. Specifically, the gliding mode is calculated by using the kinetic energy-momentum relation as shown in formulas 1 and 2: p=gRbP1 =gRb+lp2wherein, P and denote a system momentum and an angular momentum under the inertial coordinate system, respectively. P and denote a system momentum and an angular momentum under the dolphin coordinate system, respectively. gRb denotes a rotation matrix, and l denotes a vector from the origin of the inertial coordinate system to the origin of the dolphin coordinate system. Newton-Euler equations of the gliding mode are established by calculating system kinetic energy. With respect to the dolphin mode, Newton-Euler equations of the waist joint, the tail joint, the left pectoral fin joint and the right pectoral fin joint are established separately according to a multi-link dynamics theory, and the force and the moment of each joint are obtained separately through the velocity transfer of forward kinematics, so as to obtain the acceleration and the velocity of each joint. The above dynamical model is simplified by ignoring the lateral motion to be suitable for a real embedded platform, as shown in formula 3: M{dot over }=CD+,3wherein, =[u, w, q]T denotes a forward velocity, a longitudinal velocity and a pitching angular velocity under the dolphin coordinate system, respectively; M=diag{m1, mz, m3} denotes an inertial mass matrix containing added mass; m1, m2, m3 denote mass parameters; D=diag{d1, d2, d3} denotes a damping matrix simplified as a constant term; d1, d2, d3 denote damping parameters; =[0, uc, {right arrow over a}Gm sin] denotes an input matrix; C denotes a Coriolis force and centripetal force matrix, and C v = 0 0 m 2 w 0 0 - m 1 u - m 2 w m 1 u 0 ; {dot over } denotes a velocity derivative; uc denotes a real-time control quantity; {right arrow over a} denotes a position vector of the gravity center and the buoyancy center of the robot; Gm denotes the acceleration of gravity; denotes a pitch angle. Therefore, the vertical component is extracted through the simplified dynamical model according to the Coriolis force and centripetal force matrix to obtain formula 4: w . = - d 2 m 2 w + u c m 2 - m 1 m 2 u q 4 wherein, {dot over w} denotes a diving velocity derivative. During the designing of the controller, it is necessary to calculate the velocity vector of the simplified dynamical model, but the current sensors used for precise underwater positioning are mostly bulky and expensive. In this regard, the present invention provides a velocity estimation algorithm based on a sliding mode observer. 5 shows the specific process of the gliding depth control method for the biomimetic gliding robotic dolphin, wherein, dr denotes the preset gliding depth, and r denotes the preset yaw angle. Each module in 5 is illustrated as follows. Firstly, measured values of sensors are obtained, including a measured value of the depth sensor, a measured value of the inertial navigation sensor, and a measured value of the piston position sensor of the buoyancy adjustment mechanism, namely depth information, inertial navigation information, piston information. Three-axis Euler angles, an angular velocity and an acceleration of the head of the robot are obtained as navigation information by the inertial navigation sensor through a nine-axis algorithm. A real diving velocity is obtained according to the depth information d, and an estimated diving velocity gbz of the inertial coordinate system defines an estimated error s, as shown in formulas 5 and 6, respectively: gUbz=5 s=gUbzgbz6wherein, {dot over d} denotes a derivative of the depth information, and gUbz denotes the real diving velocity. Then, an estimated velocity of the global coordinate system is calculated according to a transformation relation between the inertial coordinate system and the dolphin coordinate system, as shown in formula 7: g U ~ . b = g R b ^ b U ~ b + 9 R b U ~ . b + c x c y c z sat s 7 wherein, g{tilde over {dot over U}}b denotes an estimated acceleration of the global coordinate system, cx cy cz denotes a weight vector of the sliding mode observer, and sat s denotes a saturation function. The saturation function is introduced to reduce the chattering effect of the sliding mode observer. gRb denotes a rotation matrix from the dolphin coordinate system to the inertial coordinate system, {circumflex over }b denotes a skew-symmetric matrix of the angular velocity, and {tilde over {dot over U}}b=M1CbbDb+ denotes an estimated acceleration of the dolphin coordinate system. The estimated velocity can be obtained by iterative calculation according to the estimated acceleration. Due to the gaps that typically exist between the mechanical structures, the biomimetic gliding robotic dolphin is prone to deflection in the yaw direction during the gliding process. Thus, it is necessary to modify the design of the yaw controller to satisfy the assumption of ignoring the lateral motion in the simplified dynamical model. Since the basic principle of the yaw motion depends on a differential hydrodynamic moment generated by the deflection of the pectoral fins on both sides of the biomimetic gliding robotic dolphin, and the hydrodynamic force is closely related to the velocity, when the velocity is relatively large, to avoid generating overshoot, the yaw control quantity should not be excessively large. Therefore, the present embodiment provides a design for yaw controller based on the velocity vector estimated velocity to obtain a control quantity, so as to control the deflection angles of the pectoral fins on both sides of the biomimetic gliding robotic dolphin. Firstly, a weight coefficient kf of the yaw controller is obtained according to the estimated velocity, as shown in formula 8: k f = v e x 2 + v e z 2 v max 8 wherein, max denotes a maximum gliding velocity, which is a constant value obtained by simulation, ex denotes an estimated x-axis velocity, and ez denotes an estimated z-axis velocity. Then, this coefficient is applied to a classic PID controller to obtain a final control quantity uf, as shown in formula 9: uf=kfkpe+kie+kd9wherein, kp denotes a proportional factor, ki denotes an integral factor, kd denotes a differential factor, e denotes a yaw angle error, denotes the derivative of the yaw angle error, and i denotes a subscript. The control quantity is mapped directly to the left angle l and the right angle r of the pectoral fins by adjusting PID parameters. In the present embodiment, a yaw adjustment is preferably performed on only the pectoral fin on one side. Namely, when the pectoral fin on one side undergo deflection, the angle of the pectoral fin on the other side is zero. The adjustment method is shown in formula 10: { l = u f , r = 0 , if deflected to the right l = 0 , r = u f , if deflected to the left 10 Step S300: a segmented diving velocity reference trajectory is obtained by constructing and segmenting a Bzier curve based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and a diving control quantity of the biomimetic gliding robotic dolphin is obtained by a model predictive control method in combination with the estimated velocity at the current time. The slow response of gliding motion easily causes a large delay to the system, but the model predictive control is suitable for such a system and has a low requirement for the model accuracy. Therefore, according to the simplified dynamical model and the estimated velocity obtained above, the present invention provides a depth controller based on the model predictive control to achieve high-accuracy depth control by improving the reference trajectory. The specific process is as follows. The simplified dynamical model above is converted into an expression in the form of discrete domains, as shown in formula 11: wk+1=Awk+Buck+Lk11wherein, A = 1 - d 2 m 2 , B = 1 m 2 , L = - m 1 m 2 u k q k , and k denotes a natural number. In order to facilitate the expression, the system variables are unified and defined as shown in formula 12: k | t = w k | t u c k - 1 | t 12 wherein, k|t indicates k steps are predicted on basis of time t, then the simplified dynamical model is expressed as formulas 13 and 14: k+1|t=k|t+{tilde over B}uck|t+{tilde over L}t13 k|t={tilde over C}k|t14wherein, A ~ = A B 0 1 , B ~ = B 1 , C ~ = C 0 , L ~ = L 0 , uc denotes a control increment, and k|t denotes an extended output quantity. Then, the future state variables are iteratively predicted by setting the number Nc of control steps and the number Np of prediction steps, as shown in formula 15: k + N c | t = A ~ N c k | t + A ~ N c - 1 B ~ u c k | t + + B ~ u c k + N c - 1 | t + i = 0 N c - 1 A ~ L ~ t k + N p | t = A ~ N p k | t + A ~ N p - 1 B ~ u c k | t + + A ~ N p - N c - 1 B ~ u c k + N c | t + i = 0 N c - 1 A ~ L ~ t 15 In the present embodiment, the term L maintains a constant term during one prediction process to cancel the coupling between the diving velocity, the forward velocity and the pitching angular velocity. Moreover, the predicted state variables are unified and integrated to obtain formula 16: Yt=k|t+HUct+16wherein, Yt=k+1|t, . . . , k+Nc|t, . . . , k+Np|tT, the superscript T denotes transpose operation ={tilde over C}, . . . {tilde over C}Nc, . . . , {tilde over C}NpT, = I , , i = 0 N c - 1 A ~ , , i = 0 N p - 1 A ~ T L ~ t , I denotes an identity matrix, and H = C ~ B ~ 0 0 C ~ A ~ N c - 1 B ~ C ~ B ~ C ~ A ~ N p - 1 B ~ C ~ A ~ N p - N c - 1 B ~ . Subsequently, an objective function of the model predictive control method is optimized to calculate the optimal solution of the control signal. The objective function should be chosen by considering two factors. First, the depth steady-state error difference between the target depth and the actual depth should be minimized. In the present invention, the control target is converted from the depth into the diving velocity, and the target depth can be reached by designing a suitable diving velocity reference trajectory. Second, the control increment is preferably not excessively large to avoid mechanical and electrical damage to the robot. Therefore, based on the foregoing considerations, the present invention provides an objective function based on a steady-state error and a control increment, as shown in formula 17: J t , U c t = i = 1 N p t + i | t - r e f t + i | t Q 2 + i = 0 N c - 1 u c t + i | t R 2 17 where Jt, Uct denotes the objective function based on the steady-state error and the control increment, ref denotes a reference trajectory, R denotes a control increment parameter, and Q denotes an error parameter. The objective function is integrated into a matrix form to obtain formulas 18 and 19: min U c t J t , U c t = U c t T U c t + U c t 18 s . t . { U c t [ U cmin , U cmax ] U c t [ U c min , U cmax ] 19 wherein, =HT{tilde over Q}H+{tilde over R}, =2EtT{tilde over Q}H, wherein Et=k|t+Yreft, =EtT{tilde over Q}Et, Q ~ = diag N x N p { Q , , Q } R ~ = diag N u N c { R , , R } , wherein Tref denotes the segmented diving velocity reference trajectory which is obtained in the next content, Nx denotes the number of state quantities. The control increment sequence within the control step is obtained by optimizing the feasible solution of the function above, as shown in formula 20: Uc*=uct* . . . uct+Nc1*T20wherein, Uc* denotes the control increment sequence, uct* denotes the optimal control quantity. Then, the first value in the control increment sequence is chosen as the final control increment value to obtain the control quantity, as shown in formula 21: uct=uct1+uct*21With respect to a large time-delay system, overshoot can be avoided by designing a good reference trajectory. Therefore, the present invention provides a reference trajectory design method based on a Bzier curve. The Bzier curve is typically used to smooth a path, and the shape of the curve can be changed by setting different control points. The depth control system with minimum overshoot is desired. Therefore, in the present invention, the second-order Bzier curve is first applied to the depth reference trajectory based on the prediction step, and then the real-time diving velocity reference trajectory is obtained by calculating depth reference points. The depth reference trajectory Pref is designed as shown in formula 22: Prefi=1ti2d+2ti1ti2dr+ti2dr22wherein, t = 1 N p 0 1 N p , dr denotes a preset gliding depth, and d denotes a real-time depth, namely depth information. The diving velocity reference trajectory is obtained by taking the derivative Vref={dot over P}ref of the depth reference trajectory. In actual control processes, the reference trajectory is designed differently according to different target depths. Therefore, the present invention provides a segmented reference trajectory design method, whereby reference trajectories at different phases are obtained by setting a depth threshold dtheshold as shown in formula 23: Y ref = { c 1 V r e f , if d r - d &gt; d t h e s h o l d c 2 V r e f , otherwise 23 wherein, Yref denotes the segmented diving velocity reference trajectory, and c1 and c2 denote weight coefficients. However, in practical applications, the present invention is not limited to the two-segment reference trajectory, and a multi-segment trajectory can be designed according to different target depths, and even continuous functions are designed to achieve depth control. Step S400: a target position of a piston in a buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin is obtained through a buoyancy principle according to the diving control quantity, and a control quantity of the piston is obtained according to a current position of the piston. In the present embodiment, the depth control is realized through buoyancy adjustment under the gliding mode, and the buoyancy adjustment is realized by drawing or expelling water due to a movement of the piston in the mechanism. Thus, the obtained control quantity needs to be mapped into the position of the piston in the buoyancy adjustment mechanism. Considering that the physical significance of the control quantity is force, the target position of the piston can be obtained according to the buoyancy principle, and the calculation procedure is shown in formula 24: r sr = u c t g S 24 wherein, rsr denotes the target position of the piston, denotes the density of water; g denotes the acceleration of gravity, and S denotes the bottom area of the cylindrical buoyancy adjustment mechanism. The motor has a large acceleration and deceleration under a position mode, but depth control is real-time and has a short control cycle. Thus, the position mode may cause damage to mechanical and electrical structures. In order to protect the buoyancy-driven mechanism, in the present invention, the motor is set under a velocity mode, and the PID controller is designed according to the velocity mode to realize the position loop for smooth movement. Step S500: the biomimetic gliding robotic dolphin is controlled to glide based on the control quantity of the piston and the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin. In the present embodiment, the biomimetic gliding robotic dolphin is controlled to ascend and dive according to the control quantity of the piston, and the yaw adjustment of the biomimetic gliding robotic dolphin is controlled according to the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin, thereby realizing gliding control of the biomimetic gliding robotic dolphin. 6A-6I are schematic diagrams showing the gliding depth control experiment of the biomimetic gliding robotic dolphin at different times, wherein the black dashed line denotes the target depth, and the reflection from the water surface is the inverted reflection of the dolphin. According to the second embodiment of the present invention, as shown in 2, a gliding depth control system for a biomimetic gliding robotic dolphin includes the preset value acquisition module 100, the pectoral fin control acquisition module 200, the diving control acquisition module 300, the piston control acquisition module 400, and the gliding control module 500. The preset value acquisition module 100 is configured to obtain a preset gliding depth and a preset yaw angle. The pectoral fin control acquisition module 200 is configured to obtain an estimated velocity by a sliding mode observer based on depth information and inertial navigation information of the biomimetic gliding robotic dolphin at a current time, and obtain a control quantity of pectoral fins on both sides of the biomimetic gliding robotic dolphin by a yaw controller in combination with the preset yaw angle. The diving control acquisition module 300 is configured to obtain a segmented diving velocity reference trajectory by constructing and segmenting a Bzier curve based on the preset gliding depth and the depth information of the biomimetic gliding robotic dolphin at the current time, and obtain a diving control quantity of the biomimetic gliding robotic dolphin by a model predictive control method in combination with the estimated velocity at the current time. The piston control acquisition module 400 is configured to obtain a target position of a piston in a buoyancy adjustment mechanism of the biomimetic gliding robotic dolphin through a buoyancy principle according to the diving control quantity, and obtain a control quantity of the piston according to a current position of the piston. The gliding control module 500 is configured to control the biomimetic gliding robotic dolphin to glide based on the control quantity of the piston and the control quantity of the pectoral fins on both sides of the biomimetic gliding robotic dolphin. It can be clearly understood by those skilled in the art that for the convenience and brevity of the description, the specific working process and related description of the system described above can refer to the corresponding processes in the foregoing embodiment of the method of the present invention, which will not be repeatedly described here. It should be noted that the gliding depth control system for the biomimetic gliding robotic dolphin in the above embodiment is only exemplified by the division of the above functional modules. In practical applications, the above functions may be allocated to be implemented by different functional modules as needed, that is, the modules or steps in the embodiments of the present invention are further decomposed or combined. For example, the modules in the above embodiments can be combined into one module, or can be further split into a plurality of sub-modules to complete all or a part of the functions in the above description. The designations of the modules and steps involved in the embodiments of the present invention are only intended to distinguish these modules or steps, and should not be construed as an improper limitation to the present invention. The third embodiment of the present invention provides a storage device, wherein a plurality of programs are stored in the storage device. The programs are configured to be loaded and executed by a processor to implement the gliding depth control method for the biomimetic gliding robotic dolphin described above. A processing device according to the fourth embodiment of the present invention includes a processor and a storage device. The processor is configured to execute a plurality of programs, and the storage device is configured to store the plurality of programs. The programs are configured to be loaded and executed by the processor to implement the gliding depth control method for the biomimetic gliding robotic dolphin described above. It can be clearly understood by those skilled in the art that for the convenience and brevity of the description, the specific working process and related description of the storage device and the processing device described above can refer to the corresponding processes in the foregoing embodiments of the method of the present invention, which will not be repeatedly described here. Those skilled in the art can realize that the exemplary modules and method steps described with reference to the embodiments disclosed herein can be implemented by electronic hardware, computer software or a combination of the electronic hardware and the computer software. The programs corresponding to modules of software or steps of methods can be stored in a random access memory RAM, a memory, a read-only memory ROM, an electrically programmable ROM, an electrically erasable programmable ROM, a register, a hard disk, a removable disk, a compact disc read-only memory CD-ROM, or any other form of storage mediums well-known in the art. In order to clearly illustrate the interchangeability of electronic hardware and software, in the above description, the compositions and steps of each embodiment have been generally described functionally. Whether these functions are implemented by electronic hardware or software depends on specific applications and design constraints of the technical solution. Those skilled in the art may use different methods to implement the described functions for each specific application, but such implementation should not be considered beyond the scope of the present invention. The terms first, second and the like are used to distinguish similar objects, rather than to describe or indicate a specific order or sequence. The term include/comprise or any other similar terms are intended to cover non-exclusive inclusions, so that a process, method, article or apparatus/device including a series of elements not only includes those elements but also includes other elements that are not explicitly listed, or further includes elements inherent in the process, method, article or apparatus/device. Hereto, the technical solutions of the present invention have been described in combination with the preferred implementations with reference to the drawings. However, it is easily understood by those skilled in the art that the scope of protection of the present invention is obviously not limited to these specific embodiments. Without departing from the principle of the present invention, those skilled in the art can make equivalent modifications or replacements to related technical features, and the technical solutions obtained through these modifications or replacements shall fall within the scope of protection of the present invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/86/359/109/0.pdf",
                    "CONTRADICTION_SCORE": 0.98514324426651,
                    "F_SPEC_PARAMS": [
                        "consumes a substantial amount of energy"
                    ],
                    "S_SPEC_PARAMS": [
                        "speed limitation of slider movement and buoyancy adjustment,",
                        "delay",
                        "prone to overshoot",
                        "high requirements for sealing,",
                        "prone to strong coupling effects on the yaw motion",
                        "depth error",
                        "delay",
                        "accuracy"
                    ],
                    "A_PARAMS": [
                        "movable fin surface adjustment method,",
                        "controlling the rotation angle of the pectoral fins"
                    ],
                    "F_SENTS": [
                        "However, the robotic dolphin consumes a substantial amount of energy when performing these motions."
                    ],
                    "S_SENTS": [
                        "However, due to the speed limitation of slider movement and buoyancy adjustment, the systems that are operated under these two methods experience a relatively large delay and are prone to overshoot.",
                        "However, this method has high requirements for sealing, and is prone to strong coupling effects on the yaw motion.",
                        "Aiming at the depth control problem of biomimetic robotic dolphins, by means of the movable fin surface adjustment method, Yu proveds the depth control with a depth error of 0.",
                        "5 cm by controlling the rotation angle of the pectoral fins via an improved sliding mode fuzzy controller.",
                        "The accuracy problem of the gliding depth control has not been solved satisfactorily because of the large delay and the low accuracy of the system."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Waste of Energy"
                    ],
                    "F_SIM_SCORE": 0.7940016984939575,
                    "S_TRIZ_PARAMS": [
                        "Speed",
                        "Manufacturability",
                        "Force Torque",
                        "Accuracy of Measurement"
                    ],
                    "S_SIM_SCORE": 0.5681779310107231,
                    "GLOBAL_SCORE": 1.7462330590188504
                },
                "sort": [
                    1.7462331
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11317978-20220503",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11317978-20220503",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-06-26",
                    "PUBLICATION_DATE": "2022-05-03",
                    "INVENTORS": [
                        "Hayden Cameron",
                        "Spiros Mantzavinos",
                        "Neil R. Crawford",
                        "Sanjay Joshi",
                        "Norbert Johnson",
                        "James Cascarano",
                        "Justin Larson"
                    ],
                    "APPLICANTS": [
                        "GLOBUS MEDICAL, INC.    ( Audubon , US )"
                    ],
                    "INVENTION_TITLE": "System for neuronavigation registration and robotic trajectory guidance, robotic surgery, and related methods and devices",
                    "DOMAIN": "A61B 3432",
                    "ABSTRACT": "A system for robotic surgery makes use of an end-effector which has been configured so that any selected one of a group of surgical tools may be selectively connected to such end-effector. The end-effector makes use of a tool-insert locking mechanism which secures a selected one of the surgical tools at not only a respective, predetermined height and angle of orientation, but also at a rotational position relative to an anatomical feature of the patient. The tool-insert locking mechanism may include interchangeable inserts to interconnect multiple tools to the same end-effector. In this way, different robotic operations may be accomplished with less reconfiguration of the end-effector. The end-effector may also include a tool stop which has a sensor associated with a moveable stop mechanism which may be positioned to selectively inhibit tool insertion or end-effector movement.",
                    "CLAIMS": "1. A surgical robot system for surgery on an anatomical feature of a patient, the system comprising: a surgical robot; a robot arm connected to the surgical robot; an end-effector connected to the robot arm and orientable to oppose the anatomical feature so as to be in operative proximity thereto; a plurality of surgical tools, each of the tools selectively connectable to the end-effector; a processor circuit; a memory accessible by the processor circuit and comprising machine-readable instructions configured to cause, when executed, the end-effector to move relative to the anatomical feature; a tool-insert locking mechanism located on and connected to the end-effector and configured to secure any selected one of the plurality of surgical tools at a respective, predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient; and a tool stop located on and connected to the end-effector and having a stop mechanism and a sensor; wherein the stop mechanism is configured to be selectively moveable between an engaged position to prevent any of the tools from being connected to the end-effector and a disengaged position to permit any of the tools to be selectively connected to the end-effector; wherein the sensor is configured to detect the positions of the stop mechanism; and wherein, in response to detection of the disengaged position, the sensor causes the processor to execute instructions to prevent movement of the end-effector relative to the anatomical feature. 2. The system of claim 1, wherein the end-effector comprises opposite proximal and distal surfaces; wherein the tool-insert locking mechanism is located on and connected to the proximal surface of the end-effector; wherein the tool stop is located on and connected to the distal surface of the end-effector; wherein the end-effector comprises a cylindrical bore extending between the proximal and distal surfaces; wherein the tool stop and the tool-insert locking mechanism each have circular apertures axially aligned with the bore; and wherein the apertures and the bore are adapted to receive therethrough any selected one of the plurality of surgical tools. 3. The system of claim 2, wherein the stop mechanism includes a ring axially aligned with the bore, the ring being selectively, manually rotatable so as to move the stop mechanism between the engaged position and the disengaged position. 4. The system of claim 3, wherein the stop mechanism includes a detent mechanism located on and mounted to the ring, the detent mechanism configured to lock the ring against rotational movement when the stop mechanism is in the engaged position; wherein the detent mechanism is manually actuatable to unlock the ring to permit the ring to be manually rotated to cause the stop mechanism to move from the engaged position to the disengaged position. 5. The system of claim 4, wherein the tool stop further comprises a lever arm pivotally mounted adjacent the apertures of the tool stop; wherein the lever arm is operatively connected to the stop mechanism to close the apertures of the tool stop in response to the stop mechanism being in the engaged position, thereby preventing attachment of the tools to the end-effector, and to open the apertures of the tool stop in response to the stop mechanism being in the disengaged position, thereby permitting movement of the end-effector and attachment of a selected one of the tools to the end-effector. 6. The system of claim 5, wherein the distal surface of the end-effector comprises a distal plane, and wherein the lever arm is pivotable in the distal plane to close the apertures of the tool stop sufficiently to prevent insertion of any of the plurality of the surgical tools distally past the distal plane. 7. The system of claim 2, wherein the end-effector further comprises at least one illumination element mounted on the distal surface at a location spaced from the tool stop to illuminate the anatomical feature substantially without obstruction of illumination from any of the plurality of tools when received in the tool-insert locking mechanism. 8. The system of claim 1, wherein the sensor comprises a Hall Effect sensor and at least one magnet, the Hall Effect sensor and the magnet located and movable relative to each other to generate respective magnetic fields detectable by the sensor and corresponding to the engaged position and the disengaged position, respectively. 9. The system of claim 1, wherein the tool-insert locking mechanism includes a connector configured to mate with and secure the any selected one of the plurality of tools at their respective height, angle of orientation, and rotational position relative to the anatomical feature of the patient. 10. The system of claim 9, wherein the connector comprises a rotatable flange with a slot adapted to receive therethrough a corresponding tongue associated with the selected one of the plurality of tools. 11. The system of claim 10, wherein the rotatable flange comprises a collar having a proximally oriented surface and multiple slots radially spaced on the proximally oriented surface, the multiple slots configured to receive therethrough corresponding tongues associated with the selected one of the plurality of tools; and wherein the slots and corresponding tongues are arranged to permit securing of the selected one of the plurality of tools when in the predetermined angle of orientation and rotational position relative to the anatomical feature of the patient. 12. The system of claim 10, wherein the rotatable flange is adapted to be manually rotatable between an open and a closed position, the open position permitting the corresponding tongue to be received through the slot, and the closed position bringing a portion of the rotatable flange into engagement with the corresponding tongue after being received in the slot to secure the selected one of the tools at the respective, predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient. 13. The system of claim 10, further comprising an adapter configured to interconnect at least one of the plurality of surgical tools and the end-effector. 14. The system of claim 13, wherein the corresponding tongue associated with the selected one of the plurality of tools is located on and mounted to the adapter; wherein the adapter further includes a tool receiver adapted to connect the selected one of the tools to the adapter, whereby, when the corresponding tongue is received in the slot of the rotatable flange and the selected one of the tools is connected to the tool receiver, the selected one of the tools is secured at the respective, predetermined height, angle of orientation, and rotational position relative to the anatomical feature. 15. The system of claim 14, wherein the rotatable flange has an inner perimeter defined by an inner perimeter edge and the adapter has an outer perimeter defined by an outer perimeter edge, the dimensions of the inner perimeter edge and the outer perimeter edge selected so that the edges oppose each other when the adapter is received in the rotatable flange; wherein the adapter extends between proximal and distal ends, the adapter being secured relative to the rotatable flange at the distal end; and wherein the tool receiver comprises a bore extending between the proximal and distal ends, the bore sized to receive at least one of the plurality of surgical tools therethrough. 16. The system of claim 15, further comprising a plurality of the adapters, the adapters configured to be interchangeable inserts having substantially the same, predetermined outer perimeter receivable within the inner perimeter of the rotatable flange; wherein the interchangeable adapters have bores of different, respective diameters selected to receive corresponding ones of the plurality of tools therein. 17. The system of claim 15, wherein the inner perimeter of the rotatable flange and the outer perimeter of the adapter are circular and have central, aligned axes and corresponding radii; and wherein the slot of the rotatable flange extends radially outwardly from the central axis of the rotatable flange and the tongue of the adapter extends radially outwardly from the axis of the adapter. 18. A surgical robot for surgery on an anatomical feature of a patient, the system comprising: a robot arm; an end-effector connected to the robot arm and orientable to oppose the anatomical feature so as to be in operative proximity thereto; a processor circuit; a memory accessible by the processor circuit and comprising machine-readable instructions configured to cause, when executed, the end-effector to move relative to the anatomical feature; a tool-insert locking mechanism located on and connected to the end-effector and configured to secure one of a plurality of surgical tools at a respective, predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient; and a tool stop located on and connected to the end-effector and having a stop mechanism and a sensor; wherein the stop mechanism is configured to be selectively moveable between an engaged position to prevent any of the tools from being connected to the end-effector and a disengaged position to permit any of the tools to be selectively connected to the end-effector; wherein the sensor is configured to detect the positions of the stop mechanism; and wherein, in response to detection of the disengaged position, the sensor causes the processor to execute instructions to prevent movement of the end-effector relative to the anatomical feature. 19. The surgical robot of claim 18, wherein the end-effector comprises opposite proximal and distal surfaces; wherein the tool-insert locking mechanism is located on and connected to the proximal surface of the end-effector; wherein the tool stop is located on and connected to the distal surface of the end-effector; wherein the end-effector comprises a cylindrical bore extending between the proximal and distal surfaces; wherein the tool stop and the tool-insert locking mechanism each have circular apertures axially aligned with the bore; and wherein the apertures and the bore are adapted to receive therethrough any one of the plurality of surgical tools. 20. The surgical robot of claim 18, wherein the sensor comprises a Hall Effect sensor and at least one magnet, the Hall Effect sensor and the magnet located and movable relative to each other to generate respective magnetic fields detectable by the sensor and corresponding to the engaged position and the disengaged position, respectively.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to medical devices and systems, and more particularly, systems for neuronavigation registration and robotic trajectory guidance, robotic surgery, and related methods and devices.",
                    "STATE_OF_THE_ART": "Position recognition systems for robot assisted surgeries are used to determine the position of and track a particular object in 3-dimensions 3D. In robot assisted surgeries, for example, certain objects, such as surgical instruments, need to be tracked with a high degree of precision as the instrument is being positioned and moved by a robot or by a physician, for example. Position recognition systems may use passive and/or active sensors or markers for registering and tracking the positions of the objects. Using these sensors, the system may geometrically resolve the 3-dimensional position of the sensors based on information from or with respect to one or more cameras, signals, or sensors, etc. These surgical systems can therefore utilize position feedback to precisely guide movement of robotic arms and tools relative to a patients' surgical site. Thus, there is a need for a system that efficiently and accurately provide neuronavigation registration and robotic trajectory guidance in a surgical environment. End-effectors used in robotic surgery may be limited to use in only certain procedures, or may suffer from other drawbacks or disadvantages.",
                    "SUMMARY": [
                        "According to some implementations, a surgical robot system is configured for surgery on an anatomical feature of a patient, and includes a surgical robot, a robot arm connected to such surgical robot, and an end-effector connected to the robot arm. The end-effector has a tool-insert locking mechanism connected to it. The tool-insert locking mechanism is configured to secure any one of a plurality of surgical tools at a respective, predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient. The surgical robot system may likewise include a tool stop, which is also located on and connected to the end-effector. The tool stop has a component which comprises a stop mechanism and also includes a sensor. The stop mechanism is selectively moveable between two positionsan engaged position to prevent surgical tools from being connected to the end-effector and a disengaged position in which surgical tools may be selectively connected to such end-effector. The sensor is able to detect whether the stop mechanism is in the engaged position or the disengaged position. When the sensor determines that the stop mechanism is in the disengaged position, the processor executes suitable instructions, which instructions prevent movement of the end-effector relative to the anatomical feature. According to still other implementations, the end-effector includes a cylindrical bore which extends between proximal and distal surfaces of the end-effector, with the tool insert locking mechanism located on the proximal surface and the tool stop located on the distal surface. In this implementation, the tool stop and the tool insert locking mechanism each have circular apertures which are axially aligned with the bore. As such, the apertures on the bore are sized and shaped to receive therethrough any desired one of a set of surgical tools for use with the end-effector. In still another implementation and related variations, the stop mechanism may include a ring which is also axially aligned with the bore. The ring is mounted so that it may be manually rotated when desired to move the stop mechanism between the aforementioned engaged position and disengaged position. In still another variation, the above-described stop mechanism may be constructed so that it includes a detent mechanism located on and mounted to the ring. In this way, the detent mechanism can be secured relative to the ring and the stop mechanism so as to lock the ring against rotational movement when the stop mechanism is in the engaged position. Furthermore, the detent mechanism may be released or otherwise manually actuatable to unlock the ring from its previously locked position so that the ring can be manually rotated to move from the engaged position to the disengaged position. In another implementation, the tool stop of any of the above-described surgical robot systems and variations may include a pivotable lever arm. The lever arm is mounted adjacent to the bore of the end-effector and works or is otherwise operatively connected to the stop mechanism so that when the stop mechanism is brought into the engaged position, the lever arm moves to close the aperture of the stop mechanism. In the closed position, the lever arm prevents attachment of the tools to the end-effector. Conversely, the lever arm is pivotable to open the aperture in response to the stop mechanism being moved to the disengaged position, and a selected one of the surgical tools may be attached to the end-effector. Any of the foregoing described implementations may likewise include a sensor in the form of a Hall Effect sensor which operates in conjunction with a magnet. For example, the Hall Effect sensor and the magnet may be located and movable relative to each other to generate respective magnetic fields detectable by the sensor and corresponding to the engaged position and the disengaged position of the ring, respectively. When the disengaged position is sensed, the lever arm does not block the aperture, permitting tool attachment, and the sensor signals the system 100 to prevent movement of end-effector 112. When the engaged position is sensed, the lever arm blocks the aperture and end-effector motion is permitted due to the impossibility of tools being inserted down the end-effector bore. In still further implementations of the present disclosure, the tool insert locking mechanism of any of the systems described above may include a connector configured to mate with and secure a selected one of the plurality of surgical tools at the height, angle of orientation, and rotational position desired or preferably for the connected tools in relation to the anatomical feature of the patient. The connector of the tool insert locking mechanism may include a rotatable flange, which flange has a slot adapted to receive therethrough a corresponding tongue which is associated with a selected one of the plurality of tools. In other implementations, the mating features of slot and tongue may assume other forms suitable to allow the tool to be connected to the tool insert locking mechanism and in turn to the end-effector. So, for example, the mating portion associated with the tool may be directly connected to such tool or, in other suitable implementations, the mating portion may be connected to an adapter. Such adapter may be configured to interconnect at least one of the plurality of surgical tools and the end-effector by means of selective attachment of the adapter to the tool insert locking mechanism, on the one hand, and selective attachment of one or more surgical tools to the adapter, on the other hand. The tool insert locking mechanism, in still other implementations, may make use of a rotatable flange in the form of a collar, the collar having a proximally oriented surface and multiple slots radially spaced on such proximally oriented surface. In such implementation, the multiple slots are configured to receive therethrough corresponding tongues which are either directly connected to any one of the plurality of tools or are connected instead to an adapter which is removably received in the rotatable flange. In this way, the slots in the corresponding tongues permit securing of the selected one of the plurality of tools only when such tools are at the predetermined angle of orientation and rotational position relative to the anatomical feature of the patient. The rotatable flange may be adapted, in still other implementations of the current disclosure, so as to be manually rotatable between open and closed positions. The open position is one in which one or more tongues of the adapter or the tool are received through corresponding one or more slots. The closed position is accomplished by rotating the flange after receipt of the tongues through the slots so as to bring a portion of the rotatable flange into engagement with the corresponding tongue and thereby secure the selected one of the tools, or the adapter in which the tool is received, at the respective, predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient. In those implementations that make use of an adapter, the rotatable flange has an inner perimeter edge while the adapter as an outer perimeter edge, the dimensions of each selected so that the edges oppose each other when the adapter is received in the rotatable flange. The adapter has a tongue, as mentioned for some of the implementations discussed previously, and also further includes a tool receiver adapted to connect a selected one of the tools to such adapter. As such, when the tongue of the adapter is received in the slot of the rotatable flange and locked thereto, and when the selected one of the tools is connected to the tool receiver of the adapter, such selected tool is then secured at the predetermined height, angle of orientation, and rotational position relative to the anatomical feature. In still further implementations, the surgical system may include a plurality of the adapters, the adapters being configured so as to be in the form of interchangeable inserts. Such inserts have substantially the same, predetermined outer perimeter which is sized to be receivable within the aforesaid inner perimeter of the rotatable flange. Furthermore, such interchangeable inserts have bores extending therethrough, which bores have different, respective diameters selected to receive corresponding ones of the plurality of tools therein. In this way, inserts may be selectively connected to or removed from the tool insert locking mechanism, depending on the tools associated with such adapters, and the need for such tools to be mounted to the end-effector for given procedures on the anatomical feature of the patient. The end-effector may likewise be equipped, in still other variations, with at least one illumination element which is mounted on a distal surface of the end-effector. In certain implementations, the illumination element is mounted at a location spaced from the tool stop so that tools received therein do not obstruct illumination when received in the tool insert locking mechanism. According to some embodiments of inventive concepts, a system includes a processor circuit and a memory coupled to the processor circuit. The memory includes machine-readable instructions configured to cause the processor circuit to determine, based on a first image volume comprising an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a first plurality of fiducial markers that are fixed with respect to the registration fixture, determine, for each fiducial marker of the first plurality of fiducial markers, a position of the fiducial marker relative to the image volume. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined positions of the first plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature. The machine-readable instructions are further configured to cause the processor circuit to, based on a data frame from a tracking system comprising a second plurality of tracking markers that are fixed with respect to the registration fixture, determine, for each tracking marker of the second plurality of tracking markers, a position of the tracking marker. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined positions of the second plurality of tracking markers, a position and orientation of the registration fixture with respect to a robot arm of a surgical robot. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined position and orientation of the registration fixture with respect to the anatomical feature and the determined position and orientation of the registration fixture with respect to the robot arm, a position and orientation of the anatomical feature with respect to the robot arm. The machine-readable instructions are further configured to cause the processor circuit to control the robot arm based on the determined position and orientation of the anatomical feature with respect to the robot arm. According to some other embodiments of inventive concepts, a computer-implemented method is disclosed. The computer-implemented method includes, based on a first image volume comprising an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a first plurality of fiducial markers that are fixed with respect to the registration fixture, determining, for each fiducial marker of the first plurality of fiducial markers, a position of the fiducial marker. The computer-implemented method further includes determining, based on the determined positions of the first plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature. The computer-implemented method further includes, based on a tracking data frame comprising a second plurality of tracking markers that are fixed with respect to the registration fixture, determining, for each tracking marker of the second plurality of tracking markers, a position of the tracking marker. The computer-implemented method further includes determining, based on the determined positions of the second plurality of tracking markers, a position and orientation of the registration fixture with respect to a robot arm of a surgical robot. The computer-implemented method further includes determining, based on the determined position and orientation of the registration fixture with respect to the anatomical feature and the determined position and orientation of the registration fixture with respect to the robot arm, a position and orientation of the anatomical feature with respect to the robot arm. The computer-implemented method further includes controlling the robot arm based on the determined position and orientation of the anatomical feature with respect to the robot arm. According to some other embodiments of inventive concepts, a surgical system is disclosed. The surgical system includes an intraoperative surgical tracking computer having a processor circuit and a memory. The memory includes machine-readable instructions configured to cause the processor circuit to provide a medical image volume defining an image space. The medical image volume includes an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a plurality of fiducial markers that are fixed with respect to the registration fixture. The machine-readable instructions are further configured to cause the processor circuit to, based on the medical image volume, determine, for each fiducial marker of the plurality of fiducial markers, a position of the fiducial marker with respect to the image space. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined positions of the plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature. The machine-readable instructions are further configured to cause the processor circuit to provide a tracking data frame defining a tracking space, the tracking data frame comprising positions of a first plurality of tracked markers that are fixed with respect to the registration fixture. The machine-readable instructions are further configured to cause the processor circuit to, based on the tracking data frame, determine a position of the anatomical feature with respect to the first plurality of tracked markers in the tracking space. The surgical system further includes a surgical robot having a robot arm configured to position a surgical end-effector. The surgical robot further includes a controller connected to the robot arm. The controller is configured to perform operations including, based on the tracking data frame, determining a position of the robot arm with respect to the tracking space. The controller is configured to perform operations including determining, based on the determined position and orientation of the anatomical feature with respect to the tracking space and the determined position and orientation of the robot arm with respect to the tracking space, a position and orientation of the anatomical feature with respect to the robot arm. The controller is configured to perform operations including controlling movement of the robot arm based on the determined position and orientation of the anatomical feature with respect to the robot arm to position the surgical end-effector relative to a location on the patient to facilitate surgery on the patient. Other methods and related devices and systems, and corresponding methods and computer program products according to embodiments will be or become apparent to one with skill in the art upon review of the following drawings and detailed description. It is intended that all such devices and systems, and corresponding methods and computer program products be included within this description, be within the scope of the present disclosure, and be protected by the accompanying claims. Moreover, it is intended that all embodiments disclosed herein can be implemented separately or combined in any way and/or combination.",
                        "The accompanying drawings, which are included to provide a further understanding of the disclosure and are incorporated in a constitute a part of this application, illustrate certain non-limiting embodiments of inventive concepts. In the drawings: 1A is an overhead view of an arrangement for locations of a robotic system, patient, surgeon, and other medical personnel during a surgical procedure, according to some embodiments; 1B is an overhead view of an alternate arrangement for locations of a robotic system, patient, surgeon, and other medical personnel during a cranial surgical procedure, according to some embodiments; 2 illustrates a robotic system including positioning of the surgical robot and a camera relative to the patient according to some embodiments; 3 is a flowchart diagram illustrating computer-implemented operations for determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments; 4 is a diagram illustrating processing of data for determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments; 5A-5C illustrate a system for registering an anatomical feature of a patient using a computerized tomography CT localizer, a frame reference array FRA, and a dynamic reference base DRB, according to some embodiments; 6A and 6B illustrate a system for registering an anatomical feature of a patient using fluoroscopy fluoro imaging, according to some embodiments; 7 illustrates a system for registering an anatomical feature of a patient using an intraoperative CT fixture ICT and a DRB, according to some embodiments; 8A and 8B illustrate systems for registering an anatomical feature of a patient using a DRB and an X-ray cone beam imaging device, according to some embodiments; 9 illustrates a system for registering an anatomical feature of a patient using a navigated probe and fiducials for point-to-point mapping of the anatomical feature, according to some embodiments; 10 illustrates a two-dimensional visualization of an adjustment range for a centerpoint-arc mechanism, according to some embodiments; 11 illustrates a two-dimensional visualization of virtual point rotation mechanism, according to some embodiments; 12 is an isometric view of one possible implementation of an end-effector according to the present disclosure; 13 is an isometric view of another possible implementation of an end-effector of the present disclosure; 14 is a partial cutaway, isometric view of still another possible implementation of an end-effector according to the present disclosure; 15 is a bottom angle isometric view of yet another possible implementation of an end-effector according to the present disclosure; 16 is an isometric view of one possible tool stop for use with an end-effector according to the present disclosure; 17 and 18 are top plan views of one possible implementation of a tool insert locking mechanism of an end-effector according to the present disclosure; and 19 and 20 are top plan views of the tool stop of 16, showing open and closed positions, respectively."
                    ],
                    "DESCRIPTION": "It is to be understood that the present disclosure is not limited in its application to the details of construction and the arrangement of components set forth in the description herein or illustrated in the drawings. The teachings of the present disclosure may be used and practiced in other embodiments and practiced or carried out in various ways. Also, it is to be understood that the phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of including, comprising, or having and variations thereof herein is meant to encompass the items listed thereafter and equivalents thereof as well as additional items. Unless specified or limited otherwise, the terms mounted, connected, supported, and coupled and variations thereof are used broadly and encompass both direct and indirect mountings, connections, supports, and couplings. Further, connected and coupled are not restricted to physical or mechanical connections or couplings. The following discussion is presented to enable a person skilled in the art to make and use embodiments of the present disclosure. Various modifications to the illustrated embodiments will be readily apparent to those skilled in the art, and the principles herein can be applied to other embodiments and applications without departing from embodiments of the present disclosure. Thus, the embodiments are not intended to be limited to embodiments shown, but are to be accorded the widest scope consistent with the principles and features disclosed herein. The following detailed description is to be read with reference to the figures, in which like elements in different figures have like reference numerals. The figures, which are not necessarily to scale, depict selected embodiments and are not intended to limit the scope of the embodiments. Skilled artisans will recognize the examples provided herein have many useful alternatives and fall within the scope of the embodiments. According to some other embodiments, systems for neuronavigation registration and robotic trajectory guidance, and related methods and devices are disclosed. In some embodiments, a first image having an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a first plurality of fiducial markers that are fixed with respect to the registration fixture is analyzed, and a position is determined for each fiducial marker of the first plurality of fiducial markers. Next, based on the determined positions of the first plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature is determined. A data frame comprising a second plurality of tracking markers that are fixed with respect to the registration fixture is also analyzed, and a position is determined for each tracking marker of the second plurality of tracking markers. Based on the determined positions of the second plurality of tracking markers, a position and orientation of the registration fixture with respect to a robot arm of a surgical robot is determined. Based on the determined position and orientation of the registration fixture with respect to the anatomical feature and the determined position and orientation of the registration fixture with respect to the robot arm, a position and orientation of the anatomical feature with respect to the robot arm is determined, which allows the robot arm to be controlled based on the determined position and orientation of the anatomical feature with respect to the robot arm. Advantages of this and other embodiments include the ability to combine neuronavigation and robotic trajectory alignment into one system, with support for a wide variety of different registration hardware and methods. For example, as will be described in detail below, embodiments may support both computerized tomography CT and fluoroscopy fluoro registration techniques, and may utilize frame-based and/or frameless surgical arrangements. Moreover, in many embodiments, if an initial preoperative registration is compromised due to movement of a registration fixture, registration of the registration fixture and of the anatomical feature by extension can be re-established intraoperatively without suspending surgery and re-capturing preoperative images. Referring now to the drawings, 1A illustrates a surgical robot system 100 in accordance with an embodiment. Surgical robot system 100 may include, for example, a surgical robot 102, one or more robot arms 104, a base 106, a display 110, an end-effector 112, for example, including a guide tube 114, and one or more tracking markers 118. The robot arm 104 may be movable along and/or about an axis relative to the base 106, responsive to input from a user, commands received from a processing device, or other methods. The surgical robot system 100 may include a patient tracking device 116 also including one or more tracking markers 118, which is adapted to be secured directly to the patient 210 , to a bone of the patient 210. As will be discussed in greater detail below, the tracking markers 118 may be secured to or may be part of a stereotactic frame that is fixed with respect to an anatomical feature of the patient 210. The stereotactic frame may also be secured to a fixture to prevent movement of the patient 210 during surgery. According to an alternative embodiment, 1B is an overhead view of an alternate arrangement for locations of a robotic system 100, patient 210, surgeon 120, and other medical personnel during a cranial surgical procedure. During a cranial procedure, for example, the robot 102 may be positioned behind the head 128 of the patient 210. The robot arm 104 of the robot 102 has an end-effector 112 that may hold a surgical instrument 108 during the procedure. In this example, a stereotactic frame 134 is fixed with respect to the patient's head 128, and the patient 210 and/or stereotactic frame 134 may also be secured to a patient base 211 to prevent movement of the patient's head 128 with respect to the patient base 211. In addition, the patient 210, the stereotactic frame 134 and/or or the patient base 211 may be secured to the robot base 106, such as via an auxiliary arm 107, to prevent relative movement of the patient 210 with respect to components of the robot 102 during surgery. Different devices may be positioned with respect to the patient's head 128 and/or patient base 211 as desired to facilitate the procedure, such as an intra-operative CT device 130, an anesthesiology station 132, a scrub station 136, a neuro-modulation station 138, and/or one or more remote pendants 140 for controlling the robot 102 and/or other devices or systems during the procedure. The surgical robot system 100 in the examples of 1A and/or 1B may also use a sensor, such as a camera 200, for example, positioned on a camera stand 202. The camera stand 202 can have any suitable configuration to move, orient, and support the camera 200 in a desired position. The camera 200 may include any suitable camera or cameras, such as one or more cameras , bifocal or stereophotogrammetric cameras, able to identify, for example, active or passive tracking markers 118 shown as part of patient tracking device 116 in 2 in a given measurement volume viewable from the perspective of the camera 200. In this example, the camera 200 may scan the given measurement volume and detect the light that comes from the tracking markers 118 in order to identify and determine the position of the tracking markers 118 in three-dimensions. For example, active tracking markers 118 may include infrared-emitting markers that are activated by an electrical signal , infrared light emitting diodes LEDs, and/or passive tracking markers 118 may include retro-reflective markers that reflect infrared or other light , they reflect incoming IR radiation into the direction of the incoming light, for example, emitted by illuminators on the camera 200 or other suitable sensor or other device. In many surgical procedures, one or more targets of surgical interest, such as targets within the brain for example, are localized to an external reference frame. For example, stereotactic neurosurgery may use an externally mounted stereotactic frame that facilitates patient localization and implant insertion via a frame mounted arc. Neuronavigation is used to register, , map, targets within the brain based on pre-operative or intraoperative imaging. Using this pre-operative or intraoperative imaging, links and associations can be made between the imaging and the actual anatomical structures in a surgical environment, and these links and associations can be utilized by robotic trajectory systems during surgery. According to some embodiments, various software and hardware elements may be combined to create a system that can be used to plan, register, place and verify the location of an instrument or implant in the brain. These systems may integrate a surgical robot, such as the surgical robot 102 of 1A and/or 1B, and may employ a surgical navigation system and planning software to program and control the surgical robot. In addition or alternatively, the surgical robot 102 may be remotely controlled, such as by nonsterile personnel. The robot 102 may be positioned near or next to patient 210, and it will be appreciated that the robot 102 can be positioned at any suitable location near the patient 210 depending on the area of the patient 210 undergoing the operation. The camera 200 may be separated from the surgical robot system 100 and positioned near or next to patient 210 as well, in any suitable position that allows the camera 200 to have a direct visual line of sight to the surgical field 208. In the configuration shown, the surgeon 120 may be positioned across from the robot 102, but is still able to manipulate the end-effector 112 and the display 110. A surgical assistant 126 may be positioned across from the surgeon 120 again with access to both the end-effector 112 and the display 110. If desired, the locations of the surgeon 120 and the assistant 126 may be reversed. The traditional areas for the anesthesiologist 122 and the nurse or scrub tech 124 may remain unimpeded by the locations of the robot 102 and camera 200. With respect to the other components of the robot 102, the display 110 can be attached to the surgical robot 102 and in other embodiments, the display 110 can be detached from surgical robot 102, either within a surgical room with the surgical robot 102, or in a remote location. The end-effector 112 may be coupled to the robot arm 104 and controlled by at least one motor. In some embodiments, end-effector 112 can comprise a guide tube 114, which is able to receive and orient a surgical instrument 108 used to perform surgery on the patient 210. As used herein, the term end-effector is used interchangeably with the terms end-effectuator and effectuator element. Although generally shown with a guide tube 114, it will be appreciated that the end-effector 112 may be replaced with any suitable instrumentation suitable for use in surgery. In some embodiments, end-effector 112 can comprise any known structure for effecting the movement of the surgical instrument 108 in a desired manner. The surgical robot 102 is able to control the translation and orientation of the end-effector 112. The robot 102 is able to move end-effector 112 along x-, y-, and z-axes, for example. The end-effector 112 can be configured for selective rotation about one or more of the x-, y-, and z-axis such that one or more of the Euler Angles , roll, pitch, and/or yaw associated with end-effector 112 can be selectively controlled. In some embodiments, selective control of the translation and orientation of end-effector 112 can permit performance of medical procedures with significantly improved accuracy compared to conventional robots that use, for example, a six degree of freedom robot arm comprising only rotational axes. For example, the surgical robot system 100 may be used to operate on patient 210, and robot arm 104 can be positioned above the body of patient 210, with end-effector 112 selectively angled relative to the z-axis toward the body of patient 210. In some embodiments, the position of the surgical instrument 108 can be dynamically updated so that surgical robot 102 can be aware of the location of the surgical instrument 108 at all times during the procedure. Consequently, in some embodiments, surgical robot 102 can move the surgical instrument 108 to the desired position quickly without any further assistance from a physician unless the physician so desires. In some further embodiments, surgical robot 102 can be configured to correct the path of the surgical instrument 108 if the surgical instrument 108 strays from the selected, preplanned trajectory. In some embodiments, surgical robot 102 can be configured to permit stoppage, modification, and/or manual control of the movement of end-effector 112 and/or the surgical instrument 108. Thus, in use, in some embodiments, a physician or other user can operate the system 100, and has the option to stop, modify, or manually control the autonomous movement of end-effector 112 and/or the surgical instrument 108. Further details of surgical robot system 100 including the control and movement of a surgical instrument 108 by surgical robot 102 can be found in co-pending Patent Publication 2013/0345718, which is incorporated herein by reference in its entirety. As will be described in greater detail below, the surgical robot system 100 can comprise one or more tracking markers configured to track the movement of robot arm 104, end-effector 112, patient 210, and/or the surgical instrument 108 in three dimensions. In some embodiments, a plurality of tracking markers can be mounted or otherwise secured thereon to an outer surface of the robot 102, such as, for example and without limitation, on base 106 of robot 102, on robot arm 104, and/or on the end-effector 112. In some embodiments, such as the embodiment of 3 below, for example, one or more tracking markers can be mounted or otherwise secured to the end-effector 112. One or more tracking markers can further be mounted or otherwise secured to the patient 210. In some embodiments, the plurality of tracking markers can be positioned on the patient 210 spaced apart from the surgical field 208 to reduce the likelihood of being obscured by the surgeon, surgical tools, or other parts of the robot 102. Further, one or more tracking markers can be further mounted or otherwise secured to the surgical instruments 108 , a screw driver, dilator, implant inserter, or the like. Thus, the tracking markers enable each of the marked objects , the end-effector 112, the patient 210, and the surgical instruments 108 to be tracked by the surgical robot system 100. In some embodiments, system 100 can use tracking information collected from each of the marked objects to calculate the orientation and location, for example, of the end-effector 112, the surgical instrument 108 , positioned in the tube 114 of the end-effector 112, and the relative position of the patient 210. Further details of surgical robot system 100 including the control, movement and tracking of surgical robot 102 and of a surgical instrument 108 can be found in Patent Publication 2016/0242849, which is incorporated herein by reference in its entirety. In some embodiments, pre-operative imaging may be used to identify the anatomy to be targeted in the procedure. If desired by the surgeon the planning package will allow for the definition of a reformatted coordinate system. This reformatted coordinate system will have coordinate axes anchored to specific anatomical landmarks, such as the anterior commissure AC and posterior commissure PC for neurosurgery procedures. In some embodiments, multiple pre-operative exam images , CT or magnetic resonance MR images may be co-registered such that it is possible to transform coordinates of any given point on the anatomy to the corresponding point on all other pre-operative exam images. As used herein, registration is the process of determining the coordinate transformations from one coordinate system to another. For example, in the co-registration of preoperative images, co-registering a CT scan to an MR scan means that it is possible to transform the coordinates of an anatomical point from the CT scan to the corresponding anatomical location in the MR scan. It may also be advantageous to register at least one exam image coordinate system to the coordinate system of a common registration fixture, such as a dynamic reference base DRB, which may allow the camera 200 to keep track of the position of the patient in the camera space in real-time so that any intraoperative movement of an anatomical point on the patient in the room can be detected by the robot system 100 and accounted for by compensatory movement of the surgical robot 102. 3 is a flowchart diagram illustrating computer-implemented operations 300 for determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments. The operations 300 may include receiving a first image volume, such as a CT scan, from a preoperative image capture device at a first time Block 302. The first image volume includes an anatomical feature of a patient and at least a portion of a registration fixture that is fixed with respect to the anatomical feature of the patient. The registration fixture includes a first plurality of fiducial markers that are fixed with respect to the registration fixture. The operations 300 further include determining, for each fiducial marker of the first plurality of fiducial markers, a position of the fiducial marker relative to the first image volume Block 304. The operations 300 further include, determining, based on the determined positions of the first plurality of fiducial markers, positions of an array of tracking markers on the registration fixture fiducial registration array or FRA with respect to the anatomical feature Block 306. The operations 300 may further include receiving a tracking data frame from an intraoperative tracking device comprising a plurality of tracking cameras at a second time that is later than the first time Block 308. The tracking frame includes positions of a plurality of tracking markers that are fixed with respect to the registration fixture FRA and a plurality of tracking markers that are fixed with respect to the robot. The operations 300 further include determining, for based on the positions of tracking markers of the registration fixture, a position and orientation of the anatomical feature with respect to the tracking cameras Block 310. The operations 300 further include determining, based on the determined positions of the plurality of tracking markers on the robot, a position and orientation of the robot arm of a surgical robot with respect to the tracking cameras Block 312. The operations 300 further include determining, based on the determined position and orientation of the anatomical feature with respect to the tracking cameras and the determined position and orientation of the robot arm with respect to the tracking cameras, a position and orientation of the anatomical feature with respect to the robot arm Block 314. The operations 300 further include controlling movement of the robot arm with respect to the anatomical feature, , along and/or rotationally about one or more defined axis, based on the determined position and orientation of the anatomical feature with respect to the robot arm Block 316. 4 is a diagram illustrating a data flow 400 for a multiple coordinate transformation system, to enable determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments. In this example, data from a plurality of exam image spaces 402, based on a plurality of exam images, may be transformed and combined into a common exam image space 404. The data from the common exam image space 404 and data from a verification image space 406, based on a verification image, may be transformed and combined into a registration image space 408. Data from the registration image space 408 may be transformed into patient fiducial coordinates 410, which is transformed into coordinates for a DRB 412. A tracking camera 414 may detect movement of the DRB 412 represented by DRB 412 and may also detect a location of a probe tracker 416 to track coordinates of the DRB 412 over time. A robotic arm tracker 418 determines coordinates for the robot arm based on transformation data from a Robotics Planning System RPS space 420 or similar modeling system, and/or transformation data from the tracking camera 414. It should be understood that these and other features may be used and combined in different ways to achieve registration of image space, i. e. , coordinates from image volume, into tracking space, i. e. , coordinates for use by the surgical robot in real-time. As will be discussed in detail below, these features may include fiducial-based registration such as stereotactic frames with CT localizer, preoperative CT or Mill registered using intraoperative fluoroscopy, calibrated scanner registration where any acquired scan's coordinates are pre-calibrated relative to the tracking space, and/or surface registration using a tracked probe, for example. In one example, 5A-5C illustrate a system 500 for registering an anatomical feature of a patient. In this example, the stereotactic frame base 530 is fixed to an anatomical feature 528 of patient, , the patient's head. As shown by 5A, the stereotactic frame base 530 may be affixed to the patient's head 528 prior to registration using pins clamping the skull or other method. The stereotactic frame base 530 may act as both a fixation platform, for holding the patient's head 528 in a fixed position, and registration and tracking platform, for alternatingly holding the CT localizer 536 or the FRA fixture 534. The CT localizer 536 includes a plurality of fiducial markers 532 , N-pattern radio-opaque rods or other fiducials, which are automatically detected in the image space using image processing. Due to the precise attachment mechanism of the CT localizer 536 to the base 530, these fiducial markers 532 are in known space relative to the stereotactic frame base 530. A 3D CT scan of the patient with CT localizer 536 attached is taken, with an image volume that includes both the patient's head 528 and the fiducial markers 532 of the CT localizer 536. This registration image can be taken intraoperatively or preoperatively, either in the operating room or in radiology, for example. The captured 3D image dataset is stored to computer memory. As shown by 5B, after the registration image is captured, the CT localizer 536 is removed from the stereotactic frame base 530 and the frame reference array fixture 534 is attached to the stereotactic frame base 530. The stereotactic frame base 530 remains fixed to the patient's head 528, however, and is used to secure the patient during surgery, and serves as the attachment point of a frame reference array fixture 534. The frame reference array fixture 534 includes a frame reference array FRA, which is a rigid array of three or more tracked markers 539, which may be the primary reference for optical tracking. By positioning the tracked markers 539 of the FRA in a fixed, known location and orientation relative to the stereotactic frame base 530, the position and orientation of the patient's head 528 may be tracked in real time. Mount points on the FRA fixture 534 and stereotactic frame base 530 may be designed such that the FRA fixture 534 attaches reproducibly to the stereotactic frame base 530 with minimal i. e. , submillimetric variability. These mount points on the stereotactic frame base 530 can be the same mount points used by the CT localizer 536, which is removed after the scan has been taken. An auxiliary arm such as auxiliary arm 107 of 1B, for example or other attachment mechanism can also be used to securely affix the patient to the robot base to ensure that the robot base is not allowed to move relative to the patient. As shown by 5C, a dynamic reference base DRB 540 may also be attached to the stereotactic frame base 530. The DRB 540 in this example includes a rigid array of three or more tracked markers 542. In this example, the DRB 540 and/or other tracked markers may be attached to the stereotactic frame base 530 and/or to directly to the patient's head 528 using auxiliary mounting arms 541, pins, or other attachment mechanisms. Unlike the FRA fixture 534, which mounts in only one way for unambiguous localization of the stereotactic frame base 530, the DRB 540 in general may be attached as needed for allowing unhindered surgical and equipment access. Once the DRB 540 and FRA fixture 534 are attached, registration, which was initially related to the tracking markers 539 of the FRA, can be optionally transferred or related to the tracking markers 542 of the DRB 540. For example, if any part of the FRA fixture 534 blocks surgical access, the surgeon may remove the FRA fixture 534 and navigate using only the DRB 540. However, if the FRA fixture 534 is not in the way of the surgery, the surgeon could opt to navigate from the FRA markers 539, without using a DRB 540, or may navigate using both the FRA markers 539 and the DRB 540. In this example, the FRA fixture 534 and/or DRB 540 uses optical markers, the tracked positions of which are in known locations relative to the stereotactic frame base 530, similar to the CT localizer 536, but it should be understood that many other additional and/or alternative techniques may be used. 6A and 6B illustrate a system 600 for registering an anatomical feature of a patient using fluoroscopy fluoro imaging, according to some embodiments. In this embodiment, image space is registered to tracking space using multiple intraoperative fluoroscopy fluoro images taken using a tracked registration fixture 644. The anatomical feature of the patient , the patient's head 628 is positioned and rigidly affixed in a clamping apparatus 643 in a static position for the remainder of the procedure. The clamping apparatus 643 for rigid patient fixation can be a three-pin fixation system such as a Mayfield clamp, a stereotactic frame base attached to the surgical table, or another fixation method, as desired. The clamping apparatus 643 may also function as a support structure for a patient tracking array or DRB 640 as well. The DRB may be attached to the clamping apparatus using auxiliary mounting arms 641 or other means. Once the patient is positioned, the fluoro fixture 644 is attached the fluoro unit's x-ray collecting image intensifier not shown and secured by tightening clamping feet 632. The fluoro fixture 644 contains fiducial markers , metal spheres laid out across two planes in this example, not shown that are visible on 2D fluoro images captured by the fluoro image capture device and can be used to calculate the location of the x-ray source relative to the image intensifier, which is typically about 1 meter away contralateral to the patient, using a standard pinhole camera model. Detection of the metal spheres in the fluoro image captured by the fluoro image capture device also enables the software to de-warp the fluoro image i. e. , to remove pincushion and s-distortion. Additionally, the fluoro fixture 644 contains 3 or more tracking markers 646 for determining the location and orientation of the fluoro fixture 644 in tracking space. In some embodiments, software can project vectors through a CT image volume, based on a previously captured CT image, to generate synthetic images based on contrast levels in the CT image that appear similar to the actual fluoro images i. e. , digitally reconstructed radiographs DRRs. By iterating through theoretical positions of the fluoro beam until the DRRs match the actual fluoro shots, a match can be found between fluoro image and DRR in two or more perspectives, and based on this match, the location of the patient's head 628 relative to the x-ray source and detector is calculated. Because the tracking markers 646 on the fluoro fixture 644 track the position of the image intensifier and the position of the x-ray source relative to the image intensifier is calculated from metal fiducials on the fluoro fixture 644 projected on 2D images, the position of the x-ray source and detector in tracking space are known and the system is able to achieve image-to-tracking registration. As shown by 6A and 6B, two or more shots are taken of the head 628 of the patient by the fluoro image capture device from two different perspectives while tracking the array markers 642 of the DRB 640, which is fixed to the registration fixture 630 via a mounting arm 641, and tracking markers 646 on the fluoro fixture 644. Based on the tracking data and fluoro data, an algorithm computes the location of the head 628 or other anatomical feature relative to the tracking space for the procedure. Through image-to-tracking registration, the location of any tracked tool in the image volume space can be calculated. For example, in one embodiment, a first fluoro image taken from a first fluoro perspective can be compared to a first DRR constructed from a first perspective through a CT image volume, and a second fluoro image taken from a second fluoro perspective can be compared to a second DRR constructed from a second perspective through the same CT image volume. Based on the comparisons, it may be determined that the first DRR is substantially equivalent to the first fluoro image with respect to the projected view of the anatomical feature, and that the second DRR is substantially equivalent to the second fluoro image with respect to the projected view of the anatomical feature. Equivalency confirms that the position and orientation of the x-ray path from emitter to collector on the actual fluoro machine as tracked in camera space matches the position and orientation of the x-ray path from emitter to collector as specified when generating the DRRs in CT space, and therefore registration of tracking space to CT space is achieved. 7 illustrates a system 700 for registering an anatomical feature of a patient using an intraoperative CT fixture ICT and a DRB, according to some embodiments. As shown in 7, in one application, a fiducial-based image-to-tracking registration can be utilized that uses an intraoperative CT fixture ICT 750 having a plurality of tracking markers 751 and radio-opaque fiducial reference markers 732 to register the CT space to the tracking space. After stabilizing the anatomical feature 728 , the patient's head using clamping apparatus 730 such as a three-pin Mayfield frame and/or stereotactic frame, the surgeon will affix the ICT 750 to the anatomical feature 728, DRB 740, or clamping apparatus 730, so that it is in a static position relative to the tracking markers 742 of the DRB 740, which may be held in place by mounting arm 741 or other rigid means. A CT scan is captured that encompasses the fiducial reference markers 732 of the ICT 750 while also capturing relevant anatomy of the anatomical feature 728. Once the CT scan is loaded in the software, the system auto-identifies through image processing locations of the fiducial reference markers 732 of the ICT within the CT volume, which are in a fixed position relative to the tracking markers of the ICT 750, providing image-to-tracking registration. This registration, which was initially based on the tracking markers 751 of the ICT 750, is then related to or transferred to the tracking markers 742 of the DRB 740, and the ICT 750 may then be removed. 8A illustrates a system 800 for registering an anatomical feature of a patient using a DRB and an X-ray cone beam imaging device, according to some embodiments. An intraoperative scanner 852, such as an X-ray machine or other scanning device, may have a tracking array 854 with tracking markers 855, mounted thereon for registration. Based on the fixed, known position of the tracking array 854 on the scanning device, the system may be calibrated to directly map register the tracking space to the image space of any scan acquired by the system. Once registration is achieved, the registration, which is initially based on the tracking markers 855 gantry markers of the scanner's array 854, is related or transferred to the tracking markers 842 of a DRB 840, which may be fixed to a clamping fixture 830 holding the patient's head 828 by a mounting arm 841 or other rigid means. After transferring registration, the markers on the scanner are no longer used and can be removed, deactivated or covered if desired. Registering the tracking space to any image acquired by a scanner in this way may avoid the need for fiducials or other reference markers in the image space in some embodiments. 8B illustrates an alternative system 800 that uses a portable intraoperative scanner, referred to herein as a C-arm scanner 853. In this example, the C-arm scanner 853 includes a c-shaped arm 856 coupled to a movable base 858 to allow the C-arm scanner 853 to be moved into place and removed as needed, without interfering with other aspects of the surgery. The arm 856 is positioned around the patient's head 828 intraoperatively, and the arm 856 is rotated and/or translated with respect to the patient's head 828 to capture the X-ray or other type of scan that to achieve registration, at which point the C-arm scanner 853 may be removed from the patient. Another registration method for an anatomical feature of a patient, , a patient's head, may be to use a surface contour map of the anatomical feature, according to some embodiments. A surface contour map may be constructed using a navigated or tracked probe, or other measuring or sensing device, such as a laser pointer, 3D camera, etc. For example, a surgeon may drag or sequentially touch points on the surface of the head with the navigated probe to capture the surface across unique protrusions, such as zygomatic bones, superciliary arches, bridge of nose, eyebrows, etc. The system then compares the resulting surface contours to contours detected from the CT and/or MR images, seeking the location and orientation of contour that provides the closest match. To account for movement of the patient and to ensure that all contour points are taken relative to the same anatomical feature, each contour point is related to tracking markers on a DRB on the patient at the time it is recorded. Since the location of the contour map is known in tracking space from the tracked probe and tracked DRB, tracking-to-image registration is obtained once the corresponding contour is found in image space. 9 illustrates a system 900 for registering an anatomical feature of a patient using a navigated or tracked probe and fiducials for point-to-point mapping of the anatomical feature 928 , a patient's head, according to some embodiments. Software would instruct the user to point with a tracked probe to a series of anatomical landmark points that can be found in the CT or MR image. When the user points to the landmark indicated by software, the system captures a frame of tracking data with the tracked locations of tracking markers on the probe and on the DRB. From the tracked locations of markers on the probe, the coordinates of the tip of the probe are calculated and related to the locations of markers on the DRB. Once 3 or more points are found in both spaces, tracking-to-image registration is achieved. As an alternative to pointing to natural anatomical landmarks, fiducials 954 i. e. , fiducial markers, such as sticker fiducials or metal fiducials, may be used. The surgeon will attach the fiducials 954 to the patient, which are constructed of material that is opaque on imaging, for example containing metal if used with CT or Vitamin E if used with MR. Imaging CT or MR will occur after placing the fiducials 954. The surgeon or user will then manually find the coordinates of the fiducials in the image volume, or the software will find them automatically with image processing. After attaching a DRB 940 with tracking markers 942 to the patient through a mounting arm 941 connected to a clamping apparatus 930 or other rigid means, the surgeon or user may also locate the fiducials 954 in physical space relative to the DRB 940 by touching the fiducials 954 with a tracked probe while simultaneously recording tracking markers on the probe not shown and on the DRB 940. Registration is achieved because the coordinates of the same points are known in the image space and the tracking space. One use for the embodiments described herein is to plan trajectories and to control a robot to move into a desired trajectory, after which the surgeon will place implants such as electrodes through a guide tube held by the robot. Additional functionalities include exporting coordinates used with existing stereotactic frames, such as a Leksell frame, which uses five coordinates: X, Y, Z, Ring Angle and Arc Angle. These five coordinates are established using the target and trajectory identified in the planning stage relative to the image space and knowing the position and orientation of the ring and arc relative to the stereotactic frame base or other registration fixture. As shown in 10, stereotactic frames allow a target location 1058 of an anatomical feature 1028 , a patient's head to be treated as the center of a sphere and the trajectory can pivot about the target location 1058. The trajectory to the target location 1058 is adjusted by the ring and arc angles of the stereotactic frame , a Leksell frame. These coordinates may be set manually, and the stereotactic frame may be used as a backup or as a redundant system in case the robot fails or cannot be tracked or registered successfully. The linear x,y,z offsets to the center point i. e. , target location 1058 are adjusted via the mechanisms of the frame. A cone 1060 is centered around the target location 1058, and shows the adjustment zone that can be achieved by modifying the ring and arc angles of the Leksell or other type of frame. This figure illustrates that a stereotactic frame with ring and arc adjustments is well suited for reaching a fixed target location from a range of angles while changing the entry point into the skull. 11 illustrates a two-dimensional visualization of virtual point rotation mechanism, according to some embodiments. In this embodiment, the robotic arm is able to create a different type of point-rotation functionality that enables a new movement mode that is not easily achievable with a 5-axis mechanical frame, but that may be achieved using the embodiments described herein. Through coordinated control of the robot's axes using the registration techniques described herein, this mode allows the user to pivot the robot's guide tube about any fixed point in space. For example, the robot may pivot about the entry point 1162 into the anatomical feature 1128 , a patient's head. This entry point pivoting is advantageous as it allows the user to make a smaller burr hole without limiting their ability to adjust the target location 1164 intraoperatively. The cone 1160 represents the range of trajectories that may be reachable through a single entry hole. Additionally, entry point pivoting is advantageous as it allows the user to reach two different target locations 1164 and 1166 through the same small entry burr hole. Alternately, the robot may pivot about a target point , location 1058 shown in 10 within the skull to reach the target location from different angles or trajectories, as illustrated in 10. Such interior pivoting robotically has the same advantages as a stereotactic frame as it allows the user to approach the same target location 1058 from multiple approaches, such as when irradiating a tumor or when adjusting a path so that critical structures such as blood vessels or nerves will not be crossed when reaching targets beyond them. Unlike a stereotactic frame, which relies on fixed ring and arc articulations to keep a target/pivot point fixed, the robot adjusts the pivot point through controlled activation of axes and the robot can therefore dynamically adjust its pivot point and switch as needed between the modes illustrated in 10 and 11. Following the insertion of implants or instrumentation using the robot or ring and arc fixture, these and other embodiments may allow for implant locations to be verified using intraoperative imaging. Placement accuracy of the instrument or implant relative to the planned trajectory can be qualitatively and/or quantitatively shown to the user. One option for comparing planned to placed position is to merge a postoperative verification CT image to any of the preoperative images. Once pre- and post-operative images are merged and plan is shown overlaid, the shadow of the implant on postop CT can be compared to the plan to assess accuracy of placement. Detection of the shadow artifact on post-op CT can be performed automatically through image processing and the offset displayed numerically in terms of millimeters offset at the tip and entry and angular offset along the path. This option does not require any fiducials to be present in the verification image since image-to-image registration is performed based on bony anatomical contours. A second option for comparing planned position to the final placement would utilize intraoperative fluoro with or without an attached fluoro fixture. Two out-of-plane fluoro images will be taken and these fluoro images will be matched to DRRs generated from pre-operative CT or MR as described above for registration. Unlike some of the registration methods described above, however, it may be less important for the fluoro images to be tracked because the key information is where the electrode is located relative to the anatomy in the fluoro image. The linear or slightly curved shadow of the electrode would be found on a fluoro image, and once the DRR corresponding to that fluoro shot is found, this shadow can be replicated in the CT image volume as a plane or sheet that is oriented in and out of the ray direction of the fluoro image and DRR. That is, the system may not know how deep in or out of the fluoro image plane the electrode lies on a given shot, but can calculate the plane or sheet of possible locations and represent this plane or sheet on the 3D volume. In a second fluoro view, a different plane or sheet can be determined and overlaid on the 3D image. Where these two planes or sheets intersect on the 3D image is the detected path of the electrode. The system can represent this detected path as a graphic on the 3D image volume and allow the user to reslice the image volume to display this path and the planned path from whatever perspective is desired, also allowing automatic or manual calculation of the deviation from planned to placed position of the electrode. Tracking the fluoro fixture is unnecessary but may be done to help de-warp the fluoro images and calculate the location of the x-ray emitter to improve accuracy of DRR calculation, the rate of convergence when iterating to find matching DRR and fluoro shots, and placement of sheets/planes representing the electrode on the 3D scan. In this and other examples, it is desirable to maintain navigation integrity, i. e. , to ensure that the registration and tracking remain accurate throughout the procedure. Two primary methods to establish and maintain navigation integrity include: tracking the position of a surveillance marker relative to the markers on the DRB, and checking landmarks within the images. In the first method, should this position change due to, for example, the DRB being bumped, then the system may alert the user of a possible loss of navigation integrity. In the second method, if a landmark check shows that the anatomy represented in the displayed slices on screen does not match the anatomy at which the tip of the probe points, then the surgeon will also become aware that there is a loss of navigation integrity. In either method, if using the registration method of CT localizer and frame reference array FRA, the surgeon has the option to re-attach the FRA, which mounts in only one possible way to the frame base, and to restore tracking-to-image registration based on the FRA tracking markers and the stored fiducials from the CT localizer 536. This registration can then be transferred or related to tracking markers on a repositioned DRB. Once registration is transferred the FRA can be removed if desired. Referring now to 12-18 generally, with reference to the surgical robot system 100 shown in 1A, end-effector 112 may be equipped with components, configured, or otherwise include features so that one end-effector may remain attached to a given one of robot arms 104 without changing to another end-effector for multiple different surgical procedures, such as, by way of example only, Deep Brain Stimulation DBS, Stereoelectroencephalography SEEG, or Endoscopic Navigation and Tumor Biopsy. As discussed previously, end-effector 112 may be orientable to oppose an anatomical feature of a patient in the manner so as to be in operative proximity thereto, and, to be able to receive one or more surgical tools for operations contemplated on the anatomical feature proximate to the end-effector 112. Motion and orientation of end-effector 112 may be accomplished through any of the navigation, trajectory guidance, or other methodologies discussed herein or as may be otherwise suitable for the particular operation. End-effector 112 is suitably configured to permit a plurality of surgical tools 129 to be selectively connectable to end-effector 112. Thus, for example, a stylet 113 13 may be selectively attached in order to localize an incision point on an anatomical feature of a patient, or an electrode driver 115 14 may be selectively attached to the same end-effector 112. With reference to the previous discussion of robot surgical system 100, a processor circuit, as well as memory accessible by such processor circuit, includes various subroutines and other machine-readable instructions configured to cause, when executed, end-effector 112 to move, such as by GPS movement, relative to the anatomical feature, at predetermined stages of associated surgical operations, whether pre-operative, intra-operative or post-operative. End-effector 112 includes various components and features to either prevent or permit end-effector movement depending on whether and which tools 129, if any, are connected to end-effector 112. Referring more particularly to 12, end-effector 112 includes a tool-insert locking mechanism 117 located on and connected to proximal surface 119. Tool-insert locking mechanism 117 is configured so as to secure any selected one of a plurality of surgical tools, such as the aforesaid stylet 113, electrode driver 115, or any other tools for different surgeries mentioned previously or as may be contemplated by other applications of this disclosure. The securement of the tool by tool-insert locking mechanism 117 is such that, for any of multiple tools capable of being secured to locking mechanism 117, each such tool is operatively and suitably secured at the predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient, such that multiple tools may be secured to the same end-effector 112 in respective positions appropriate for the contemplated procedure. Another feature of the end-effector 112 is a tool stop 121 located on distal surface 123 of end-effector 112, that is, the surface generally opposing the patient. Tool stop 121 has a stop mechanism 125 and a sensor 127 operatively associated therewith, as seen with reference to 16, 19, and 20. Stop mechanism 125 is mounted to end-effector 112 so as to be selectively movable relative thereto between an engaged position to prevent any of the tools from being connected to end-effector 112 and a disengaged position which permits any of the tools 129 to be selectively connected to end-effector 112. Sensor 127 may be located on or within the housing of end-effector 112 at any suitable location 12, 14, 16 so that sensor 127 detects whether stop mechanism 125 is in the engaged or disengaged position. Sensor 127 may assume any form suitable for such detection, such as any type of mechanical switch or any type of magnetic sensor, including Reed switches, Hall Effect sensors, or other magnetic field detecting devices. In one possible implementation, sensor 127 has two portions, a Hall Effect sensor portion not shown and a magnetic portion 131, the two portions moving relative to each other so as to generate and detect two magnetic fields corresponding to respective engaged and disengaged position. In the illustrated implementation, the magnetic portion comprises two rare earth magnets 131 which move relative to the complementary sensing portion not shown mounted in the housing of end effector 112 in operative proximity to magnets 131 to detect change in the associated magnetic field from movement of stop mechanism 125 between engaged and disengaged positions. In this implementation the Hall effect sensor is bipolar and can detect whether a North pole or South pole of a magnet opposes the sensor. Magnets 131 are configured so that the North pole of one magnet faces the path of the sensor and the South pole of the other magnet faces the path of the sensor. In this configuration, the sensor senses an increased signal when it is near one magnet for example, in disengaged position, a decreased signal when it is near the other magnet for example, in engaged position, and unchanged signal when it is not in proximity to any magnet. In this implementation, in response to detection of stop mechanism 125 being in the disengaged position shown in 13 and 19, sensor 127 causes the processor of surgical robot system 100 to execute suitable instructions to prevent movement of end-effector 112 relative to the anatomical feature. Such movement prevention may be appropriate for any number of reasons, such as when a tool is connected to end-effector 112, such tool potentially interacting with the anatomical feature of the patient. Another implementation of a sensor 127 for detecting engaged or disengaged tool stop mechanism 125 could comprise a single magnet behind the housing not shown and two Hall Effect sensors located where magnets 131 are shown in the preferred embodiment. In such a configuration, monopolar Hall Effect sensors are suitable and would be configured so that Sensor 1 detects a signal when the magnet is in proximity due to the locking mechanism being disengaged, while Sensor 2 detects a signal when the same magnet is in proximity due to the locking mechanism being engaged. Neither sensor would detect a signal when the magnet is between positions or out of proximity to either sensor. Although a configuration could be conceived in which a sensor is active for engaged position and inactive for disengaged position, a configuration with three signals indicating engaged, disengaged, or transitional is preferred to ensure correct behavior in case of power failure. End-effector 112, tool stop 121, and tool-insert locking mechanism 117 each have co-axially aligned bores or apertures such that any selected one of the plurality of surgical tools 129 may be received through such bores and apertures. In this implementation end-effector has a bore 133 and tool stop 121 and tool-insert locking mechanism 117 have respective apertures 135 and 137. Stop mechanism 125 includes a ring 139 axially aligned with bore 133 and aperture 135 of tool stop 121. Ring 139 is selectively, manually rotatable in the directions indicated by arrow A 16 so as to move stop mechanism 125 between the engaged position and the disengaged position. In one possible implementation, the selective rotation of ring 139 includes features which enable ring 139 to be locked in either the disengaged or engaged position. So, for example, as illustrated, a detent mechanism 141 is located on and mounted to ring 139 in any suitable way to lock ring 139 against certain rotational movement out of a predetermined position, in this case, such position being when stop mechanism 125 is in the engaged position. Although various forms of detent mechanism are contemplated herein, one suitable arrangement has a manually accessible head extending circumferentially outwardly from ring 139 and having a male protrusion not shown spring-loaded axially inwardly to engage a corresponding female detent portion not shown. Detent mechanism 141, as such, is manually actuatable to unlock ring 139 from its engaged position to permit ring 139 to be manually rotated to cause stop mechanism 125 to move from the engaged position 20 to the disengaged position 19. Tool stop 121 includes a lever arm 143 pivotally mounted adjacent aperture 135 of tool stop 121 so end of lever arm 143 selectively pivots in the directions indicated by arrow B 16, 19 and 20. Lever arm 143 is operatively connected to stop mechanism 125, meaning it closes aperture 135 of tool stop 121 in response to stop mechanism 125 being in the engaged position, as shown in 20. Lever arm 143 is also operatively connected so as to pivot back in direction of arrow B to open aperture 135 in response to stop mechanism 125 being in the disengaged position. As such, movement of stop mechanism 125 between engaged and disengaged positions results in closure or opening of aperture 135, respectively, by lever arm 143. Lever arm 143, in this implementation, is not only pivotally mounted adjacent aperture 135, but also pivots in parallel with a distal plane defined at a distal-most point of distal surface 123 of end-effector 112. In this manner, any one of the surgical tools 129, which is attempted to be inserted through bore 133 and aperture 135, is stopped from being inserted past the distal plane in which lever arm 143 rotates to close aperture 135. Turning now to tool-insert locking mechanism 117 13, 17, 18, a connector 145 is configured to meet with and secure any one of the surgical tools 129 at their appropriate height, angle of orientation, and rotational position relative to the anatomical feature of the patient. In the illustrated implementation, connector 145 comprises a rotatable flange 147 which has at least one slot 149 formed therein to receive therethrough a corresponding tongue 151 associated with a selected one of the plurality of tools 129. So, for example, in 14, the particular electrode driver 115 has multiple tongues, one of which tongue 151 is shown. Rotatable flange 147, in some implementations, may comprise a collar 153, which collar, in turn, has multiple ones of slots 149 radially spaced on a proximally oriented surface 155, as best seen in 12. Multiple slots 147 arranged around collar 153 are sized or otherwise configured so as to receive therethrough corresponding ones of multiple tongues 151 associated with a selected one of the plurality of tools 129. Therefore, as seen in 13, multiple slots 149 and corresponding tongues 151 may be arranged to permit securing of a selected one of the plurality of tools 129 only when selected tool is in the correct, predetermined angle of orientation and rotational position relative to the anatomical feature of the patient. Similarly, with regard to the electrode driver shown in 14, tongues 151 one of which is shown in a cutaway of 14 have been received in radially spaced slots 149 arrayed so that electrode driver 115 is received at the appropriate angle of orientation and rotational position. Rotatable flange 147 has, in this implementation, a grip 173 to facilitate manual rotation between an open and closed position as shown in 17 and 18, respectively. As seen in 17, multiple sets of mating slots 149 and tongues 151 are arranged at different angular locations, in this case, locations which may be symmetric about a single diametric chord of a circle but otherwise radially asymmetric, and at least one of the slots has a different dimension or extends through a different arc length than other slots. In this slot-tongue arrangement, and any number of variations contemplated by this disclosure, there is only one rotational position of the tool 129 or adapter 155 discussed later to be received in tool-insert locking mechanism 117 when rotatable flange 147 is in the open position shown in 17. In other words, when the user of system 100 moves a selected tool 129 or tool adapter 155 to a single appropriate rotational position, corresponding tongues 151 may be received through slots 149. Upon placement of tongues 151 into slots 149, tongues 151 confront a base surface 175 within connector 145 of rotatable flange 147. Upon receiving tongues 151 into slots 149 and having them rest on underlying base surface 175, dimensions of tongues 151 and slots 149, especially with regard to height relative to rotatable flange 147, are selected so that when rotatable flange 147 is rotated to the closed position, flange portions 157 are radially translated to overlie or engage portions of tongues 151, such engagement shown in 18 and affixing tool 129 or adapter 155 received in connector 145 at the desired, predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient. Tongues 151 described as being associated with tools 129 may either be directly connected to such tools 129, and/or tongues 151 may be located on and mounted to the above-mentioned adapter 155, such as that shown in 12, 17 and 18, such adapter 155 configured to interconnect at least one of the plurality of surgical tools 129 with end-effector 112. In the described implementation, adapter 155 includes two operative portionsa tool receiver 157 adapted to connect the selected one or more surgical tools 129, and the second operative part being one or more tongues 151 which may, in this implementation, be mounted and connected to the distal end of adapter 155. Adapter 155 has an outer perimeter 159 which, in this implementation, is sized to oppose an inner perimeter 161 of rotatable flange 147. Adapter 155 extends between proximal and distal ends 163, 165, respectively and has an adapter bore 167 extending between ends 163, 165. Adapter bore 167 is sized to receive at least one of the plurality of surgical tools 129, and similarly, the distance between proximal and distal ends 163, 165 is selected so that at least one of tools 129 is secured to end-effector 112 at the predetermined, appropriate height for the surgical procedure associated with such tool received in adapter bore 167. In one possible implementation, system 100 includes multiple ones of adapter 155, configured to be interchangeable inserts 169 having substantially the same, predetermined outer perimeters 159 to be received within inner perimeter 161 of rotatable flange 147. Still further in such implementation, the interchangeable inserts 169 have bores of different, respective diameters, which bores may be selected to receive corresponding ones of the tools 129 therein. Bores 167 may comprise cylindrical bushings having inner diameters common to multiple surgical tools 129. One possible set of diameters for bores 167 may be 12, 15, and 17 millimeters, suitable for multiple robotic surgery operations, such as those identified in this disclosure. In the illustrated implementation, inner perimeter 161 of rotatable flange 147 and outer perimeter 159 of adapter 155 are circular, having central, aligned axes and corresponding radii. Slots 149 of rotatable flange 147 extend radially outwardly from the central axis of rotatable flange 147 in the illustrated implementation, whereas tongues 151 of adapter 155 extend radially outwardly from adapter 155. In still other implementations, end-effector 112 may be equipped with at least one illumination element 171 14 and 15 orientable toward the anatomical feature to be operated upon. Illumination element 171 may be in the form of a ring of LEDs 177 14 located within adapter 167, which adapter is in the form of a bushing secured to tool locking mechanism 117. Illumination element 171 may also be a single LED 179 mounted on the distal surface 123 of end-effector 112. Whether in the form of LED ring 177 or a single element LED 179 mounted on distal surface of end-effector 112, or any other variation, the spacing and location of illumination element or elements 171 may be selected so that tools 129 received through bore 133 of end-effector 112 do not cast shadows or otherwise interfere with illumination from element 171 of the anatomical feature being operated upon. The operation and associated features of end-effector 112 are readily apparent from the foregoing description. Tool stop 121 is rotatable, selectively lockable, and movable between engaged and disengaged positions, and a sensor prevents movement of end-effector 112 when in such disengaged position, due to the potential presence of a tool which may not be advisably moved during such disengaged position. Tool-insert locking mechanism 117 is likewise rotatable between open and closed positions to receive one of a plurality of interchangeable inserts 169 and tongues 151 of such inserts, wherein selected tools 129 may be received in such inserts 169; alternately, tongues 151 may be otherwise associated with tools 129, such as by having tongues 151 directly connected to such tools 129, which tongue-equipped tools likewise may be received in corresponding slots 149 of tool-insert locking mechanism 117. Tool-insert locking mechanism 117 may be rotated from its open position in which tongues 151 have been received in slots 149, to secure associated adapters 155 and/or tools 129 so that they are at appropriate, respective heights, angles of orientation, and rotational positions relative to the anatomical feature of the patient. For those implementations with multiple adapters 155, the dimensions of such adapters 155, including bore diameters, height, and other suitable dimensions, are selected so that a single or a minimized number of end-effectors 112 can be used for a multiplicity of surgical tools 129. Adapters 155, such as those in the form of interchangeable inserts 169 or cylindrical bushings, may facilitate connecting an expanded set of surgical tools 129 to the end-effector 112, and thus likewise facilitate a corresponding expanded set of associated surgical features using the same end-effector 112. In the above-description of various embodiments of present inventive concepts, it is to be understood that the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of present inventive concepts. Unless otherwise defined, all terms including technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which present inventive concepts belong. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of this specification and the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein. When an element is referred to as being connected, coupled, responsive, or variants thereof to another element, it can be directly connected, coupled, or responsive to the other element or intervening elements may be present. In contrast, when an element is referred to as being directly connected, directly coupled, directly responsive, or variants thereof to another element, there are no intervening elements present. Like numbers refer to like elements throughout. Furthermore, coupled, connected, responsive, or variants thereof as used herein may include wirelessly coupled, connected, or responsive. As used herein, the singular forms a, an and the are intended to include the plural forms as well, unless the context clearly indicates otherwise. Well-known functions or constructions may not be described in detail for brevity and/or clarity. The term and/or includes any and all combinations of one or more of the associated listed items. It will be understood that although the terms first, second, third, etc. may be used herein to describe various elements/operations, these elements/operations should not be limited by these terms. These terms are only used to distinguish one element/operation from another element/operation. Thus a first element/operation in some embodiments could be termed a second element/operation in other embodiments without departing from the teachings of present inventive concepts. The same reference numerals or the same reference designators denote the same or similar elements throughout the specification. As used herein, the terms comprise, comprising, comprises, include, including, includes, have, has, having, or variants thereof are open-ended, and include one or more stated features, integers, elements, steps, components or functions but does not preclude the presence or addition of one or more other features, integers, elements, steps, components, functions or groups thereof. Furthermore, as used herein, the common abbreviation , which derives from the Latin phrase exempli gratia, may be used to introduce or specify a general example or examples of a previously mentioned item, and is not intended to be limiting of such item. The common abbreviation i. e. , which derives from the Latin phrase id est, may be used to specify a particular item from a more general recitation. Example embodiments are described herein with reference to block diagrams and/or flowchart illustrations of computer-implemented methods, apparatus systems and/or devices and/or computer program products. It is understood that a block of the block diagrams and/or flowchart illustrations, and combinations of blocks in the block diagrams and/or flowchart illustrations, can be implemented by computer program instructions that are performed by one or more computer circuits. These computer program instructions may be provided to a processor circuit of a general purpose computer circuit, special purpose computer circuit, and/or other programmable data processing circuit to produce a machine, such that the instructions, which execute via the processor of the computer and/or other programmable data processing apparatus, transform and control transistors, values stored in memory locations, and other hardware components within such circuitry to implement the functions/acts specified in the block diagrams and/or flowchart block or blocks, and thereby create means functionality and/or structure for implementing the functions/acts specified in the block diagrams and/or flowchart blocks. These computer program instructions may also be stored in a tangible computer-readable medium that can direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable medium produce an article of manufacture including instructions which implement the functions/acts specified in the block diagrams and/or flowchart block or blocks. Accordingly, embodiments of present inventive concepts may be embodied in hardware and/or in software including firmware, resident software, micro-code, that runs on a processor such as a digital signal processor, which may collectively be referred to as circuitry, a module or variants thereof. It should also be noted that in some alternate implementations, the functions/acts noted in the blocks may occur out of the order noted in the flowcharts. For example, two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order, depending upon the functionality/acts involved. Moreover, the functionality of a given block of the flowcharts and/or block diagrams may be separated into multiple blocks and/or the functionality of two or more blocks of the flowcharts and/or block diagrams may be at least partially integrated. Finally, other blocks may be added/inserted between the blocks that are illustrated, and/or blocks/operations may be omitted without departing from the scope of inventive concepts. Moreover, although some of the diagrams include arrows on communication paths to show a primary direction of communication, it is to be understood that communication may occur in the opposite direction to the depicted arrows. Although several embodiments of inventive concepts have been disclosed in the foregoing specification, it is understood that many modifications and other embodiments of inventive concepts will come to mind to which inventive concepts pertain, having the benefit of teachings presented in the foregoing description and associated drawings. It is thus understood that inventive concepts are not limited to the specific embodiments disclosed hereinabove, and that many modifications and other embodiments are intended to be included within the scope of the appended claims. It is further envisioned that features from one embodiment may be combined or used with the features from a different embodiments described herein. Moreover, although specific terms are employed herein, as well as in the claims which follow, they are used only in a generic and descriptive sense, and not for the purposes of limiting the described inventive concepts, nor the claims which follow. The entire disclosure of each patent and patent publication cited herein is incorporated by reference herein in its entirety, as if each such patent or publication were individually incorporated by reference herein. Various features and/or potential advantages of inventive concepts are set forth in the following claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/78/179/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.922062337398529,
                    "F_SPEC_PARAMS": [
                        "need to be tracked with a high degree of precision"
                    ],
                    "S_SPEC_PARAMS": [
                        "limited to use in only certain procedures,",
                        "drawbacks"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "<s> Position recognition systems for robot assisted surgeries are used to determine the position of and track a particular object in 3-dimensions 3D. In robot assisted surgeries, for example, certain objects, such as surgical instruments, need to be tracked with a high degree of precision as the instrument is being positioned and moved by a robot or by a physician, for example."
                    ],
                    "S_SENTS": [
                        "End-effectors used in robotic surgery may be limited to use in only certain procedures, or may suffer from other drawbacks or disadvantages."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement"
                    ],
                    "F_SIM_SCORE": 0.6500094532966614,
                    "S_TRIZ_PARAMS": [
                        "Convenience of Use"
                    ],
                    "S_SIM_SCORE": 0.5916016101837158,
                    "GLOBAL_SCORE": 1.7428678691387176
                },
                "sort": [
                    1.7428678
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11446545-20220920",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11446545-20220920",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-12-06",
                    "PUBLICATION_DATE": "2022-09-20",
                    "INVENTORS": [
                        "Panagiotis Polygerinos",
                        "Frederick Sebastian",
                        "Qiushi Fu",
                        "Marco Santello"
                    ],
                    "APPLICANTS": [
                        "Panagiotis Polygerinos    ( Gilbert , US )"
                    ],
                    "INVENTION_TITLE": "Soft robotic haptic interface with variable stiffness for rehabilitation of sensorimotor hand function",
                    "DOMAIN": "A63B 2316",
                    "ABSTRACT": "A pneumatically-actuated soft robotics-based variable stiffness haptic interface device for rehabilitation of a hand includes a body having a flexible outer wall and a cavity defined by the outer wall, the outer wall including a plurality of grooves configured to receive a fiber wound around the outer wall. The device further includes a pneumatic actuator in communication with the cavity and configured to provide pressure to the cavity.",
                    "CLAIMS": "1. A pneumatically-actuated soft robotics-based variable stiffness haptic interface device for rehabilitation of a hand, the device comprising: a body including a flexible outer wall and a cavity defined by the outer wall, the outer wall including a plurality of grooves configured to receive a fiber wound around the outer wall, wherein the body is sized and shaped to be gripped by the hand during use; and a pneumatic actuator in communication with the cavity and configured to provide pressure to the cavity; wherein in an open loop mode the pneumatic actuator is configured to provide a predetermined pressure to the cavity and an internal pressure of the cavity is allowed to increase with an increased force applied to the device, and wherein in a closed loop mode the pneumatic actuator is configured to provide constant control, the cavity is given a starting pressure, and the internal pressure is configured to be maintained at the starting pressure as increased force is applied to the device. 2. The device of claim 1, wherein the outer wall comprises silicone. 3. The device of claim 1, further comprising a first end cap secured to a first end of the outer wall and a second end cap secured to a second end of the outer wall. 4. The device of claim 3, further comprising a rod secured to the first end cap and the second end cap and extending between the first end cap and the second end cap inside the cavity. 5. The device of claim 1, wherein the outer wall comprises a first layer of shore hardness 10A silicone rubber. 6. The device of claim 5, wherein the outer wall comprises a second layer of shore hardness 20A silicone rubber. 7. The device of claim 1, further comprising the fiber, wherein the fiber is wound around the body in clockwise and counter clockwise directions. 8. The device of claim 1, further comprising: a controller configured to set the predetermined pressure in the cavity, a solenoid valve in communication with the controller, the solenoid valve configured to remain closed, wherein the pneumatic actuator is in communication with the solenoid valve, and a pressure sensor in communication with the pneumatic actuator and the cavity, the pressure sensor configured to monitor pressure variations in the cavity. 9. The device of claim 1, further comprising: a controller configured to set the predetermined pressure in the cavity, a pressure sensor configured to monitor pressure in the cavity, the pressure sensor in communication with the controller, and a solenoid valve in communication with the controller and configured to regulate the pressure in the cavity to the set pressure based on feedback from the pressure sensor, and wherein the pneumatic actuator is in communication with the solenoid valve and the pressure sensor. 10. The device of claim 1, wherein the body is cylindrical, and has a diameter between 35 mm and 45 mm and a height between 115 mm and 125 mm. 11. A pneumatically-actuated soft robotics-based variable stiffness haptic interface device for rehabilitation of a hand, the device comprising: a cylindrical body including a flexible outer wall and a cavity defined by the outer wall; a pneumatic actuator in communication with the cavity and configured to provide pressure to the cavity, a pressure sensor to monitor a pressure in the cavity, and a valve configured to regulate the pressure in the cavity in response to a user supplied force that acts radially on the flexible outer wall; wherein the pressure sensor is configured to measure the pressure of the cavity, and wherein the pressure measured by the pressure sensor of the cavity is greater than a predetermined pressure as the user supplied force applied to the device increases. 12. The device of claim 11, further comprising an end cap secured to a first end of the outer wall, the end cap including a pneumatic tube for providing fluid communication between the cavity and the pneumatic actuator. 13. The device of claim 11, wherein the outer wall includes a plurality of grooves and a fiber is disposed in the plurality of grooves and wound around the outer wall. 14. The device of claim 11, wherein the outer wall comprises silicone. 15. The device of claim 11, further comprising a controller coupled to the valve, wherein the controller is configured to open and close the valve to regulate a flow of air into and out of the cavity and to maintain a constant pressure within the cavity when the user applies the radial user supplied force. 16. The device of claim 15, wherein the body is cylindrical and has a diameter between 35 mm and 45 mm, and has a height between 115 mm and 125 mm, wherein the body is configured to be gripped by a hand of the user. 17. A pneumatically-actuated soft robotics-based variable stiffness haptic interface device for rehabilitation of a hand, the device comprising: a cylindrical body including a flexible outer wall and a cavity defined by the outer wall; a pneumatic actuator in communication with the cavity and configured to provide pressure to the cavity; a pressure sensor to monitor a pressure in the cavity; a valve configured to regulate the pressure in the cavity in response to a user supplied force that acts radially on the flexible outer wall; a controller coupled to the valve, wherein the controller is configured to open and close the valve to regulate a flow of air into and out of the cavity and to maintain a constant pressure within the cavity when the user applies the radial user supplied force; wherein the body is cylindrical and has a diameter between 35 mm and 45 mm, and has a height between 115 mm and 125 mm, wherein the body is configured to be gripped by a hand of the user.",
                    "STATE_OF_THE_ART": "The human hand is a complex sensorimotor apparatus that consists of many joints, muscles, and sensory receptors. Such complexity allows for skillful and dexterous manual actions in activities of daily living. When the sensorimotor function of hand is impaired by neurological diseases or traumatic injuries, the quality of life of the affected individual could be severely impacted. For example, stroke is a condition that is broadly defined as a loss in brain function due to necrotic cell death stemming from a sudden loss in blood supply within the cranium. This event can lead to a multitude of repercussions on sensorimotor function, one of which being impaired hand control such as weakened grip strength. Other potential causes of impaired hand function include cerebral palsy, multiple sclerosis, and amputation. Therefore, effective rehabilitation to help patients regain functional hand control is critically important in clinical practice. It has been shown that recovery of sensory motor function relies on the plasticity of the central nervous system to relearn and remodel the brain. Specifically, there are several factors that are known to contribute to neuroplasticity: specificity, number of repetition, training intensity, time, and salience. However, existing physical therapy of hand is limited by the resource and accessibility, leading to inadequate dosage and lack of patients' motivation. Robot-assisted hand rehabilitation has recently attracted a lot attention because robotic devices has the advantage to provide 1 enriched environment to strengthen motivation, 2 increase number of repetition through automated control, and 3 progressive intensity levels that adapts to patient's need.",
                    "SUMMARY": [
                        "The human hand comprises complex sensorimotor functions that can be impaired by neurological diseases and traumatic injuries. Effective rehabilitation can bring the impaired hand back to a functional state because of the plasticity of the central nervous system to relearn and remodel the lost synapses in the brain. Synaptic plasticity can be further augmented by training specific parts of the brain with motor tasks in increasing difficulty. Current rehabilitation therapies focus on strengthening motor skills, such as grasping, employing multiple objects of varying stiffness so that affected persons can experience a wide range of strength training. These objects also have limited range of stiffness due to the rigid mechanisms employed in their variable stiffness actuators. Certain embodiments described herein provide a soft robotic haptic device for neuromuscular rehabilitation of the hand, which is designed to offer adjustable stiffness and can be utilized in both clinical and home settings. The device eliminates the need for multiple objects by utilizing a pneumatic soft structure made with highly compliant materials that act as the actuator and the body of the haptic interface. It is made with interchangeable sleeves that can be customized to include materials of varying stiffness to increase the upper limit of the variable stiffness range. The device is fabricated using 3-D printing technologies, and polymer molding and casting techniques thus keeping the cost low and throughput high. The haptic interface is linked to either an effective open-loop or closed-loop control system depending on the desired mode of actuation. The former allows for an increased pressure during usage, while the latter provides pressure regulation in accordance to the stiffness the user specifies. Preliminary evaluation was performed to characterize the effective controllable region of variance in stiffness. The two control systems were tested to derive relationships between internal pressure, grasping force exertion on the surface, and displacement using multiple probing points on the haptic device. Additional quantitative evaluation was performed with study participants and juxtaposed to a qualitative analysis to ensure adequate perception in compliance variance. In one embodiment, the invention provides a pneumatically-actuated soft robotics-based variable stiffness haptic interface device for rehabilitation of a hand. The device comprises a body including a flexible outer wall and a cavity defined by the outer wall, the outer wall including a plurality of grooves configured to receive a fiber wound around the outer wall, and a pneumatic actuator in communication with the cavity and configured to provide pressure to the cavity. Other aspects of the invention will become apparent by consideration of the detailed description and accompanying drawings.",
                        "1 illustrates a pneumatically-actuated device for supporting rehabilitation of sensorimotor function of hands according to an embodiment of the present invention. 2 is a cross-sectional view of the device illustrated in 1. 3 is a block diagram of an open-loop control system of an isometric mode of operation. 4 is a block diagram of a closed-loop control system of a constant pressure mode of operation. 5A illustrates the device of 1 marked for a stiffness characterization experiment to determine the stiffness profile of the grasping area. 5B illustrates a testing apparatus for conducting the stiffness characterization experiment. 6A graphically illustrates results of the characterization test of the device illustrated in 1. 6B graphically illustrates exerted force and displacement of the device illustrated in 1 with varying pressures using the constant pressure system illustrated in 4. 7 graphically illustrates the relationship between stiffness, displacement, and force, and indicates that a controllable increased stiffness with varying pneumatic actuation in the device enables the device to increase its stiffness when a gradual force is exerted on it. 8A illustrates several devices having varying Shore hardness values. 8B graphically illustrates subjects' attempts at matching stiffness of the device with its pressure setting."
                    ],
                    "DESCRIPTION": "Before any embodiments of the invention are explained in detail, it is to be understood that the invention is not limited in its application to the details of construction and the arrangement of components set forth in the following description or illustrated in the following drawings. The invention is capable of other embodiments and of being practiced or of being carried out in various ways. Haptic interfaces and variable stiffness mechanisms are usually incorporated into robotic rehabilitation devices to provide varying difficulties by adjusting force output or stiffness. These devices and systems, however, are either costly or bulky due to complex mechanical design, or have limited range of stiffness due to passive mechanical components. To overcome these limitations, the design of a novel pneumatically-actuated soft robotics-based variable stiffness haptic interface 10 is presented to support rehabilitation of sensorimotor function of hands 1. Soft robotics is a rapidly growing field that utilizes highly compliant materials that are fluidic actuated to effectively adapt to shapes and constraints that traditionally rigid machines are unable to. Several soft-robotics devices have been developed to provide assistance to stroke patients, but none of these have been designed as resistive training devices. An example of an existing device includes the use of soft actuators that bend, twist, and extend through finger-like motions in a rehabilitative exoglove to be worn by stroke patients. A variable stiffness device that employs soft-robotics allows a greater range of stiffness to be implemented since there is minimal or no impedance to the initial stiffness of the device. Additionally, soft robotics methods allow devices to be manufactured with lowered cost and have much less complexity, thus suitable to be used not only inpatient but also outpatient hand rehabilitative services. As shown in 2, the device 10 may include a cylindrical handle 14 having a diameter. In the illustrated embodiment, the diameter is 40 mm since this diameter has been shown to be most effective in enabling high grip forces in humans. In other constructions, the handle 14 also is capable of having other suitable dimensions for the diameter, such as, for example, 35 mm to 45 mm. The average male hand width, defined as the distance from the second to the fifth metacarpophalangeal joints, is approximately 83 mm. The handle 14 includes a height, and in one embodiment, the height is 120 mm. In other constructions, the height of the handle 14 is capable of having other suitable dimensions, such as, for example, 115 mm to 125 mm. The approximately 40 mm additional length was added to ensure the entire body 14 of the device 10 fits in a patient's grip, accommodate for hand widths larger than the average, and to account for higher stiffness in areas closer to the end caps 18 of the device 10 see 6. The male hand width is used as the basis of the design since on average the male hand is larger than the female hand. The device 10 was modeled using computer-aided design CAD software before the device was made. In the illustrated embodiment, a mold was made for its body 14 and the end caps 18 were 3-D printed. The body 14 was cast out of silicon elastomer material, although other materials may be used. In the illustrated embodiment, the body 14 is hollow and a wall of the body 14 defines a cavity 24. The end caps 18 coupled to the body 14 enclose the cavity 24. A radial constraint , a wound fiber 38 is coupled to the body 14. In the illustrated embodiment, the mold of the body 14 included grooves 20 in a helical pattern along the body 14 of the device 10 to facilitate the fiber winding process during fabrication, as described below. In some embodiments, the body 14 of the device 10 may be fabricated based on a multistep molding and casting technique that has been established for creating fiber-reinforced soft actuators. However, some features and components may be modified according to the goal of constraining the device from expanding vertically and horizontally, as well as to prevent bending and twisting motions. Instead of a hemisphere or a rectangle, the body of the mold may be made in a circular design to achieve a cylindrical hand-held device, and 3D-printed. The first layer 22 may be casted with the printed mold using a shore hardness 10A silicone rubber with 2 mm thickness. End caps 18 of 50 mm diameter and 5 mm thickness may be 3-D printed. The caps 18 may include a hole in the center to introduce a threaded rod 26, acting as a core, which was positioned within the cavity of the body 14 and was fastened on both ends with locking nuts 30. In the illustrated embodiment, the hole has a diameter of 6 mm, and the threaded rod 26 has a length of 178 mm. In other constructions, the core 26 may be formed from a member other than the threaded rod. Additionally, a hole off the edge of the first hole is used to introduce a tube 34 for pneumatic actuation. In the illustrated embodiment, the hole has a diameter of 3 mm, and is spaced approximately 4 mm off the edge of the first hole. The end caps 18 are attached to the body of the actuator 10 using silicone adhesive Sil-Poxy Adhesive, Smooth-on Inc. , PA, USA. This adhesive may also be used around the connecting parts to prevent air leaks, i. e. , around the base of the cap 18 and the body 14, and at the ends of the core 26. A single Kevlar fiber 38 is wound along the grooves 20 made from the mold in clockwise and counter clockwise directions, and a thin layer of silicone was applied on the fiber threading 38 to anchor it in place and prevent it from moving during actuation and grasping. A second layer 2-mm thick was made with the same casting techniques, but with a shore hardness 20A silicone rubber, and used as a sleeve over the first layer 22. Although certain example embodiments described in this application achieve radial constraint through the inclusion of a wound fiber , fiber 38, those of ordinary skill in the art will, having studied the teachings in this application, recognize and appreciate that, in certain embodiments, the device may be configured to achieve radial constraint in other ways including, but not limited to, through the inclusion of a stiffer silicone or different stiffness elastomer patterns, electroactive polymer patterns, or otherwise without the use of a wound fiber , plastic rings, elastic rings, fabric strips, or braided meshes. In certain embodiments, device 10 may include one or more radial constraints, one example of which includes, but is not limited to, a wound fiber such as fiber 38. The first layer 22 of the device 10 may be made with very flexible rubber to ensure the lower limit of the device's stiffness is kept at a minimum while it is directly exposed to pressure. However, the high compliance of the first layer 22 compromises its structural integrity. Therefore, a secondary layer of the same compliance may be made as a sleeve over the first 22. The user may utilize a third sleeve with less compliant materials to increase the upper limit of the device's stiffness range. The interchangeability of sleeves provides greater customization and adaptability for the user's specific needs. Additionally, the interchangeability feature allows for improved sanitary environments by allowing physicians to swap sleeves between patients quickly. There are two modes of operation of the soft robotic haptic interface: 1 isometric 100 and 2 constant pressure 200. The former mode 100 is a system with no pressure regulation. Therefore, the device is given a starting pressure greater than 0 kPa 105 and the internal pressure is allowed to increase with an increased force exertion on the device 10. This actuation system is shown on the open-loop control system block diagram in 3. The latter mode 200 of operation involves regulated pressure. Therefore, the device 10 is given a starting pressure greater than 0 kPa 205, and the internal pressure is maintained at that pressure as the hand grasping force exerted on the device 10 is increased. This actuation system is shown on the closed-loop control system block diagram in 4. In the open-loop mode 100, the pressure sensor 125 is utilized to monitor the pressure variations inside the device. The microcontroller 110 is set to keep the solenoid valves 115 closed, thereby preventing a pressure drop in the actuator 120 once the initial pressure 205 has been set. The design for the closed-loop system 200 is achieved by employing solenoid valves 215 to both pressurize and depressurize the actuator 220 based on the user's input. The pressure input 205 is fed through solenoid valves Series 11 Miniature Solenoid Valves, Parker Hannifin Corp. , OH, USA 215 before they split to equal pressures in the haptic interface and a fluidic pressure sensor ASDXAVX100PGAA5, Honeywell International Inc. , Morris Plains, N. J. 225. The pressure sensor 225 provides feedback to a microcontroller Arduino Uno R3, Arduino LLC. , Italy 210 to turn the solenoid valves 215 on and off to regulate the pressure to an approximate accuracy of 0. 69 kPa. When the pressure sensor 225 reads the pressure input to be higher or lower than the desired preset input 205, it will depressurize or pressurize, respectively. Generally, an object's stiffness is described by the Young's Modulus, which is the ratio of the pressure force per unit area applied on the object and its relative deformation. However, for small strains, as expected in this case, the compliance of the soft haptic interface 10 can still be characterized by the ratio of the force exerted on it and the resulting displacement. The equation describing this characterization is shown in Eq. 1, where k, x, and F represent stiffness, displacement and force applied, respectively. k=F/xEq. 1 A stiffness characterization experiment was performed to determine the stiffness profile of the grasping area of the soft robotic haptic interface 10. This was done by marking the device's soft body with nine linear points with spacing of 15 mm in between in each point 5A. Point 1 is the point closest to the end cap 18 on the side with a pneumatic tubing 34, and Point 9 is at the furthest opposite end. The device 10 is fixed in place by the core 26 using a bar clamp not shown with the marked points being exposed upwards. The clamp is attached to the lower grip of a uniaxial testing machine 50 while a probe 54 of 6-mm diameter is attached on the upper grip 5B. The probe 54 is positioned right above the point to be tested, and force and position of the probe 54 are set to 0 N and 0 mm, respectively. In a quasi-static, cyclical loading-unloading experiment the probe 54 is set to lower a maximum of 10 mm into the soft material body 14 of the device 10 while a preset pressure is provided at the beginning of the experiment. The resulting force and displacement of the probe 54 are recorded. A total of three trials are performed per probing point, and the exerted force and displacement are averaged. The characterization experiment is repeated with preset pressurizations of 3. 45, 6. 89, 13. 79, and 20. 68 kPa. For the constant pressure mode of operation, a similar test to the characterization experiment is performed but the closed-loop system 200 is utilized instead. Additionally, the mid-point on the device Point 5 is selected as the only probing location to record the resulting force. A total of three trials are performed, and the exerted force is averaged. This is repeated with pressurizations of 3. 45, 6. 89, 13. 79, and 20. 68 kPa. For the isometric mode 100 of operation, this quasi-static experiment is performed while using the open loop system. This experiment also utilized the mid-point Point 5 on the device as the only probing location. However, the probe 54 is set to probe four times with 2. 5-mm intervals between each vertical probing distance starting at 2. 5 mm for a given starting pressurization. The resulting pressure and the force exerted on the device 10 was then recorded. The stiffness per displacement is then calculated using Eq. 1 and plotted against the pressure recorded for that displacement. Three trials per displacement were performed, and the exerted force and pressure were averaged. This experiment was repeated with pressurizations of 3. 45, 6. 89, 13. 79, and 20. 68 kPa. To maximize the efficacy of this variable stiffness device 10, the change in compliance is adequately perceived by the person using the device. This is because the essence of this technology is to have variance in stiffness that begins with as minimal resistance as possible to better the rigidity experienced in existing variable stiffness devices. Therefore, the end user needs to be able to readily differentiate the stiffness of the device 10 from the lowest stiffness setting up to the highest. More importantly, perception of stiffness often involves a variety of somatosensory modalities such as mechanoreceptors, muscle spindles, and Golgi tendon, as well as the ability to coordinate joint positions and contact forces. Therefore, these types of tasks could have potential application in the rehabilitation of sensorimotor function of hands. To test the stiffness perception, the soft haptic device 10 was set at a constant pressure utilizing the open-loop control system 100. The stiffness per pressure setting 3. 45, 6. 89, or 20. 68 kPa is approximated to three distinct Shore Hardness values 00-10, 00-30, and 00-50, respectively. As shown in 8A, three cylindrical objects of Shore Hardness 00-10 object 70, 00-30 object 74, and 00-50 object 78 of the same dimensions as the soft haptic device 10 were then fabricated but with a filled center. Subjects were asked to grasp the three filled cylindrical objects 70, 74, 78 and then grasp the soft haptic device 10 that is set at a pressure setting unknown to them. The number of attempts it took the subject to match it to our set Shore Hardness for the given pressurization was then recorded. This qualitative experiment is repeated with the same subject but at a different pressure setting. This experiment was conducted with 17 healthy participants who gave their full written and oral consent before participation. The stiffness profile versus the points on the device with varying pressures is presented in 6A-B. The device 10 was expected to be stiffer as one moves away from the middle Point 5 of the device. This expectation was consistent with experimental results from the characterization test of the soft haptic device 10 6A. The device 10 has greater stiffness at points closer to the end caps 18 and therefore the regions of effective variable stiffness can be identified between points 3 and 7 where the stiffness for each pressure appears to be relatively linear. The greater stiffness towards either end of the device 10 is mainly due to the influence of the bond between the end caps 18 and the body 14 of the actuator 10. For this reason, Points 1 and 9 were excluded from the data. The graph of the exerted force and displacement with varying pressures using the constant pressure system is presented in 6B. Using this plot the end user has the ability to select a fixed stiffness value when using the soft haptic interface 10 in a constant pressure mode 200 to perform grasping exercises where the haptic feel remains the same irrespective of the grasping force exerted on the device 10. Conversely, the stiffness reduced for every increment in displacement in the isometric testing 7, however, the drop was consistent for every pressure input. This validates the concept of a controllable increased stiffness with varying pneumatic actuation in the soft haptic interface 10, which enables the device 10 to increase its stiffness when a gradual force is exerted on it. Overall, the two modes 100, 200 allow for stiffness values to be adjusted on demand to higher or lower ranges through variations of the initial stiffness of the sleeves and the internal pneumatic pressure. Additionally, the efficacy of the device 10 was tested using 34 test subjects to grasp the device 10 at varying stiffness settings. Out of the 34 test subjects, 23 of them or 68% matched the stiffness of the device 10 correctly in their first attempt as seen in 8B. This number was then further broken down for the three stiffness settings and it was found that 67%, 73%, and 64% of the subjects matched the stiffness correctly in their first attempt for the Shore 00-10 70, Shore 00-30 74, and Shore 00-50 78 cylinders, respectively, as shown in 8B. A novel design of a variable stiffness haptic interface 10 based on soft robotics that is pneumatically actuated to assist hand rehabilitation is described herein. The fabrication process of this device 10 is simple and cost-effective since it closely adheres to existing multistep casting and molding techniques utilized for fiber-reinforced soft actuators. The utilization of highly compliant materials silicone elastomers allowed for the device to present stiffness ranges that existing variable stiffness devices are not able to achieve due to the rigidity of their mechanical designs. Experiments were conducted to characterize the effective regions of variable stiffness in the soft haptic device 10 due to design constraints that include regions of exponential stiffness. A closed-loop and open-loop control system 200, 100 were presented and tested. Finally, the variance of stiffness in the device was tested with healthy subjects to ensure that the induced variance in stiffness translates adequately to a qualitative measure as well. One of the most challenging aspects of creating a device of variable stiffness is to ensure the variance in compliance is appropriately perceived by the users. This is challenging due to the multitude of factors involved in human perception of stiffness Bergmann Tiest 2010; Jones and Hunter 1990. The experiment results show that healthy subjects could effectively distinguish the variance in stiffness of the soft haptic device 10, and that the qualitative measurement could be matched to a quantitative value Shore Hardness. This allows for a more cohesive mapping of the soft haptic device 10, and therefore provides the device's users the tool necessary to utilize the device 10 effectively. The main findings and potential applications of the soft-robotics device for rehabilitation of sensorimotor function of hands are discussed. The central region Points 3 to 7, 6A is characterized by an increasing stiffness that could be manipulated on demand by the end user or physical therapist in a controlled fashion by increasing the pressure input to the device 10. It is important to note that only four different pressure settings were tested in this work as a proof-of-concept. If desired, additional pressure settings can be utilized for this particular design. However, the maximum pressure input presented was 20. 68 kPa so as to prevent the device 10 from buckling under greater internal pressure. To increase the upper limit of the pressure input, a greater number of sleeves can be added to the device 10, sleeves of higher stiffness can be incorporated into the design, and/or the number of windings 38 on the first layer 10 could be increased. This once again proves the versatility of this device to be used in stroke rehabilitation given the importance of tailoring task difficulty or characteristics to individual patients' sensorimotor deficits. The constant pressure test support using the device to calculate the stiffness a user can expect when using the device 10 at a given regulated pressure. This could be eventually used to formulate a chart for quick reference if a particular setting is desired for a rehabilitative exercise to be performed. This setting can be utilized for strength training that requires a large number of hand grasping/squeezing repetitions since high repetitions have shown to increase neural plasticity in stroke recovery. The isometric mode 100 provides the user with an option to increase the force needed to squeeze the device 10 at a given pressure, thus being useful for users who need consistent increases in difficulty for each rehabilitative exercise. These two different modes 100, 200 can be utilized by the physician depending on the needs of the stroke patient. However, the results of this testing showed that the stiffness dropped for 2. 5 mm increments in the displacement using the isometric system 100. Given that the stiffness increased during characterization which utilized the same control system, it appears that the pressure in the soft haptics is escaping when small displacements occur in the device. The results demonstrated great potential to use the device in a variety of hand rehabilitation exercises. For instance, patients who need fixed stiffness with increased repetitions of grasping exercise could use the constant pressure control mode 200; and patients who need increasing difficulty could utilize the isometric control mode 100. Furthermore, with a sensor added to the device 10, patients can use it as a controller at home to perform exercises in combination with video games to mimic augmented reality feedback that currently exists for rehabilitation devices Khademi et al. 2012. Lastly, the device 10 has the unique feature that the entire grasp area is compliant due to the implementation of soft robotics techniques. Unlike hand rehabilitation devices with rigid mechanisms, our design could promote the practice of natural coordination among all fingers which is important in ADL tasks. Various features and advantages of certain embodiments are set forth in the following claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/45/465/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9921385645866394,
                    "F_SPEC_PARAMS": [
                        "sensorimotor function,",
                        "grip strength"
                    ],
                    "S_SPEC_PARAMS": [
                        "accessibility,",
                        "inadequate dosage",
                        "lack of patients' motivation"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "This event can lead to a multitude of repercussions on sensorimotor function, one of which being impaired hand control such as weakened grip strength.",
                        "Other potential causes of impaired hand function include cerebral palsy, multiple sclerosis, and amputation."
                    ],
                    "S_SENTS": [
                        "However, existing physical therapy of hand is limited by the resource and accessibility, leading to inadequate dosage and lack of patients' motivation."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Strength"
                    ],
                    "F_SIM_SCORE": 0.6694357395172119,
                    "S_TRIZ_PARAMS": [
                        "Convenience of Use",
                        "Harmful Side Effects"
                    ],
                    "S_SIM_SCORE": 0.5650940537452698,
                    "GLOBAL_SCORE": 1.7427367945512136
                },
                "sort": [
                    1.7427368
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11345054-20220531",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11345054-20220531",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2020-06-01",
                    "PUBLICATION_DATE": "2022-05-31",
                    "INVENTORS": [
                        "Te Li",
                        "Yongqing Wang",
                        "Haibo Liu",
                        "Qile Bo",
                        "Boyao Cui",
                        "Jianhui Deng",
                        "Kuo Liu"
                    ],
                    "APPLICANTS": [
                        "DALIAN UNIVERSITY OF TECHNOLOGY    ( Liaoning , CN )"
                    ],
                    "INVENTION_TITLE": "Magnetic-induced stiffness changed soft robot drive module and production method thereof",
                    "DOMAIN": "B25J 19068",
                    "ABSTRACT": "A magnetic-induced stiffness changed soft robot drive module includes magnetic-induced stiffness changed layer, two-degree-of-freedom pneumatic driver, magnetic core and sealing fixing device. The magnetic-induced stiffness changed layer and two-degree-of-freedom pneumatic driver are printed and formed. The magnetic core can be deformed together with the driver, and a magnetic field can be generated when it is energized. After the magnetic core is installed into the two-degree-of-freedom pneumatic driver, then assembled with the sealing fixing device, a soft robot drive module with one end fixed is finished. The magnetic-induced stiffness changed layer has the fast, reversible and controllable stiffness adjustment ability under the action of electromagnetic field. As its hardness is greater than that of the two-degree-of-freedom pneumatic driver and its position is outside the air cavity, the two-degree-of-freedom pneumatic driver can be restricted from over-expansion and over-extension in the axial direction, making its pneumatic bending deformation controllable.",
                    "CLAIMS": "1. A magnetic-induced stiffness changed soft robot drive module, including magnetic-induced stiffness changed layer, two-degree-of-freedom pneumatic drive, magnetic core, sealing fixing device; the magnetic-induced stiffness changed layer, two-degree-of-freedom pneumatic drive are main body of the drive module, the magnetic core is encapsulated in the center after integral forming of the magnetic-induced stiffness changed layer, two-degree-of-freedom pneumatic drive, and finally connected to the fixing device so as to assemble into the magnetic-induced stiffness changed soft robot drive module; the magnetic-induced stiffness changed layer is a magnetron smart elastomer composed of magnetizable particles and a silicon rubber matrix; middle part of the magnetron smart elastomer is an arc-shaped structure, and upper and lower ends are fan-shaped structures; among them, there is an arc-shaped opening in middle of the lower fan-shaped structure for passing through connection part of the two-degree-of-freedom pneumatic drive; main body of the two-degree-of-freedom pneumatic drive is cylindrical structure, including four parts: air cavity, channel for the magnetic-induced stiffness changed layer, magnetic core cavity, the connection part; center of the cylindrical structure is provided with a cylindrical cavity along axial direction, the upper end of the cylindrical cavity is open and the lower end is closed, the cylindrical cavity serves as a magnetic core cavity channel for accommodating the magnetic core; upper and lower surfaces of the cylindrical structure are provided with three fan-shaped grooves at equal intervals along circumferential direction of the cylindrical cavity, and the fan-shaped grooves are matched with the fan-shaped structures at the upper and lower ends of the magnetic-induced stiffness changed layer; interior of the cylindrical structure is provided with three arc-shaped cavities at equal intervals along longitudinal direction; the connection part includes three groups, which are set in the three arc-shaped cavities, and each group includes two curved plate-shaped structures, area between one plate-shaped structure and the outer side of the arc-shaped cavity is the channel for the magnetic-induced stiffness changed layer, which is used to place the magnetic-induced stiffness changed layer, and the other plate-shaped structure is in contact with the inner surface of the arc-shaped cavity inside the cylindrical structure, area between the two plate-shaped structures is the air cavity for accommodating driving gas, and the air cavity is closed at top and penetrates through bottom; the length of the connection part is greater than the length of the two-degree-of-freedom pneumatic drive, upper end of the connection part is in contact with lower surface of the fan-shaped groove on upper end surface of the two-degree-of-freedom pneumatic drive, protruding part of lower end is inserted into the arc-shaped groove of the sealing fixing device; the magnetic core is used as a magnetic field generator, which is made of silicon steel sheet and silica gel layer by bonding and lamination, and the outer surface of the magnetic core is wrapped with a copper conductive coil; the magnetic core wrapped with the copper conductive coil is encapsulated in the magnetic core cavity and integrated with the two-degree-of-freedom pneumatic drive; by adjusting number of turns and current of the copper conductive coil, the stiffness of the drive module is adjusted; the sealing fixing device includes an outer flange, a middle sealing splint, and an inner flange; the outer flange is a hollow trapezoidal round table structure, the middle sealing splint and the inner flange are placed in the hollow, convex structure of the outer flange is provided with connecting holes, corresponding positions of the middle sealing splint and inner flange are provided with connecting holes; the bolts pass through the connecting holes along horizontal direction and make a matching connection with the connection part, which plays a sealing role; the outer flange has mounting holes on outer ring for fixing the end of the magnetic-induced stiffness changed soft robot drive module; the middle sealing splint is a ring structure, two sides of circumference of the ring structure has three arc-shaped groove structures at equal intervals, which are used to insert the arc-shaped plate-shaped structure of the connection part; the ring structure is also provided with a vent hole that communicates with the air cavity, which is connected with a quick connector. 2. The magnetic-induced stiffness changed soft robot drive module according to claim 1, wherein the magnetizable particles are submicron hydroxy iron powder, and the rubber matrix is polydimethylsiloxane PDMS. 3. A production method of the magnetic-induced stiffness changed soft robot drive module according to claim 1, comprising steps of: step 1: preparing printing material for the magnetic-induced stiffness changed layer; the printing material of the magnetic-induced stiffness changed layer includes silicone rubber, fumed silica, and high-purity hydroxyl iron powder; the silicone rubber includes basic components and curing agents; mixing basic component A, curing agent B, high-purity hydroxy iron powder, and fumed silica in a mass ratio of 10:1:6:1. 25, and putting them into a vacuum box to remove bubbles, then obtaining the printing material of the magnetic-induced stiffness changed layer; step 2: preparing printing material for the two-degree-of-freedom pneumatic drive; the printing material of the two-degree-of-freedom pneumatic drive includes silicone rubber and fumed silica; the silicone rubber includes basic components and curing agents; mixing basic component A, curing agent B, and fumed silica in a mass ratio of 10:1:1. 25, and putting them into a vacuum box to remove bubbles, then obtaining the printing material of the two-degree-of-freedom pneumatic drive; step 3: 3D printing of the magnetic-induced stiffness changed layer and two-degree-of-freedom pneumatic drive; firstly, loading the printing materials of the magnetic-induced stiffness changed layer and the two-degree-of-freedom pneumatic drive into two low-temperature print heads; secondly, combining two model to slice, then giving two print heads the printing material properties of the magnetic-induced stiffness changed layer and the two-degree-of-freedom pneumatic drive; leaving space for the air cavity normally when printing, and printing the magnetic-induced stiffness changed layer corresponding to the position of channel for the magnetic-induced stiffness changed layer layer by layer directly following the two-degree-of-freedom pneumatic drive; after the printing is completed, the magnetic-induced stiffness changed layer and the two-degree-of-freedom pneumatic drive are naturally combined into a whole; step 4: fabrication and package of magnetic core; firstly, using the printing material of the two-degree-of-freedom pneumatic drive to print 5 silica gel layers; after curing, bonding 5 silica gel layers alternately with 5 silicon steel sheet to make a magnetic core; then wrapping the copper conductive coil around the magnetic core; finally, after the magnetic-induced stiffness changed layer and the two-degree-of-freedom pneumatic drive are cured, putting the magnetic core wrapped with the copper conductive coil into the magnetic core cavity channel, and then injecting liquid insulating silica gel into remaining space of the magnetic core cavity channel, waiting for the liquid insulating silica gel to be completely cured before the package; step 5: assembling the sealing fixing device; firstly, aligning the inner flange with center of the inner ring of the connection part, then gluing and fixing the inner flange; secondly, putting the middle sealing splint between the connection part and the inner flange, and aligning middle sealing splint connecting hole and inner flange connecting hole; third, putting the outer flange on outer ring of middle sealing splint and connection part and rotating to the outer flange connecting hole and middle sealing splint connecting hole to align; finally, using bolts to connect outer flange, middle sealing splint, inner flange and connection part; sealing the side connection gap between the two-degree-of-freedom pneumatic drive and the sealing fixing device by glass glue; step 6: performing pneumatic control and magnetic stiffness adjustment on the magnetic-induced stiffness changed soft robot drive module; connecting three quick connectors to vent hole through threads; each quick connector is externally connected to an air pipe, a proportional valve and an air pump separately, to achieve three-channel high-pressure gas modular pneumatic drive.",
                    "FIELD_OF_INVENTION": "The invention belongs to the technical field of soft robots, and relates to a magnetic-induced stiffness changed soft robot drive module and production method thereof.",
                    "STATE_OF_THE_ART": "Soft robot technology is one of the emerging frontiers in the robotics field in recent years. The design of the soft robot is inspired by soft creatures such as elephant trunks, octopus tentacles, and inchworms. Therefore, soft robots no longer use rigid structural joints, but mainly use soft materials such as silicone rubber and hydrogel as their bodies. They have good human-machine-environment interaction and safety. They are used in aerospace, underwater operations, high-end manufacturing, biomedicine and other fields have broad application prospects. However, the current practical application of soft robots has the design problem of more flexibility but not enough rigidity, which hinders the application process of soft robots. Therefore, in order to make soft robots have both friendly interaction and operational load capacity, soft robots with variable stiffness adjustment capabilities have become a hot research direction. In 2017, Hongliang Ren of the National University of Singapore and others proposed a method for adjusting the stiffness of the actuator by controlling the positive and negative pressures of the internal cavity of the origami structure in the invention patent application number 201710258747. 0. However, the production of the origami structure is more complicated and the drive speed is slow. In 2019, Xufeng Dong of Dalian University of Technology and others proposed a variable stiffness layer made of electrorheological fluid in the invention patent application number 201910853243. 2, so as to realize the stiffness adjustment ability of the bending deformation actuator under electronic control, but electrorheological fluid has the problems of poor agglomeration stability and sedimentation stability, and the manufacturing process is not easy. In summary, fast, controllable, reversible, and integrated variable stiffness capability and flexible motion capability have become the technical goal of the design of the soft robot variable stiffness drive. At present, the overall performance of the variable stiffness drive design still needs to be improved.",
                    "SUMMARY": [
                        "In view of the problems existing in the current technology, the present invention provides a magnetic-induced stiffness changed soft robot drive module and production method thereof. This method uses a magnetic-induced stiffness changed layer made of PDMS and high-purity hydroxyl iron powder as raw materials to achieve rapid, reversible, and controllable stiffness adjustment capabilities under the action of an electromagnetic field; An integrated design configuration of variable stiffness actuators is adopted and a soft robot pneumatic driver that realizes the coexistence of two-degree-of-freedom motion and stiffness adjustment capabilities; the 3D printing process steps of direct ink writing is proposed, and the manufacture of stiffness changed drive module is realized. In order to achieve the above-mentioned objective, the technical solution adopted by the present invention is as follows:A magnetic-induced stiffness changed soft robot drive module, including a magnetic-induced stiffness changed layer 1, a two-degree-of-freedom pneumatic driver 2, a magnetic core 3, and a sealing fixing device 4; the magnetic-induced stiffness changed layer 1 and two-degree-of-freedom pneumatic driver 2 are main body of the drive module, they are integrally formed by 3D printing with direct ink writing, and then the magnetic core 3 is packaged in the center, and finally connected with the sealing fixing device 4 to be assembled into the magnetic-induced stiffness changed soft robot drive module. The magnetic-induced stiffness changed layer 1 is a magnetic control intelligent elastomer composed of magnetizable particles and a silicone rubber matrix, the middle part of which is an arc-shaped structure, the upper and lower ends are fan-shaped structures, and the fan-shaped structure of lower end is provided with an arc-shape opening. The opening is used to pass through a connecting part 204 of the two-degree-of-freedom pneumatic driver 2. The magnetizable particles are submicron hydroxy iron powder, and the rubber matrix is PDMMain body of the two-degree-of-freedom pneumatic drive 2 is a cylindrical structure, and includes four parts: air cavity 201, magnetic-induced stiffness changed layer cavity 202, magnetic core cavity 203, and the connection part 204. The center of the cylindrical structure is provided with a cylindrical cavity along the axial direction. The upper end surface of the cylindrical cavity is open and the lower end surface is closed. The cylindrical cavity serves as a magnetic core cavity 203 for accommodating the magnetic core 3. The upper and lower surfaces of the cylindrical structure are provided with three fan-shaped grooves at equal intervals along the circumferential direction of the cylindrical cavity. The fan-shaped groove is matched with the fan-shaped structure at the upper and lower ends of the magnetic-induced stiffness changed layer 1, and the center angle of the fan-shaped groove on the upper end surface is smaller than the center angle of the fan-shaped groove on the lower end surface. Three through arc-shaped cavities are arranged at equal intervals in the longitudinal direction inside the cylindrical structure. The connection part 204 includes three groups, which are arranged in the three arc-shaped cavities inside the cylindrical structure. Each group includes two plate-shaped structures with a certain arc on the outer side and the inner side, and the length of the connection part 204 is greater than length of two-degree-of-freedom pneumatic drive 2, the upper end of connection part 204 is in contact with the lower surface of the fan-shaped groove on the upper end of the two-degree-of-freedom pneumatic drive 2, and the protruding part of the lower end is inserted into the arc-shaped groove of the sealing fixing device 4. The area between the outer plate-shaped structure and the outer side of the arc-shaped cavity is the magnetic-induced stiffness changed layer cavity 202, which is used to place the magnetic-induced stiffness changed layer 1, and the length of the middle arc structure of the magnetic-induced stiffness changed layer 1 is the same as that of the magnetic-induced stiffness changed layer cavity 202. When printing, the two models are merged and directly integrated into one piece, the area between the two plate-shaped structures is air cavity 201, which is used to accommodate driving gas, and the top of air cavity 201 is closed and the bottom penetrates. The bottom of the connection part 204 can be sealed through the sealing fixing device 4 by matching with the sealing fixing device 4; the inner plate-shaped structure is in contact with the inner surface of the arc-shaped cavity inside the cylindrical structure. The magnetic core 3 is used as a magnetic field generating device which is made by bonding silicon steel sheet 301 and silica gel layer 302 layer by layer. The outer surface of the magnetic core 3 is wrapped with copper conductive coil 303, and it is encapsulated in the magnetic core cavity 203, so integrated with two-degree-of-freedom pneumatic drive 2. Among them, the alternate structure of silicon steel sheet 301 and silica gel layer 302 can ensure that the magnetic core 3 has a certain deformation ability, and will not affect the overall deformation of the driver. A current is applied to the copper conductive coil 303 wound around the outer circumference of the magnetic core 3, and the magnetic core 3 generates a magnetic field. In addition, the magnetic-induced stiffness changed layer 1 and the magnetic core 3 form a closed magnetic circuit; Under the action of a strong magnetic field, the magnetized particles of the magnetic-induced stiffness changed layer 1 can be magnetized, and the interaction energy between the particles is increased, thereby increasing the stiffness of the magnetic-induced stiffness changed layer 1, thereby effectively improving the overall stiffness of the drive module; By adjusting the number of turns and current of the copper conductive coil 303, dynamic stiffness adjustment can be achieved. After the magnetic field is removed, the magnetizable particles are demagnetized, and the interaction energy between the particles is reduced to zero, so that the magnetic-induced stiffness changed layer 1 is demagnetized and restored to its original state. As shown in 8, when a magnetic field is applied, due to the larger cross-sectional area of the upper and lower ends of the magnetic-induced stiffness changed layer 1, a sector-shaped structure can ensure that the magnetic core 3 and the magnetic-induced stiffness changed layer 1 form a closed-loop magnetic circuit when it is energized. The sealing fixing device 4 includes an outer flange 401, a middle sealing splint 402, and an inner flange 403. The outer flange 401 is a hollow trapezoidal round table structure, the middle sealing splint 402 and the inner flange 403 are placed in the hollow, and the middle sealing splint 402 is located between the outer flange 401 and the inner flange 403. The convex circumferential structure of the outer flange 401 is provided with three outer flange connecting holes 401-1 at equal distance along the circumferential direction, and the corresponding positions of the middle sealing splint 402 and the inner flange 403 are provided with middle sealing splint connecting hole 402-1 and inner flange connecting hole 403-1. The bolts pass through the outer flange connecting hole 401-1, the middle sealing splint connecting hole 402-1, the inner flange connecting hole 403-1 and the connection part 204 in a horizontal direction in order to be connected, and play a sealing role. The outer ring of the outer flange 401 is also provided with 8 outer flange Mounting holes 401-2 at equal intervals for fixing the ends of the magnetic-induced stiffness changed soft robot drive module. The middle sealing splint 402 is a circular ring structure, and three sets of arc-shaped groove structures are equally spaced on both sides of the circumference. Each group of groove structures includes an inner groove structure close to the inner flange 403 and an outer groove structure close to the outer flange 401. The outer groove structure is used to pass through the two arc-shaped plate structures of connection part 204; each group of groove structures is provided with vent hole 402-2 in the middle vent hole 402-2 is the vertical direction, which is used to communicate with SP-20 quick connector, and the vent hole 402-2 communicates with the air cavity 201. A production method of the magnetic-induced stiffness changed soft robot drive module includes the following steps:Step 1: Prepare the Printing Material for the Magnetic-Induced Stiffness Changed Layer 1The printing materials of the magnetic-induced stiffness changed layer 1 include PDMS, fumed silica, and high-purity hydroxyl iron powder. The particle size of the high-purity hydroxyl iron powder is less than 10 microns, the purity is greater than or equal to 99% iron content, and the appearance is black ultrafine powder without any impurities. The PDMS is selected from Dow Corning Syglard 184 silicone rubber, which includes basic component A and curing agent B. After the two components are mixed, they can be cured within 48 hours at room temperature. The preparation process of the material is as follows: First, add basic component A, curing agent B, high-purity hydroxyl iron powder, and fumed silica in a beaker in a mass ratio of 10:1:6:1. 25. The addition of fumed silica as a thixotropic agent can make the raw material obtain the mechanical properties required for direct ink writing. Then, put the stirred printing materials into a vacuum box for 5-10 minutes at room temperature to remove air bubbles. Finally, the printing material of the magnetic-induced stiffness changed layer 1 is obtained, which looks like a thicker black colloid. Step 2: Prepare the Printing Material for the Two-Degree-of-Freedom Pneumatic Drive 2The printing materials of the two-degree-of-freedom pneumatic drive 2 include PDMS and fumed silica. The PDMS is made of Dow Corning 184 silicone rubber, composed of two components A and B, including basic component A and curing agent B. The material preparation process is similar to the printing material preparation of the magnetic-induced stiffness changed layer 1: First, add the basic component A, curing agent B, and fumed silica in a beaker at a mass ratio of 10:1:1. 25, then stir thoroughly. Among them, the addition of fumed silica as a thixotropic agent can make the raw material obtain the mechanical properties required for direct ink writing. Then, put the stirred printing materials into a vacuum box for 5-10 minutes at room temperature to remove air bubbles. Finally, the printing material of two-degree-of-freedom pneumatic drive 2 is obtained, and the appearance is a relatively viscous off-white colloid. Step 3: 3D Printing of the Magnetic-Induced Stiffness Changed Layer 1 and Two-Degree-of-Freedom Pneumatic Drive 2The magnetic-induced stiffness changed layer 1 and two-degree-of-freedom pneumatic drive 2 can be printed and formed continuously from bottom to top, specifically: First, put the printing materials which are made in the Step 1 and Step 2 corresponding to the magnetic-induced stiffness changed layer 1 and the two-degree-of-freedom pneumatic drive 2 into two low-temperature printing heads. Secondly, combine the two models and slice them together then give the two print heads the printing material properties of the magnetic-induced stiffness changed layer 1 and the two-degree-of-freedom pneumatic drive 2 respectively. The air cavity 201 normally leaves space when printing, the magnetic-induced stiffness changed layer 1 corresponding to the position of channel for the magnetic-induced stiffness changed layer 202, directly follows the two-degree-of-freedom pneumatic drive 2 for layer-by-layer printing. When the printing is completed, they are combined into one. Step 4: Fabrication and Package of Magnetic Core 3First, five silica gel layers 302 are printed by printing materials of two-degree-of-freedom pneumatic drive 2. After the silica gel layer 302 is cured, it is alternately bonded with 5 silicon steel sheets 301 to make magnetic core 3. Then, the copper conductive coil 303 is wound around the magnetic core 3. Finally, after the magnetic-induced stiffness changed layer 1 and two-degree-of-freedom pneumatic drive 2 are cured in the Step 3, the magnetic core 3 wrapped with the copper conductive coil 303 is put into the magnetic core cavity channel 203. Then, inject liquid heat insulating silica gel into the remaining space of the magnetic core cavity channel 203, and wait for it to be completely solidified to complete the packaging. Step 5: Assemble the Sealing Fixing Device 4First, align the inner flange 403 with the center of the inner ring of the connection part 204, and glue and fix it. Secondly, put the middle sealing splint 402 between the connection part 204 and the inner flange 403, and align the middle sealing splint connecting hole 402-1 and the inner flange connecting hole 403-1. Third, put the outer flange 401 on the outer ring of middle sealing splint 402 and connection part 204, and rotate to the outer flange connecting hole 401-1 and middle sealing splint connecting hole 402-1 to align. Finally, use three bolts to connect outer flange 401, middle sealing splint 402, inner flange 403 and connection part 204 through outer flange connecting hole 401-1, middle sealing splint connecting hole 402-1, and inner flange connecting hole 403-1. The side connection gap between the two-degree-of-freedom pneumatic drive 2 and the sealing fixing device 4 is sealed by glass glue to prevent air leakage when large deformation occurs. Step 6: Perform Pneumatic Control and Magnetic Stiffness Adjustment on the Magnetic-Induced Stiffness Changed Soft Robot Drive ModuleConnect three SP-20 quick connectors to vent hole 402-2 through threads, and vent hole 402-2 communicates with air cavity 201; Each SP-20 quick connector is externally connected to an air pipe, a proportional valve and an air pump separately, to achieve three-channel high-pressure gas modular pneumatic drive, so that the magnetic-induced stiffness changed soft robot drive module 1 has the ability of individual control and coordinated control. Further, the hardness of the magnetic-induced stiffness changed layer 1 is higher than the hardness of the two-degree-of-freedom pneumatic drive 2, and the magnetic-induced stiffness changed layer 1 can also limit radial over-expansion and axial expansion of two-degree-of-freedom pneumatic drive 2. The limitation makes the aerodynamic bending deformation of the two-degree-of-freedom pneumatic drive more controllable; At the same time, the magnetic-induced stiffness changed layer 1 is beneficial to increase the load capacity of the drive module of the magnetic-induced stiffness changed soft robot drive module. The beneficial effect of the present invention is that a magnetic-induced stiffness changed soft robot drive module is designed by adopting the principle of magneto-induced stiffness changed, and a manufacturing process method is proposed. The magnetic-induced stiffness changed soft robot drive module can achieve 2 degrees of freedom bending under the drive of high-pressure gas, and can be used as the basic drive unit of a continuous body robot or a multi-finger dexterous hand; After the copper conductive coil 303 entwined on the magnetic core 3 generated the magnetic field, the driver module relies on magnetic-induced stiffness changed layer 1 to achieve a fast response, controllable and reversible stiffness adjustment ability; The magnetic-induced stiffness changed soft robot drive module is a highly integrated unit module without external device to generate magnetic field, and has a compact and reliable design structure. Its design principle is universal, can be used for variable stiffness soft robots with other motion forms.",
                        "1 is a schematic diagram of the overall structure of the magnetic-induced stiffness changed soft robot drive module of the present invention; 2 is a schematic diagram of the internal structure of the magnetic-induced stiffness changed soft robot drive module of the present invention; 3 is a schematic diagram of the structure of the sealing fixing device; 4 is a schematic diagram of the structure of the two-degree-of-freedom pneumatic driver; 5 is a schematic diagram of the structure of the magnetic core; 6A and 6B are a top view and a section of the two-degree-of-freedom pneumatic driver; 7 is a bottom view of the two-degree-of-freedom pneumatic driver; 8 is a schematic diagram of the structure of the magnetic-induced stiffness changed layer;In the picture: 1 the magnetic-induced stiffness changed layer; 2 two-degree-of-freedom pneumatic drive; 3 magnetic core; 4 sealing fixing device; 401 outer flange; 402 middle sealing splint; 403 inner flange; 401-2 outer flange Mounting hole; 401-1 outer flange connecting hole; 402-1 middle sealing splint connecting hole; 403-1 inner flange connecting hole; 402-2 vent hole; 201 air cavity; 202 Channel for the magnetic-induced stiffness changed layer; 203 magnetic core cavity channel; 204 connection part; 301 silicon steel sheet; 302 silica gel layer; 303 copper conductive coil."
                    ],
                    "DESCRIPTION": "The following describes the embodiments of the present invention in detail with reference to the drawings and technical solutions, but the scope of protection of the present invention is more than the above. The magnetic-induced stiffness changed soft robot drive module of the present invention is composed of the magnetic-induced stiffness changed layer 1, two-degree-of-freedom pneumatic drive 2, magnetic core 3, and sealing fixing device 4. The main body is made by 3D printing with direct ink writing. Among them, the structural design of the magnetic-induced stiffness changed layer 1 and two-degree-of-freedom pneumatic drive 2 satisfy the need for integrated printing and does not require separate production and combination, and has the advantages of compactness, reliability, and simplicity. The magnetic-induced stiffness changed layer 1 has an arc-shaped column with a length of 112 mm in the middle, and a fan-shaped column with a length of 4 mm at the upper and lower ends respectively, which are embedded in the two-degree-of-freedom pneumatic drive 2. For the lower fan-shaped, there is an arc-shaped opening in the middle of the structure for passing through the connection part 204 of the two-degree-of-freedom pneumatic drive 2. It is a magnetron smart elastomer in which submicron hydroxy iron powder is dispersed into the PDMS matrix. Under the action of the magnetic field generated by the magnetic core 3, the magnetic particles are magnetized, so that the magnetization vector of the particles is aligned with the applied magnetic field, and the interaction energy between the particles is increased, thereby increasing its rigidity. After the magnetic field disappears, the magnetizable particles are demagnetized, the interaction energy between particles is reduced to zero, and the initial state can be restored; the presence of high-purity hydroxyl iron powder also makes the magnetic-induced stiffness changed layer 1 increase its hardness, thereby limiting the effect of excessive radial and axial deformation of the drive. As shown in 8, when a magnetic field is applied, the upper and lower ends of the magnetic-induced stiffness changed layer 1 have a fan-shaped structure with a larger surface area, which can ensure that the magnetic core 3 can form a closed-loop magnetic circuit that along the axis of the magnetic core, along the fan shape, along the longitudinal direction of the arc-shaped cylinder, and along the horizontal direction of the sector. The two-degree-of-freedom pneumatic drive 2 is the main part of the magnetic-induced stiffness changed soft robot drive module. The shape is approximately a cylindrical structure with a diameter of 60 mm, including air cavity 201 and channel for the magnetic-induced stiffness changed layer 202, magnetic core cavity channel 203, connection part 204. The cylindrical structure is provided with three arc-shaped cavity structures at equal intervals 120 along the longitudinal direction, and there are also three sector-shaped groove structures arranged at equal intervals along the horizontal at the upper and lower ends. It is the channel for the magnetic-induced stiffness changed layer 202 used to accommodate the magnetic-induced stiffness changed layer 1; the connection part 204 is a three-group plate structure set in three arc-shaped cavities inside the cylindrical structure, and the length of the connection part 204 is 20 mm longer than the two-degree-of-freedom pneumatic drive. The upper end of connection part 204 is in contact with the lower surface of the fan-shaped groove on the upper end of the two-degree-of-freedom pneumatic drive 2, and the lower end of the protrusion is inserted into the arc of the sealing fixing device 4. The area between the outer plate-shaped structure and the outer side of the arc-shaped cavity is the channel for the magnetic-induced stiffness changed layer 202, which is used to place the magnetic-induced stiffness changed layer 1. When printing, the two models are merged and formed directly into one piece. The area between the set of two plate-shaped structures is air cavity 201, which is used to accommodate driving gas, and the air cavity 201 is closed at the top and penetrated at the bottom, with a length of 110 mm, and the bottom of the connection part 204 passes through and sealing fixing device 4. The position for penetration can be sealed by matching; The inner plate-shaped structure is in contact with the inner surface of the arc-shaped cavity inside the cylinder. The magnetic core cavity 203 is a cylindrical cavity with a depth of 70 mm, it penetrated at the bottom and located at the axis of the two-degree-of-freedom pneumatic drive 2, which is used to accommodate the magnetic core 3 and is subsequently filled with heat-insulating silica gel. As shown in 5, the magnetic core 3 is composed of a silicon steel sheet 301 and a silica gel layer 302 laminated on each other. The copper conductive coil 303 wound around the magnetic core 3 and it can be energized to apply a magnetic field;The sealing fixing device 4 is composed of an outer flange 401, a middle sealing splint 402, and an inner flange 403. The hollow of the outer flange 401 is used to accommodate the middle sealing splint 402 and the inner flange 403. The middle sealing splint 402 is located on the outer ring of the inner flange 403. The convex circumferential structure of the outer flange 401 is provided with three outer flange connecting holes 401-1 at equal intervals along the circumferential direction. The corresponding positions of the middle sealing splint 402 and the inner flange 403 are also provided with middle sealing splint connecting hole 402-1, inner flange connecting hole 403-1. The bolts pass through outer flange connecting hole 401-1, middle sealing splint connecting hole 402-1, inner flange connecting hole 403-1 and connection part 204 in order in the horizontal direction, so to achieve the sealing effect. The outer ring of the outer flange 401 is also provided with 8 outer flange mounting holes 401-2 at equal intervals for fixing the ends of the magnetic-induced stiffness changed soft robot drive module. The middle sealing splint 402 has three groups of arc-shaped groove structures at equal intervals on both sides of the circumference, and each group of groove structures includes an inner groove structure close to the inner flange 403 and an outer groove structure close to the outer flange 401 structure, and the arc length of the inner groove structure is smaller than that of the outer groove structure. There is a vent hole 402-2 in the middle of each group of groove structure vent hole 402-2 is the vertical direction, which is used to connect with SP-20 quick connectors, and the vent hole 402-2 communicates with the air cavity 201. Among them, the convex circumferential structure of the outer flange 401 has the same diameter as the two-degree-of-freedom pneumatic drive 2. A manufacturing method of the magnetic-induced stiffness changed soft robot drive module is specifically as follows:Step 1: Prepare the Printing Material for the Magnetic-Induced Stiffness Changed Layer 1The magnetic-induced stiffness changed layer 1 is a magnetron intelligent elastomer in which sub-micron hydroxy iron powder is dispersed into the PDMS matrix. The printing material is mainly composed of PDMS, fumed silica and high-purity hydroxy iron powder that are used to change its performance. The particle size of the high-purity hydroxy iron powder is less than 10 microns, the purity is greater than or equal to 99% iron content, and the appearance is black ultrafine powder without any impurities. The PDMS is selected from the United States Dow Corning Syglard 184 silicone rubber, composed of two components A and B, including basic component A and curing agent B. After the two components are mixed, they can be cured within 48 hours at room temperature. The preparation process of the material is as follows: First, add the basic component A, curing agent B, high-purity hydroxyl iron powder, and fumed silica in a beaker with a mass ratio of 10:1:6:1. 25, then stir fully. Among them, the addition of fumed silica as a thixotropic agent can make the raw material obtain the mechanical properties required for direct ink writing. The fumed silica is sieved with a metal mesh before adding to the break in order to prevent the agglomerated fumed silica from clogging the printing needle; Second, the stirred printing materials are put into a vacuum box for 5-10 minutes at room temperature to remove bubbles. Make sure that no bubbles are visible to the naked eye in the printing material. At this point, the printing material of the magnetic-induced stiffness changed layer 1 is prepared, and the appearance is a thicker black colloid. Step 2: Prepare the Printing Material for the Two-Degree-of-Freedom Pneumatic Drive 2The printing material of the two-degree-of-freedom pneumatic drive 2 is composed of PDMS and fumed silica. The PDMS is selected from Dow Corning 184 silicone rubber, which is composed of two components A and B, including basic component A and curing agent B. The material preparation process is similar to the material preparation of the magnetic-induced stiffness changed layer 1: First, the basic component A, curing agent B, and fumed silica are added in a beaker with a mass ratio of 10:1:1. 25 and fully stirred. Among them, the addition of fumed silica as a thixotropic agent can make the raw materials obtain the mechanical properties required for direct ink writing. Then, the stirred printing raw materials are put into a vacuum box for 5-10 minutes at room temperature to remove bubbles. At this point, the material configuration is completed, and the appearance is a relatively viscous off-white colloid. Step 3: 3D Printing of the Magnetic-Induced Stiffness Changed Layer 1 and Two-Degree-of-Freedom Pneumatic Drive 2First, combine the magnetic-induced stiffness changed layer 1 and two-degree-of-freedom pneumatic drive 2 model to slice in the slicing software of the printer. Secondly, the two prepared materials are placed in two low-temperature print heads and the two print heads are assigned their material properties in the control software. Finally, modify the speed, pressure and other parameters according to the trial printing situation, and then start printing. The material of the two-degree-of-freedom pneumatic drive 2 has high consumption, and the material of one cylinder is not enough to complete the printing. So it is necessary to prepare materials during the printing. The magnetic-induced stiffness changed soft robot drive module includes the main structure: the magnetic-induced stiffness changed layer 1 and the two-degree-of-freedom pneumatic drive 2, they can be combined into one body and printed continuously from bottom to top. The air cavity 201 normally leaves space when printing, and the magnetic-induced stiffness changed layer 1 corresponding to the position of channel for the magnetic-induced stiffness changed layer 202 is printed layer by layer directly following the two-degree-of-freedom pneumatic drive 2. After the printing is completed, the two are combined into one, without cumbersome steps such as bonding and combining. Step 4: Fabrication and Package of Magnetic Core 3First, use material of the two-degree-of-freedom pneumatic drive 2 to print 5 silica gel layers 302. After curing, it is alternately bonded with 5 silicon steel sheets 301 to make a magnetic core 3, and a copper conductive coil 303 is wrapped around it. After the magnetic-induced stiffness changed layer 1 and the two-degree-of-freedom pneumatic drive 2 are cured, put the manufactured magnetic core 3 into the magnetic core cavity channel 203, and then inject the liquid insulating silica gel into the remaining space of the magnetic core cavity channel 203, and wait for it to be completely cured before the package. The laminated structure of the silicon steel sheet 301 and silica gel layer 302 contained in the magnetic core 3 is to prevent the purely silicon steel sheet material from being too hard and hindering the deformation of the two-degree-of-freedom pneumatic drive 2. Since the magnetic core generates heat when it is energized, it may melt the silicone rubber material of the drive, so insulating silicone is adopted. Step 5: Assemble the Sealing Fixing Device 4The sealing fixing device 4 includes an outer flange 401, a middle sealing splint 402, and an inner flange 403, as shown in 3. The outer ring diameter of the inner flange 403 is 30 mm, which is the same as the inner diameter of the connection part 204. The outer flange 401 has an outer ring diameter of 60 mm, an inner ring diameter of 42 mm, and a base diameter of 80 mm. The outer flange Mounting hole 401-2 can be used to fix one end of the magnetic-induced stiffness changed soft robot drive module. The specific assembly process is as follows: First, align the inner flange 403 with the center of the connection part 204 then glue and fixed it. Secondly, put the middle sealing splint 402 between the connection part 204 and the inner flange 403, and align the middle sealing splint connecting hole 402-1 and the inner flange connecting hole 403-1. Third, put the outer flange 401 on the outer ring of the middle sealing splint 402 and rotate it to the outer flange connecting hole 401-1 and the middle sealing splint connecting hole 402-1 to align. Finally, use three bolts to connect outer flange 401, middle sealing splint 402, inner flange 403 and connection part 204 through outer flange connecting hole 401-1, middle sealing splint connecting hole 402-1, and inner flange connecting hole 403-1. Furthermore, on the cylindrical side of the two-degree-of-freedom pneumatic drive 2, the joint with the sealing fixing device 4 is coated with glass glue to seal it to prevent air leakage during large deformation. Step 6: Perform Pneumatic Control and Magnetic Stiffness Adjustment on the Magnetic-Induced Stiffness Changed Soft Robot Drive ModuleConnect three SP-20 quick connectors to the vent hole 402-2 through threads. The vent hole 402-2 ensures that the input driving gas can enter the air cavity 201. Each SP-20 quick connector is externally connected to the air pipe, proportional valve and air pump separately, so as to realize the three-channel high-pressure gas pneumatic modularization drive, so that the magnetic-induced stiffness changed soft robot drive module has the ability of independent control and coordinated control. Ventilate three air cavity 201 at the same time, the magnetic-induced stiffness changed soft robot drive module will elongate axially. After applying air pressure to one or two of the air cavity 201, the ventilation of air cavity 201 with the non-ventilated air cavity corresponding to the part of the magnetic-induced stiffness changed soft robot drive module will produce asymmetrical elongation, so the whole body will be bent. Among them, the hardness of the magnetic-induced stiffness changed layer 1 is higher than the hardness of the two-degree-of-freedom pneumatic drive 2, which limits the radial and axial over expansion generated by the two-degree-of-freedom pneumatic drive 2 and increase its the output load capacity. By energizing the magnetic core 3 to apply a magnetic field, the magnetic-induced stiffness changed layer 1 will be magnetized to increase the stiffness. After testing, under normal conditions, it is found that the zero field modulus of the magnetic-induced stiffness changed layer 1 is 320 Kpa, under the excitation of 100 mT magnetic field, its elastic modulus is 450 KPa; Under the excitation of 200 mT magnetic field, its elastic modulus is 650 KPa; Under the excitation of a 300 mT magnetic field, its elastic modulus is 800 KPa. It can be seen that the magnetic-induced stiffness changed layer 1 can effectively increase its stiffness under magnetic field excitation, and the increase in stiffness can be controlled by the strength of the magnetic field, so that real-time stiffness of the magneto-induced stiffness changed soft robot drive module can be realized in the process of movement. The above-mentioned examples only express the implementation of the present invention, but cannot therefore be understood as a limitation on the scope of the patent of the present invention. It should be pointed out that for technicians in the related field, without departing from the concept of the present invention, several modifications and improvements can also be made, all of which belong to the protection scope of the present invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/54/450/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9987081289291382,
                    "F_SPEC_PARAMS": [
                        "flexibility",
                        "rigidity,"
                    ],
                    "S_SPEC_PARAMS": [
                        "production of the origami structure is more complicated",
                        "drive speed is slow"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "However, the current practical application of soft robots has the design problem of more flexibility but not enough rigidity, which hinders the application process of soft robots."
                    ],
                    "S_SENTS": [
                        "However, the production of the origami structure is more complicated and the drive speed is slow."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Adaptability",
                        "Shape"
                    ],
                    "F_SIM_SCORE": 0.5989261567592621,
                    "S_TRIZ_PARAMS": [
                        "Speed"
                    ],
                    "S_SIM_SCORE": 0.6102553606033325,
                    "GLOBAL_SCORE": 1.7366322209437688
                },
                "sort": [
                    1.7366322
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11442429-20220913",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11442429-20220913",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-12-06",
                    "PUBLICATION_DATE": "2022-09-13",
                    "INVENTORS": [
                        "Emil Laftchiev",
                        "Diego Romeres"
                    ],
                    "APPLICANTS": [
                        "Mitsubishi Electric Research Laboratories, Inc.    ( Cambridge , US )"
                    ],
                    "INVENTION_TITLE": "Systems and methods for advance anomaly detection in a discrete manufacturing process with a task performed by a human-robot team",
                    "DOMAIN": "G05B 194063",
                    "ABSTRACT": "A system for detection of an anomaly in a discrete manufacturing process DMP with human-robot teams executing a task. Receive signals including robot, worker and DMP signals. Predict a sequence of events SOEs from DMP signals. Determine whether the predicted SOEs in the DMP signals is inconsistent with a behavior of operation of the DMP described in a DMP model, and if the predicted SOEs from DMP signals is inconsistent with the behavior, then an alarm is to be signaled. Input worker data into a Human Performance HP model, to obtain a state of the worker based on previously learned boundaries of human state. The state of the HW is then input into the HRI model and the DMP model to determine a classification of anomaly or no anomaly. Update a Human-Robot Interaction HRI model to obtain a control action of a robot or a type of an anomaly alarm.",
                    "CLAIMS": "1. A process control system for detecting an anomaly in a discrete manufacturing process DMP with human-robot teams executing at least one task within the process, comprising: a memory configured to store data including robot data, manufacturing process MP data, human data, and executable models; an input interface configured to receive test signals including DMP signals from DMP sensors that includes robot operational signals, and human worker HW signals from human worker HW sensors; a hardware processor in communication with the memory and the input interface, the hardware processor configured to: extract a predicted sequence of events from the DMP signals, and determine whether the predicted sequence of events in the DMP signals is inconsistent with a behavior of operation of the DMP described in a DMP model, and if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled; extract from the HW signals, a task completion time, measurements relating to a state of a human worker HW and a next predicted sequenced task, and input into a Human performance HP model, the HP model determines the state of the HW based on previously learned boundaries of the state of the HW, such that the state of the HW is inputted into a Human-Robot Interaction HRI model, and outputs from a human worker HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of anomaly or no anomaly, wherein the HP model for the HW is previously configured to have learned different states of human worker HW performance that correspond as a set of boundaries in the human data, and wherein the DMP model is previously configured to have learned different the operation of the manufacturing process that is used to assist in issuing classifications of anomalies or no anomaly detection; update the HRI model with the robot operation signals, the HW signals and the classified anomaly; determine a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly; and an output interface to output the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly. 2. The process control system of claim 1, wherein the HRI model is previously configured to have learned mappings between the different states of the HW and optimal robot actions. 3. The process control system of claim 1, wherein an event transition table is used to specify discovered positional relationships between pairs of events from training data of the behavior, and if the predicted sequence of events from the test signals is inconsistent with the behavior, then the alarm is to be signaled. 4. The process control system of claim 3, wherein the event transition table is built by each entry in the event transition table and is initialized with a symbol label, the memory is scanned sequentially, and for any pair of events where one event immediately follows another event, a corresponding entry of the event transition table is changed to a &gt; symbol label, upon completion of the scanning, any two events are either in the &gt; symbol label or the symbol label relations, then derive relations , , , and # computed on the basis of the relations of the &gt; symbol label and the symbol label from the memory, using a set of rules: AB, if A&gt;B and BA; AB, if AB; and B&gt;A; AB, if A&gt;B and B&gt;A; and A #B, if AB and BA, wherein the event transition table and probability distributions over task durations measured in intervals of time, specifies the discovered positional relationships between pairs of events from training data of the behavior of the operation of the DMP, wherein A is an event and B is an event, and A&gt;B is where the event A immediately follows the event B. 5. The process control system of claim 1, wherein an event transition table is constructed from training signals during a training phase, such that the training signals are acquired from a monitoring system of the DMP, during operation of the DMP, such that the hardware processor includes determining minimal and maximal durations for the at least one task and for multiple tasks. 6. The process control system of claim 5, wherein the hardware processor is further configured to: determine if the predicted sequence of events is feasible given the event transition table and a predicted completion time of a human-robot team of the human robot teams. 7. The process control system of claim 1, wherein the input interface acquires training data from the DMP sensors during a training operation of the DMP while in an off-line training period, before receiving the test signals, and the hardware processor is configured to: extract events from the training signals as a sequence to construct an event transition table of ordering relations of allowed positional relationships between pairs of observed events during the training operation of the DMP, and store the predicted sequence of events in the memory. 8. The process control system of claim 1, wherein the DMP signals include one or more of DMP component training data, DMP assembly line training data, DMP operational training data, DMP management training data, and wherein the DMP signals include other data comprising one or more of DMP component data, DMP assembly line data, DMP operational data, or DMP management data, and wherein the HW signals from HW sensors include a time series of measurements of tasks completed by the HW. 9. The process control system of claim 1, wherein-some of the classifications of anomaly detections include robot actions associated with the state of the HW, that include, different levels of speed, X-axis, Y-axis and Z-axis movements of the robot, voice announcements, making calls, maintaining robot positions for one or more periods of time, adjusting environmental conditions via commands sent to a controller, and wherein some of the types of classification of anomalies also include detection of future anomalies, maintenance related anomalies, safety related anomalies, lost production anomalies, potential failure of components anomalies, quality anomalies and assembly line anomalies. 10. The process control system of claim 1, wherein the HP model is constructed from Human worker HW training signals of completed training tasks during a training phase prior to receiving the HW signals, such that the HW training signals includes data for each completed training task of the completed training tasks that includes a training task name, multiple training states of the HW for the completed training task, and a next sequenced training task, such that the HW training signals and the HW signals are acquired from sensors associated with the HW during a training operation or an operation of the DMP with the human-robot teams. 11. The process control system of claim 1, wherein the HP model is constructed by a Human Task Execution HTE model and a model of the state of the HW, wherein the HTE model is constructed using at least one predictive model trained using Human worker HW training signals obtained during a training phase completing a sequence of training tasks, and wherein the model of the state of the HW is constructed using at least one classification model trained using the HW training signals, such that each completed training task is associated with multiple states of the HW, and is stored in the memory. 12. The process control system of claim 11, wherein the at least one predictive model is configured to learn expected completion times for each completed task, identify or capture patterns of movements of the HW observed in sensor data obtained from sensors, and wherein at least one statistical model learning approach, includes one or more predictive model, one or more classification model, or both, that is capable of producing estimates of a completion time of an on-going task given sensor measurements of the HW while the HW is interactively working with the robot in completing the at least one task. 13. The process control system of claim 11, wherein the at least one classification model is configured to learn a state of the HW from the HW signals by first determining a task label of a task completed and a next sequenced task, then uses a gaze detection algorithm to determine the state of the HW, such as an amount of a level of focus and an amount of a level of energy of the HW at a time of completing the task. 14. The process control system of claim 1, wherein, if the no anomaly is determined, then the state of the HW is compared to a predetermined level of Human worker HW performance thresholds of the HW model, and if greater than, a HW peak performance threshold, indicating a peak performance by the HW to complete the task, then, the HP model is updated to model peak performance by the HW, and wherein the level of the HW performance is determined by extracting data from the received HW signals, such as an adherence of the HW measurements to the learned statistical models, a degradation of model performance, or a specific learning model that is used to predict the obtained state of the HW. 15. The process control system of claim 1, wherein the HW signals includes data for each completed training task including one or a combination of, patterns of movements by the HW, an energy level of the HW, a skill level associated with a set of HW skill levels, and historical levels of states of the HW corresponding to a performance matrix associated with each completed training task label. 16. The process control system of claim 1, wherein some of the previously learned boundaries of different types of anomalies and no anomalies learned from the Human data by the HP model include a HW that is no longer working, a distracted HW, a HW experiencing a level of energy indicating the HW is tired or underperforming according to the previously learned boundaries, or a HW experiencing a level of energy indicating the HW is energetic or performing at a high energy level according to the previously learned boundaries, or a HW experiencing a level of energy indicating the HW is not tired or energetic such as an average energetic level, or performing at a level of energy associated with an average HW performance according to the previously learned boundaries. 17. A method for a process control system for detecting an anomaly in a discrete manufacturing DMP with human-robot teams executing at least one task within the process, the method comprising steps of: receiving test signals that includes DMP signals from DMP sensors that includes robot operational signals, and human worker HW signals from human worker HW sensors corresponding to a human worker HW, the HW signals including a time series of measurements of tasks completed by the HW; extracting a predicted sequence of events from the DMP signals and determine whether the predicted sequence of events in the DMP signals is inconsistent with a behavior of operation of the DMP described in a DMP model, and if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled; extracting from the HW signals, a task completion time, a task name, measurements relating to a state of the HW and a next predicted sequenced task, and input into a Human Performance HP model, to obtain a state of the HW based on previously learned boundaries of the state of the HW, the state of the HW is then inputted into a Human-Robot Interaction HRI model, such that outputs of a Human Worker HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of an anomaly or no anomaly, wherein the HP model for the HW is previously configured to have learned different states of human worker HW performance that correspond as a set of boundaries in the human data, and wherein the DMP model is previously configured to have learned different the operation of the manufacturing process that is used to assist in issuing classifications of anomalies or no anomaly detection; updating the HRI model with the robot operation signals, the HW signals and the classified anomaly; determining a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly; and outputting the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly, wherein the steps are implemented by a hardware processor communicatively coupled with a memory. 18. The method of claim 17, wherein an event transition table specifies discovered positional relationships between pairs of events from training data of the behavior, if the predicted sequence of events from the test signals is inconsistent with the behavior, then the alarm is to be signaled, and wherein previous DMP data, previous Human data and previous Human-Robot data are obtained prior to the receiving of the test signals and are stored in the memory. 19. The method of claim 17, wherein the type of anomaly alarm includes one or a combination of, a suspected assembly line mechanical failure, a suspected material supply problem to the assembly line, an under production problem due to the HW, a suspected robot related problem, an operator related task or a suspected electronic failure. 20. A non-transitory computer readable storage medium embodied thereon a program executable by a computer for performing a method for a process control system for detecting an anomaly in a discrete manufacturing process DMP with human-robot teams executing at least one task within the process, the method comprising steps of: receiving test signals that includes DMP signals from DMP sensors that includes robot operational signals, and human worker HW signals from human worker HW sensors corresponding to a human worker HW; extracting a predicted sequence of events from the DMP signals and determine whether the predicted sequence of events in the DMP signals is inconsistent with a behavior of operation of the DMP described in a DMP model, and if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled; extracting from the HW signals, a task completion time, measurements relating to a state of the HW and a next predicted sequenced task, and input into a Human Performance HP model, to obtain a state of the HW based on previously learned boundaries of the state of the HW, the state of the HW is then inputted into a Human-Robot Interaction HRI model, such that outputs of a Human Worker HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of anomaly or no anomaly, wherein the HP model is constructed from Human worker HW training signals of completed training tasks during a training phase prior to receiving the HW signals, such that the HW training signals includes data for each completed training task of the completed training tasks that includes a training task name, multiple training states of the HW for the completed training task, and a next sequenced training task, such that the HW training signals and the HW signals are acquired from sensors associated with the HW during a training operation or an operation of the DMP with the human-robot teams; updating the HRI model with the robot operation signals, the HW signals and the classified anomaly, then determine a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly; and outputting the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly, wherein the steps are implemented by a hardware processor connected to a memory. 21. A process control system for detecting an anomaly in a discrete manufacturing process DMP with human-robot teams executing at least one task within the process, comprising: a memory configured to store data including robot data, manufacturing process MP data, human data, and executable models; an input interface configured to receive test signals including DMP signals from DMP sensors that includes robot operational signals, and human worker HW signals from human worker HW sensors corresponding to a human worker HW; a hardware processor in communication with the memory and the input interface, the hardware processor configured to: extract a predicted sequence of events from the DMP signals, and determine whether the predicted sequence of events in the DMP signals is inconsistent with a behavior of operation of the DMP described in a DMP model, and if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled; extract from the HW signals, a task completion time, measurements relating to a state of the HW and a next predicted sequenced task, and input into a Human performance HP model, wherein the HP model is constructed by a Human Task Execution HTE model and a model of the state of the HW, wherein the HTE model is constructed using at least one predictive model trained using Human worker HW training signals obtained during a training phase completing a sequence of training tasks, and wherein the model of the state of the HW is constructed using at least one classification model trained using the HW training signals, such that each completed training task is associated with multiple states of the HW, and is stored in the memory, and wherein the HP model determines the state of the HW based on previously learned boundaries of the state of the HW, such that the state of the HW is inputted into a Human-Robot Interaction HRI model, and outputs from a human worker HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of anomaly or no anomaly; update the HRI model with the robot operation signals, the HW signals and the classified anomaly; determine a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly; and an output interface to output the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly.",
                    "FIELD_OF_INVENTION": "The present disclosure relates generally to systems and methods of model learning technologies, and more specifically to systems and designs of model learning technologies for a discrete manufacturing process where some steps in the process are executed by joint human-robot operation teams.",
                    "STATE_OF_THE_ART": "Conventional machine learning technologies can allow intelligent systems such as robots and personal assistants to acquire knowledge and solve difficult problems by learning from examples or instruction. However, there are many difficulties with these conventional learning models to control robotic systems in the fact that very complex physical laws, called Rigid Body Dynamics RBD and only a crude representation of these physical laws, govern the motion of a robotic system are usually known. Moreover, measurements of physical quantities need to compute these laws, such as position, velocity and acceleration of each component of the robot, which often is unavailable. Sensors mounted on the robotic systems typically only measure a position component encoders, potentiometers, proximity sensors . . . , while velocity and acceleration are not measured. Some conventional machine-learning approaches tend to be from a subfield of computer vision, which poses a significant list of limitations in terms of applicability of a solution. For example, CN105389859A patent relates to a monitoring system for monitoring a sanitation worker at a working state. This method is based on intelligent lamppost applications that uses RFID and smart lampposts to monitor worker attendance. However, this these methods do not learn what the worker is doing and what the state of health of the worker might be. Further, monitoring and controlling safety and quality are very important in manufacturing, where fast and powerful machines can execute complex sequences of operations at very high speeds. Deviations from an intended sequence of operations or timing can degrade quality, waste raw materials, cause down times and broken equipment, decrease output. Danger to workers is a major concern. For this reason, extreme care must be taken to carefully design manufacturing processes to minimize unexpected events, and safeguards need to be designed into the production line, using a variety of sensors and emergency switches. Some types of manufacturing are process and discrete manufacturing. In process manufacturing, products are generally undifferentiated, for example oil, natural gas and salt. Discrete manufacturing produces distinct items, , automobiles, furniture, toys, and airplanes. A conventional approach to increasing the safety and minimizing the loss of material and output is to detect when a production line is operating abnormally, and stop the line if necessary in such cases. To implement this, the conventional approach is to use a description of normal operation of the production line in terms of ranges of measurable variables, for example temperature, pressure, etc. , defining an admissible operating region, and detecting operating points out of that region. This conventional method is common in process manufacturing industries, for example oil refining, where there is usually a good understanding of permissible ranges for physical variables, and quality metrics for the product quality are often defined directly in terms of these variables. However, the nature of the working process in discrete manufacturing is different from that in process manufacturing, and deviations from the normal working process can have very different characteristics. Discrete manufacturing includes a sequence of operations performed on work units, such as machining, soldering, assembling, etc. Anomalies can include incorrect execution of one or more of tasks, or an incorrect order of the tasks. Even in anomalous situations, often no physical variables, such as temperature or pressure are out of range, so direct monitoring of such variables cannot detect such anomalies reliably. Accordingly, there is a need to develop advanced technologies for learning systems that learn to characterize a discrete manufacturing process. In particular, there is a need to develop learning systems geared to discrete manufacturing processes whose sub-steps may be executed by joint human-robot teams. Here the learning system can operate at two levels, the first level is learning a method to optimize the process at a human-robot collaboration level that adjusts help that a robot can provide to a human worker, subject to a condition of the human worker. The second level, is at the system level that learns to detect anomalies in the total discrete manufacturing process given that some steps are executed by a robots and humans.",
                    "SUMMARY": [
                        "The present disclosure relates generally to systems and methods of model learning technologies, and more specifically to systems and designs of model learning technologies for a discreet manufacturing process with steps executed by joint human-robot teams. In particular, some systems and methods of the present disclosure overcome the conventional human-robot collaboration process problems by optimizing speed along a total manufacturing process, and optimizing the interaction between the human and robot to optimize speed and quality of the product. In addition, the systems and methods presented in this disclosure are capable of detecting anomalies in the total manufacturing process while taking into account the variation of conditions possible of the human worker. The total manufacturing process is a discrete manufacturing process DMP where some steps are performed by joint human-robot teams. Here anomalies in the DMP, such as assembly lines, can also be detected according to the present disclosure. During training, data can be acquired during normal operation of the DMP. The normal operation can be understood as a description of normal operation of a production line in terms of ranges of measurable variables, for example temperature, pressure, etc. , defining an admissible operating region, for a period of time without detection of abnormal operation out of the description of normal operation. Importantly, when some tasks in the process are executed by joint human-robot teams, normal operation includes work performed by a worker during the various possible physical states of that worker, ex. Healthy, tired, sick, sleepy, etc. Thus during normal operation of the DM, we observe also the normal range of operation of the worker. Such training data collection can be learned during test periods of a production line which allow for the determination of permissible ranges for physical variables, and quality metrics for product quality, among other measuring techniques, to determine the normal operation. Upon determining the normal operation of the production line, data obtained from sensors is obtained and processed to form a stream of discrete events. Wherein, an event relationship table for a normal operation is constructed from the events. Then, during on-line time processing, the table is used to detect anomalies in the operation of the DMP using data acquired in real-time. There are several advantages for developing the table. By non-limiting example, the table can represent the correct workflow or normal operation of the DMP. Second, in contrast to conventional modeling formalisms, such as finite automata, the table represents compactly and accurately multiple sub-processes that are performed in parallel. Such sub-processes can either be independent, or coordinated. Third, the table represents the coordination between multiple sub-processes. Some embodiments about the human-robot interaction can include learning the typical or normal operation of a human worker performance, i. e. determining the typical motions performed by the human worker and the levels of alertness or fatigue experienced by the worker. Learning a personalized task execution model, which depends on, or is influenced by, the state of the human worker, was found to have many benefits. A first realization is that recognizing changes and anomalies in task execution and the state of the human worker can provide a much earlier anomaly detection in the human-robot manufacturing process as compared to conventional methods. That is, task execution and human health of the worker can provide the early warning of an impending manufacturing process anomaly. A second realization included the recognition that understanding of the typical performance and state of the human worker means that this information can be used in the collaboration between the robot and the human itself such that the combined human/robot performance can be improved. These improvements stein from an optimization of the interaction of the robot with the human worker. As a non-limiting example, if an anomaly can be detected in the performance level either of the human worker, or a state of health of the work, based upon a threshold approach. Then, in an assembly line environment where a robot assists a human in completing a task, early detection of the human worker's anomaly can alert the robot to change a robot action in order to accommodate for the human worker's anomaly. For example, the robot can slow its actions to match those of a tired human worker. Alternate examples of robot actions could include performing additional tasks, calling a supervisor for help, holding parts closer to the human worker at the cost of additional time, improving the comfort of the immediate area by adding light, heating/cooling, etc. , and others. This can be understood that the process control system can be designed to optimize the process at the human-robot collaboration level, by adjusting the help that the robot is providing to the human worker subject to the condition of the worker. In addition, the process control system knows a holistic description of the human/robot system; it is also capable of detecting future anomalies in a total manufacturing process. For example, during on-line time processing of the DMP, the table can be used to detect future/upcoming anomalies while the DMP is operating in real-time. The use of the table can lead to optimizing the speed of the total manufacturing process, correct workflow issues, as well as a quality of the manufactured product. Some aspects anticipated in mixed Human-Robot assembly processes, are that it is insufficient to learn discrete anomaly models for the process, models of human behavior, and models of robot manipulation, independently. The realization here is that a system must be designed to utilize these components to jointly optimize the speed of the process and the quality of the product. Thus, learning human behavior to learn the precise method of work performed by the human is an important component in designing a process control system. To learn the method of work of the human worker, the worker must be outfitted with sensors that collect information about the ongoing work process to be described below. In addition, the information collected from the human worker ex. Arm motion can be augmented by information in the process control system, by collecting task information such as task label assembly, inspection, paint, stitch, , expected task duration typical duration, specified duration, and human worker skill level can be expressed in years of employment. Together these features can be recorded in a training database and used to learn a model of the human performance. Some embodiments of the present disclosure can learn human performance via machine learning methods that capture a description of the human worker performance in a statistical model. Here two types of models may be utilized: predictive models and classification models. Predictive models can be used to learn expected completion time, effectively capturing the patterns observed in sensor data that indicate how a human worker is performing a task. Classification models can be used to learn human worker state ex. Energetic, tired, slow, or task performed assembly, inspection, paint, stitch, . Some embodiments of the present disclosure summarize the knowledge gained from the predictive models and classification models by creating anomaly detection mechanisms and predicting task completion times. For example, when multiple concurrent processes are taking place, using event relationship tables it is possible to generate sequences of possible steps in the human-robot manufacturing process. This information characterizes the human-robot manufacturing process. For a fixed execution time, these tables allow us to generate the exact sequence of normal events in a human-robot manufacturing process. Importantly the sequence of events is deterministic with each event following a single prior event. However, if one or more events have variable completion times, then multiple events may follow any single event. Here a number of possible event sequences can be generated to represent normal completions of the DMP. Using the predicted task completion time, and the event transition tables, or generated complete sequences of a DMP, it is now possible to anticipate an anomaly in the process based on the current state of the worker and the human-worker collaboration. In other words, given the current predicted completion time, the sequence of events that will occur either do not complete the DMP in the event transition table or is not a member of the possible normal sequences of operation of the DMP. For example, when using these anomaly detection tables and the predicted task completion time it is possible to anticipate an anomaly that will occur in the process. This occurs when a sequence of events in the future does not match a valid sequence in the anomaly detection table. Suppose the human robot team is predicting a normal completion time of the task. Then the event sequences generated from this moment until the completion of the process using the event transition table will be complete. However, suppose now that the predicted completion time of a task is too long. In this case the event sequences generated using the event transition table will be incomplete. This means that the long event duration led to an illegal transition from one event to the next in the event transition table, and thus the process was not completed. By this manner, being unable to generate a complete manufacturing event sequence, we know that there is an upcoming anomaly in the DM and that it stems from the joint human-robot task. Detection of an immediate problem with the human worker can be completed using the classification algorithms. Wherein the classification algorithm can determine a current task that is performed by the human worker and the next worker task that is extracted from current human worker signals. The classification algorithm i. e. which is part of a Human Performance model determines the state of the human worker. Then, based on previously learned boundaries of different types of anomalies and no anomalies learned from human worker data by the classification algorithm along with the state of the human worker, obtains an anomaly classification or no anomaly. A Human-Robot Interaction HRI model is updated with current robot operation signals, the current human worker signals and the classified anomaly. Wherein a control action of a robot interacting with the human worker or a type of an anomaly alarm using the updated HRI model and the classified anomaly can be determined. Here either an anomaly alarm can be given or an action of the robot helper can be altered to improve the human worker performance. The robot can then learn to alter its behavior by being provided examples of correct behavior given a level of human performance, including in real-time. Importantly, this anomaly alarm can also be triggered ahead of time, before the anomaly occurs because predictions issued by a human performance model can generate future sequences of task executions. As will be discussed later, these examples can be explicitly provided or discovered during a learning procedure. Another optional approach may include using the classification algorithm to compare the predicted task and action to an expected task and action from the control system pre-trained and stored in a memory, and a decision can be rendered on the state of the human worker. Another aspect of developing the systems and methods of the present disclosure can include combining the robot learning with human performance monitoring. Experimentation included better understanding of the interaction between the robot and the human during the human-robot manufacturing process. At least one realization was to learn this interaction using statistical models machine learning which were capable of making inferences and reacting to the ongoing interaction between the robot and the human operator. To learn these models data was collected from built-in sensors on the robots and from external sensors like cameras. The external sensors can be the same sensors previously used to learn the characterization of the human collaborator, by non-limiting example. The learned model can then use to determine a control method of the robot, which is capable of interacting with the Human collaborator during his/her unique physical states Energized, tired, slow, . Importantly, this collaboration is learned with the goal of completing the human-robot manufacturing task and maximizing product quality. For example, the robot can learn to alter its behavior by being provided examples of correct behavior given a level of human performance. As will be discussed later, these examples can be explicitly provided or discovered during a learning procedure. According to an embodiment of the present disclosure, a process control system for detecting an anomaly in a discrete manufacturing process DMP with human-robot teams executing at least one task within the process. The process control system including a memory configured to store data including robot data, manufacturing process MP data, human data, and executable models. An input interface configured to receive signals including DMP signals from DMP sensors that includes robot operational signals, and human worker HW signals from HW sensors. A hardware processor in communication with the memory and input interface is configured to extract a predicted sequence of events from the DMP signals. Determine whether the predicted sequence of events in the DMP signals is inconsistent with a behavior of operation of the DMP described in a DMP model, and if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled. Extract from the HW signals, a task completion time, measurements relating to a state of the HW and a next predicted sequenced task, and input into a Human performance HP model. The HP model determines the state of the HW based on previously learned boundaries of the state of the HW, the human state is then inputted into a Human-Robot Interaction HRI model, and outputs from the HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of anomaly or no anomaly. Update the HRI model with the robot operation signals, the HW signals and the classified anomaly. Determine a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly. An output interface to output the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly. According to an embodiment of the present disclosure, a method for a process control system for detecting an anomaly in a discrete manufacturing DMP with human-robot teams executing at least one task within the process. The method can include steps of: receiving test signals having DMP signals from DMP sensors that includes robot operational signals, and human worker HW signals from HW sensors including a time series of measurements of tasks completed by the HW. Predicting a sequence of events from the DMP signals and an expected human-robot team completion time. Determining whether this predicted sequence of events in the DMP signals is inconsistent with a behavior of an operation of the DMP as described in a DMP model. Such that, if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled. Extracting from the HW signals, a task completion time, a task name, measurements relating to a state of the HW performance and a next predicted sequenced task, and input into a Human Performance HP model obtained from previous Human data, to obtain a state of the HW performance based on previously learned boundaries of the state of the HW; the state of the HW is then input into a Human-Robot Interaction HRI model, and outputs from the HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of anomaly or no anomaly. Updating the HRI model obtained from previous Human-Robot data with the robot operation signals, the HW signals and the classified anomaly. Then, determines a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly. Outputting the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly. Wherein the steps are implemented by a hardware processor connected to a memory. According to an embodiment of the present disclosure, a non-transitory computer readable storage medium embodied thereon a program executable by a computer for performing a method for a process control system for detecting an anomaly in a discrete manufacturing process DMP with human-robot teams executing at least one task within the process. The method having some steps of receiving test signals that includes DMP signals from DMP sensors that includes robot operational signals, and human worker HW signals from HW sensors including a time series of measurements of tasks completed by the HW. Predicting a sequence of events from the DMP signals and an expected human-robot team completion time. Determining whether this predicted sequence of events in the DMP signals is inconsistent with a behavior of an operation of the DMP described in a DMP model. Such that, if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled. Extracting from the HW signals, a task completion time, measurements relating to a state of the HW and a next predicted sequenced task, and input into a Human Performance HP model obtained from previous Human data, to obtain a state of the HW based on previously learned boundaries of the state of the HW; the state of the HW is then inputted into a Human-Robot Interaction HRI model, and outputs from the HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of anomaly or no anomaly. Updating the HRI model obtained from previous Human-Robot data with the robot operation signals, the HW signals and the classified anomaly. Then, determine a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly. Outputting the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly. Wherein the steps are implemented by a hardware processor connected to a memory.",
                        "The presently disclosed embodiments are explained with reference to the attached drawings. The drawings are not necessarily to scale, with emphasis instead placed upon illustrating the principles of the presently disclosed embodiments. 1A is a block diagram illustrating a method, according to an embodiment of the present disclosure; 1B is a schematic illustrating some components used for implementing the method of 1A, according to some embodiments of the present disclosure; 1C is a schematic diagram illustrating a robot system and a human worker executing a task in an assembly line of a discrete manufacturing process, according to some embodiments of the present disclosure; 2A is a schematic illustrating a diagram of a method for detecting anomalies in a discrete manufacturing process sDMP, according to some embodiments of the present disclosure; 2B is a table illustrating a diagram of a log-based ordering relationship table, according to some embodiments of the present disclosure; and 2C is a schematic illustrating a diagram of a method for detects anomalies from the event sequences by determining consistency between the sequence and the model, according to some embodiments of the present disclosure. 3A is a schematic diagram illustrating data collection from the sensors associated with the human worker to obtain training data to be stored in a training database, and after the collection of the training data, obtain current human worker data, according to some embodiments of the present disclosure; 3B is a schematic diagram illustrating some data collection from sensors that monitor human worker performance tasks, according to some embodiments of the present disclosure; 3C is a block diagram illustrating some learning process steps of a model of human worker performance, and then doing process calibration for the model of human worker performance, according to some embodiments of the present disclosure; 4A is a block diagram illustrating some process steps of a model of human worker performance, and then exploiting the model of human worker performance, according to some embodiments of the present disclosure; 4B is a block diagram illustrating some learning process steps of the model of human worker performance of 4A, and then some process steps for exploiting the model of human worker performance of 4A, according to some embodiments of the present disclosure; 5A is a schematic diagram illustrating a robot learning-control apparatus including a Robot Model Learning Program, , Derivative-Free Semi-Parametric Gaussian Process SPGP learning model, connected to a robotic system, according to some embodiments of the present disclosure; 5B is a block diagram illustrating a flowchart describing the robot model learning program, i. e. a derivative-free Semi-Parametric Gaussian Process SPGP learning model of 5A, that initiates the robot policy to start the learning process of the robot system for a predetermined period of time, according to some embodiments of the present disclosure; 6A is a schematic diagram illustrating interaction between the robot and the human during the human-robot manufacturing process, according to some embodiments of the present disclosure; 6B is a block diagram illustrating combining the human model and the robot model into a joint model, according to some embodiments of the present disclosure; 6C is a block diagram illustrating combining the human model and the robot model into a joint model, which includes training the human model using data collected from the human worker, according to some embodiments of the present disclosure; 6D is a block diagram illustrating learning an individual model for both the human and robot, then learns a joint model that is used to improve the robot policy and finally, this robot policy can be used during the operations; 7A is a block diagram illustrating some sensing data including biometric and other data that can be collected and be used in assisting the process control system in detecting anomalies, according to some embodiments of the present disclosure; and 7B is a schematic illustrating a wrist device, according to some embodiments of the present disclosure. While the above-identified drawings set forth presently disclosed embodiments, other embodiments are also contemplated, as noted in the discussion. This disclosure presents illustrative embodiments by way of representation and not limitation. Those skilled in the art can devise numerous other modifications and embodiments, which fall within the scope and spirit of the principles of the presently disclosed embodiments."
                    ],
                    "DESCRIPTION": "The present disclosure relates generally to systems and methods of model learning technologies, and more specifically to systems and designs of model learning technologies for joint human-robot manufacturing process. 1A is a block diagram of a method, according to an embodiment of the present disclosure, that includes a process control system for detecting an anomaly in a discrete manufacturing DMP with human-robot teams executing at least one task within the process. The process control method can detect anomalies in an execution of a task in a sequence of tasks in a discrete manufacturing process DMP where some tasks are performed by joint human-robot teams. Some of the steps include:Step 15A of 1A, receiving test signals that includes DMP signals from DMP sensors that includes robot operational signals and other DMP component signals, and human worker HW signals from HW sensors including a time series of measurements of tasks completed by the HW. Step 20A of 1A includes predicting a sequence of events from the DMP signals and an expected human-robot team completion time, and determining whether this predicted sequence of events in the DMP signals is inconsistent with a behavior of an operation of the DMP described in a DMP model, and, if the predicted sequence of events from the DMP signals is inconsistent with the behavior, then an alarm is to be signaled. Step 25A of 1A illustrates extracting from the HW signals, task data and measurements relating to a state of the HW, and input into a Human Performance HP model obtained from previous Human data, to obtain a state of the HW based on previously learned boundaries of the state of the HW, the state of the HW is then inputted into a Human-Robot Interaction HRI model, and outputs from the HW model, the HRI model, or both, are inputted into the DMP model to determine a classification of anomaly or no anomaly. Step 30A of 1A includes updating the HRI model obtained from previous Human-Robot data with the robot operation signals, the HW signals and the classified anomaly, then determine a control action of a robot interacting with the HW or a type of an anomaly alarm using the updated HRI model and the classified anomaly. The control action can include one or a combination of: a adjusting an amount of a robot speed according to the state of the human-worker; b adjusting a direction of the robot including one or a combination of an X-axis direction, Y-axis direction or Z-axis direction; or c initiating an audible voice command such as indicating a change of robot operation according to the control action. Contemplated is that other control actions may include maintenance related actions for the robot, safety related actions to both the human and the robot, as well as diagnostic related actions for the robot. Further, some classifications of anomaly detections can be associated with robot actions such as levels of speed of the robot, movements of the robot, voice announcements, making calls, maintaining robot positions for one or more periods of time, adjusting environmental conditions via commands sent to a controller. Other classifications of anomaly detections can include types of anomalies such as detection of future anomalies, maintenance related anomalies, safety related anomalies, lost production anomalies, failure of components anomalies, quality anomalies and assembly line anomalies. Step 35A of 1A includes outputting the control action of the robot to change a robot action, or output the type of the anomaly alarm to a management system of the DMP, based on the updated HRI model and the classified anomaly, wherein the steps are implemented by a hardware processor connected to a memory. Also contemplated is outputting the signaled alarm upon receiving the determination that the sequence of events is inconsistent with the behavior of the operation to a management system of the mixed human-robot processes. Some types of anomaly alarms can be anomaly alarms intended for immediate actions, however, more importantly, this anomaly alarm can also be triggered ahead of time, before a more major or detrimental anomaly occurs because predictions can be discovered by the human performance model, Human-Robot Interaction model and the DMP model, can generate future sequences of task executions. Some examples of the immediate or future anomaly alarms can include one or a combination of, a suspected assembly line mechanical failure, a suspected material supply problem to the assembly line, an under production problem due to the HW, a suspected robot related problem, an operator related task or a suspected electronic failure. According to some embodiments of the present disclosure, some advantages of the systems and methods of the present disclosure overcome the conventional human-robot collaboration process problems by optimizing speed along the total manufacturing process, and optimizing the interaction between the human and robot to optimize speed and quality of the product. This can be accomplished by optimizing the process at the human-robot collaboration level by adjusting the help that the robot is providing to the human worker subject to the condition of the worker. In addition, the embodiments presented in this disclosure provide a forward looking anomaly detection in the total manufacturing process which was not previously possible without learning a detailed model of the human worker. 1B is a schematic illustrating some components used for implementing the method of 1A, according to some embodiments of the present disclosure. For example, 1B can be used for implementing embodiments of the robot-human models combined together. 1B includes a process control system 100 used for controlling a robotic system 58 via bus 55, that may include a network interface controller NIC 51 adapted to connect through a bus 56 to a network 57. Wherein data can be communicated to and/or from the robotic system 58, i. e. robotic systems, and data 59 including input measurements such as those used to monitor the human worker and other possible data. A memory 140 can be used to store computer-executable programs in a storage 130 including a data preprocess program 131, a statistical model learning program for human and robot models 134, a Control Program based on the statistical models 135, an initialization program to initialize the learning of the statistical models 137, and a processor 120 or more than one processor, in connection with the memory 140. Also, stored in the storage 130 can be object state history data not shown and robot state history data not shown. There can be components including an input/output interface 80, control interface 82 connected to devices 84 via bus 83. Some optional components of the process control system can include a human machine interface HMI 60 connected via bus 61 to a keyboard 62 and bus 63 to pointing device/medium 64. Other optional components can include a display interface 66 connected via bus 73 to display device 67, imaging interface 68 connected via bus 74 to imaging device 69, printer interface 71 connected via bus 75 to printing device 72. 1C is a schematic diagram illustrating a robot system and a human worker in an assembly line of a discrete manufacturing process performing a discrete task, according to embodiments of the present disclosure. In later figures, a model of human performance, a robot learning process and other processes will be explained in detail. Contemplated is that the assembly line includes multiple stations and at each station includes at least one task to be completed by at least one robot that assists at least one human worker to complete the at least one task. The robot system 90 includes a controller 91, a robot state detector for example a positional encoder 93, wherein the positional encoder 93 can produce robot state signals 92. The robot system 90 can also include an object state detector for example a camera 94, wherein the camera 94 can produce object state signals of an object 95 to be manipulated by the robot system 90 in a workspace or conveyor 12 of a worktable 11. Wherein the robot system 90 assists at least one human worker 13 in completing at least one task on the worktable 11, such that the workspace or conveyor is capable of moving in a forward direction and a reverse direction in order to assist either the robot or human worker in completing the task. Note that these components 11-12 and 90-94 are here represented as an example but they might vary for different applications since the embodiment of the present disclosure is robust to different applications. In addition, the robot operational data can optionally, depending upon a user specific interest, be sent or received wirelessly to a robot learning process 101. Still referring to 1C, a human worker 13 can have sensors for gathering data including a wrist device 104, motion monitors 102, on-body motion sensors 106A, 106B, time devices 108 and environmental sensors 109. The data from these devices facilitates the learning process for the models specific to the human worker. These sensors all together are an example of the human state detector. The human states are considered together with the state of the object to be manipulated 95 and with the state of the robot system 90 to learn the control policy of the robot. Anomaly Detection in Discrete Manufacturing Processes DMP 2A shows a method for detecting anomalies in a discrete manufacturing process DMP 200 according to embodiments of the present disclosure. Note that this DMP does not have any human workers in the process. The embodiments of the present disclosure provide a method for detecting anomalies in discrete manufacturing processes DMP, such as assembly lines. During training, data are acquired during normal operation of the DMP. The data are processed to form a stream of discrete events. An event relationship table for a normal operation is constructed from the events. Then, during on-line time processing, the table is used to detect anomalies in the operation of the DMP using data acquired in real-time. The table has several advantages. First, the table can represent the correct workflow or normal operation of the DMP. Second, in contrast to conventional modeling formalisms, such as finite automata, the table represents compactly and accurately multiple sub-processes that are performed in parallel. Such sub-processes can be either independent, or coordinated. Third, the table represents the coordination between multiple sub-processes. Some embodiments include off-line training and real-time processing. The training can be a one-time preprocessing task. Alternatively, the training is done as needed, , to adapt to changing processing conditions. The method can be performed in a processing device connected to memory and input/output interfaces by buses as known in the art. The DMP includes bins 201-202, manufacturing robots 203-204, conveyors 205-206 and an assembler 207. During operation, the robots pick parts from the bin, and place the parts on the conveyer to be assembled. Training: During training, signals 209 are acquired from various sensors, switches, and the like used by the DMP. The sensors can be connected to the various operational components of the DMP, , the bins, robots, conveyer and assembler. A sequence of events 221 is extracted 210 from the signals. The events are stored in a training database 251. The events are used to build a relationship table 270 of log-based ordering relations between all pairs of observed events of the normal operation of the DMP based on the table 270. Detecting: The table is used to detect 230 anomalies in the signals 209 acquired in real-time by determining an anomaly score 231. If the score exceeds a predetermined threshold, then an alarm can be signaled 240. Machine learning: The present disclosure uses machine learning to construct a relationship table of the event sequence from data measurements. These measurements are obtained from factory devices, robots, and workers. Event sequence: The first step of the method is to acquire such signals from all devices, workers and robots in the DMP. Relationship table: In practice, the relationship table can be built by means of a simple and efficient two-step procedure. Initially, all entries in the relationship table are initialized with the symbol. During the first step, the database is scanned sequentially, and for any pair of events where one event immediately follows another event, the corresponding entry of the relationship table is changed to the &gt; symbol. After the first step, any two events can be either in the &gt; or the relations. During the second step, the derived relations , , , and # are computed on the basis of the relations &gt; and discovered during the first step, using the following rules: AB if A&gt;B and BA; AB if AB and B&gt;A; AB if A&gt;B and B&gt;A; and A #B if A B and BA. 2B is a table illustrating a diagram of an example log-based ordering relationship table, according to some embodiments of the present disclosure. It is noted that the anomaly detection is based on the relationship table, and not a complex complete mined model as in the prior art. The relationship table is not a model. Instead, the relationship table represents summary statistics of the data logs about the discovered positional relationships between pairs of events during normal operation of the DMP as represented by a log-based ordering relationship table. Task duration: The task duration is a difference between the time of the current task and the latest time among the events corresponding to all predecessor tasks of the current task in the event trace. For example, in an event trace contains events t1, Op2, t2, Op1, t3, Op3, with t1&lt;t2&lt;t3, then the duration of task Op3 is dOp3=t3-t2, because Op1 and Op2 are both predecessors of Op3 in the SWN, and Op1 occurred later at time t2, thus enabling the task Op3 at that time, with the corresponding event emitted t3-t2 time later. Note that the event corresponding to a given task can be emitted at the beginning, end, middle, or a random time during the task. When the event is always emitted at the end of each task, the collected task durations d actually represents the execution times of the respective tasks. In any of the other cases, the task duration only includes the time until the corresponding event is emitted, but may also include the time between one or more predecessor tasks emitted their events and actually were completed. Regardless of which case is present in a DMP, the task durations are an important property of the process and can be included in its model for the purposes of anomaly detection. Optionally, task durations for robot tasks can be included in the model in several ways. One way is to determine the minimal dminT and maximal dmaxT durations for the tasks, and record the durations with the task descriptor. Another way is to assume a specific parametric form for the probability distribution fTd over the possible durations d of task T, such as a Gaussian, Beta, or Weibull distribution, and determine the corresponding parameters for that distribution, , for example, mean and standard deviation for the Gaussian distribution. For simple operations, task duration can be approximated as the mean of the Gaussian distribution. Real-Time Anomaly Detection: 2C is a schematic illustrating a diagram of a method for detects anomalies from the event sequences by determining consistency between the sequence and the model, according to some embodiments of the present disclosure. The real-time processing has two stages: determining 210 whether the sequence of events 221 are consistent with normal operation as represented by the model; and determining 220 whether the task durations are consistent with the table. We note here that this is for the robot executed tasks only. It can be determined whether the sequence of events is consistent with the model, and if the sequence is inconsistent, a maximal anomaly score Smax 311 can be assigned to the sequence. Another method for verifying the correctness of the sequence of tasks is to compare the relative order of tasks, as they are observed in the event stream, with the entries of the log-based ordering relation table 270 constructed during the training phase. For example, if the sequence of events AB is observed, but the relation AB is present in the relation table, an anomaly can be signaled. Recall that AB signifies that A was never followed by B in the training database; if such behavior is observed currently, it is clearly inconsistent with the training data. Combining with Discrete Anomaly Detection MethodThe above method relies on event relationship tables that describe the sequence of events that occur. Critically, when multiple concurrent processes are taking place, these tables capture the timing information that describes the range of possible sequences in the manufacturing process. This information is the characterizing information that describes the manufacturing process. For a fixed execution time, these tables allow us to generate sequences of normal events in a manufacturing process. Importantly the sequence of events is deterministic with each event following a single prior event. However, if one or more events are variable, then multiple events may follow any single event. Using these anomaly detection tables and the predicted task completion time it is then possible to anticipate an anomaly that will occur in the process. This occurs when a sequence of events in the future does not match a valid sequence in the anomaly detection table. As an example, suppose the human robot team is predicting a normal completion time of the task. Then the event sequences generated from this moment until the completion of the process using the event transition table will be complete. However, suppose now that the predicted completion time of a task is too long. In this case the event sequences generated using the event transition table will be incomplete. This means that the long event duration led to an illegal transition from one event to the next in the event transition table, and thus the process was not completed. By this manner, being unable to generate a complete manufacturing event sequence, we know that there is an upcoming anomaly in the DM and that it stems from the joint human-robot task. The detection of an immediate problem with the worker can be completed using the classification algorithms. As an example, a procedure is shown in 2C. Here the classification algorithm determines the current task that is performed by the worker and the next worker task. Examples of such algorithms include worker gaze detection algorithms that can determine if the user is focused on his/her on-going task. The predicted task and action is compared to the expected task and action from the control system and a decision is rendered on the state of the worker. As an example, suppose that the control system knows, whether from the event transition table or by design, that the current task to be executed is welding. Suppose then that the classification algorithm is consistently at each time step determining that the worker is welding and that the prediction algorithm is predicting an ordinary completion time, then the worker must be healthy. On the other hand, suppose the classification system is vacillating in its decision between welding and cutting, and the prediction of the completion time is very long, then the worker must be tired. As more data is collected about the worker these states can be refined as well as related to the degradation of the statistical model performance. For example, initially the model will always determine that the worker is tired, but a model can be trained to take into account past decisions as well as the current state. Then as more data is collected, the model can determine that a worker is, for example, becoming tired vs has arrived at work tired. It is important to note that the detection of a problem with the worker can be treated as whole process anomaly or as an action given to the robot helper is altered to improve the worker performance. In fact, prior to declaring an anomaly on the DMP, the robot controller should have taken steps to aid the human worker. To this end, the human prediction/classification models can be used either: in the controller which combines the robot model and the human model, or as additional dimensions in the robot learning state space. The latter is an important realization because it facilitates automatic learning of the type of help provided by the robot. That is, the robot may learn specific actions to take in each of the human health states to improve the final product quality and speed of the manufacturing process. As noted above, 2C shows the process of learning the model of human performance and then exploiting the model of human performance. In the first step of learning the historical database of data collected using the approach shown in 2B can be accessed. Here the historical database can be used to learn both the predictive and the classification models. Learning Models of the Human Worker 3A is a schematic diagram illustrating data collection from the sensors associated with the human worker to obtain training data to be stored in a training database 303, and after the collection of the training data, obtain current human worker data 301, according to embodiments of the present disclosure. For example, the human worker 313 can have a sensor such as a wrist device 304 or some other human body sensor, i. e. attached to or embedded into, the worker 313, and can collect data such as body temperature, blood pressure, heart rate/pulse, breather rate, O2 saturation, skin conductance and skin temperature. Other sensors such as motion monitors 302, on-body motion sensors 306A, 306B, can collect biometric data such as behavioral identifiers such as physical movements, engagement patterns, physical movements, and physical identifiers such as photos and videos, physiological recognition, voice and boy attributes. Also other sensors can collect time stamp data via time devices 308 and environmental data from environmental sensors 307, such data can include air temperature, air velocity, humidity, air quality and radiant temperature. 3B is a schematic diagram illustrating some data collection from sensors that monitor worker performance tasks 311, according to embodiments of the present disclosure. Wherein the data collected can be used for learning a normal or typical operation method of a human worker along with a typical state of the worker. Some of the data collected is from sensors that monitor worker performance tasks that can include external sensors can be used to monitor motion 317, such as using gaze detectors, biometric data 315, environmental sensors 307, time stamps 308 and on-body motion sensors 306A, 306B. Also, included for worker performance task data 320 includes historical task process information, such as task information 322 that is previously stored in a memory 140 of 1B. The data collected for the process control system can be task labelling assembly, inspection, paint, stitch, , expected task durations typical duration, specified duration, and worker skill levels can be expressed in years of employment, by non-limiting example. The environmental data sensing may include an I/O module, which may include wireless communication components or an on-device user interface, a data processor or control module, a power supply that may be a removable or rechargeable battery, or a wireless power converter. The environmental data sensing may include one or more sensors that measure various characteristics of the environment, such as air temperature, air velocity, humidity, air quality 328 and/or radiant temperature. Additionally, sensors such as, but not limited to, turbulence and CO2 sensors are included in the environmental data sensing. The one or more sensors are located at the vicinity of the worker. Although indicated as separate items, it is contemplated that a single sensor of the environmental data sensing may measure more than one variable. For example, an omnidirectional anemometer may be used to measure air velocity as well as turbulence intensity. In another example, the radiant temperature may be determined based on data from an IR camera or by using a separate sensor, such as a glob thermometer. In some embodiments, the environmental data may include a model of the environment and distributions of variables of the model of the environment. The model of the environment includes location of windows and location of doors and walls and the variables of the model of the environment indicate whether the windows and the doors are open or closed. Further, the model of the environment includes a location and a type of a heat source computer, oven, workers, in the environment and the variables of the model of the environment indicate a state of the heat source. 3C is a block diagram illustrating some learning process steps of a model of human worker performance 380, and then exploiting the model of human worker performance, according to an embodiment of the present disclosure. The model of human worker performance 380 is learned via machine learning methods that capture a description of the worker performance in a statistical model. Wherein two types of models can be used, 1 predictive models and 2 classification models. Step 1 of 3C can be a learning process step for learning the model of human worker performance 380 by accessing the training database 303 that was collected using the approach shown in 3B. Wherein the training database 303 can be used to learn both a predictive model and a classification model of the model of human worker performance 380. The predictive models can be used to effectively learn via training data in the training database 303, operational methods performed by humans to learn expected completion times for different sub-tasks in the series of task, and to effectively capture patterns observed in sensor data that indicate how workers perform tasks in the series of tasks. The predictive models can be trained using historical past worker data i. e. collected prior to collecting current worker data. However, contemplated is that other workers having a similar profile as the worker being assessed, such training data may be collected. Some aspects of a similar profile of other workers can include all of the data obtained about the worker, including years of experience, age, education, physical body characteristics, health condition rating, etc. , by non-limiting example. Some collected task information can include task label assembly, inspection, paint, stitch, , expected task duration typical duration, specified duration, and worker skill level can be expressed in years of employment. Together these features are recorded in the training database 303 and used to assist in learning the model of the human performance. There are many types of predictive algorithms. For simplicity, we describe a linear regression predictive model. A linear regression model assumes a linear dependency between the outputs and the inputs: the inputs also called regressors are multiplied by a set of parameters and then summed up to estimate the output. As an example, suppose we have collected one multidimensional regressor point X=[x1, x2, x3, . . . , xN] where each xi with {i=1, . . . , N} represents one of the described variables such as worker heart rate, worker motion in the x,y, or z direction, room temperature etc. Then suppose that we have estimated a set of parameters, A=[a1,a2, . . . , aN], through machine learning or system identification techniques to predict the completion time of the current task, tc. Then in this case, the completion time can be predicted as fX which is determined as {circumflex over t}c={circumflex over f}X=a1x1+a2x2+ . . . . +aNxN. Still referring to Step 1 of 3C, the trained predictive models stored in the training database 303 can include levels of expected completion times for different tasks in the series of task, and patterns of how workers perform tasks in the series of tasks, in order to create thresholds relating to levels of performance that may be used to assist in anomaly detectionThe classification models can be used to learn the style of work performed by the human. Wherein the classification model, i. e. algorithm, can determine a current task that is performed by the worker and a next worker task, from training data stored in the training database 303. Examples of such algorithms include worker gaze detection algorithms that can determine if the user is focused on his/her on-going task. An example gaze detection algorithm learns a distribution of the worker gaze location x, y coordinates during the completion of a task. This distribution is assumed to be unique for each particular task. Then for a known task, the gaze of the worker can be input into this distribution and assigned a probability of belonging to the particular task. If the probability is low, the worker is may be fatigued, distracted, or simply taking a break. Still referring to Step 1 of 3C, other methods of tracking worker performance can be built using measurements from inertial measurement sensors, biological sensors, external monitors in the environment, time information, and external motion sensors such as gaze detectors, that may be used to obtain such information as the worker state ex. Energetic, tired, slow, or task performed assembly, inspection, paint, stitch, . As an example of a classification model, we use a Gaussian distribution. This distribution is parametrized using mean and variance. Specifically, we want to learn a Gaussian distribution of the worker Gaze. Here because worker Gaze is tracked in a plane, the learned distribution is 2 dimensional with a mean and a covariance matrix of size 22, . Model learning comprises of estimating the mean and the covariance matrix from stored gaze data. Then at every point x, y we can determine the value of the distribution, fx,y, |, . To classify if a worker is paying attention to the task, each new gaze location, xn, yn, is input into the distribution fxn,yn|, . A high probability point, p, means that the worker is working on the assigned task. A low probability means that the worker is distracted, tired, taking a break, or working on a different task. Used in this fashion the classification model determines if task completion is normal. However, this approach can also be used to distinguish between tasks. Suppose we have learned a distribution for each task. Then the distribution which yields the highest probability determines task belongingi. e. classification of the ongoing task. The trained classification models stored in the training database 303 can include levels of health/alertness of the human workers, in order to create thresholds relating levels of health of workers that may be used to assist in anomaly detection. For example, using the example above, different distributions could be learned for alertness levels of the worker. Here a healthy alert worker might have a narrow Gaussian distribution while a tired worker might have a broad Gaussian distribution. For a known task, comparing these different distributions reveals the alertness level of the worker. For a single distribution, it might be possible to track the change in the covariance matrix to determine changes in the worker. In general, worker health either can be explicitly tracked using levels of health, or implicitly tracked by studying the changes in predictions/classifications of the known models. Still referring to Step 1 of 3C, once the model of human worker performance or performance model 380 is learned, the performance model 380 is continually updated in order to learn good human worker performance. For example, the performance model is updated when peak performance is observed by the worker. Here peak performance is interpreted in context of the desired application, for example in the scene depicted in 1C the worker and the robot are collaboratively working on an assembly line. Here peak performance might be the desired products per hour, the desired faulty rate per hour, or a combination thereof. When these conditions are met, the data collected in essence describes method, actions, and state of the worker that produces the desired peak performance. Noted, is that the worker is assumed capable of achieving the target performance rate. This approach will let the performance model 380 develop precise models of good performance and thus precise detection of deteriorating performance. At least one aspect of understanding the typical performance and state of the worker can be that the collaboration between a robot and a human itself may be in identifying changes in robot actions or human worker action, that further improve performance, i. e. for robot, human worker, task completion time, improve an overall speed and quality of the assembly line or product produced. Wherein the typical normal performance is defined as meeting a desired product output or fault rate threshold as per the example above. An example of adjusting robot performance to the human state, thereby linking the model of human performance to the robot task learning, is adjusting robot speed according to the human state. Here the robot has learned a set of rules that map the human state to a speed. For example, suppose the robot has learned that an energetic worker performs the task at speed A, a tired worked performs the task at speed 0. 1A and a sick worker performs the task at speed 0. 5A. Then, in real time using the prediction algorithms, the robot can choose which speed to select given the determined worker state. Noted, is that the robot can move in any direction, speed, etc. to adapt to human performance current data per task. Step 2 of 3C describes this learning of the peak performance of the worker. Here data is continuously collected from the monitoring sensors 319, the wearable sensors 304, and the known task information 320. In addition to this data, there is an available database 303 of prior data that was collected from this worker. When the observed worker performance as desired in the particular application reaches the target performance, then the data is saved to the training database. Similar databases can be created for other conditions of the worker such as tired, sick, unhappy, etc. The data from this these databases 303 is then used to find the classification and predictions models discussed earlier. The collection of this data is very important because it provides the context needed for anomaly detection and process monitoring which will be described in the upcoming steps. Step 3 of 3C can be thought of as the exploitation of the statistical models learned in Step 1 above. Here the classification model can be used for immediate anomaly detection 384, which can detect quickly if a worker's performance is deviating from normal. In addition, as discussed the changes in the classification model can be used to observe deterioration in the worker before a real anomaly is detected. The prediction model can be used to determine when the worker will complete his/her current task. In addition to providing an anomaly detection 384 capability, this also facilitates anomaly detection in the entire process. This is because this information can be used to generate a sequence of events that will happen from this point forward using event transition tables. The anomaly detection 384 and process calibration 386 are then part of Step 4 of the process. Step 4 of 3C can be thought of as the model exploitation step. Here we leverage the models learned 380 using the stored data 303 and the currently collected data 304, 319, 320. Many anomaly detection mechanisms can be constructed using this experimental setup. One example is directly using the completion time prediction model to determine if a worker is performing well or anomalously. A more complex example is using the completion time in an event transition table and generating sequences of possible manufacturing actions given the predicted task completion time. In each case, the models can directly send an alarm about an immediate anomaly. However, a subtle realization is that the anomaly models can also be used for process calibration 386. For example, if the Gaussian method of gaze detection describe above is showing a growing covariance in the gaze plane, it may be possible to change the manufacturing process to improve speed and quality. For example, a robot could be trained to slow its actions to match the worker actions thereby reducing errors. Alternatively, a robot could be trained to bring the parts closer to the worker to improve focus. Overall, the series of manufacturing events might be rerouted such that other workers replace some of the load observed by the current worker. This means that the statistical models monitoring the worker can also be used to improve the human-robot manufacturing process adaptively with respect to the worker condition. 4A is a block diagram illustrating some process steps of a model of human worker performance, and then exploiting the model of human worker performance, according to some embodiments of the present disclosure. This figure specifically describes this learning of the peak performance of the worker. Here there is an available database 400 of prior data that was collected from this worker. The stored data, because of its method of collection specifically captures the performance of a worker when they are performing well. Separately, data could also be annotated for other conditions of the worker such as tired, sick, unhappy, etc. The data from this database 400 is then used to find the classification and predictions models. These models are determined in Steps 402 and 402 and can be thought of as the prediction and classification models previously described. The models can then be updated when the corresponding worker condition is detected and new data is collected 404 as previously described. 4B is a block diagram illustrating some steps of using the learned model of human worker performance of 4A in event transition tables. These tables describe the normal sequence of events in the overall human-robot manufacturing process and are a computationally efficient method of anomaly detection. In 4B, the historical database 400 is used to train classification 401 and prediction 402 models. The classification model can then be used to learn the event transition table because it predicts the event, which is currently taking place. In essence, the model helps to detect the transition of the worker from one task to the next. The prediction model can be used to augment the event transition tables by learning a distribution of the task completion times. The event transition table can be updated in a similar fashion to models based on application specific conditions. For example, if a worker has achieved the desired product output at a desired fault rate, the typical sequences of manufacture can be created and the event transition table updated. Transition tables can also be created for different worker states. 5A is a schematic diagram illustrating a robot learning-control apparatus including Robot Model Learning Program, , Derivative-Free Semi-Parametric Gaussian Process SPGP learning model, connected to a robotic system, according to some embodiments of the present disclosure. The robot learning is also an integral part of the present disclosure, which made possible by a learning system for a robot. Contemplated is that other systems can be used such as different types of controllers from different types of a machine learning state of the art approaches using other Model Based Reinforcement Learning techniques, or even from classical control state of the art approaches where the model is learned from data using, for example, Gaussian Processes or Neural Networks, where a control law could be given for example by a Model Predictive Control, by non-limiting example, according to some embodiments of the present disclosure. The components 590, 591, 595 and the worktable 511 define an example of an application to which some of the embodiments of the present disclosure can be applied. Continuing with the robot learning system of 5A, a robot learning-control apparatus 500 is used for controlling a robot system, and transmits to the robot controller 591 via interface 550, an initialized and updated robot policy. The robot controller 591 implements the initialized and updated robot policy to execute the task on the object 595. Further, robot state signals 592 of the robot system 590 detected by at least one robot state detector, i. e. a positional encoder 593, object state signals of the object 595 detected by the object state detector, and the initialized robot policy are sent to the Data preprocessing program 531, where the signals are preprocessed and stored in memory as robot states history 532 and object states history 533. Contemplated is that the object state signals can be detected by at least one object detector for example a camera 594. These components 590, 591, 595, 594, 593 are here represented as an example but they may vary depending upon the specific user application or different task related applications, according to some embodiments of the present disclosure. Still referring to 5A, in the beginning of the learning training process an initial robot policy 537 may move the robot in an exploratory fashion for a predetermined period of time be sent to the robot system 590 using the interface 550. The initial robot policy is an arbitrary signal, which could be for example a sinusoidal signal, a sum of sinusoids or a random signal. During the predetermined period of time, the data collected are the object state and the robot system state detected by 594 and 593, respectively, that the input/output interface 550 sends to the data-preprocessing program 531. In the data-preprocessing program 531, the data are subject to some preprocessing, then then stored in a memory as the robot state history 532 and as the object state history 533, these quantities at each time step contain a finite history of the past positions of the robot and of the object, respectively. The Robot Model-learning program 534, can be for example the Derivative-free SPGP DF-SPGP which takes as an input the robot states history 532, the object states history 533 and the initial robot policy. In performing the DF-SPGP Model learning program 534, the Derivative-free SPGP DF-SPGP kernel learning program not shown and the Derivative-free SPGP Model learning program are trained. The Derivative-free SPGP model obtained in 534 together with the task specification 536 of the task that the robot has to compute on the objects 595 are used to compute the updated robot policy in 535. In 535, the robot policy can be for example, the Iterative Linear Quadratic Gaussian iLQG, but it could be replaced with any trajectory optimization technique model-based. Once the updated robot policy is learned in 535 this can be sent to the robot system via the input/output interface 550 and the controller 591. The robot system 590 performs now the task on the object 595. The Derivative-free SPGP DF-SPGP Model-learning program in 534 and the policy algorithm Iterative Linear Quadratic Gaussian iLQG in 535 are only an example that has been shown to be successful for robot manipulation. Accordingly, to some embodiments of the present disclosure, the model-learning program 534 and the policy computation 535 are not restricted to be the one illustrated here. The model 534 could be for example a standard Gaussian process, a deep neural network or any other function approximators for the forward dynamics. The policy 535 could also be any other model-based controller such as Model Predictive Control. The component 534 and 535 could also be combined together to determine a policy without a specific model, using therefor model-free policy algorithms such as PID controllers or model-free reinforcement learning algorithms. 5B is a block diagram illustrating a flowchart describing the robot model learning program, i. e. a derivative-free Semi-Parametric Gaussian Process SPGP learning model of 5A, that initiates the robot policy to start the learning process of the robot system for a predetermined period of time, according to embodiments of the present disclosure. Referring to 5B and 5A, step 1 of 5B computes and initializes the robot policy to initiate the learning process of the robot system. Step 2 of 5B transmits the initialized robot policy via a control signal to the robot system 590 of 5A, via the interface 550 of 5A, which sends the control signal to the controller 591 of 5A, which makes the robot system 590 of 5A move accordingly to the initialized robot policy in the next step step 3. Step 3 of 5B receives the control signal which is used by the robot system 590 of 5A to manipulate the object 595 of 5A for the predetermined period of time. Step 4 of 5B collects the robot states signal 592 of 5A and the object states signals from the object state-detector 594 of 5A and sends the signals to the robot learning-control apparatus 500 of 5A via the interface 550 of 5A. Step 5 of 5B receives and sends the signals along with the initialized robot policy to the Data preprocessing program 531 of 5A, where the data are preprocessed and stored in memory as robot states history 532 of 5A and object states history 533 of 5A. Step 6 of 5B updates the Derivative-free SPGP learning model 534 of 5A with the received data of the previous step step 5. Step 7 of 5B defines a task 536 of 5A that the robot system should compute on the manipulated object 595 of 5A. Step 8 of 5B uses the defined task together with the derivative-free SPGP learning model to update the robot policy using iLQG 535 of 5A. Step 9 of 5B the updated robot policy is then transmitted to the robot system 590 of 5A using the interface 550 of 5A that is connected to the controller of 591 5A. Step 10 of 5B executes the updated robot policy via robot system 590 of 5A which performs the task manipulating of the object 594 of 5A accordingly to the updated policy 535 of 5A obtained using the Derivative-free SPGP learning model 534 of 5A. Referring to 5A, according to aspects of the present disclosure a model learning system as for example the derivative-free model learning program described above can be arranged in a robot learning-control apparatus that can be arranged within the process control system 100 of 1B, or arranged as a separate unit that can include at least the components of the process control systems of 1B, according to embodiments of the present disclosure. The robot learning-control apparatus can be used for controlling the robot system 590 and configured to transmit an initial and updated policy program to the robot system 590, via interface 550. Wherein the robot state signals 592 of the robot system 590 and object state signals are received with respect to the object 595 to be manipulated by the robot system 590 on a worktable 511. The object state signals are detected by at least one object detector 594, and the memory 140 of 1B can store computer-executable programs in the storage 130 of 1B including a data preprocess program 131, object state history data 133, robot state history data 132, a statistical model learning program 134, an update-policy program 135, an initial policy program 137, a robot states history 132 and an object states history 133, and a processor 120 or more than one processor, in connection with the memory 140 of 1B. The processor 120 of 1B is configured to transmit the initial policy program 137 of 1B to the robot system 195 of 1B via a network 57 for initiating a learning process that operates the robot system manipulating the object while a preset period of time. In this case, a processor can update the Model learning program 534 according to the object state history data and the robot state history data converted, using the data preprocess program 531, from sets of the robot state and object state signals having received in the preset period of time, wherein the processor updates the update-policy program 535 according to the updated Model learning program 534. Combining the Robot Learning with Human Performance Monitoring 6A is a schematic diagram illustrating interaction between the robot and the human during the human-robot manufacturing process, according to some embodiments of the present disclosure. To learn this interaction statistical models are used and are based on machine learning capable of making inferences and reacting to the ongoing interaction between the robot and the human operator. To learn these models, we collect data from built-in sensors on the robots from external sensors like cameras. The external sensors can be the same sensors previously used to learn the characterization of the human collaborator, for example, or other external sensors located in proximity to the robot system. Wherein the learned model can be a combined representation of the robot and of the human collaborator. At a conceptual level, this learned model could be as the addition of adding features that represent the human collaborator and the robot during the learning process, or learning individual representations separately, and then later combining these into a global model. Still referring to 6A, specifically, as an illustrative example, we take the case of where the robot model is learned jointly on the robot state and action space and on the human state and action space. The robot model is learned during a training phase. The training phase might consist on a first phase that gathers human data, such as a worker or a population of workers that can be observed by external sensors ex. one or more cameras and other sensors. With this human data, a predictive model can be learned to infer quantities like predicted completion time or predicted worker movements and a classification model to infer a state of the workers and the task on which the worker is currently working. Then second, we initialize a control law or policy that provides the robot to perform reasonably well in the operations the robot has to accomplish to help the worker. An expert engineer can do this, by kinesthetic teaching or through simulation via machine learning techniques such as reinforcement learning. The final stage of training can consist of training a global model that is defined on the state space including the state space of the robot and of the human. The policy of the robot is now adaptively improved accordingly to this learning model as the worker and the robot work together to accomplish the task. These adaptive online machine-learning techniques can be used to update the learning model and the robot policy as data is collected from the built-in sensors of the robot and human worker. In this way, the robot will not only perfect the robot's own policy, but will also be able to adapt to different situations in which the state of the worker changes. The robot should be a co-bot and/or controlled in compliance mode in order to guarantee the safety of the worker. The resulting joint model of robot/human can be very different for different states of the human. This is illustrated in 6A, and by necessity this figure is two-dimensional, but it can be logically extended to any number of dimensions. Here the robot only has one possible action speed up or slow down. The human has only two possible states tired or energetic. The robot must learn that when the human is energetic it can have a faster speed and when the human is tired it can have a slower speed. When the statistical models are learned the robot will learn that there is a higher cost to fast action if the worker is tired, and an optimal cost to lower speed. The same is true in reverse if the worker is energetic. 6A also demonstrates the need for human performance models. It is certainly true that this same principle could be accomplished by directly feeding human measurements to the robot. However, this would cause the state space of the robot to become much large adding noise and computational complexity to the problem. Thus, it is advantageous to first learn a model of the human state evolution and include this in the combined human/robot model. 6B, 6C and 6D, illustrate multiple methods of developing the joint human-robot model, wherein three possible training approaches can yield the combined human-robot model described conceptually above, i. e. 6B, 601B; 6C, 608C; and 6D, 609D. 6B illustrates the first approach 601B that focuses on combining the human model 604B and the robot model 607B into a joint model 605B. This model uses its corresponding data, human data 602B and a training phase 603B and robot data 606B and a training phase 611B that is unique to the given data set. The resulting model 605B relies on the output of both previously learned models 604B and 607B. This approach can be a direct method of combining these models. A policy for the robot based on the robot model 607B and on the task 617B, the robot has to achieve, , help the human worker in the assembly line task, is computed. The policy can be achieved with any policy optimization algorithm 612B, with model based reinforcement learning or optimal control as described above. When, the joint human-robot model 605B is computed, this can be used to improve the robot policy 612B, that can be updated considering not only the robot model 607B and the task 617B, but also having information of the human model 604B, in order to have new robot policy 612B. 6C is a block diagram illustrating combining the human model and the robot model into a joint model, which includes training the human model using data collected from the human worker, according to some embodiments of the present disclosure. This second approach 608C to combining these models begins by training 603C a human model 604C using data collected from the human worker 602C. The human model 604C is then used with the robot data 606C during training 611C of the joint model 605C. When, the joint human-robot model 605C is computed, this can be used to learn a new robot policy 612C. At least one advantage of this second approach is that we are learning a unified model whose input features include the model learned about the human worker. 6D is a block diagram illustrating learning an individual model for both the human and robot, then learns a joint model that is used to improve the robot policy and finally, this robot policy can be used during the operations, such that while the robot and the human are operating, data can be collected, so that the robot model and the robot policy are continuously updated using the collected data, according to some embodiments of the present disclosure. The third approach 609D begins similarly to the first approach 601B. Here a human model 604D and a robot model 607D are combined into a joint model 605D. Each model uses its corresponding data, human data 602D and a training phase 603D and robot data 606D and a training phase 611D that is unique to the given data set. The resulting model 605D relies on the output of both previously learned models 604D and 607D. This model is used to learn a first robot policy 612D. As a difference with the first approach in 601B, the robot model 607D and the robot policy 612D are updated online. While the human is co-working with the robot, which is controlled under the robot policy 612D, data from both the human and the robot are collected in 619D. The data are the same as described in some embodiments of the present disclosure above. These data are then used online to improve the robot model 607D and the robot policy 612D. The updated robot model and robot policy can be thought of as corrections to the initial robot model 607D and to the initial robot policy 612D that are used with the original robot model 607D to ensure proper robot operation. In all cases, the learned joint model can then be used to determine a control method of the robot, which is capable of interacting with the collaborator during his/her physical states Energized, tired, slow, uniquely, represented by his/her measurements. Importantly, this collaboration is learned with the goal of completing the human-robot manufacturing task and maximizing product quality. Because collecting data in this setting may be time consuming, and because the robot in this system is an engineered device. The models learned here can also incorporate physical knowledge , equations of motion of the robot, characteristics of the human operator and possibly task dependent features as prior information to the machine learning algorithm. 7A is a block diagram illustrating some sensing data including biometric and other data that can be collected and be used in assisting the process control system in detecting anomalies, according to some embodiments of the present disclosure. For example, some biometric data can include behavioral identifiers 701 that may include physical movements 703, engagement patterns 705 such as behavioral characteristics and behavior patterns, physical movements 707. Some other biometric data can include physical identifiers 702 that may include photo &amp; videos 704, physiological recognition 706, voice 708 and body attributes 710. Other data that can be collected can include vital signs of the worker 720, skin conductance 722 and skin temperature. The biometric data sensing may include an I/O module, which can include wireless communication components or an on-device user interface, control module, a power supply that may be a removable or rechargeable battery. The biometric data sensing may include one or more sensors that measure heart rate, vital signs of the worker 720, skin temperature 724, and/or skin conductance 722. The one or more sensors are located at the vicinity of the worker. A heart rate monitor or a heart rate sensor may measure the heart rate of the worker. The heart rate sensor should have accuracy sufficient to differentiate between the LF band and the HF band. Further, based on the heart rate measurements, a processor module can be used to determine a ratio of low spectral frequency LF heart rate variability to high spectral frequency HF heart rate variability. A higher ratio of LF to HF corresponds to a higher level of discomfort for the worker. The vital signs of worker 720 may be obtained by utilizing a remote photo plethysmography RPPG sensor. In some embodiments, a wearable device may be used for measuring the vital signs of the worker, wherein during an operation of the process control system, the wearable device can be in remote communication with the input interface of the process control system. Still referring to 7A and 7B, 7A shows the skin conductance 722 may be measured as the galvanic skin response reflecting the change in electrical properties of the worker's skin associated with evaporative transfer from the worker's skin. The skin temperature 724 may be used to quantify the heat transfer between the worker and the environment. Each of the skin conductance and the skin temperature measurements may be compared to respective predefined ranges indicative of a particular worker's preference. The biometric data sensing may include an IR camera that outputs IR image of the worker. Analysis of the IR image may provide an estimate of clothing level by comparing temperature of the outer most layer of clothing to the skin temperature of the worker. To obtain the biometric data one or more sensors may be in direct contact with the skin of the worker. For example, the sensing may be a wearable device, such as a watch, bracelet, necklace, shoe insert, or armband. 7B is a schematic illustrating an example of a wearable wrist device, according to some embodiments of the present disclosure. The wearable device 746 may be disposed on wrist of the worker 748. The wearable device 746 can be configured to display a list 750 as well as obtain data for each item in the list 750. For example, such display and data obtained in the list 750 can include body temperature 751, blood pressure 752, pulse rate 753, breathing rate 754, 02 saturation 756, skin conductance 757 and skin temperature 758. Model Based Reinforcement Learning Using Gaussian Process RegressionHere we describe the standard model-learning framework using Gaussian Process Regression GPR and a trajectory optimization algorithm adopted in Model Based Reinforcement Learning MBRL. This is to give a technical explanation of how the statistical model learning 134 of 1B, the Control Program 135 of 1B, the model learning 534 of 5A, the Update Policy 535 of 5A can be computed. A Markov Decision Process MDP formally defines an environment for RL. Consider a discrete-time system {tilde over x}k+1=f{tilde over x}k, uk subject to the Markov property, where {tilde over x}k |Rns and uk Rnu are the state vector and the input vector at the time instant k. When considering a mechanical system with generalized coordinates qk=qk1, . . . , qknRn, the dynamics equations obtained through Rigid Body Dynamics, suggest that, in order to satisfy the Markov property, the state vector {tilde over x} should consist of positions, velocities, and accelerations of the generalized coordinates, i. e. {tilde over x}k=[qk, {dot over q}k, {umlaut over q}k]R3n, or possibly of a subset of these variables, depending on the task definition. Model-based RL algorithms derive the policy {tilde over x}k starting from {tilde over f}{tilde over x}k, uk, an estimate of the system evolution. Gaussian Process Regression: GPR can be used to learn {circumflex over f}{tilde over x}k, uk. Typically, the variables composing {tilde over x}k+1 are assumed to be conditionally independent given {tilde over x}k+1 and uk, and each state dimension is modeled by a separate GPR. The components of {circumflex over f}{tilde over x}k, uk denoted by {circumflex over f}i{tilde over x}k, uk, with i=1 . . . ns, are inferred and updated based on {X, yi}, a data set of input-output noisy observations. Let N be the number of samples available and define the set of GPR inputs as X=[{tilde over x}1, . . . , {tilde over x}N] where {tilde over x}k=[{tilde over x}k, uk] Rm with m=ns+nu. As regards the outputs yi=[y1i, . . . , yNi], two definitions have been proposed in the literature. In particular, yki can be defined as {tilde over x}k+1i, the i-th component of the state at the next time instant, or as yki={tilde over x}k+1i{tilde over x}ki, leading to {tilde over {circumflex over x}}k+1 {tilde over x}k+{circumflex over f}{tilde over x}k, uk. In both cases, GPR models the observations as y i = [ f i x 1 f i x N ] + [ e 1 e N ] = f i X + e , 1 where e is Gaussian i. i. d. noise with zero mean and covariance n2, and fi X:NmfiX,KfiX, X. The matrix KfiX, X RNN is called the kernel matrix, and is defined through the kernel function kfi. ,. , which is the kernel learning program. Under these assumptions, the posterior distribution of fi is Gaussian and is available in closed form. In GPR, which is the model learning program, the crucial aspect is the selection of the prior functions for fi, defined by mfi, usually considered 0, and kfi,. . In the following we will refer to f and k,. as one of the f components and the relative kernel function, respectively. In the literature, when GPR is applied to modeling of physical systems, the kernel function or kernel learning program is often defined in one of the following cases. Physically inspired kernels: When the physical model of the system is derived by first principles, the model information might be used to identify a feature space over which the evolution of the system is linear. More precisely, assume that the model can be written in the form yk=xkTw, where xk: RmRq is a known nonlinear function obtained by first principles that maps the GPR inputs vector xk onto the physically inspired features space, and w is the vector of unknown parameters, modeled as a zero mean Gaussian random variable, i. e. w: N0, PI, with PI Rqq often chosen to be diagonal. The expression of the physically inspired kernel PI is kxk,xj=xkTPIxj,2 namely a linear kernel in the features . The efficiency of PI kernels in terms of estimation performances is closely related the adherence between the model and the behaviors of the real system. When the model is accurate these kernels exhibits good performances in terms of accuracy and generalization. For later convenience, we define also the homogeneous polynomial kernel in , which is a more general case of 2, kpolypxk,xj=xkTPIxjp. 3Notice that the linear kernel is obtained when P=1. The hyperparameters to be estimated remain the diagonal elements of the matrix PI. Nonparametric kernel: When there is no known structure of the process to be modeled, the kernel has to be chosen by the user accordingly to their understanding of the process to be modeled. A common option is the Radial Basis Function kernel RBF: k RBF x _ k , x _ j = Ae - x _ k - x _ j T RBF - 1 x _ k - x _ j 2 , 4 where is a positive constant called the scaling factor, and RBF is a positive definite matrix that defines the norm over which the distance between xk and xj is computed. The scaling factor and the elements of RBF are unknown parameters called hyperparameters; for this reason, it is called a nonparametric NP kernel. Several options to parametrize RBF have been proposed , a diagonal matrix or a full matrix defined by the Cholesky decomposition, namely RBF=LLT. In this case, the hyperparameters of RBF are the elements of the lower triangular matrix L, where the elements along the diagonal are constrained to be positive. Notice that with this choice, all the positive definite matrices are parameterized. Semiparametric kernel: This approach combines the physically inspired and the non-parametric kernels. Here the kernel function is defined as the sum of the covariance's: kxk,xj=xkTPIxj+kNPx,xj. 5 where kNP,. can be for example the RBF kernel 4. The semi-parametric SP kernel takes advantage of the global property of the parametric kernel kPI as well as of the flexibility of the nonparametric kernel kNP. Using SP kernels has been shown to have a model learning program which generalize well also to area of the state space not well explored by the data, typical behavior of model learning programs obtained with nonparametric kernels, and at the same time to have higher accuracy performance than the model learning programs obtained with parametric kernels which suffer of unmodeled dynamics. Trajectory Optimization using iLQG: Some embodiments of the present disclosure are based on recognition that the iLQG algorithm can be used for trajectory optimization. Given a discrete time dynamics such as 1 and a cost function, the algorithm computes local linear models and quadratic cost functions for the system along a trajectory. These linear models are then used to compute optimal control inputs and local gain matrices by iteratively solving the associated LQG problem. The cost function for controller design is a function of e, the state deviation from the target state x, and of the input saturation. As concerns the state cost, typically the smooth-abs function is used, given by x* e={square root over e22+2}, where e22 is the square of the Euclidean norm of e and is a parameter that controls the smoothness of the function around zero. In order to account for eventual constraints on input saturation, the cosine-hyperbolic function can be used, ui u=2 coshui/1, that ensures that the costs exponentially grow to infinity outside of the desired control volume, the parameter governs this volume. This cost function is optimized by linearizing the GP models and performing backward and forward search using the iLQG algorithm. Derivative-Free Framework for Reinforcement Learning AlgorithmsIn this section, a novel learning framework to model the evolution of a physical system is proposed. Several issues need to be addressed in the standard modeling approach described above. We list here the main problems to be solved by some embodiments of the present disclosure. First, the numerical differentiation: The Rigid Body Dynamics of any physical system computed from physical first principles are functions of joint positions, velocities and accelerations. However, a common issue is that often joint velocities and accelerations cannot be measured and computing them by means of numerical differentiation starting from the possibly noisy measurements of the joint positions might severely hamper the final solution. This is a very well-known and often discussed problem and it is usually partially addressed by ad-hoc filter design. However, this requires significant user knowledge and experience in tuning the filters' parameters, and is still prone to introducing various errors and delays. Second, the conditional independence assumption: The assumption of conditional independence among the fixk with i=1 . . . d given xk in 1 might be a very imprecise approximation of the real system's behavior, in particular when the outputs considered are position, velocity or acceleration of the same variable, which are correlated by nature. This fact is both an issue for estimation performance and an issue because one separate GP for each output needs to be estimated for modeling variables intrinsically correlated, leading to redundant modeling design and testing work, and a waste of computational resources and time. This last aspect might be particularly relevant when considering systems with a considerable number of degrees of freedom. Third, delays and nonlinearities in the dynamics: Finally, physical systems often are affected by intrinsic delays and nonlinear effects that have an impact on the system over several time instants, contradicting the first-order Markov assumption; an instance of such behavior is discussed later. Derivative-Free State definitionTo overcome the aforementioned limitations, we define the system state in a derivative-free fashion, considering as state elements the history of the position measurements: xk:=qk, . . . ,qk-kpRnkp+1. 6 where kp R is a positive integer. The definitions of the states are described as follows. In some cases, the object state data may represent a set of sequential measurement data of positions of the object in a predetermined period of time, and the robot state data may represent a set of sequential measurement data of positions of the robot in a predetermined period of time. The definition above can be understood that when velocities and accelerations measures are not available, if kp is chosen sufficiently large, then the history of the positions contains all the system information available at time k, leaving to the model learning algorithm the possibility of estimate the state transition function. Indeed, velocities and accelerations computed through causal numerical differentiation are the outputs of digital filters with finite impulse response or with finite past instants knowledge for non-linear filters, which represent a statistic of the past raw position data. These statistics cannot be exact in general, and might be severely corrupted by, for example, the delay introduced when a low-pass filter is used to reject the noise, or by the compound error propagation if several filters are applied, leading to a loss of information for the learning algorithm. Instead, this loss of information is kept in the proposed derivative-free framework which is some embodiment of the present disclosure. The state transition function becomes deterministic and known i. e. , the identity function for all the qk-1, . . . , qk-kq components of the state. Consequently, the problem of learning the evolution of the system is restricted to learning only the functions qk+1=fxk, uk reducing the number of models to learn and avoiding erroneous conditional independence assumptions. Finally, the MDP has some state information rich enough to be robust to intrinsic delays and to obey the first-order Markov property. State Transition Learning with PIDF KernelThe proposed state definition entails the need of a modeling technique for the MDP's state transition function. Derivative-free GPRs were already introduced only for nonparametric derivative-free GPR. However, as pointed in the above, the generalization performance of data-driven models might not be sufficient to guarantee robust learning performance, and exploiting eventual prior information coining from the physical model is crucial. On the other hand, physical models depend on positions, velocities and accelerations, and their use in a derivative-free framework is not possible in the standard formulation, the embodiments of the present disclosure solve this issue. In the following the procedure to obtain the so called Physically Inspired Derivative-Free PIDF kernel is proposed. Define qki=qki, . . . , qk-kpi and assume that a physical model of the type yk=qk, {dot over q}k, {umlaut over q}kukw, is known. Then, we propose a set of guidelines based on qk, {dot over q}k, {umlaut over q}k, uk to derive a PIDF kernel, which is an important component of the DFSPGP model learning program. PIDF Kernel Guidelines: Each and every position, velocity or acceleration term in is replaced by a distinct polynomial kernel kpolyp. ,. of degree p, where p is the degree of the original term; , kpoly2 . ,. . The input of each of the kernels kpolyp. ,. in 1 is a function of qki, the history of the position qi corresponding to the independent variable of the substituted term; , kpoly2qk1,. . If a state variable appears into transformed by a function g, the input to kpolyp. ,. becomes the input defined at point 2 transformed by the same function g, , sinqikpoly1 sinqki, sinqji. . Applying this guidelines will generate a kernel function kPIDF. ,. which incorporate the information given by the physics without knowing velocity and acceleration. The extension to semi-parametric derivative-free SPDF kernels become trivial when combing, as described in section Semiparametric kernel, the proposed kPIDF xk,. with a NP kernel with derivative-free state, kNPDF xk,. : kSPDFxk,xj=kPIDFxk,xj+kNPDFxk,xj. 7 which is the DF-SPGP kernel learning program. These guidelines formalize the solution to the non-trivial issue of modeling real systems using the physical models but without measuring velocity and acceleration. In other words, the DF-SPGP Model learning program, which is defined based on the DF-SPGP kernel learning program the DF-SPGP kernel learning program may define the DF-SPGP Model learning program, can predict behaviors of the robot and/or the object manipulated by the robot. FeaturesContemplated is that one or a combination of aspects can be included in independent claim 1 to create one or more different embodiments. For example, some of the one or a combination of aspects can include the following:An aspect can include that wherein the HP model for the HW is previously configured to have learned different states of HW performance that correspond as a set of boundaries in the human data. Wherein the DMP model is previously configured to have learned different the operation of the manufacturing process that is used to assist in issuing classifications of anomalies or no anomaly detection. Wherein the HRI model is previously configured to have learned mappings between the different states of the HW and optimal robot actions. Another aspect can be an event transition table is used to specify discovered positional relationships between pairs of events from training data of the behavior, if the sequence of events from the test signals is inconsistent with the behavior, then the alarm is to be signaled. It is possible an aspect can be that an event transition table can be constructed from training signals during a training phase, such that the training signals are acquired from a monitoring system of the DMP, during operation of the DMP, such that the hardware processor includes determining minimal and maximal durations for the at least one task and for multiple tasks. Still, an aspect may be determining, if a sequence of events is feasible given the event transition table and a predicted completion time of the human robot team. Another aspect could be that the input interface acquires training data from the DMP sensors during a training operation of the DMP while in an off-line training period, before acquiring the test data, and upon receiving the training data. The hardware processor can be configured to extract events from the training signals as a sequence to construct the event transition table of ordering relations of allowed positional relationships between pairs of observed events during the training operation of the DMP, and store the sequence of events in the memory. Further, an aspect can be that the DMP data includes other data such as DMP component training data, DMP assembly line training data, DMP operational training data, DMP management training data, and wherein the DMP signals include other data such as DMP component data, DMP assembly line data, DMP operational data, DMP management data, and wherein the HW signals from HW sensors include a time series of measurements of tasks completed by the HW. Another aspect may be that some of the classifications of anomaly detections can include robot actions associated with the state of the HW, that include, different levels of speed, X-axis, Y-axis and Z-axis movements of the robot, voice announcements, making calls, maintaining robot positions for one or more periods of time, adjusting environmental conditions via commands sent to a controller, and wherein some of the types of classification of anomalies also include detection of future anomalies, maintenance related anomalies, safety related anomalies, lost production anomalies, potential failure of components anomalies, quality anomalies and assembly line anomalies. Further still, another aspect can be the HP model is constructed from HW training signals of completed training tasks during a training phase prior to receiving the HW signals, such that the training signals includes data for each completed training task of the completed training tasks that includes a training task name, multiple training states of the HW for the completed training task, and a next sequenced training task, such that the HW training signals and the HW signals are acquired from sensors associated the HW during a training operation or an operation of the DMP with the human-robot teams. Another aspect can be the HP model is constructed by a Human Task Execution HTE model and a model of the state of the HW, wherein the HTE model is constructed using at least one predictive model trained using HW training signals obtained during a training phase completing a sequence of training tasks, and wherein the model of the state of the HW is constructed using at least one classification model trained using the HW training signals, such that each completed training task is associated with multiple states of the HW, and is stored in the memory. Wherein the at least one predictive model is configured to learn expected completion times for each completed task, identify or capture patterns of movements of the HW observed in sensor data obtained from sensors, and wherein at least one statistical model learning approach, includes one or more predictive model, one or more classification model, or both, that is capable of producing estimates of a completion time of an on-going task given sensor measurements of the HW while the HW is interactively working with the robot in completing the at least one task. Wherein the at least one classification model is configured to learn a state of the HW from the HW signals by first determining a task label of a task completed and a next sequenced task, then uses a gaze detection algorithm to determine the state of the HW, such as an amount of a level of focus and an amount of a level of energy of the HW at a time of completing the task. An aspect is that if no anomaly detection is produced, then the state of the HW is compared to a predetermined level of HW performance thresholds of the HW model, and if greater than, a HW peak performance threshold, indicating a peak performance by the HW to complete the task, then, the HP model is updated to model peak performance by the HW, and wherein the level of the HW performance is determined by extracting data from the received HW signals, such as an adherence of the HW measurements to the learned statistical models, a degradation of model performance, or a specific learning model that is used to predict the obtained state of the HW. Or, an aspect may be the HW data includes data for each completed training task, such as one or a combination of, patterns of movements by the HW, an energy level of the HW, a skill level associated with a set of HW skill levels, and historical levels of states of the HW corresponding to a performance matrix associated with each completed training task label. Another aspect is that an event transition table or a log-based ordering relationship table can be built by each entry in the event transition table and is initialized with a symbol label, the memory is scanned sequentially, and for any pair of events where one event immediately follows another event, a corresponding entry of the event transition table is changed to a &gt; symbol label, upon completion of the scanning, any two events are either in the &gt; symbol label or the symbol label relations, then derive relations , , , and # computed on the basis of the relations of the &gt; symbol label and the symbol label from the memory, using a set of rules: AB, if A&gt;B and BA; AB, if AB and B&gt;A; AB, if A&gt;B and B&gt;A; and A #B, if A B and BA, wherein the event transition table and probability distributions over task durations measured in intervals of time, specifies the discovered positional relationships between pairs of events from training data of the behavior of the operation of the DMP, wherein A is an event and B is an event, and A&gt;B is where the event A immediately follows the event B. An aspect can be that wherein some of the previously learned boundaries of different types of anomalies and no anomalies learned from the Human data by the HP model include a HW that is no longer working, a distracted HW, a HW experiencing a level of energy indicating the HW is tired or underperforming according to the previously learned boundaries, or a HW experiencing a level of energy indicating the HW is energetic or performing at a high energy level according to the previously learned boundaries, or a HW experiencing a level of energy indicating the HW is not tired or energetic such as an average energetic level, or performing at a level of energy associated with an average HW performance according to the previously learned boundaries.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/29/424/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9259697198867798,
                    "F_SPEC_PARAMS": [
                        "velocity",
                        "velocity"
                    ],
                    "S_SPEC_PARAMS": [
                        "incorrect execution of one or more of tasks,"
                    ],
                    "A_PARAMS": [
                        "measurements of physical quantities"
                    ],
                    "F_SENTS": [
                        "However, there are many difficulties with these conventional learning models to control robotic systems in the fact that very complex physical laws, called Rigid Body Dynamics RBD and only a crude representation of these physical laws, govern the motion of a robotic system are usually known.",
                        "Moreover, measurements of physical quantities need to compute these laws, such as position, velocity and acceleration of each component of the robot, which often is unavailable.",
                        "Sensors mounted on the robotic systems typically only measure a position component encoders, potentiometers, proximity sensors .",
                        ", while velocity and acceleration are not measured."
                    ],
                    "S_SENTS": [
                        "However, the nature of the working process in discrete manufacturing is different from that in process manufacturing, and deviations from the normal working process can have very different characteristics.",
                        "Anomalies can include incorrect execution of one or more of tasks, or an incorrect order of the tasks.",
                        "Even in anomalous situations, often no physical variables, such as temperature or pressure are out of range, so direct monitoring of such variables cannot detect such anomalies reliably."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Speed"
                    ],
                    "F_SIM_SCORE": 0.7172702550888062,
                    "S_TRIZ_PARAMS": [
                        "Reliability"
                    ],
                    "S_SIM_SCORE": 0.4880649149417877,
                    "GLOBAL_SCORE": 1.7286373049020767
                },
                "sort": [
                    1.7286373
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11413772-20220816",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11413772-20220816",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2017-02-22",
                    "PUBLICATION_DATE": "2022-08-16",
                    "INVENTORS": [
                        "Emil Fjllstrm"
                    ],
                    "APPLICANTS": [
                        "ABB Schweiz AG    ( Baden , CH )"
                    ],
                    "INVENTION_TITLE": "Industrial robot system with supervision sensor",
                    "DOMAIN": "B25J 190066",
                    "ABSTRACT": "An industrial robot system includes a manipulator with a drive chain including a motor unit. A controller in the industrial robot system is electrically connected to the drive chain by a set of power transmission lines and is operable to transmit electrical power on the set of power transmission lines so as to impart a controlled movement of the manipulator. A supervision sensor is arranged in the manipulator and configured to sense a property of the manipulator. The supervision sensor is electrically connected to at least a subset of the power transmission lines for transmission of sensor data representing the property to the controller.",
                    "CLAIMS": "1. An industrial robot system, comprising: a manipulator having a base, a moveable part, and a drive chain for moving the moveable part in relation to the base, wherein the drive chain includes a motor unit with a driven shaft which is mechanically coupled to the moveable part, a controller which is electrically connected to the drive chain by a set of power transmission lines and is operable to transmit electrical power on the set of power transmission lines so as to impart a controlled movement of the moveable part in relation to the base, and a supervision sensor arranged in the manipulator and configured to sense a property of the manipulator, wherein the supervision sensor is electrically connected to at least a subset of the power transmission lines for transmission of sensor data representing said property to the controller, wherein the sensor data includes an identifier of the supervision sensor, and one or more sensing values representing the property. 2. The industrial robot system of claim 1, wherein the supervision sensor is further electrically connected to at least one of the power transmission lines for receiving electrical power for powering the supervision sensor. 3. The industrial robot system of claim 1, wherein the supervision sensor is electrically connected to said at least a subset of power transmission lines by a signal conditioning unit, which is configured to encode the sensor data into a power signal which is applied by the controller onto said at least a subset of the power transmission lines. 4. The industrial robot system of claim 3, wherein the signal conditioning unit is further configured to divert power from the power signal and distribute the diverted power to the supervision sensor for powering the supervision sensor. 5. The industrial robot system of claim 1, wherein the set of power transmission lines include one or more first power transmission lines connected to the motor unit and one or more second power transmission lines connected to a brake device in the drive chain. 6. The industrial robot system of claim 5, wherein the brake device is configured to switch from a first, default state to a second state when energized, wherein the controller is configured to transmit electrical power on the one or more second power transmission lines to switch the brake device from the first state to the second state, and wherein the supervision sensor is connected to the one or more second power transmission lines for transmission of the sensor data. 7. The industrial robot system of claim 6, wherein the controller is configured to transmit the electrical power on the one or more second power transmission lines as a DC signal. 8. The industrial robot system of claim 6, wherein the brake device is configured to prevent the controlled movement of the moveable part in the first state, and to allow the controlled movement of the moveable part in the second state. 9. The industrial robot system of claim 1, further comprising a position detector which is associated with the drive chain and configured to generate position indicating data for the motor unit and to transmit the position indicating data to the controller on one or more dedicated signal transmission lines. 10. The industrial robot system of claim 1, wherein the supervision sensor is configured to sense a property included in the group including: pressure, temperature, moisture, humidity, liquid, dust, force, torque, and presence of a general or specific liquid or substance. 11. The industrial robot system of claim 1, wherein the supervision sensor is configured to sense presence of oil. 12. The industrial robot system of claim 1, wherein the drive chain further includes a gearbox containing lubrication oil, wherein said supervision sensor is configured and arranged to detect leakage of said lubrication oil from the gearbox. 13. The industrial robot system of claim 12, wherein the supervision sensor is arranged at a mechanical interface between the motor unit and the gearbox and/or between the gearbox and the moveable part. 14. The industrial robot system of claim 1, wherein the base, the moveable part, the drive chain, and the supervision sensor are included in a manipulator device, and wherein the controller is located remotely from the manipulator device. 15. The industrial robot system of claim 2, wherein the supervision sensor is electrically connected to said at least a subset of power transmission lines by a signal conditioning unit, which is configured to encode the sensor data into a power signal which is applied by the controller onto said at least a subset of the power transmission lines. 16. The industrial robot system of claim 2, wherein the set of power transmission lines include one or more first power transmission lines connected to the motor unit and one or more second power transmission lines connected to a brake device in the drive chain. 17. The industrial robot system of claim 7, wherein the brake device is configured to prevent the controlled movement of the moveable part in the first state, and to allow the controlled movement of the moveable part in the second state. 18. The industrial robot system of claim 2, further comprising a position detector which is associated with the drive chain and configured to generate position indicating data for the motor unit and to transmit the position indicating data to the controller on one or more dedicated signal transmission lines.",
                    "FIELD_OF_INVENTION": "The present invention generally relates to industrial robot systems, in particular to a technique of incorporating one or more supervision sensors in such systems.",
                    "STATE_OF_THE_ART": "Industrial robot systems are well known in the art and are used in a variety of environments. It is important that the reliability of the industrial robot system is high, since malfunctions may result in production stops, downtime for repair and need to acquire costly spare parts. To obviate such malfunctions, industrial robot systems are regularly subject to manual inspection. These manual inspections are time-consuming and labor-intensive, and there is a risk that potential problems are overlooked. In one example, manual inspections are regularly scheduled to look for leaks of lubrication oil within the robot. One source of oil leaks is the gearbox in the mechanical drive chain for the respective joint of the robot. Oil leaks are known to occur at the mechanical interface between the gearbox and the motor in the drive chain, and some robots are designed with a dedicated evacuation channel at this mechanical interface to facilitate detection of oil leaks. A manual inspection for oil leaks requires service personnel to partly dismantle the robot for separate inspection of every joint or drive chain in the manipulator of the industrial robot. For example, a six axis manipulator has six drive chains to be inspected. Further, more than one location may need to be inspected for oil leaks in each drive chain. The need for manual inspection may be reduced by installation of supervision sensors in the manipulator and automated monitoring of the sensor output for detection and prediction of malfunctions. However, a wired installation of supervision sensors in the manipulator may increase the complexity and cost of the industrial robot system. Further, installation of supervision sensors may actually add a potential cause of malfunction to the industrial robot system, malfunctions caused by the wiring to the sensors. Different wired installations of sensors in industrial robot systems are disclosed in US2016/0271807, DE202016104294U1, 6,882,125, US2005/0103148 and 7,703,349.",
                    "SUMMARY": [
                        "It is an objective of the invention to at least partly overcome one or more limitations of the prior art. Another objective is to provide an industrial robot system with a simple and robust electrical installation of one or more supervision sensors. One or more of these objectives, as well as further objectives that may appear from the description below, are at least partly achieved by an industrial robot system according to the independent claim, example embodiments being defined by dependent claims. Still other objectives, as well as features, aspects and advantages of the present invention, may appear from the following detailed description, from the attached claim as well as from the drawings.",
                        "Embodiments of the invention will now be described in more detail with reference to the accompanying schematic drawings. 1 illustrates an industrial robot system comprising a controller and a robotic manipulator. 2 is an elevated side view of the robotic manipulator in 1. 3 is a block diagram of a mechanical drive chain for an actuated joint in the manipulator in 1-2. 4 is a section view of an actuated joint and an associated drive chain. 5 is a perspective view of a robot positioner for an industrial robot system. 6 is a perspective view of a workpiece manipulator for an industrial robot system."
                    ],
                    "DESCRIPTION": "Embodiments of the present invention will now be described more fully hereinafter with reference to the accompanying drawings, in which some, but not all, embodiments of the invention are shown. Indeed, the invention may be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided so that this disclosure may satisfy applicable legal requirements. Like numbers refer to like elements throughout. Also, it will be understood that, where possible, any of the advantages, features, functions, devices, and/or operational aspects of any of the embodiments of the present invention described and/or contemplated herein may be included in any of the other embodiments of the present invention described and/or contemplated herein, and/or vice versa. In addition, where possible, any terms expressed in the singular form herein are meant to also include the plural form and/or vice versa, unless explicitly stated otherwise. As used herein, at least one shall mean one or more and these phrases are intended to be interchangeable. Accordingly, the terms a and/or an shall mean at least one or one or more, even though the phrase one or more or at least one is also used herein. As used herein, except where the context requires otherwise owing to express language or necessary implication, the word comprise or variations such as comprises or comprising is used in an inclusive sense, that is, to specify the presence of the stated features but not to preclude the presence or addition of further features in various embodiments of the invention. Embodiments of the invention are directed to a technique of incorporating one or more supervision sensors into a manipulator of an industrial robot system while minimizing the need to redesign the electrical wiring within the manipulator. As used herein, an industrial robot system comprises one or more manipulators and one or more controllers for operating the manipulators. Such a manipulator may be of different generic types. One generic type is a robotic manipulator which is a jointed manipulation structure for attachment of an end effector, which is designed according to the work task to be performed by the robot. The robotic manipulator is typically configured for movement of the end effector in two or three dimensions. Generally, a robotic manipulator may be defined as an automatically controlled, reprogrammable, multipurpose manipulator with a plurality of degrees of freedom and an ability to perform work tasks independently. When combined with a controller, the robotic manipulator forms an industrial robot. Many different robotic manipulators are known in the art, and the industrial robot is often characterized based on the movement of the robotic manipulator, as a Cartesian robot, cylindrical robot, spherical polar robot, articulated robot, gantry robot, etc. Another generic type of manipulator is a robot positioner, which is configured to carry and position the robotic manipulator in one, two or three dimensions. A further generic type of manipulator is a workpiece manipulator, which is configured to carry and position a workpiece to be processed by the end effector of an industrial robot. For example, the workpiece manipulator may be configured to translate and/or rotate the workpiece. It is understood that an industrial robot system may comprise any combination of these manipulators, which may be connected to a common controller or separate controllers. In the following, with reference to 1-4, embodiments of the invention will be described in relation to industrial robots. 1 shows such an industrial robot 1, which comprises a controller or control unit 2 and a robotic manipulator 3. The controller 2 is electrically connected to the robotic manipulator 3 by a motor cable 4a and a measurement cable 4b, which alternatively may be combined into a single cable. The robotic manipulator 3 is an articulated manipulator and comprises a base 5 and a sequence of links 6 and joints 7 attached to the base 5. In 1, the links 6 and joints 7 are only schematically indicated. The links 6 are rigid members connecting the joints 7, and the joints 7 are movable components that enable relative motion between adjoining links 6. Each joint 7 defines an axis or degree of freedom of the manipulator. Generally, the respective joint 7 may be characterized as either a linear prismatic joint, which exhibits a sliding or linear translational motion along an axis, or a rotary revolute joint, which exhibits a rotary motion about an axis. The distal link 6 of the robotic manipulator 3 is configured for attachment of an end effector not shown. The movement at the respective joint 7 is governed by a drive chain for the joint 7. As used herein, the drive chain of a joint 7 designates the mechanical sequence of structurally interconnected components that collectively operate to generate a force and mechanically convert the force into a relative motion of the links 6 connected at the joint 7. The force of the respective drive chain is generated by an actuator, which may but need not be located at its associated joint. In the following, it is presumed that the actuator is an electrical motor, specifically a servomotor. Depending on implementation, the servomotor may be a stepper motor, a DC-brush motor, a DC brushless motor, an AC synchronous motor or an AC asynchronous motor. The controller 2 is configured to operate the drive chains of the manipulator so as to achieve a desired robot motion, a trajectory of the end effector. In the example of 1, the controller 2 comprises a main computer 10, an axis computer 11 and a set of drive units or drivers 12 one shown. The main computer 10 operates a software program that defines the desired robot motion. Based on the software program, the main computer 10 sends motion commands to the axis computer 11. The axis computer 11 is configured to convert the respective motion command to a desired movement of the respective joint 7. The controller 2 may comprise one driver 12 for each drive chain in the manipulator 3, where each driver 12 is configured to convert the desired movement of the respective joint 7 into a motor power signal P1 for the servomotor of the drive chain associated with the joint 7. Typically, the servomotor is directly controlled by the signal P1, in that the signal P1 contains power pulses that energize the servomotor to cause a desired motion of a driven shaft of the servomotor. In other words, the signal P1 defines a modulated power flow to the servomotor for execution of a desired motion. For servomotors, the driver 12 may comprise a so-called servo amplifier for generating the signal P1. In a variant, not shown, all or part of the controller 2 may be installed in the manipulator 3. For example, at least part of the functionality of the drivers 12 may be implemented by one or more local drive units in the manipulator 3. As indicated in 1, the respective driver 12 transmits the signal P1 to the servomotor via the motor cable 4a, and the controller 2 also transmits a brake power signal P2 for a brake see below associated with the servomotor. 1 also indicates that a measurement signal S1 is provided from the manipulator 3 to the axis computer 11 via the measurement cable 4b. The measurement signal S1 indicates the actual position of the respective joint and thereby allows the axis computer 11 to precisely control the movement of the manipulator 3. As explained further below, the measurement signal S1 contains measurement data generated by a position sensor, possibly in combination with a revolution sensor, associated with the respective drive chain. The measurement cable 4b may be electrically connected to each position sensor and revolution sensor, if included. Alternatively, the manipulator 3 may include a dedicated data collection unit 8 indicated by dotted lines in 1 for collecting data from these sensors and providing the measurement signal S1 as a digital signal via the measurement cable 4b. Such a dedicated data collection unit 8 is commonly known as a serial measurement board SMB. It should be understood that the motor cable 4a includes a plurality of wires for transmitting the signals P1, P2 to the servomotors and the brakes. Within the manipulator 3, one or more wires extend from the motor cable 4a to the respective servomotor and to the respective brake. Likewise, one or more wires extend inside the manipulator 3, from the respective position detector to the measurement cable 4b or to the data collection unit 8, if present. 2 schematically illustrates the location of servomotors 31 in the robotic manipulator 3 of 1. As seen, the servomotors 31 and thus the drive chains are distributed throughout the manipulator. 2 also indicates the extent of the wiring 20 that includes the above-mentioned wires. As understood, the wiring 20 needs to be carefully guided through the manipulator 3 so as to not obstruct its motion or to get damaged by tensioning or pinching between moving parts. The wiring 20 is typically arranged in a flexible cable harness not shown that holds the included wires together and shields them from mechanical damage. 3 is a block diagram of a drive chain 30 for a joint cf. 7 in 1. The drive chain 30 is attached to and extends between two links cf. 6 in 1, which are designated 6A, 6B in 3. The drive chain 30 is configured to effect a controlled movement of link 6B relative to link 6A. In 3, thick lines between boxes represent mechanical connections/components and thin lines represent electrical connections. Although the electrical connections are represented by pairs of wires in 3, it is realized that any number of wires may be used, including a single wire. In the illustrated example, the drive chain 30 comprises a servomotor 31 which is attached to link 6A. The servomotor 31 is electrically connected to wires 4a1 motor power wires that transmit the signal P1 from the driver 12 for the drive chain 30 1. The servomotor 31 has a driven shaft, i. e. a shaft that is driven to rotate around its axis. The driven shaft is connected by a coupling 32 to an input element of a transmission 33, which is also attached to link 6A. The transmission 33 comprises one or more gears or gear trains arranged to convert the rotational speed and/or torque between the input element and an output element of the transmission 33, or to transform the rotational movement of the input element to a linear movement of the output element of the transmission 33. At least part of the transmission 33 may be contained in a dedicated housing defining a gearbox, which holds oil for lubrication of the included components. The output element of the transmission 33 is engaged with link 6B. 3 also illustrates a position detector 34 represented by a dotted box, which is associated with the drive chain 30 and arranged to sense the instantaneous rotational position of the servomotor 31. The position detector 34 may be attached to the output shaft of the servomotor 31 as shown, or to any suitable component of the transmission 33, or even be incorporated into the servomotor 31. As is known in the art, the position detector 34 may be a resolver, which is configured to generate an analog signal, or a rotary encoder, which is configured to generate a digital signal. The position detector 34 is electrically connected to wires 4b1 that transmit the measurement signal S1 to the axis computer 11 through the measurement cable 4b, optionally via the dedicated data collection unit 8 1. Although not shown in 3, the drive chain 30 may also be associated with a revolution counter, a tachometer, which senses the rotational speed of the output shaft of servomotor 31. Such a revolution counter may be electrically connected to a dedicated wire pair in analogy with the position detector 34. The drive chain 30 further comprises a brake 35, which is configured to selectively lock on position and release off position the drive chain 30, so as to prevent and allow, respectively, relative movement between links 6A, 6B. The brake 35 is electrically connected to wires brake power wires 4a2 to receive the brake power signal P2 from the controller 2 1. The signal P2 controls the switching between on and off positions. For example, the brake 35 may be configured to selectively bring a dedicated friction element not shown into locking engagement with any suitable component of the drive chain 30, the output shaft of the servomotor 31 as shown, or a component of the transmission 33. Alternatively, the brake 35 may be incorporated into the servomotor 31. In one embodiment, the brake 35 is a fail-safe brake, i. e. a brake that switches to the on position when an electrical characteristic of the signal P2 falls below a threshold, given in terms of voltage, current or power. Such fail-safe brakes 35 are common in industrial robots. When power is restored above the threshold, the fail-safe brake 35 switches to and stays in the off position. Many different types of fail-safe brakes are known in the art, including spring-set brakes and permanent-magnet brakes. In the following, it is assumed that the brake 35 is a fail-safe brake. This means that the controller 2 supplies sufficient power to the brake, via the signal P2, at all times during movement of the manipulator 3. Depending on the configuration of the brake 35, the signal P2 may be either a DC signal or an AC signal. In one embodiment, the signal P2 is a 24V DC signal, and the above-mentioned threshold may be set to about 22V or less. As noted above 2, the wiring 20 is a critical component for industrial robot systems when it comes to reliability. It is generally desirable to keep the number of wires in the wiring 20 to a minimum. At the same time, it may be desirable to include one or more dedicated supervision sensors in the manipulator 3 to monitor one or more properties. Such properties may be any one of temperature, pressure, humidity, moisture, dust, force, torque, as well as presence of a general or specific liquid or substance. Reverting to 3, the illustrated embodiment obviates or reduces the need to include additional dedicated wires in the wiring 20, for supplying power to the supervision sensors 37 and/or for transmitting sensor data from the supervision sensors 37 to the controller 2. In the illustrated embodiment, this is achieved by connecting the respective sensor 37 to transmit the sensor data and receive power via the wires 4a2. A signal conditioning unit SCU 38 is arranged to receive an analog or digital sensor signal from the respective sensor 37 and to transmit sensor data given by the sensor signal on the wires 4a2. The SCU 38 thereby operates as a signal transmitter. The controller 2 comprises a corresponding signal receiver configured to retrieve the sensor data from the wires 4a2. Typically, the controller 2 is configured to actuate all brakes 35 in the manipulator 3 in unison, and therefore the wires 4a2 from all brakes 35 may be connected to a single node in the controller 2, and thus to a common signal receiver. In the example of a fail-safe brake 35, in which the signal P2 transmits power to the brake during operation of the manipulator 3, the SCU 38 may be configured to encode the sensor data in the signal P2, by superposition or modulation of a low energy information signal onto signal P2, according to any known power-line communication PLC technique, as is well-known in the art. Since the brake 35 and the controller 2 are both connected to ground, the signal P2 with the encoded sensor data will be available at the controller 2 for decoding by the signal receivers. The sensor data may be communicated according to any suitable communication protocol. If the wires 4a2 are connected to a single node in the controller 2, the sensor data may include a unique identifier of the respective sensor 37 in addition to one or more sensor values representing the measured property. In the illustrated embodiment, the SCU 38 is also configured to divert power from the wires 4a2 and supply an adequate amount of power to the respective sensor 37. In the example of a fail-safe brake 35, the diverted power should be less than the difference between the power supplied by the driver 12 and the power level corresponding to the above-mentioned threshold. The power required by the sensor 37 is typically much smaller than the power required to release the fail-safe brake 35. It should be noted that the SCU 38 may be configured to supply power and transmit sensor data also when the fail-safe brake 35 is set in the on position, provided that the controller 2 provides power on the wires 4a2 also in the on position, i. e. at a level below the power level corresponding to above-mentioned the threshold. To ensure an ability to transmit sensor data in the event of a complete power outage, the SCU 38 may be connected to a local battery or capacitance that provides backup power. In an alternative embodiment, the SCU 38 is connected to divert power for powering the sensor 37 from one or more other power transmitting wires, wires 4a1. In a further alternative embodiment, the SCU 38 is only configured to transmit the sensor data and does not divert power to the respective sensor 37. The sensor 37 may instead be powered by the controller 2 via one or more dedicated power wires included in the wiring 20. Alternatively, the sensor 37 may be powered by a power source in the manipulator 3, such as a battery, capacitor, energy harvesting device, solar cell, etc. In these embodiments, the SCU 38 may be able to transmit the sensor data also in absence of power on the wires 4a2. In a further alternative embodiment, the SCU 38 is only configured to divert power to the respective sensor 37 and does not transmit sensor data. The sensor 37 may instead be configured for wireless transmission of sensor data to the controller 2. The skilled person readily understands that the foregoing techniques for transmitting sensor data and/or diverting power are equally applicable for a non-fail-safe brake 35. In a further alternative embodiment, each of the supervision sensors 37 is instead electrically connected to the wires 4a1, in the same way as described above for the wires 4a2. However, it is currently believed that both transmission of sensor data and supply of power to the sensorss 37 are facilitated by connection to the wires 4a2, in view of the fact that the signal P2 generally contains less high frequency components and typically carries lower power than the signal P1. It should be understood that the supervision sensors 37 may have any location within the manipulator 3, depending on the property to be monitored. In one embodiment, at least one supervision sensor 37 is associated with each drive chain 30. The sensor 37 may be attached to the input or output element of the transmission 33 as shown in 3, to a link 6A, 6B, onto or within a gearbox in the transmission 33, to a bearing, onto or within the servomotor 31, etc. In one specific implementation, the supervision sensor 37 is arranged to detect leakage of oil in the drive chain 30. The oil leakage sensor 37 may or may not be capable of differentiating between liquids containing hydrocarbons and other liquids including water. It should be understood that leakage of oil from the drive chain 30 may reduce the life of the industrial robot and result in need for substantial repair and operational standstill, if not detected in time. Many manufacturers therefore recommend regular, visual inspections of the drive chain 30 for oil leakage. It is realized that this is a demanding task, since the inspections follow strict routines. Further, the drive chains 30 are distributed within the manipulator 3 and it is necessary to access each drive chain 30 for inspection. These visual inspections may be obviated by installation of one or more oil leakage sensors 37 in each drive chain 30 in accordance with embodiments of the invention. An example of such an installation is given in 4, which is a section view of a drive chain 30 comprising a servomotor 31, a brake 35 and a gearbox 33. All internal details of the servomotor 31, gearbox 33 and brake 35 have been omitted in 4. The gearbox 33 holds lubrication oil. In the illustrated example, one oil leakage sensor 37 is installed at an evacuation channel 40 for oil evacuation in case of oil leakage. The evacuation channel 40 extends in the servomotor 31 at the mechanical interface 41 between the servomotor 31 and the gearbox. Another oil leakage sensor 37 is installed at the mechanical interface 42 between the gearbox 33 and the link coupled to the output element of the gearbox 33. Further oil sensors may be installed in the drive chain 30, at lubrication points of an automatic lubrication system, at bearings. In the illustrated example, the above-mentioned SCU cf. 38 in 3 is integrated with the respective sensor 37. All of the embodiments described in the foregoing involve an industrial robot system comprising a manipulator, which comprises a base, a moveable part, and a drive chain for moving the moveable part in relation to the base, where the drive chain comprises a motor unit with a driven shaft which is mechanically coupled to the moveable part. The industrial robot system also comprises a controller which is electrically connected to the drive chain by power transmission lines and is operable to transmit electrical power on the power transmission lines so as to impart a controlled movement of the moveable part in relation to the base, and a supervision sensor arranged in the manipulator and configured to sense a property of the manipulator, wherein the supervision sensor is electrically connected to transmit sensor data representing the property on at least a subset of the power transmission lines. In the example of 1-3, the base corresponds to base 5, the moveable part corresponds to any of the links 6, link 6A or 6B in 3, the drive unit corresponds to drive unit 30, the motor unit corresponds to servomotor 31, the controller corresponds to controller 2, the supervision sensor corresponds to sensor 37, and the power transmission lines correspond to wires 4a1, 4a2. It is realized that such an industrial robot system may be conveniently obtained by retrofitting existing manipulators with one or more supervision sensors, in a simple and cost-effective way, without the need to update the wiring 20 that extends through the manipulator 3. Further, such an industrial robot system may be manufactured without the need to redesign the wiring 20 of existing manipulators. The manipulator of this industrial robot system need not be a robotic manipulator as exemplified in 1-3, but could be any other type of manipulator as used in industrial robot systems, the above-mentioned robot positioner or workpiece manipulator. Thus, all of the foregoing embodiments are equally applicable to such other types of manipulators. 5 shows an example of a robot positioner 3, also known as track motion. The robot positioner 3 comprises a support element 5 base, and a carriage 6 moveable part, which is arranged for movement along a rail on the support element 5. The carriage 6 defines a platform 50 for attachment of a robotic manipulator. The robot positioner 3 comprises an electric motor 31, which forms a motor unit and is part of a drive chain not shown that imparts a movement to the carriage 6 in relation to the support element 5. The robot positioner 3 may be controlled by a controller to move the carriage 6 in coordination with a robotic manipulator mounted onto the platform 50. 6 shows an example of a workpiece manipulator 3, also known as work-piece positioner. The illustrated workpiece manipulator 3 is configured to impart a horizontal rotation to a workpiece. The workpiece manipulator 3 comprises a support element 5 base and a driven mounting plate 6 moveable part, which is arranged for rotational movement around a horizontal axis. The driven mounting plate 6 and an opposing free-turning mounting plate 60 define a mounting region for a workpiece for processing by a robotic manipulator. The workpiece manipulator 3 comprises an electric motor 31, which forms a motor unit and is part of a drive chain not shown that imparts a movement to mounting plate 6 in relation to the support element 5. The workpiece manipulator 3 may be controlled by a controller to rotate the plate 6 in coordination of with a robotic manipulator that operates on a workpiece mounted between the plates 6, 60.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/72/137/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9938190579414368,
                    "F_SPEC_PARAMS": [
                        "time-consuming",
                        "labor-intensive,"
                    ],
                    "S_SPEC_PARAMS": [
                        "complexity",
                        "cost"
                    ],
                    "A_PARAMS": [
                        "wired installation of supervision sensors"
                    ],
                    "F_SENTS": [
                        "These manual inspections are time-consuming and labor-intensive, and there is a risk that potential problems are overlooked."
                    ],
                    "S_SENTS": [
                        "However, a wired installation of supervision sensors in the manipulator may increase the complexity and cost of the industrial robot system.",
                        "Further, installation of supervision sensors may actually add a potential cause of malfunction to the industrial robot system, malfunctions caused by the wiring to the sensors."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "F_SIM_SCORE": 0.5977427363395691,
                    "S_TRIZ_PARAMS": [
                        "Complexity of Control",
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.584314838051796,
                    "GLOBAL_SCORE": 1.7181811784704526
                },
                "sort": [
                    1.7181811
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11328170-20220510",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11328170-20220510",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2020-02-19",
                    "PUBLICATION_DATE": "2022-05-10",
                    "INVENTORS": [
                        "Krishna Shankar"
                    ],
                    "APPLICANTS": [
                        "TOYOTA RESEARCH INSTITUTE, INC.    ( Los Altos , US )"
                    ],
                    "INVENTION_TITLE": "Unknown object identification for robotic device",
                    "DOMAIN": "G06K 96215",
                    "ABSTRACT": "A method for identifying objects includes generating a feature vector of an unknown object from an image of an environment. The method also includes comparing the feature vector of the unknown object to feature vectors of known objects. The method further includes determining whether a similarity between the feature vector of the unknown object and the feature vector of one of the known objects satisfies a threshold. Furthermore, the method includes identifying the unknown object based on the determination.",
                    "CLAIMS": "1. A method for identifying objects, comprising: generating a feature vector of an unknown object from an image of an environment, in which the feature vector encodes information into a series of numbers to provide a numerical fingerprint for differentiating one feature from another; comparing the numerical fingerprint of the unknown object to numerical fingerprints of known objects of a trained object class, in which each of the known objects comprises multiple numerical fingerprints, and each of the multiple numerical fingerprints is based on images obtained from different views of the known objects to identify the known objects in the different views; determining whether a similarity between the numerical fingerprint of the unknown object and the numerical fingerprints of one of the known objects satisfies a threshold; identifying the unknown object based on the determining; and inferring that the unknown object is in the same trained object class as one of the known objects of the trained object class when a difference between the numerical fingerprints of the known objects and the unknown object is less than the threshold and/or the similarity between the numerical fingerprints of the known objects and the unknown object is greater than the threshold. 2. The method of claim 1, in which the numerical fingerprint of the unknown object and the numerical fingerprints of each of the known objects comprise a multi-dimensional vector. 3. The method of claim 1, in which the numerical fingerprint of the unknown object and the numerical fingerprints of each of the known objects correspond to characteristics comprising at least one of shape, color, size, and texture. 4. The method of claim 1, in which: the numerical fingerprint of the unknown object comprises a same number of rows and columns as the image, and the numerical fingerprint of the unknown object comprises N-channels for each pixel or descriptor of the image. 5. A system for identifying objects, comprising: a memory; and at least one processor, the at least one processor configured: to generate a feature vector of an unknown object from an image of an environment, in which the feature vector encodes information into a series of numbers to provide a numerical fingerprint for differentiating one feature from another; to compare the numerical fingerprint of the unknown object to numerical fingerprints of known objects of a trained object class, in which each of the known objects comprises multiple numerical fingerprints, and each of the multiple numerical fingerprints is based on images obtained from different views of the known objects to identify the known objects in the different views; to determine whether a similarity between the numerical fingerprint of the unknown object and the numerical fingerprints of one of the known objects satisfies a threshold; to identify the unknown object based on the determination, and to infer that the unknown object is in the same trained object class as one of the known objects of the trained object class when a difference between the numerical fingerprints of the known objects and the unknown object is less than the threshold and/or the similarity between the numerical fingerprints of the known objects and the unknown object is greater than the threshold. 6. The system of claim 5, in which the numerical fingerprint of the unknown object and the numerical fingerprints of each of the known objects comprise a multi-dimensional vector. 7. The system of claim 5, in which the numerical fingerprint of the unknown object and the numerical fingerprints of each of the known objects correspond to characteristics comprising a shape, color, size, and/or texture. 8. The system of claim 5, in which: the numerical fingerprint of the unknown object comprises a same number of rows and columns as the image, and the numerical fingerprint of the unknown object comprises N-channels for each pixel or descriptor of the image. 9. A non-transitory computer-readable medium having program code recorded thereon to identify objects, the program code being executed by a processor and comprising: program code to generate a feature vector of an unknown object from an image of an environment, in which the feature vector encodes information into a series of numbers to provide a numerical fingerprint for differentiating one feature from another; program code to compare the numerical fingerprint of the unknown object to numerical fingerprints of known objects of a trained object class, in which each of the known objects comprises multiple numerical fingerprints, and each of the multiple numerical fingerprints is based on images obtained from different views of the known objects to identify the known objects in the different views; program code to determine whether a similarity between the numerical fingerprint of the unknown object and the numerical fingerprints of one of the known objects satisfies a threshold; program code to identify the unknown object based on the determination; and program code to infer that the unknown object is in the same trained object class as one of the known objects of the trained object class when a difference between the numerical fingerprints of the known objects and the unknown object is less than the threshold and/or the similarity between the numerical fingerprints of the known objects and the unknown object is greater than the threshold. 10. The non-transitory computer-readable medium of claim 9, in which the numerical fingerprint of the unknown object and the numerical fingerprints of each of the known objects comprise a multi-dimensional vector. 11. The non-transitory computer-readable medium of claim 9, in which the numerical fingerprint of the unknown object and the numerical fingerprints of each of the known objects correspond to characteristics comprising shape, color, size, and/or texture. 12. The non-transitory computer-readable medium of claim 9, in which: the numerical fingerprint of the unknown object comprises a same number of rows and columns as the image, and the numerical fingerprint of the unknown object comprises N-channels for each pixel or descriptor of the image.",
                    "FIELD_OF_INVENTION": "Certain aspects of the present disclosure generally relate to robotic devices and, more particularly, to a system and method for teaching a robotic device unknown objects through visual recognition and analysis of elements of an already learned object.",
                    "STATE_OF_THE_ART": "As robotic assistance technology improves, robots may be programmed to execute a wide variety of tasks people perform in an environment, such as a house. Conventionally, a classifier identifies an object by training the classifier with a training set that includes instances of the specific object. In such cases, when the classifier is confronted with an object that is not included in the training set, the classifier selects the closest known object. However, this selection may be incorrect. It is desirable to improve classifiers to identify unknown objects based on elements and features of learned/known objects.",
                    "SUMMARY": [
                        "A method for identifying objects includes generating a feature vector of an unknown object from an image of an environment. The method also includes comparing the feature vector of the unknown object to feature vectors of known objects. The method further includes determining whether a similarity between the feature vector of the unknown object and the feature vector of one of the known objects satisfies a threshold. Furthermore, the method includes identifying the unknown object based on the determination. A system for identifying objects is described. The system having a memory and one or more processors coupled to the memory. The processors is configured to generate a feature vector of an unknown object from an image of an environment. The processors is also configured to compare the feature vector of the unknown object to feature vectors of known objects. The processors is further configured to determine whether a similarity between the feature vector of the unknown object and the feature vector of one of the known objects satisfies a threshold. Furthermore, the processor is configured to identify the unknown object based on the determination. A non-transitory computer-readable medium with non-transitory program code recorded thereon is described. The program code to identify objects. The program code is executed by a processor and includes program code to generate a feature vector of an unknown object from an image of an environment. The program code also includes program code to compare the feature vector of the unknown object to feature vectors of known objects. The program code further includes program code to determine whether a similarity between the feature vector of the unknown object and the feature vector of one of the known objects satisfies a threshold. Furthermore, the program code includes program code to identify the unknown object based on the determination. This has outlined, rather broadly, the features and technical advantages of the present disclosure in order that the detailed description that follows may be better understood. Additional features and advantages of the present disclosure will be described below. It should be appreciated by those skilled in the art that this present disclosure may be readily utilized as a basis for modifying or designing other structures for carrying out the same purposes of the present disclosure. It should also be realized by those skilled in the art that such equivalent constructions do not depart from the teachings of the present disclosure as set forth in the appended claims. The novel features, which are believed to be characteristic of the present disclosure, both as to its organization and method of operation, together with further objects and advantages, will be better understood from the following description when considered in connection with the accompanying figures. It is to be expressly understood, however, that each of the figures is provided for the purpose of illustration and description only and is not intended as a definition of the limits of the present disclosure.",
                        "The features, nature, and advantages of the present disclosure will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify correspondingly throughout. 1 illustrates an example of a classifier system. 2 illustrates an example architecture of a classifier, according to aspects of the present disclosure. 3 illustrates an image of a training environment used to train a robot, according to aspects of the present disclosure. 4 illustrates an example of an image of an environment captured by a sensor of a robotic device, according to aspects of the present disclosure 5 is a diagram illustrating an example of a hardware implementation for a robotic device to recognize unknown objects, according to aspects of the present disclosure. 6 illustrates a method for a robotic device to recognize unknown objects, according to an aspect of the present disclosure."
                    ],
                    "DESCRIPTION": "The detailed description set forth below, in connection with the appended drawings, is intended as a description of various configurations and is not intended to represent the only configurations in which the concepts described herein may be practiced. The detailed description includes specific details for the purpose of providing a thorough understanding of the various concepts. It will be apparent to those skilled in the art, however, that these concepts may be practiced without these specific details. In some instances, well-known structures and components are shown in block diagram form in order to avoid obscuring such concepts. Based on the teachings, one skilled in the art should appreciate that the scope of the present disclosure is intended to cover any aspect of the present disclosure, whether implemented independently of or combined with any other aspect of the present disclosure. For example, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth. In addition, the scope of the present disclosure is intended to cover such an apparatus or method practiced using other structure, functionality, or structure and functionality in addition to, or other than the various aspects of the present disclosure set forth. It should be understood that any aspect of the present disclosure may be embodied by one or more elements of a claim. The word exemplary is used herein to mean serving as an example, instance, or illustration. Any aspect described herein as exemplary is not necessarily to be construed as preferred or advantageous over other aspects. Although particular aspects are described herein, many variations and permutations of these aspects fall within the scope of the present disclosure. Although some benefits and advantages of the preferred aspects are mentioned, the scope of the present disclosure is not intended to be limited to particular benefits, uses or objectives. Rather, aspects of the present disclosure are intended to be broadly applicable to different technologies, system configurations, networks and protocols, some of which are illustrated by way of example in the figures and in the following description of the preferred aspects. The detailed description and drawings are merely illustrative of the present disclosure rather than limiting, the scope of the present disclosure being defined by the appended claims and equivalents thereof. A robotic device may use one or more sensors to identify objects in an environment. The sensors may include a red-green-blue RGB camera, radio detection and ranging RADAR sensor, light detection and ranging LiDAR sensor, or another type of sensor. Conventionally, perception software is used to recognize objects that were included in a training set. Objects in the training set may be referred to as known objects or old objects. Objects that were not used for training may be referred to as unknown objects or new objects. When a classifier is confronted with an unknown object, the classifier may select a most similar known object. However, this selection may be incorrect. Aspects of the present disclosure are directed to systems and methods for training a robotic system to identify new or unknown objects based on elements and features of learned/known objects. The robotic system may also be referred to as a robotic device or robot. The robotic system may be autonomous or semi-autonomous. The objects may include man-made objects , chairs, desks, cars, books, , natural objects , rocks, fruits, trees, animals, , and humans. In one aspect, the robotic system predicts an N dimensional vector or multi-dimensional vector for each detected object. For example, the robotic system predicts a sixteen 16 dimensional vector. In some aspects, the multiple dimensional vector may include any value between ten 10 and twenty 20. Of course, other values may be used. A multi-dimensional vector may be predicted for each pixel of multiple pixels associated with each known object. A feature vector representing characteristics of the known object are allocated to each pixel of the known objects. Each pixel may be associated with multiple feature vectors that are based on images obtained from different views/angles. Given a training set of objects, a neural network is encoded with N dimensional vectors or multiple dimensional vectors for different objects. The values of the multiple dimensional vector may capture feature vectors or embeddings of the objects, such as shape, color, size, texture, or the like. As a result, an unknown object, which is different from an object that the classifier is trained on, can be identified based on the multiple dimensional vectors of the different objects. For example, a neural network may be trained on five different types of fruits and may be later introduced to an unknown fruit that is different than all the others. In one aspect, an image of an environment including the unknown fruit may be captured by a sensor of the robotic device. For example, a neural network associated with the robotic device generates an output of the unknown fruit from the image. The output includes a feature vector of the unknown fruit. The feature vector may be a multiple dimensional vector. The feature vector may be determined for each pixel of the unknown fruit. A neural network architecture used in the present disclosure includes a network layer , a convolutional layer that receives an input, and performs a number of convolutions, with the final output being a tensor. In one aspect, a feature vector associated with the tensor includes a same number of rows and columns as the image. However, instead of having three channels as in conventional tensors, the tensor may include N-channels , sixteen for each pixel , pixel coordinate. For example, for every pixel of an initial image that is processed by the network there may be sixteen values for each pixel at the output. The neural network may use features , the feature vectors of the unknown fruit to infer the type of the unknown fruit. The neural network associated with the robotic device compares the feature vector of the unknown fruit to feature vectors of the known fruits. Based on the comparison, the neural network may classify the unknown fruit. For example, the neural network may determine the color of the fruit based on knowledge of the color of known fruits. Moreover, the neural network may determine that the texture is more similar to a texture of a known fruit. The texture, and the color may be defined by one or more values of the multiple dimensional vector that classify the unknown fruit. Additionally, a shape of the fruit may be identified based on the shapes of known fruits. The shape is also be defined as one or more of the values of the multiple dimensional vector. When a classifier is trained on objects corresponding to different classes, an unknown object that shares characteristics with an object of a training class can be identified based on the similarity to trained characteristics. The neural network determines the differences and similarities between the unknown object and the known objects. For example, the neural network determines whether a difference and/or a similarity between the feature vector of the unknown fruit and the feature vector of a known fruit satisfies a threshold. The selected known fruit may be the known fruit that is most similar to the unknown fruit. For example, the texture and the shape of the unknown fruit is similar to the lime. Thus, in this case the selected one of the known fruits is the lime. The neural network then infers or identifies the unknown fruit based on the determination using the threshold. For example, the unknown fruit is identified as a lime when the similarity between the feature vector of the unknown fruit and the one or more feature vectors of the lime is greater than a similarity threshold. In addition to the similarity comparison, a difference comparison may also be used to infer the type of the unknown fruit. For example, the unknown fruit is identified as a lime when the difference between the feature vector of the unknown fruit and the one or more feature vectors of the lime is less than a difference threshold. In one aspect the unknown fruit may not be identifiable when the similarity between the feature vector of the sixth fruit and the one or more feature vectors of a specified known fruit , the lime less than the similarity threshold. Alternatively or additionally, the unknown fruit may not be identifiable when the difference between the feature vector of the unknown fruit and the one or more feature vectors of the specified known fruit is greater than the difference threshold. Accordingly, aspects of the present disclosure provide a network that is capable of learning unknown objects based on elements of trained objects. Specifically, the network learns , identifies unknown objects by determining similarities and differences between characteristics of trained objects and the unknown object. 1 illustrates an example of a classifier system 100. The classifier system 100 is implemented to perform methods and other functions as disclosed herein. It should be appreciated, that while the classifier system 100 is illustrated as being a single contained system, in various embodiments, the classifier system 100 is a distributed system that is comprised of components that can be provided as a centralized server, a cloud-based service, and so on. The classifier system 100 includes a processor 110. The processor 110 may represent a distributed processing resource, an individual local processor , a CPU, GPU, or application specific processor, or the classifier system 100 may access the processor 110 through a data bus or another communication path. In one aspect, the classifier system 100 includes a memory 120. The memory 120 is a random-access memory RAM, read-only memory ROM, a hard-disk drive, a flash memory, a processor cache, or other suitable memory. Aspects of the classifier system 100 may be implemented as cloud-based services , software as a service, centralized services, and/or other types of services. Additionally, the classifier system 100, in one or more aspects, includes a database 150 that is stored in a separate data store, in the memory 120, or in another suitable configuration. The database 150 may include image data of an object. A machine learning module includes, in one aspect, a machine learning implementation such as a convolutional neural network CNN or other suitable deep learning algorithm. In further aspects, the classifiers can be provided to a robotic device within which the classifier is to be implemented such that the classifier is stored in a local memory thereof. Thus, the robotic device may implement the classifier to analyze objects from a camera 190 so that the robotic device can recognize objects within an environment. 2 provides an example architecture of a classifier 200. As illustrated, the classifier 200 includes a convolutional neural network CNN. The classifier 200 may include a transform module to provide a transform of the input image. The classifier 200 pairs the machine learning implementation with a transformation implementation that perform pre-processing on input data , images. For example, the transformation module accepts inputs of images, executes the pre-processing on the input, and passes the transformed input to layers of convolutional nodes that form the CNN. In one configuration, each convolutional layer is followed by a max pooling layer, connected by, in one or more aspects, rectified linear activations ReLu. The final convolutional layer feeds into a fully-connected ReLu layer. 3 illustrates an image 301 of a training environment 302 used to train a robotic device 300, according to aspects of the present disclosure. In the example of 3, the robotic device 300 is a humanoid robot and the training environment 302 is a kitchen. Aspects of the present disclosure are not limited to a humanoid robot. The robotic device 300 may be any type of autonomous or semi-autonomous device, such as a drone or a vehicle. Additionally, the robotic device 300 may be in any type of environment. The robotic device 300 may be trained to identify unknown objects according to aspects of the present disclosure. In one configuration, the robotic device 300 obtains the image 301 of the training environment 302 via one or more sensors of the robotic device 300. The robotic device 300 may detect and localize one or more objects in the image 301. Localization refers to determining the location , coordinates of a detected object within the image 301. In conventional object detection systems, a bounding box may indicate the location of an object detected in the image 301. The detected objects may be one or more specific classes of objects, such as a table 304, a pushed-in chair 306, a closed window 308, a bottle 310, utensils 320 and 322, a counter 340, a sink 342, a cabinet 330 having a handle 332, or all objects in the image 301. The objects may be detected and identified using an object detection system, such as a pre-trained object detection neural network. A 3D camera on the robotic device 300 captures images of the training environment 302 and an object , a lime or apple on the table from different views/angles. A 3D model of the environment is generated from the captured images. The 3D model is used to create images from viewpoints that differ from the viewpoints of the image 301 captured by the 3D camera. For example, during training, the 3D camera captures the image of the object , apple from an angle. Feature vectors for the object may be determined for the angle , view of the 3D camera. In one aspect, the view from which the object is captured is extrapolated to a circle or some path around the object. Feature vectors from each view of the object are collected as an example of the object. In the context of neural networks, embeddings or feature vectors are low-dimensional, learned continuous vector representations of discrete variables. Aspects of the present disclosure are directed to a system that uses an output , a sixteen dimensional output of the neural network to teach the robotic device to identify unknown objects. For example, given the training set of objects, a neural network is encoded with N dimensional vectors for the different objects. The values of the N dimensional vector may capture feature vectors of the objects, such as shape, color, size, texture, or the like for each selected pixel of each of the training set of objects. As a result, an unknown object, which is different from an object that the classifier is trained on, can be identified based on the N dimensional vectors of the different trained objects. Thus, the robotic device identifies the unknown object after a training phase of the neural network. For example, when a robotic device is tasked with identifying an unknown object, the neural network of the robotic device receives an image , an RGB image as an input and outputs a feature vector including values assigned to each pixel of the image. The feature vector may include characteristics of the image encoded into a series of numbers to provide a numerical fingerprint for differentiating one feature from another. The feature vectors act as a feature descriptor that may establish per pixel correspondences of different images. Because feature vectors may be defined at each pixel, keyframes images may be as broad as including every pixel in the image, or as narrow as only using specified pixels or pixels in a user-defined mask. In one aspect, the neural network , a dense embeddings network processes the input image and outputs the N dimensional feature vector of the pixels of the image. Each pixel of the input image may correspond to a feature vector that is sixteen dimensional. A user may only select one pixel, or descriptor, of an image. The feature vectors and image may be combined to generate a voxel map. The feature vectors may also be input to a matcher , keyframe or image matcher that compares the feature vectors of the pixels of the unknown object with feature vectors , feature vectors of known objects of pixels of known objects. In one aspect, the images of the set of training objects and the unknown object may be obtained from different view/angles. Each of the set of objects , an apple, a bottle, may be taught from different angles. In one aspect, the robotic device may extrapolate a selected pixel into a three dimensional representation. Thus, when the robotic device moves around, it can match feature vectors without interaction from a user. As such, an accuracy of the classifier may increase without further interaction from the user. Accordingly, the user interaction time is reduced. More data may be obtained due to the reduced user interaction time. For example, the apple is taught from one angle, then the angle is extrapolated to a circle or some path around the apple. Feature vectors may be obtained for each view of the apple as an example of the apple. A number of examples of the apple can be obtained, which increases reliability of identification of the unknown object as the unknown object can be compared to feature vectors corresponding to different views/angles of the known object. The feature of extrapolating a single pixel into a three dimensional representation of feature vectors is used to train the robotic device with different sets of training objects. The data from the training or teaching stage is fed into the robotic device and is used to recognize an unknown object. For example, consider a soda can that has a front and a back label where the front label is different from the back label. The ability to identify both the front label and the back label with a single selected pixel is advantageous because of the reduced interaction from the user. In one aspect, the robotic device maps a pixel of an object to a three dimensional location using depth information and three dimensional features. As the robotic device moves around the same object, the robotic device can select the pixel of the object using depth information. The selected pixel is a three dimensional point, which is a first projected point corresponding to an initial pixel and an initial view point. As the robot moves, the three dimensional point can be re-projected into a new camera position or view point that represents the object. The feature vectors associated with the different viewpoints are matched to pixels of the object and stored for use in recognizing the unknown object. 4 illustrates an example of an image 450 of an environment 402 captured by a sensor of a robotic device 400, according to aspects of the present disclosure. The sensor may be a three dimensional camera. The environment 402 is a dining area , a restaurant dining area that includes an unknown object , an unknown fruit 410 on a table 404. The environment also includes a chair 406, a window 408, a cabinet 430 and utensils 420, 422 on the cabinet 430. In one example, the neural network associated with the robotic device 400 is trained on five different types of fruits , a banana, an apple, a lime, a blueberry, and a grape. The neural network is later introduced to the unknown fruit 410 that is different than all the others. The neural network associated with the robotic device 400 may use feature vectors to infer or identify the type of the unknown fruit 410. In one aspect, the neural network compares the feature vector of the unknown fruit 410 to feature vectors of known fruits. For example, the neural network may determine the color of the unknown fruit 410 based on knowledge of the color of known fruit. Moreover, the neural network may determine that the texture of the unknown fruit 410 based on texture of known fruit. The round shape of the unknown fruit 410 may be determined to be the same as a shape of a known fruit. The neural network determines the differences and similarities between the feature vectors of the unknown fruit 410 and the known fruits. For example, the neural network determines whether a difference and/or a similarity between the feature vector of the unknown fruit 410 and the feature vector of a selected one , the lime of the known fruits meets a threshold. The neural network identifies the unknown fruit 410 based on the determination using the threshold. For example, the unknown fruit 410 is identified as a lime when the similarity between the feature vector of the unknown fruit 410 and the one or more feature vectors of the lime is greater than a similarity threshold. In addition to the similarity comparison, a difference comparison may also be used to infer the type of the unknown fruit 410. In this example, the unknown fruit 410 is identified as a lime when the difference between the feature vector of the unknown fruit and the one or more feature vectors of the lime is less than a difference threshold. 5 is a diagram illustrating an example of a hardware implementation for an object identification system 500, according to aspects of the present disclosure. The object identification system 500 may be a component of a vehicle, a robotic device, or another device. For example, as shown in 5, the object identification system 500 is a component of a robotic device 400. Aspects of the present disclosure are not limited to the object identification system 500 being a component of the robotic device 400. Other devices, such as a bus, boat, drone, or vehicle, are also contemplated for using the object identification system 500. The robotic device 400 may operate in at least an autonomous operating mode and a manual operating mode. The object identification system 500 may be implemented with a bus architecture, represented generally by a bus 550. The bus 550 may include any number of interconnecting buses and bridges depending on the specific application of the object identification system 500 and the overall design constraints. The bus 550 links together various circuits including one or more processors and/or hardware modules, represented by a processor 520, a communication module 522, a location module 524, a sensor module 502, a locomotion module 526, a navigation module 528, and a computer-readable medium 530. The bus 550 may also link various other circuits such as timing sources, peripherals, voltage regulators, and power management circuits, which are well known in the art, and therefore, will not be described any further. The object identification system 500 includes a transceiver 540 coupled to the processor 520, the sensor module 502, an object identification module 510, an object matching module 512, the communication module 522, the location module 524, the locomotion module 526, the navigation module 528, and the computer-readable medium 530. The transceiver 540 is coupled to an antenna 542. The transceiver 540 communicates with various other devices over a transmission medium. For example, the transceiver 540 may receive commands via transmissions from a user or a remote device. As another example, the transceiver 540 may transmit statistics and other information from the object identification module 510 to a server not shown. The object identification system 500 includes the processor 520 coupled to the computer-readable medium 530. The processor 520 performs processing, including the execution of software stored on the computer-readable medium 530 to provide functionality according to the disclosure. The software, when executed by the processor 520, causes the object identification system 500 to perform the various functions described for a particular device, such as the robotic device 400, or any of the modules 502, 510, 512, 522, 524, 526, and 528. The computer-readable medium 530 may also be used for storing data that is manipulated by the processor 520 when executing the software. The sensor module 502 may obtain measurements via different sensors, such as a first sensor 504 and a second sensor 506. The first sensor 504 may be a vision sensor, such as a stereoscopic camera or a red-green-blue RGB camera, for capturing 3D images. The second sensor 506 may be a ranging sensor, such as a light detection and ranging LiDAR sensor or a radio detection and ranging RADAR sensor. Of course, aspects of the present disclosure are not limited to the aforementioned sensors as other types of sensors, such as, for example, thermal, sonar, and/or lasers are also contemplated for either of the first sensor 504 and the second sensor 506. The measurements of the first sensor 504 and the second sensor 506 may be processed by one or more of the processor 520, the sensor module 502, the object identification module 510, the communication module 522, the location module 524, the locomotion module 526, the navigation module 528, in conjunction with the computer-readable medium 530 to implement the functionality described herein. In one configuration, the data captured by the first sensor 504 and the second sensor 506 may be transmitted to an external device via the transceiver 540. The first sensor 504 and the second sensor 506 may be coupled to the robotic device 400 or may be in communication with the robotic device 400. The location module 524 may determine a location of the robotic device 400. For example, the location module 524 may use a global positioning system GPS to determine the location of the robotic device 400. The communication module 522 may facilitate communications via the transceiver 540. For example, the communication module 522 may be configured to provide communication capabilities via different wireless protocols, such as Wi Fi, long term evolution LTE, 5G, etc. The communication module 522 may also be used to communicate with other components of the robotic device 400 that are not modules of the object identification system 500. The locomotion module 526 may facilitate locomotion of the robotic device 400. As another example, the locomotion module 526 may be in communication with one or more power sources of the robotic device 400, such as a motor and/or batteries. The locomotion may be proved via wheels, moveable limbs, propellers, treads, fins, jet engines, and/or other sources of locomotion. The robotic device 400 may use one or more sensors, such as the first sensor 504 and the second sensor 506 to identify objects in an environment. The sensors may include a red-green-blue RGB camera, radio detection and ranging RADAR sensor, light detection and ranging LiDAR sensor, or another type of sensor. In one aspect, an image of the environment is received by the object identification system 500 of the robotic device 400. The object identification system 500 includes an object identification module 510 that generates an output of the unknown object identified in the image of the environment. The output includes a feature vector of the unknown object. The object identification module 510 may include an object matching module 512 , keyframe or image matcher. For example, the feature vectors may be input to the object matching module 512, which compares the feature vectors of the pixels of the unknown object with feature vectors of pixels of known objects. The object identification module 510 determines whether a difference and/or a similarity between the feature vector of the unknown object and the feature vector of a selected one of the known objects meets a threshold. The object identification module 510 then identifies the unknown object based on the determination. The modules may be software modules running in the processor 520, resident/stored in the computer-readable medium 530, one or more hardware modules coupled to the processor 520, or some combination thereof. The object identification module 510 may be in communication with the sensor module 502, the transceiver 540, the processor 520, the communication module 522, the location module 524, the locomotion module 526, the navigation module 528, and the computer-readable medium 530. In one configuration, the object identification module 510 receives sensor data from the sensor module 502. The sensor module 502 may receive the sensor data from the first sensor 504 and the second sensor 506. According to aspects of the present disclosure, the sensor module 502 may filter the data to remove noise, encode the data, decode the data, merge the data, extract frames, or perform other functions. In an alternate configuration, the object identification module 510 may receive sensor data directly from the first sensor 504 and the second sensor 506. In one configuration, the object identification module 510 identifies detected objects based on information from the processor 520, the location module 524, the computer-readable medium 530, the first sensor 504, and/or the second sensor 506. 6 illustrates a method 600 for a robotic device to recognize an unknown objects, according to an aspect of the present disclosure. At block 602, a feature vector of an unknown object from an image of an environment is generated. At block 604, the feature vector of the unknown object is compared to feature vectors of known objects. At block 606, the robotic device determines whether a similarity between the feature vector of the unknown object and the feature vector of one of the known objects satisfies a threshold. At block 608, the robotic device identifies the unknown object based on the determination. The various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions. The means may include various hardware and/or software components and/or modules, including, but not limited to, a circuit, an application specific integrated circuit ASIC, or processor. Generally, where there are operations illustrated in the figures, those operations may have corresponding counterpart means-plus-function components with similar numbering. As used herein, the term determining encompasses a wide variety of actions. For example, determining may include calculating, computing, processing, deriving, investigating, looking up , looking up in a table, a database or another data structure, ascertaining and the like. Additionally, determining may include receiving , receiving information, accessing , accessing data in a memory and the like. Furthermore, determining may include resolving, selecting, choosing, establishing, and the like. As used herein, a phrase referring to at least one of a list of items refers to any combination of those items, including single members. As an example, at least one of: a, b, or c is intended to cover: a, b, c, a-b, a-c, b-c, and a-b-c. The various illustrative logical blocks, modules and circuits described in connection with the present disclosure may be implemented or performed with a processor configured according to the present disclosure, a digital signal processor DSP, an application specific integrated circuit ASIC, a field programmable gate array signal FPGA or other programmable logic device PLD, discrete gate or transistor logic, discrete hardware components or any combination thereof designed to perform the functions described herein. The processor may be a microprocessor, controller, microcontroller, or state machine specially configured as described herein. A processor may also be implemented as a combination of computing devices, , a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or such other special configuration, as described herein. The steps of a method or algorithm described in connection with the present disclosure may be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two. A software module may reside in storage or machine readable medium, including random access memory RAM, read only memory ROM, flash memory, erasable programmable read-only memory EPROM, electrically erasable programmable read-only memory EEPROM, registers, a hard disk, a removable disk, a CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. A software module may comprise a single instruction, or many instructions, and may be distributed over several different code segments, among different programs, and across multiple storage media. A storage medium may be coupled to a processor such that the processor can read information from, and write information to, the storage medium. In the alternative, the storage medium may be integral to the processor. The methods disclosed herein comprise one or more steps or actions for achieving the described method. The method steps and/or actions may be interchanged with one another without departing from the scope of the claims. In other words, unless a specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims. The functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in hardware, an example hardware configuration may comprise a processing system in a device. The processing system may be implemented with a bus architecture. The bus may include any number of interconnecting buses and bridges depending on the specific application of the processing system and the overall design constraints. The bus may link together various circuits including a processor, machine-readable media, and a bus interface. The bus interface may connect a network adapter, among other things, to the processing system via the bus. The network adapter may implement signal processing functions. For certain aspects, a user interface , keypad, display, mouse, joystick, may also be connected to the bus. The bus may also link various other circuits such as timing sources, peripherals, voltage regulators, power management circuits, and the like, which are well known in the art, and therefore, will not be described any further. The processor may be responsible for managing the bus and processing, including the execution of software stored on the machine-readable media. Software shall be construed to mean instructions, data, or any combination thereof, whether referred to as software, firmware, middleware, microcode, hardware description language, or otherwise. In a hardware implementation, the machine-readable media may be part of the processing system separate from the processor. However, as those skilled in the art will readily appreciate, the machine-readable media, or any portion thereof, may be external to the processing system. By way of example, the machine-readable media may include a transmission line, a carrier wave modulated by data, and/or a computer product separate from the device, all which may be accessed by the processor through the bus interface. Alternatively, or in addition, the machine-readable media, or any portion thereof, may be integrated into the processor, such as the case may be with cache and/or specialized register files. Although the various components discussed may be described as having a specific location, such as a local component, they may also be configured in various ways, such as certain components being configured as part of a distributed computing system. The processing system may be configured with one or more microprocessors providing the processor functionality and external memory providing at least a portion of the machine-readable media, all linked together with other supporting circuitry through an external bus architecture. Alternatively, the processing system may comprise one or more neuromorphic processors for implementing the neuron models and models of neural systems described herein. As another alternative, the processing system may be implemented with an application specific integrated circuit ASIC with the processor, the bus interface, the user interface, supporting circuitry, and at least a portion of the machine-readable media integrated into a single chip, or with one or more field programmable gate arrays FPGAs, programmable logic devices PLDs, controllers, state machines, gated logic, discrete hardware components, or any other suitable circuitry, or any combination of circuits that can perform the various functions described throughout this present disclosure. Those skilled in the art will recognize how best to implement the described functionality for the processing system depending on the particular application and the overall design constraints imposed on the overall system. The machine-readable media may comprise a number of software modules. The software modules may include a transmission module and a receiving module. Each software module may reside in a single storage device or be distributed across multiple storage devices. By way of example, a software module may be loaded into RAM from a hard drive when a triggering event occurs. During execution of the software module, the processor may load some of the instructions into cache to increase access speed. One or more cache lines may then be loaded into a special purpose register file for execution by the processor. When referring to the functionality of a software module below, it will be understood that such functionality is implemented by the processor when executing instructions from that software module. Furthermore, it should be appreciated that aspects of the present disclosure result in improvements to the functioning of the processor, computer, machine, or other system implementing such aspects. If implemented in software, the functions may be stored or transmitted over as one or more instructions or code on a computer-readable medium. Computer-readable media include both computer storage media and communication media including any storage medium that facilitates transfer of a computer program from one place to another. Additionally, any connection is properly termed a computer-readable medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line DSL, or wireless technologies such as infrared IR, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. Disk and disc, as used herein, include compact disc CD, laser disc, optical disc, digital versatile disc DVD, floppy disk, and Blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Thus, in some aspects computer-readable media may comprise non-transitory computer-readable media , tangible media. In addition, for other aspects computer-readable media may comprise transitory computer-readable media , a signal. Combinations of the above should also be included within the scope of computer-readable media. Thus, certain aspects may comprise a computer program product for performing the operations presented herein. For example, such a computer program product may comprise a computer-readable medium having instructions stored and/or encoded thereon, the instructions being executable by one or more processors to perform the operations described herein. For certain aspects, the computer program product may include packaging material. Further, it should be appreciated that modules and/or other appropriate means for performing the methods and techniques described herein can be downloaded and/or otherwise obtained by a user terminal and/or base station as applicable. For example, such a device can be coupled to a server to facilitate the transfer of means for performing the methods described herein. Alternatively, various methods described herein can be provided via storage means, such that a user terminal and/or base station can obtain the various methods upon coupling or providing the storage means to the device. Moreover, any other suitable technique for providing the methods and techniques described herein to a device can be utilized. It is to be understood that the claims are not limited to the precise configuration and components illustrated above. Various modifications, changes, and variations may be made in the arrangement, operation, and details of the methods and apparatus described above without departing from the scope of the claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/70/281/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9904590845108032,
                    "F_SPEC_PARAMS": [
                        "turning radius",
                        "scuffing"
                    ],
                    "S_SPEC_PARAMS": [
                        "front wheels are dragged, the left or right rear wheel slips, and the right or left rear wheel slides",
                        "turn more sharply,"
                    ],
                    "A_PARAMS": [
                        "locking the right rear wheel",
                        "Independent brakes"
                    ],
                    "F_SENTS": [
                        "Because of mechanical limits of traditional steer axles, this decreased turning radius by braking the inside wheel results in scuffing."
                    ],
                    "S_SENTS": [
                        "For tractors known in the art, when the inside wheel is locked, the steering angles of the front wheels do not allow the axes of the front wheels to intersect at the rear inside wheel.",
                        "Known steered wheels, especially driven ones, do not turn that sharply.",
                        "The result of locking the right rear wheel on a traditional tractor, even when the front wheels are steered all the way to the right or left, is that the front wheels are dragged, the left or right rear wheel slips, and the right or left rear wheel slides.",
                        "Independent brakes help a tractor turn more sharply, but the tractor prevents a pivot turn."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Area of Moving Object"
                    ],
                    "F_SIM_SCORE": 0.5732467770576477,
                    "S_TRIZ_PARAMS": [
                        "Shape"
                    ],
                    "S_SIM_SCORE": 0.47627103328704834,
                    "GLOBAL_SCORE": 1.7152179896831512
                },
                "sort": [
                    1.715218
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11270601-20220308",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11270601-20220308",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-06-25",
                    "PUBLICATION_DATE": "2022-03-08",
                    "INVENTORS": [
                        "Haoran Yu",
                        "Pablo Eduardo Garcia Kilroy",
                        "Bernard Fai Kin Siu",
                        "Eric Mark Johnson"
                    ],
                    "APPLICANTS": [
                        "VERB SURGICAL INC.    ( Mountain View , US )"
                    ],
                    "INVENTION_TITLE": "Virtual reality system for simulating a robotic surgical environment",
                    "DOMAIN": "G09B 900",
                    "ABSTRACT": "A virtual reality system may generate a virtual robotic surgical environment using a client application, where the virtual robotic surgical environment includes at least one virtual robotic component. In response to a user input to move the at least one virtual robotic component in the virtual robotic surgical environment, the system may pass status information regarding the at least one virtual robotic component from the client application to a server application, generate an actuation command based on the user input and the status information using the server application, pass the actuation command from the server application to the client application, and move the at least one virtual robotic component based on the actuation command. The client application and the server application may be run on a shared processor device, or on separate processor devices.",
                    "CLAIMS": "1. A virtual reality system for simulating a robotic surgical environment, the system comprising: one or more processors configured to render a computer-generated virtual robotic surgical environment in a client application to a head-mounted display, the computer-generated virtual robotic surgical environment comprising one or more virtual robotic arms mounted on a virtual patient table wherein the one or more virtual robotic arms are generated based on kinematic models and control modes of a real robotic arm; and one or more handheld devices configured to receive a user input to move one of the one or more virtual robotic arms; wherein the one or more processors are further configured to: transmit a status of the one of the one or more virtual robotic arms to a server application; generate an actuation command in the server application based on the user input, and the status of the one of the one or more virtual robotic arms; and transmit the actuation command back to the client application for rendering, to the head-mounted display, a motion of the one of the one or more virtual robotic arms in response to the user input in the client application. 2. The virtual reality system of claim 1, further comprising a model database storing a plurality of models of virtual components in the computer-generated virtual robotic surgical environment. 3. The virtual reality system of claim 2, wherein the plurality of models of virtual components comprises configuration files for a virtual operating room, the one or more virtual robotic arms including respective one or more joints and links, the virtual patient table, a virtual cannula, a virtual trocar, a virtual surgical instrument, a virtual patient and a virtual staff. 4. The virtual reality system of claim 3, wherein the one or more processors are further configured to receive the configuration files from the model database for rendering the computer-generated virtual robotic surgical environment. 5. The virtual reality system of claim 2, wherein the model database further stores the kinematic models and the control modes of the real robotic arm that are associated with the plurality of models of virtual components. 6. The virtual reality system of claim 5, wherein generating the actuation command is based on the kinematic models and the control modes of the real robotic arm that are associated with the plurality of models of virtual components. 7. The virtual reality system of claim 1, wherein the user input comprises activating a virtual touchpoint on a surface of the one or more virtual robotic arms for manipulating the control modes. 8. The virtual reality system of claim 1, wherein the status of the one of the one or more virtual robotic arms comprises of position, orientation, and velocity of one or more joints of the one of the one or more virtual robotic arms. 9. The virtual reality system of claim 1, wherein transmitting the status of the one of the one or more virtual robotic arms and the actuation command comprises communicating between the client application and the server application through an application programming interface. 10. The virtual reality system of claim 1, wherein the client application and the server application are run on a shared processor device or separate devices. 11. A computer-implemented method for operating a computer-generated virtual operating room, comprising: generating, by one or more processors, the computer-generated virtual operating room comprising at least one virtual component; presenting the computer-generated virtual operating room to a head-mounted display; receiving a user input to interact with the at least one virtual component in the computer-generated virtual operating room; in response to the user input, requesting an actuation command from a server by sending the user input and a status of the at least one virtual component to the server; and rendering, to the head-mounted display, a motion of the at least one virtual component based on a received actuation command. 12. The computer-implemented method of claim 11, further comprises receiving one or more models for the at least one virtual component from a model database for gene rating the computer-generated virtual operating room. 13. The computer-implemented method of claim 11, wherein the at least one virtual component comprises the computer-generated virtual operating room, a virtual robotic arm including one or more joints and links, a virtual patient table, a virtual cannula, a virtual trocar, a virtual surgical instrument, a virtual patient and a virtual staff. 14. The computer-implemented method of claim 11, wherein requesting and receiving the actuation command from the server are through an application programming interface comprising a plurality of definitions of data structures for the at least one virtual component. 15. The computer-implemented method of claim 11, wherein the at least one virtual component is a virtual robotic arm mounted on a virtual patient table in the computer-generated virtual operating room. 16. The computer-implemented method of claim 11, wherein the at least one virtual component includes a virtual robotic arm, and the status of the at least one virtual component comprises position, orientation, and velocity of one or more joints of the virtual robotic arm. 17. A computer-implemented method for operating a computer-generated virtual operating room, comprising: receiving from a client, a request for an actuation command fora virtual robotic arm mounted on a virtual patient table in the computer-generated virtual operating room, the request comprising a user input to move the virtual robotic arm around the virtual patient table and a status of the virtual robotic arm; in response to the request, generating, by one or more processors, the actuation command based on the user input and the status of the virtual robotic arm; and sending the actuation command to the client for rendering a motion of the virtual robotic arm. 18. The computer-implemented method of claim 17, wherein receiving the request and sending the actuation command are through an application programming interface comprising a plurality of data structures for the virtual robotic arm and the virtual patient table. 19. The computer-implemented method of claim 17, wherein generating the actuation command is further based on a kinematic model and a control mode of the virtual robotic arm. 20. The computer-implemented method of claim 19, wherein the user input comprises of activating a virtual touchpoint on the virtual robotic arm for manipulating a control mode of the virtual robotic arm.",
                    "FIELD_OF_INVENTION": "This invention relates generally to the field of robotic surgery, and more specifically to new and useful systems and methods for providing virtual robotic surgical environments.",
                    "STATE_OF_THE_ART": "Minimally-invasive surgery MIS, such as laparoscopic surgery, involves techniques intended to reduce tissue damage during a surgical procedure. For example, laparoscopic procedures typically involve creating a number of small incisions in the patient , in the abdomen, and introducing one or more surgical instruments , an end effector, at least one camera, through the incisions into the patient. The surgical procedures may then be performed using the introduced surgical instruments, with the visualization aid provided by the camera. Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery. In some embodiments, MIS may be performed with robotic systems that include one or more robotic arms for manipulating surgical instruments based on commands from an operator. A robotic arm may, for example, support at its distal end various devices such as surgical end effectors, imaging devices, cannulae for providing access to the patient's body cavity and organs, etc. Robotic surgical systems are generally complex systems performing complex procedures. Accordingly, a user , surgeons generally may require significant training and experience to successfully operate a robotic surgical system. Such training and experience is advantageous to effectively plan the specifics of MIS procedures , determine optimal number, location, and orientation of robotic arms, determine optical number and location of incisions, determine optimal types and sizes of surgical instruments, determine order of actions in a procedure, . Additionally, the design process of robotic surgical systems may also be complicated. For example, improvements in hardware , robotic arms are prototyped as physical embodiments and physically tested. Improvements in software , control algorithms for robotic arms may also require physical embodiments. Such cyclical prototyping and testing is generally cumulatively expensive and time-consuming.",
                    "SUMMARY": [
                        "Generally, a virtual reality system for providing a virtual robotic surgical environment may include a virtual reality processor , a processor in a computer implementing instructions stored in memory for generating a virtual robotic surgical environment, a head-mounted display wearable by a user, and one or more handheld controllers manipulable by the user for interacting with the virtual robotic surgical environment. The virtual reality processor may, in some variations, be configured to generate a virtual robotic surgical environment based on at least one predetermined configuration file describing a virtual component , virtual robotic component in the virtual environment. The head-mounted display may include an immersive display for displaying the virtual robotic surgical environment to the user , with a first-person perspective view of the virtual environment. In some variations, the virtual reality system may additionally or alternatively include an external display for displaying the virtual robotic surgical environment. The immersive display and the external display, if both are present, may be synchronized to show the same or similar content. The virtual reality system may be configured to generate a virtual robotic surgical environment within which a user may navigate around a virtual operating room and interact with virtual objects via the head-mounted display and/or handheld controllers. The virtual reality system and variations thereof, as further described herein may serve as a useful tool with respect to robotic surgery, in applications including but not limited to training, simulation, and/or collaboration among multiple persons. In some variations, a virtual reality system may interface with a real or actual non-virtual operating room. The virtual reality system may enable visualization of a robotic surgical environment, and may include a virtual reality processor configured to generate a virtual robotic surgical environment comprising at least one virtual robotic component, and at least one sensor in a robotic surgical environment. The sensor may be in communication with the virtual reality processor and configured to detect a status of a robotic component corresponding to the virtual robotic component. The virtual reality processor is configured to receive the detected status of the robotic component and modify the virtual robotic component based at least in part on the detected status such that the virtual robotic component mimics the robotic component. For example, a user may monitor an actual robotic surgical procedure in a real operating room via a virtual reality system that interfaces with the real operating room , the user may interact with a virtual reality environment that is reflective of the conditions in the real operating room. Detected positions of robotic components during a surgical procedure may be compared with their expected positions as determined from surgical pre-planning in a virtual environment, such that deviations from the surgical plan may trigger a surgeon to perform adjustments to avoid collisions , change a pose of a robotic arm, . In some variations, the one or more sensors may be configured to detect characteristics or status of a robotic component such as position, orientation, speed, and/or velocity. As an illustrative example, the one or more sensors in the robotic surgical environment may be configured to detect position and/or orientation of a robotic component such as a robotic arm. The position and orientation of the robotic arm may be fed to the virtual reality processor, which moves or otherwise modifies a virtual robotic arm corresponding to the actual robotic arm. As such, a user viewing the virtual robotic surgical environment may visualize the adjusted virtual robotic arm. As another illustrative example, one or more sensors may be configured to detect a collision involving the robotic component in the robotic surgical environment, and the system may provide an alarm notifying the user of the occurrence of the collision. Within the virtual reality system, various user modes enable different kinds of interactions between a user and the virtual robotic surgical environment. For example, one variation of a method for facilitating navigation of a virtual robotic surgical environment includes displaying a first-person perspective view of the virtual robotic surgical environment from a first vantage point within the virtual robotic surgical environment, displaying a first window view of the virtual robotic surgical environment from a second vantage point and displaying a second window view of the virtual robotic surgical environment from a third vantage point. The first and second window views may be displayed in respective regions of the displayed first-person perspective view. Additionally, the method may include, in response to a user input associating the first and second window views, sequentially linking the first and second window views to generate a trajectory between the second and third vantage points. Window views of the virtual robotic surgical environment may be displayed at different scale factors , zoom levels, and may offer views of the virtual environment from any suitable vantage point in the virtual environment, such as inside a virtual patient, overhead the virtual patient, etc. In response to a user input indicating selection of a particular window view, the method may include displaying a new first-person perspective view of the virtual environment from the vantage point of the selected window view. In other words, the window views may, for example, operate as portals facilitating transportation between different vantage points within the virtual environment. As another example of user interaction between a user and the virtual robotic surgical environment, one variation of a method for facilitating visualization of a virtual robotic surgical environment includes displaying a first-person perspective view of the virtual robotic surgical environment from a first vantage point within the virtual robotic surgical environment, receiving a user input indicating placement of a virtual camera at a second vantage point within the virtual robotic surgical environment different from the first vantage point, generating a virtual camera perspective view of the virtual robotic surgical environment from the second vantage point, and displaying the virtual camera perspective view in a region of the displayed first-person perspective view. The camera view may, for example, provide a supplemental view of the virtual environment to the user that enables the user to monitor various aspects of the environment simultaneously while still maintaining primary focus on a main, first-person perspective view. In some variations, the method may further include receiving a user input indicating a selection of a virtual camera type , movie camera configured to be placed outside a virtual patient, an endoscopic camera configured to be placed inside a virtual patient, a 360-degree camera, and displaying a virtual model of the selected virtual camera type at the second vantage point within the virtual robotic surgical environment. Other examples of user interactions with the virtual environment are described herein. In another variation of a virtual reality system, the virtual reality system may simulate a robotic surgical environment in which a user may operate both a robotically-controlled surgical instrument using a handheld controller and a manual laparoscopic surgical instrument , while adjacent a patient table, or over the bed. For example, a virtual reality system for simulating a robotic surgical environment may include a virtual reality controller configured to generate a virtual robotic surgical environment comprising at least one virtual robotic arm and at least one virtual manual laparoscopic tool, a first handheld device communicatively coupled to the virtual reality controller for manipulating the at least one virtual robotic arm in the virtual robotic surgical environment, and a second handheld device comprising a handheld portion and a tool feature representative of at least a portion of a manual laparoscopic tool, wherein the second handheld device is communicatively coupled to the virtual reality controller for manipulating the at least one virtual manual laparoscopic tool in the virtual robotic surgical environment. For example, in some variations, the tool feature may include a tool shaft and a shaft adapter for coupling the tool shaft to the handheld portion of the second handheld device , the shaft adapter may include fasteners. The second handheld device may be a functioning manual laparoscopic tool or a mock-up , facsimile or genericized version of a manual laparoscopic tool, whose movements , in the tool feature may be mapped by the virtual reality controller to correspond to movements of the virtual manual laparoscopic tool. The second handheld device may be modular. For example, the tool feature may be removable from the handheld portion of the second handheld device, thereby enabling the second handheld device to function as a laparoscopic handheld device for controlling a virtual manual laparoscopic tool when the tool feature is attached to the handheld portion, as well as a non-laparoscopic handheld device , for controlling a robotically-controlled tool or robotic arm when the tool feature is detached from the handheld portion. In some variations, the handheld portion of the second handheld device may be substantially similar to the first handheld device. The handheld portion of the second handheld device may include an interactive feature, such as a trigger or button, which actuates a function of the virtual manual laparoscopic tool in response to engagement of the interactive feature by a user. For example, a trigger on the handheld portion of the second handheld device may be mapped to a virtual trigger on the virtual manual laparoscopic tool. As an illustrative example, in a variation in which the virtual manual laparoscopic tool is a virtual manual laparoscopic stapler, a trigger on the handheld portion may be mapped to firing a virtual staple in the virtual environment. Other aspects of the system may further approximate the virtual tool setup in the virtual environment. For example, the virtual reality system may further include a patient simulator , mock patient abdomen including a cannula configured to receive at least a portion of the tool feature of the second handheld device, to thereby further simulate the user feel of a manual laparoscopic tool. Generally, a computer-implemented method for operating a virtual robotic surgical environment may include generating a virtual robotic surgical environment using a client application, where the virtual robotic surgical environment includes at least one virtual robotic component, and passing information between two software applications in order to effect movements of the virtual robotic component. For example, in response to a user input to move the at least one virtual robotic component in the virtual robotic surgical environment, the method may include passing status information regarding the at least one virtual robotic component from the client application to a server application, generating an actuation command based on the user input and the status information using the server application, passing the actuation command from the server application to the client application, and moving the at least one virtual robotic component based on the actuation command. The client application and the server application may be run on a shared processor device, or on separate processor devices. In some variations, passing status information and/or passing the actuation command may include invoking an application programming interface API to support communication between the client and server applications. The API may include one or more definitions of data structures for virtual robotic components and other virtual components in the virtual environment. For example, the API may include a plurality of data structures for a virtual robotic arm, a virtual robotic arm segment , link, a virtual patient table, a virtual cannula, and/or a virtual surgical instrument. As another example, the API may include a data structure for a virtual touchpoint for allowing manipulation of at least one virtual robotic component , virtual robotic arm or other virtual component. For example, the method may include passing status information regarding a virtual robotic arm, such as position and orientation , pose of the virtual robotic arm. The client application may pass such status information to the server application, whereupon the server application may generate an actuation command based on kinematics associated with the virtual robotic arm. As described herein, there are various applications and uses for the virtual reality system. In one variation, the virtual reality system may be used to expedite the R&amp;D cycle during development of a robotic surgical system, such as by allowing simulation of potential design without the time and significant expense of physical prototypes. For example, a method for designing a robotic surgical system may include generating a virtual model of a robotic surgical system, testing the virtual model of the robotic surgical system in a virtual operating room environment, modifying the virtual model of the robotic surgical system based on the testing, and generating a real model of the robotic surgical system based on the modified virtual model. Testing the virtual model may, for example, involve performing a virtual surgical procedure using a virtual robotic arm and a virtual surgical instrument supported by the virtual robotic arm, such as through the client application described herein. During a test, the system may detect one or more collision events involving the virtual robotic arm, which may, for example, trigger a modification to the virtual model , modifying the virtual robotic arm in link length, diameter, in response to the detected collision event. Further testing of the modified virtual model may then be performed, to thereby confirm whether the modification reduced the likelihood of the collision event occurring during the virtual surgical procedure. Accordingly, testing and modifying robotic surgical system designs in a virtual environment may be used to identify issues before testing physical prototypes of the designs. In another variation, the virtual reality system may be used to test a control mode for a robotic surgical component. For example, a method for testing a control mode for a robotic surgical component may include generating a virtual robotic surgical environment, the virtual robotic surgical environment comprising at least one virtual robotic component corresponding to the robotic surgical component, emulating a control mode for the robotic surgical component in the virtual robotic surgical environment, and, in response to a user input to move the at least one virtual robotic component, moving the at least one virtual robotic component in accordance with the emulated control mode. In some variations, moving the virtual robotic component may include passing status information regarding the at least one virtual robotic component from a first application , virtual operating environment application to a second application , kinematics application, generating an actuation command based on the status information and the emulated control mode, passing the actuation command from the second application to the first application, and moving the at least one virtual robotic component in the virtual robotic surgical environment based on the actuation command. For example, the control mode to be tested may be a trajectory following control mode for a robotic arm. In trajectory following, movement of the robotic arm may be programmed then emulated using the virtual reality system. Accordingly, when the system is used to emulate a trajectory following control mode, the actuation command generated by a kinematics application may include generating an actuated command for each of a plurality of virtual joints in the virtual robotic arm. This set of actuated commands may be implemented by a virtual operating environment application to move the virtual robotic arm in the virtual environment, thereby allowing testing for collision, volume or workspace of movement, etc. Other variations and examples of virtual reality systems, their user modes and interactions, and applications and uses of the virtual reality systems, are described in further detail herein.",
                        "1A depicts an example of an operating room arrangement with a robotic surgical system and a surgeon console. 1B is a schematic illustration of one exemplary variation of a robotic arm manipulator, tool driver, and cannula with a surgical tool. 2A is a schematic illustration of one variation of a virtual reality system. 2B is a schematic illustration of an immersive display for displaying an immersive view of a virtual reality environment. 3 is a schematic illustration of components of a virtual reality system. 4A is an exemplary structure for a communication between a virtual reality environment application and a kinematics application for use in a virtual reality system. 4B and 4C are tables summarizing exemplary data structures and fields for an application program interface for communication between the virtual reality environment application and the kinematics application. 5A is a schematic illustration of another variation of a virtual reality system including an exemplary variation of a laparoscopic handheld controller. 5B is a schematic illustration of an immersive display for displaying an immersive view of a virtual reality environment including a virtual manual laparoscopic tool controlled by the laparoscopic handheld controller. 6A is a perspective view of an exemplary variation of a laparoscopic handheld controller. 6B is a schematic illustration of a virtual manual laparoscopic tool overlaid on part of the laparoscopic handheld controller shown in 6A. 6C-6E are a side view, a detailed partial perspective view, and a partial cross-sectional view, respectively, of the laparoscopic handheld controller shown in 6A. 7 is a schematic illustration of another variation of a virtual reality system interfacing with a robotic surgical environment. 8 is a schematic illustration of a displayed menu for selecting one or more user modes of one variation of a virtual reality system. 9A-9C are schematic illustrations of a virtual robotic surgical environment with exemplary portals. 10A and 10B are schematic illustrations of an exemplary virtual robotic surgical environment viewed in a flight mode. 10C is a schematic illustration of a transition region for modifying the view of the exemplary virtual robotic surgical environment in flight mode. 11 is a schematic illustration of a virtual robotic surgical environment viewed from a vantage point providing an exemplary dollhouse view of a virtual operating room. 12 is a schematic illustration of a view of a virtual robotic surgical environment with an exemplary heads-up display for displaying supplemental views. 13 is a schematic illustration of a display provided by one variation of a virtual reality system operating in a virtual command station mode. 14 is a flowchart of an exemplary variation of a method for operating a user mode menu for selection of user modes in a virtual reality system. 15 is a flowchart of an exemplary variation of a method for operating in an environment view rotation mode in a virtual reality system. 16 is a flowchart of an exemplary variation of a method for operating a user mode enabling snap points in a virtual environment."
                    ],
                    "DESCRIPTION": "Examples of various aspects and variations of the invention are described herein and illustrated in the accompanying drawings. The following description is not intended to limit the invention to these embodiments, but rather to enable a person skilled in the art to make and use this invention. Robotic Surgical System OverviewAn exemplary robotic surgical system and surgical environment is illustrated in 1A. As shown in 1A, a robotic surgical system 150 may include one or more robotic arms 160 located at a surgical platform , table, bed, , where end effectors or surgical tools are attached to the distal ends of the robotic arms 160 for executing a surgical procedure. For example, a robotic surgical system 150 may include, as shown in the exemplary schematic of 1B, at least one robotic arm 160 coupled to a surgical platform, and a tool driver 170 generally attached to a distal end of the robotic arm 160. A cannula 100 coupled to the end of the tool driver 170 may receive and guide a surgical instrument 190 , end effector, camera, . Furthermore, the robotic arm 160 may include a plurality of links that are actuated so as to position and orient the tool driver 170, which actuates the surgical instrument 190. The robotic surgical system may further include a control tower 152 , including a power supply, computing equipment, and/or other suitable equipment for supporting functionality of the robotic components. In some variations, a user such as a surgeon or other operator may use a user console 100 to remotely manipulate the robotic arms 160 and/or surgical instruments , tele-operation. The user console 100 may be located in the same procedure room as the robotic system 150, as shown in 1A. In other embodiments, the user console 100 may be located in an adjacent or nearby room, or tele-operated from a remote location in a different building, city, or country. In one example, the user console 100 comprises a seat 110, foot-operated controls 120, one or more handheld user interface devices 122, and at least one user display 130 configured to display, for example, a view of the surgical site inside a patient. For example, as shown in the exemplary user console shown in 1C, a user located in the seat 110 and viewing the user display 130 may manipulate the foot-operated controls 120 and/or handheld user interface devices to remotely control the robotic arms 160 and/or surgical instruments. In some variations, a user may operate the robotic surgical system 150 in an over the bed OTB mode, in which the user is at the patient's side and simultaneously manipulating a robotically-driven tool driver/end effector attached thereto , with a handheld user interface device 122 held in one hand and a manual laparoscopic tool. For example, the user's left hand may be manipulating a handheld user interface device 122 to control a robotic surgical component, while the user's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the user may perform both robotic-assisted MIS and manual laparoscopic techniques on a patient. During an exemplary procedure or surgery, the patient is prepped and draped in a sterile fashion, and anesthesia is achieved. Initial access to the surgical site may be performed manually with the robotic system 150 in a stowed configuration or withdrawn configuration to facilitate access to the surgical site. Once access is completed, initial positioning and/or preparation of the robotic system may be performed. During the surgical procedure, a surgeon or other user in the user console 100 may utilize the foot-operated controls 120 and/or user interface devices 122 to manipulate various end effectors and/or imaging systems to perform the procedure. Manual assistance may also be provided at the procedure table by sterile-gowned personnel, who may perform tasks including but not limited to retracting organs, or performing manual repositioning or tool exchange involving one or more robotic arms 160. Non-sterile personnel may also be present to assist the surgeon at the user console 100. When the procedure or surgery is completed, the robotic system 150 and/or user console 100 may be configured or set in a state to facilitate one or more post-operative procedures, including but not limited to robotic system 150 cleaning and/or sterilization, and/or healthcare record entry or printout, whether electronic or hard copy, such as via the user console 100. In 1A, the robotic arms 160 are shown with a table-mounted system, but in other embodiments, the robotic arms may be mounted in a cart, ceiling or sidewall, or other suitable support surface. The communication between the robotic system 150, the user console 100, and any other displays may be via wired and/or wireless connections. Any wired connections may be optionally built into the floor and/or walls or ceiling. The communication between the user console 100 and the robotic system 150 may be wired and/or wireless, and may be proprietary and/or performed using any of a variety of data communication protocols. In still other variations, the user console 100 does not include an integrated display 130, but may provide a video output that can be connected to output to one or more generic displays, including remote displays accessible via the internet or network. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. In other examples, additional user consoles 100 may be provided, for example to control additional surgical instruments, and/or to take control of one or more surgical instruments at a primary user console. This will permit, for example, a surgeon to take over or illustrate a technique during a surgical procedure with medical students and physicians-in-training, or to assist during complex surgeries requiring multiple surgeons acting simultaneously or in a coordinated manner. Virtual Reality SystemA virtual reality system for providing a virtual robotic surgical environment is described herein. As shown in 2A, a virtual reality system 200 may include a virtual reality processor 210 , a processor in a computer implementing instructions stored in memory for generating a virtual robotic surgical environment, a head-mounted display 220 wearable by a user U, and one or more handheld controllers 230 manipulable by the user U for interacting with the virtual robotic surgical environment. As shown in 2B, the head-mounted display 220 may include an immersive display 222 for displaying the virtual robotic surgical environment to the user U , with a first-person perspective view of the virtual environment. The immersive display may, for example, be a stereoscopic display provided by eyepiece assemblies. In some variations, the virtual reality system 200 may additionally or alternatively include an external display 240 for displaying the virtual robotic surgical environment. The immersive display 222 and the external display 240, if both are present, may be synchronized to show the same or similar content. As described in further detail herein, the virtual reality system and variations thereof, as further described herein may serve as a useful tool with respect to robotic surgery, in applications including but not limited to training, simulation, and/or collaboration among multiple persons. More specific examples of applications and uses of the virtual reality system are described herein. Generally, the virtual reality processor is configured to generate a virtual robotic surgical environment within which a user may navigate around a virtual operating room and interact with virtual objects via the head-mounted display and/or handheld controllers. For example, a virtual robotic surgical system may be integrated into a virtual operating room, with one or more virtual robotic components having three-dimensional meshes and selected characteristics , dimensions and kinematic constraints of virtual robotic arms and/or virtual surgical tools, number and arrangement thereof, . Other virtual objects, such as a virtual control towers or other virtual equipment representing equipment supporting the robotic surgical system, a virtual patient, a virtual table or other surface for the patient, virtual medical staff, a virtual user console, etc. , may also be integrated into the virtual reality operating room. In some variations, the head-mounted display 220 and/or the handheld controllers 230 may be modified versions of those included in any suitable virtual reality hardware system that is commercially available for applications including virtual and augmented reality environments , for gaming and/or military purposes and are familiar to one of ordinary skill in the art. For example, the head-mounted display 220 and/or the handheld controllers 230 may be modified to enable interaction by a user with a virtual robotic surgical environment , a handheld controller 230 may be modified as described below to operate as a laparoscopic handheld controller. The handheld controller may include, for example, a carried device , wand, remote device, and/or a garment worn on the user's hand , gloves, rings, wristbands, and including sensors and/or configured to cooperate with external sensors to thereby provide tracking of the user's hands, individual fingers, wrists, etc. Other suitable controllers may additionally or alternatively be used , sleeves configured to provide tracking of the user's arms. Generally, a user U may don the head-mounted display 220 and carry or wear at least one handheld controller 230 while he or she moves around a physical workspace, such as a training room. While wearing the head-mounted display 220, the user may view an immersive first-person perspective view of the virtual robotic surgical environment generated by the virtual reality processor 210 and displayed onto the immersive display 222. As shown in 2B, the view displayed onto the immersive display 222 may include one or more graphical representations 230 of the handheld controllers , virtual models of the handheld controllers, virtual models of human hands in place of handheld controllers or holding handheld controllers, . A similar first-person perspective view may be displayed onto an external display 240 , for assistants, mentors, or other suitable persons to view. As the user moves and navigates within the workspace, the virtual reality processor 210 may change the view of the virtual robotic surgical environment displayed on the immersive display 222 based at least in part on the location and orientation of the head-mounted display and hence the user's location and orientation, thereby allowing the user to feel as if he or she is exploring and moving within the virtual robotic surgical environment. Additionally, the user may further interact with the virtual robotic surgical environment by moving and/or manipulating the handheld controllers 230. For example, the handheld controllers 230 may include one or more buttons, triggers, touch-sensitive features, scroll wheels, switches, and/or other suitable interactive features that the user may manipulate to interact with the virtual environment. As the user moves the handheld controllers 230, the virtual reality processor 210 may move the graphical representations 230 of the handheld controllers or a cursor or other representative icon within the virtual robotic surgical environment. Furthermore, engaging one or more interactive features of the handheld controllers 230 may enable the user to manipulate aspects of the virtual environment. For example, the user may move a handheld controller 230 until the graphical representation 230 of the handheld controller is in proximity to a virtual touchpoint , selectable location on a virtual robotic arm in the environment, engage a trigger or other interactive feature on the handheld controller 230 to select the virtual touchpoint, then move the handheld controller 230 while engaging the trigger to drag or otherwise manipulate the virtual robotic arm via the virtual touchpoint. Other examples of user interactions with the virtual robotic surgical environment are described in further detail below. In some variations, the virtual reality system may engage other senses of the user. For example, the virtual reality system may include one or more audio devices , headphones for the user, speakers, for relaying audio feedback to the user. As another example, the virtual reality system may provide tactile feedback, such as vibration, in one or more of the handheld controllers 230, the head-mounted display 220, or other haptic devices contacting the user , gloves, wristbands, . Virtual Reality ProcessorThe virtual reality processor 210 may be configured to generate a virtual robotic surgical environment within which a user may navigate around a virtual operating room and interact with virtual objects. A general schematic illustrating an exemplary interaction between the virtual reality processor and at least some components of the virtual reality system is shown in 3. In some variations, the virtual reality processor 210 may be in communication with hardware components such as the head-mounted display 220, and/or handheld controllers 230. For example, the virtual reality processor 210 may receive input from sensors in the head-mounted display 220 to determine location and orientation of the user within the physical workspace, which may be used to generate a suitable, corresponding first-person perspective view of the virtual environment to display in the head-mounted display 220 to the user. As another example, the virtual reality control 210 may receive input from sensors in the handheld controllers 230 to determine location and orientation of the handheld controllers 230, which may be used to generate suitable graphical representations of the handheld controllers 230 to display in the head-mounted display 220 to the user, as well as translate user input for interacting with the virtual environment into corresponding modifications of the virtual robotic surgical environment. The virtual reality processor 210 may be coupled to an external display 240 , a monitor screen that is visible to the user in a non-immersive manner and/or to other persons such as assistants or mentors who may wish to view the user's interactions with the virtual environment. In some variations, the virtual reality processor 210 or multiple processor machines may be configured to execute one or more software applications for generating the virtual robotic surgical environment. For example, as shown in 4, the virtual reality processor 210 may utilize at least two software applications, including a virtual operating environment application 410 and a kinematics application 420. The virtual operating environment application and the kinematics application may communicate via a client-server model. For example, the virtual operating environment application may operate as a client, while the kinematics application may operate as a server. The virtual operation environment application 410 and the kinematics application 420 may be executed on the same processing machine, or on separate processing machines coupled via a computer network , the client or the server may be a remote device, or the machines may be on a local computer network. Additionally, it should be understood that in other variations, the virtual operating environment application 410 and/or the kinematics application 420 may interface with other software components. In some variations, the virtual operating environment application 410 and the kinematics application 520 may invoke one or more application program interfaces APIs, which define the manner in which the applications communicate with one another. The virtual operating environment 410 may allow for a description or definition of the virtual operating room environment , the operating room, operating table, control tower or other components, user console, robotic arms, table adapter links coupling robotic arms to the operating table, . At least some descriptions of the virtual operating room environment may be saved , in a model virtual reality component database 202 and provided to the processor as configuration files. For example, in some variations, as shown in 3, the virtual reality processor such as through the virtual operating environment application 410 described above may be in communication with a model virtual reality component database 202 , stored on a server, local or remote hard drive, or other suitable memory. The model virtual reality component database 202 may store one or more configuration files describing virtual components of the virtual robotic surgical environment. For example, the database 202 may store files describing different kinds of operating rooms , varying in room shape or room dimensions, operating tables or other surfaces on which a patient lies , varying in size, height, surfaces, material construction, , control towers , varying in size and shape, user console , varying in user seat design, robotic arms , design of arm links and joints, number and arrangement thereof, number and location of virtual touchpoints on the arm, , table adapter links coupling robotic arms to an operating table , design of table adapter links and joints, number and arrangement thereof, , patient types , varying in sex, age, weight, height, girth, and/or medical personnel , generic graphical representations of people, graphical representations of actual medical staff, . As one specific example, a configuration file in Unified Robot Description Format URDF may store a configuration of a particular robotic arm, including definitions or values for fields such as number of arm links, number of arm joints connecting the arm links, length of each arm link, diameter or girth of each arm link, mass of each arm link, type of arm joint , roll, pitch, yaw , etc. Additionally, kinematic constraints may be loaded as a wrapper over a virtual robotic component , arm to further define the kinematic behavior of the virtual robotic component. In other variations, the virtual reality processor 210 may receive any suitable descriptions of virtual components to load and generate in the virtual robotic surgical environment. Accordingly, the virtual reality processor 210 may receive and utilize different combinations of configuration files and/or other descriptions of virtual components to generate particular virtual robotic surgical environments. In some variations, as shown in 3, the virtual reality processor 210 may additionally or alternatively be in communication with a patient records database 204, which may store patient-specific information. Such patient-specific information may include, for example, patient imaging data , X-ray, MRI, CT, ultrasound, , medical histories, and/or patient metrics , age, weight, height, , though other suitable patient-specific information may additionally or alternatively be stored in the patient records database 204. When generating the virtual robotic surgical environment, the virtual reality processor 210 may receive patient-specific information from the patient records database 204 and integrate at least some of the received information into the virtual reality environment. For example, a realistic representation of the patient's body or other tissue may be generated and incorporated into the virtual reality environment , a 3D model generated from a combined stack of 2D images, such as MRI images, which may be useful, for example, for determining desirable arrangement of robotic arms around the patient, optimal port placement, etc. specific to a particular patient, as further described herein. As another example, patient imaging data may be overlaid over a portion of the user's field of view of the virtual environment , overlaying an ultrasound image of a patient's tissue over the virtual patient's tissue. In some variations, the virtual reality processor 210 may embed one or more kinematics algorithms via the kinematics application 420 to at least partially describe behavior of one or more components of the virtual robotic system in the virtual robotic surgical environment. For example, one or more algorithms may define how a virtual robotic arm responds to user interactions , moving the virtual robotic arm by selection and manipulation of a touchpoint on the virtual robotic arm, or how a virtual robotic arm operates in a selected control mode. Other kinematics algorithms, such as those defining operation of a virtual tool driver, a virtual patient table, or other virtual components, may additionally or alternatively be embedded in the virtual environment. By embedding in the virtual environment one or more kinematics algorithms that accurately describe behavior of an actual real robotic surgical system, the virtual reality processor 210 may permit the virtual robotic surgical system to function accurately or realistically compared to a physical implementation of a corresponding real robotic surgical system. For example, the virtual reality processor 210 may embed at least one control algorithm that represents or corresponds to one or more control modes defining movement of a robotic component , arm in an actual robotic surgical system. For example, the kinematics application 420 may allow for a description or definition of one or more virtual control modes, such as for the virtual robotic arms or other suitable virtual components in the virtual environment. Generally, for example, a control mode for a virtual robotic arm may correspond to a function block that enables the virtual robotic arm to perform or carry out a particular task. For example, as shown in 4, a control system 430 may include multiple virtual control modes 432, 434, 436, etc. governing actuation of at least one joint in the virtual robotic arm. The virtual control modes 432, 434, 436, etc. may include at least one primitive mode which governs the underlying behavior for actuation of at least one joint and/or at least one user mode which governs higher level, task-specific behavior and may utilize one or more primitive modes. In some variations, a user may activate a virtual touchpoint surface of a virtual robotic arm or other virtual object, thereby triggering a particular control mode , via a state machine or other controller. In some variations, a user may directly select a particular control mode through, for example, a menu displayed in the first-person perspective view of the virtual environment. Examples of primitive virtual control modes include, but are not limited to, a joint command mode which allows a user to directly actuate a single virtual joint individually, and/or multiple virtual joints collectively, a gravity compensation mode in which the virtual robotic arm holds itself in a particular pose, with particular position and orientation of the links and joints, without drifting downward due to simulated gravity, and trajectory following mode in which the virtual robotic arm may move to follow a sequence of one or more Cartesian or other trajectory commands. Examples of user modes that incorporate one or more primitive control modes include, but are not limited to, an idling mode in which the virtual robotic arm may rest in a current or default pose awaiting further commands, a setup mode in which the virtual robotic arm may transition to a default setup pose or a predetermined template pose for a particular type of surgical procedure, and a docking mode in which the robotic arm facilitates the process in which the user attaches the robotic arm to a part, such as with gravity compensation, . Generally, the virtual operating environment application 410 and the kinematics application 420 may communicate with each other via a predefined communication protocol, such as an application program interface APIs that organizes information , status or other characteristics of virtual objects and other aspects of the virtual environment. For example, the API may include data structures that specify how to communicate information about virtual objects such as a virtual robotic arm in whole and/or on a segment-by-segment basis a virtual table, a virtual table adapter connecting a virtual arm to the virtual table, a virtual cannula, a virtual tool, a virtual touchpoint for facilitating user interaction with the virtual environment, user input system, handheld controller devices, etc. Furthermore, the API may include one or more data structures that specify how to communicate information about events in the virtual environment , a collision event between two virtual entities or other aspects relating to the virtual environment , reference frame for displaying the virtual environment, control system framework, . Exemplary data structures and exemplary fields for containing their information are listed and described in 4B and 4C, though it should be understood that other variations of the API may include any suitable types, names, and numbers of data structures and exemplary field structures. In some variations, as generally illustrated schematically in 4A, the virtual operating environment application 410 passes status information to the kinematics application 420, and the kinematics application 420 passes commands to the virtual operating environment application 410 via the API, where the commands are generated based on the status information and subsequently used by the virtual reality processor 210 to generate changes in the virtual robotic surgical environment. For example, a method for embedding one or more kinematics algorithms in a virtual robotic surgical environment for control of a virtual robotic arm may include passing status information regarding at least a portion of the virtual robotic arm from the virtual operating environment application 410 to the kinematics application 420, algorithmically determining an actuation command to actuate at least one virtual joint of the virtual robotic arm, and passing the actuation command from the kinematics application 420 to virtual operating environment application 410. The virtual reality processor 210 may subsequently move the virtual robotic arm in accordance with the actuation command. As an illustrative example for controlling a virtual robotic arm, a gravity compensation control mode for a virtual robotic arm may be invoked, thereby requiring one or more virtual joint actuation commands in order to counteract simulated gravity forces on the virtual joints in the virtual robotic arm. The virtual operating environment application 410 may pass to the kinematics application 420 relevant status information regarding the virtual robotic arm , position of at least a portion of the virtual robotic arm, position of the virtual patient table to which the virtual robotic arm is mounted, position of a virtual touchpoint that the user may have manipulated to move the virtual robotic arm, joint angles between adjacent virtual arm links and status information , direction of simulated gravitational force on the virtual robotic arm. Based on the received status information from the virtual operating environment application 410 and known kinematic and/or dynamic properties of the virtual robotic arm and/or virtual tool drive attached to the virtual robotic arm , known from a configuration file, , the control system 430 may algorithmically determine what actuated force at each virtual joint is required to compensate for the simulated gravitational force acting on that virtual joint. For example, the control system 430 may utilize a forward kinematic algorithm, an inverse algorithm, or any suitable algorithm. Once the actuated force command for each relevant virtual joint of the virtual robotic arm is determined, the kinematics application 420 may send the force commands to the virtual operating environment application 410. The virtual reality processor subsequently may actuate the virtual joints of the virtual robotic arm in accordance with the force commands, thereby causing the virtual robotic arm to be visualized as maintaining its current pose despite the simulated gravitational force in the virtual environment , instead of falling down or collapsing under simulated gravitational force. Another example for controlling a virtual robotic arm is trajectory following for a robotic arm. In trajectory following, movement of the robotic arm may be programmed then emulated using the virtual reality system. Accordingly, when the system is used to emulate a trajectory planning control mode, the actuation command generated by a kinematics application may include generating an actuated command for each of a plurality of virtual joints in the virtual robotic arm. This set of actuated commands may be implemented by a virtual operating environment application to move the virtual robotic arm in the virtual environment, thereby allowing testing for collision, volume or workspace of movement, etc. Other virtual control algorithms for the virtual robotic arm and/or other virtual components , virtual table adapter links coupling the virtual robotic arm to a virtual operating table may be implemented via similar communication between the virtual operating environment application 410 and the kinematics application 420. Although the virtual reality processor 210 is generally referred to herein as a single processor, it should be understood that in some variations, multiple processors may be used to perform the processors described herein. The one or more processors may include, for example, a processor of a general purpose computer, a special purpose computer or controller, or other programmable data processing apparatus or component, etc. Generally, the one or more processors may be configured to execute instructions stored on any suitable computer readable media. The computer readable media may include, for example, magnetic media, optical media, magneto-optical media and hardware devices that are specially configured to store and execute program code, such as application-specific integrated circuits ASICs, programmable logic devices PLDs, ROM and RAM devices, flash memory, EEPROMs, optical devices , CD or DVD, hard drives, floppy drives, or any suitable device. Examples of computer program code include machine code, such as produced by a compiler, and files containing higher-level code that are executed by a computer using an interpreter. For example, one variation may be implemented using C++, JAVA, or other suitable object-oriented programming language and development tools. As another example, another variation may be implemented in hardwired circuitry in place of, or in combination with, machine-executable software instructions. Head-Mounted Display and Handheld ControllersAs shown in 2A, a user U may wear a head-mounted display 220 and/or hold one or more handheld controllers 230. The head-mounted display 220 and handheld controllers 230 may generally enable a user to navigate and/or interact with the virtual robotic surgical environment generated by the virtual reality processor 210. The head-mounted display 220 and/or handheld controllers 230 may communicate signals to the virtual reality processor 210 via a wired or wireless connection. In some variations, the head-mounted display 220 and/or the handheld controllers 230 may be modified versions of those included in any suitable virtual reality hardware system that is commercially available for applications including virtual and augmented reality environments. For example, the head-mounted display 220 and/or the handheld controllers 230 may be modified to enable interaction by a user with a virtual robotic surgical environment , a handheld controller 230 may be modified as described below to operate as a laparoscopic handheld controller. In some variations, the virtual reality system may further include one or more tracking emitters 212 that emit infrared light into a workspace for the user The tracking emitters 212 may, for example, be mounted on a wall, ceiling, fixture, or other suitable mounting surface. Sensors may be coupled to outward-facing surfaces of the head-mounted display 220 and/or handheld controllers 230 for detecting the emitted infrared light. Based on the location of any sensors that detect the emitted light and when such sensors detect the emitted light after the light is emitted, the virtual reality processor 220 may be configured to determine , through triangulation the location and orientation of the head-mounted display 220 and/or handheld controllers 230 within the workspace. In other variations, other suitable means , other sensor technologies such as accelerometers or gyroscopes, other sensor arrangements, may be used to determine location and orientation of the head-mounted display 220 and handheld controllers 230. In some variations, the head-mounted display 220 may include straps , with buckles, elastic, snaps, that facilitate mounting of the display 220 to the user's head. For example, the head-mounted display 220 may be structured similar to goggles, a headband or headset, a cap, etc. The head-mounted display 220 may include two eyepiece assemblies providing a stereoscopic immersive display, though alternatively may include any suitable display. The handheld controllers 230 may include interactive features that the user may manipulate to interact with the virtual robotic surgical environment. For example, the handheld controllers 230 may include one or more buttons, triggers, touch-sensitive features, scroll wheels, switches, and/or other suitable interactive features. Additionally, the handheld controllers 230 may have any of various form factors, such as a wand, pinchers, generally round shapes , ball or egg-shaped, etc. In some variations, the graphical representations 230 displayed on the head-mounted display 220 and/or external display 240 may generally mimic the form factor of the actual, real handheld controllers 230. In some variations, the handheld controller may include a carried device , wand, remote device, and/or a garment worn on the user's hand , gloves, rings, wristbands, and including sensors and/or configured to cooperate with external sensors to thereby provide tracking of the user's hands, individual fingers, wrists, etc. Other suitable controllers may additionally or alternatively be used , sleeves configured to provide tracking of the user's arms. Laparoscopic Handheld ControllerIn some variations, as shown in the schematic of 5A, the handheld controller 230 may further include at least one tool feature 232 that is representative of at least a portion of a manual laparoscopic tool, thereby forming a laparoscopic handheld controller 234 that may be used to control a virtual manual laparoscopic tool. Generally, for example, the tool feature 232 may function to adapt the handheld controller 230 into a controller substantially similar in form , user feel and touch to a manual laparoscopic tool. The laparoscopic handheld controller 234 may be communicatively coupled to the virtual reality processor 210 for manipulating a virtual manual laparoscopic tool in the virtual robotic surgical environment, and may help enable the user to feel as if he or she is using an actual manual laparoscopic tool while interacting with the virtual robotic surgical environment. In some variations the laparoscopic handheld device may be a mock-up , facsimile or genericized version of a manual laparoscopic tool, while in other variations the laparoscopic handheld device may be a functioning manual laparoscopic tool. Movements of at least a portion of the laparoscopic handheld controller may be mapped by the virtual reality controller to correspond to movements of the virtual manual laparoscopic tool. Thus, in some variations, the virtual reality system may simulate use of a manual laparoscopic tool for manual MIAs shown in 5A, the laparoscopic handheld controller 234 may be used with a mock patient setup to further simulate the feel of a virtual manual laparoscopic tool. For example, the laparoscopic handheld controller 234 may be inserted into a cannula 250 , an actual cannula used in MIS procedures to provide realistic feel of a manual tool within a cannula, or a suitable representation thereof, such as a tube with a lumen for receiving a tool shaft portion of the laparoscopic handheld controller 234. The cannula 250 may be placed in a mock patient abdomen 260, such as a foam body with one or more insertion sites or ports for receiving the cannula 250. Alternatively, other suitable mock patient setups may be used, such as a cavity providing resistance , with fluid, with similar feel as an actual patient abdomen. Additionally, as shown in 5B, the virtual reality processor may generate a virtual robotic surgical environment including a virtual manual laparoscopic tool 236 and/or a virtual cannula 250 relative to a virtual patient , the graphical representation 250 of the cannula depicted as inserted in the virtual patient. As such, the virtual environment with the virtual manual laparoscopic tool 236 and virtual cannula 250 may be displayed on the immersive display provided by the head-mounted display 220, and/or the external display 240. A calibration procedure may be performed to map the laparoscopic handheld controller 234 to the virtual manual laparoscopic tool 236 within the virtual environment. Accordingly, as the user moves and manipulates the laparoscopic handheld controller 234, the combination of the at least one tool feature 234 and the mock patient setup may allow the user to tactilely feel as if he or she is using a manual laparoscopic tool in the virtual robotic surgical environment. Likewise, as the user moves and manipulates the laparoscopic handheld controller 234, the corresponding movements of the virtual manual laparoscopic tool 236 may allow the user to visualize the simulation that he or she is using a manual laparoscopic tool in the virtual robotic surgical environment. In some variations, the calibration procedure for the laparoscopic handheld controller generally maps the laparoscopic handheld controller 234 to the virtual manual laparoscopic tool 236. For example, generally, the calibration procedure may zero its position relative to a reference point within the virtual environment. In an exemplary calibration procedure, the user may insert the laparoscopic handheld controller through a cannula 250 into a mock patient abdomen 260, which may be placed on a table in front of the user , at a height that is representative of the height of a real operating patient table. The user may continue inserting the laparoscopic handheld controller into the mock patient abdomen 260 into a suitable depth representative of depth achieved during a real laparoscopic procedure. Once the laparoscopic handheld controller is suitably placed in the mock patient abdomen 260, the user may provide an input , squeeze a trigger or push a button on the laparoscopic handheld controller, by voice command, to confirm and orient the virtual patient to the location and height of the mock patient abdomen 260. Additionally, other aspects of the virtual environment may be calibrated to align with the real, tangible aspects of the system, such as by depicting virtual components adjustably movable to target locations and allowing user input to confirm new alignment of the virtual component with target locations , by squeezing a trigger or pushing a button on the laparoscopic handheld controller, voice command, . Orientation of virtual components , rotational orientation of a shaft may be adjusted with a touchpad, trackball, or other suitable input on the laparoscopic handheld controller or other device. For example, the virtual operating room may be aligned with the real room in which the user is standing, a distal end of the virtual cannula or trocar may be aligned with the real entry location in the mock patient abdomen, etc. Furthermore, in some variations, a virtual end effector , endocutter, clipper may be located and oriented via the laparoscopic handheld controller to a new target location and orientation in similar manners. In some variations, as shown in 5B, the system may include both a handheld controller 230 and a laparoscopic handheld controller 234. Accordingly, the virtual reality processor may generate a virtual environment including both a graphical representation 230 of a handheld controller 230 with no laparoscopic attachment and a virtual manual laparoscopic tool 236 as described above. The handheld controller 230 may be communicatively coupled to the virtual reality processor 210 for manipulating at least one virtual robotic arm, and the laparoscopic handheld controller 234 may be communicatively coupled to the virtual reality processor 210 for manipulating a virtual manual laparoscopic tool 236. Thus, in some variations, the virtual reality system may simulate an over the bed mode of using a robotic surgical system, in which an operator is at the patient's side and manipulating both a robotic arm , with one hand providing robotic-assisted MIS, and a manual laparoscopic tool providing manual MIThe tool feature 232 may include any suitable feature generally approximating or representing a portion of a manual laparoscopic tool. For example, the tool feature 232 may generally approximate a laparoscopic tool shaft , include an elongated member extending from a handheld portion of the controller. As another example, the tool feature 232 may include a trigger, button, or other laparoscopic interactive feature similar to that present on a manual laparoscopic tool that engages an interactive feature on the handheld controller 230 but provides a realistic form factor mimicking the feel of a manual laparoscopic tool , the tool feature 232 may include a larger trigger having a realistic form factor that is overlaid with and engages a generic interactive feature on the handheld controller 230. As yet another example, the tool feature 232 may include selected materials and/or masses to create a laparoscopic handheld controller 234 having a weight distribution that is similar to a particular kind of manual laparoscopic tool. In some variations, the tool feature 232 may include plastic , polycarbonate, acrylonitrile butadiene styrene ABS, nylon, that is injection molded, machined, 3D-printed, or other suitable material shaped in any suitable fashion. In other variations, the tool feature 232 may include metal or other suitable material that is machined, casted, etc. In some variations, the tool feature 236 may be an adapter or other attachment that is formed separately from the handheld controller 230 and coupled to the handheld controller 230 via fasteners , screws, magnets, , interlocking features , threads or snap fit features such as tabs and slots, , epoxy, welding , ultrasonic welding, etc. The tool feature 236 may be reversibly coupled to the handheld controller 230. For example, the tool feature 236 may be selectively attached to the handheld controller 230 in order to adapt a handheld controller 230 when a laparoscopic-style handheld controller 230 is desired, while the tool feature 236 may be selectively detached from the handheld controller 230 when a laparoscopic-style handheld controller 230 is not desired. Alternatively, the tool feature 236 may be permanently coupled to the handheld portion 234, such as during manufacturing. Furthermore, in some variations, the handheld portion 234 and the tool feature 236 may be integrally formed , injection molded together as a single piece. One exemplary variation of a laparoscopic handheld controller is shown in 6A. The laparoscopic handheld controller 600 may include a handheld portion 610 , similar to handheld controller 230 described above, a tool shaft 630, and a shaft adapter 620 for coupling the tool shaft 630 to the handheld portion 610. As shown in 6B, the laparoscopic handheld controller 600 may generally be used to control a virtual manual laparoscopic stapler tool 600, though the laparoscopic handheld controller 600 may be used to control other kinds of virtual manual laparoscopic tools , scissors, dissectors, graspers, needleholders, probes, forceps, biopsy tools, etc. For example, the handheld portion 610 may be associated with a virtual handle 610 of the virtual manual laparoscopic stapler tool 600 having a stapler end effector 640, such that the user's manipulation of the handheld portion 610 is mapped to manipulation of the virtual handle 610. Similarly, the tool shaft 630 may correspond to a virtual tool shaft 630 of the virtual manual laparoscopic stapler tool 600. The tool shaft 630 and the virtual tool shaft 630 may be inserted into a cannula and a virtual cannula, respectively, such that movement of the tool shaft 630 relative to the cannula is mapped to movement of the virtual tool shaft 630 within the virtual cannula in the virtual robotic surgical environment. The handheld portion 610 may include one or more interactive features, such as finger trigger 612 and/or button 614, which may receive user input from the user's fingers, palms, etc. and be communicatively coupled to a virtual reality processor. In this exemplary embodiment, the finger trigger 612 may be mapped to a virtual trigger 612 on the virtual manual laparoscopic stapler tool 600. The virtual trigger 612 may be visualized as actuating the virtual end effector 640 , causing the virtual members of the virtual end effector 640 to close and fire staplers for stapling virtual tissue in the virtual environment. Accordingly, when the user actuates the finger trigger 612 on the laparoscopic handheld controller, the signal from finger trigger 612 may be communicated to the virtual reality processor, which modifies the virtual manual laparoscopic stapler tool 600 to interact within the virtual environment in simulation of an actual manual laparoscopic stapler tool. In another variation, a trigger attachment may physically resemble , in shape and form the virtual trigger 612 on the virtual manual laparoscopic stapler tool 600 and may be coupled to the finger trigger 612, which may enable the laparoscopic handheld controller 600 to even more closely mimic the user feel of the virtual manual laparoscopic stapler tool 600. As shown in 6C-6E, the shaft adapter 620 may generally function to couple the tool shaft 630 to the handheld portion 610, which may, for example, adapt a handheld controller similar to handheld controller 210 described above into a laparoscopic handheld controller. The shaft adapter 620 may generally include a first end for coupling to the handheld portion 610 and a second end for coupling to the tool shaft 630. As shown best in 6E, the first end of the shaft adapter 620 may include a proximal portion 620a and distal portion 620b configured to clamp on a feature of the handheld portion 610. For example, the handheld portion 610 may include generally ring-like portion defining a central space 614 which receives the proximal portion 620a and the distal portion 620b. The proximal portion 620a and the distal portion 620b may clamp on either side of the ring-like portion at its inner diameter, and be fixed to the ring-like portion via fasteners not shown passing through fastener holes 622, thereby securing the shaft adapter 620 to the handheld portion 610. Additionally or alternatively, the shaft adapter 620 may couple to the handheld portion 610 in any suitable fashion, such as an interference fit, epoxy, interlocking features , between the proximal portion 620a and the distal portion 620b, etc. As also shown in 6E, the second end of the shaft adapter 620 may include a recess for receiving the tool shaft 620. For example, the recess may be generally cylindrical for receiving a generally cylindrical end of a tool shaft portion 630, such as through a press fit, friction fit, or other interference fit. Additionally or alternatively, the tool shaft 620 may be coupled to the shaft adapter 620 with fasteners , screws, bolts, epoxy, ultrasonic welding, . The tool shaft 630 may be any suitable size , length, diameter for mimicking or representing a manual laparoscopic tool. In some variations, the shaft adapter 620 may be selectively removable from the handheld portion 610 for permitting selective use of the handheld portion 610 both as a standalone handheld controller , handheld controller 210 and as a laparoscopic handheld controller 600. Additionally or alternatively, the tool shaft 630 may be selectively removable from the shaft adapter 620 , although the shaft adapter 620 may be intentionally fixed to the handheld portion 610, the tool shaft 620 may be selectively removable from the shaft adapter 620 to convert the laparoscopic handheld control 600 to a standalone handheld controller 210. Generally, the tool feature of the laparoscopic handheld controller 600, such as the shaft adapter 620 and the tool shaft 630, may be made of a rigid or semi-rigid plastic or metal, and may be formed through any suitable manufacturing process, such as 3D printing, injection molding, milling, turning, etc. The tool feature may include multiple kinds of materials, and/or weights or other masses to further simulate the user feel of a particular manual laparoscopic tool. System VariationsOne or more aspects of the virtual reality system described above may be incorporated into other variations of systems. For example, in some variations, a virtual reality system for providing a virtual robotic surgical environment may interface with one or more features of a real robotic surgical environment. For example, as shown in 3, a system 700 may include one or more processors , a virtual reality processor 210 configured to generate a virtual robotic surgical environment, and one or more sensors 750 in a robotic surgical environment, where the one or more sensors 750 is in communication with the one or more processors. Sensor information from the robotic surgical environment may be configured to detect status of an aspect of the robotic surgical environment, such for mimicking or replicating features of the robotic surgical environment in the virtual robotic surgical environment. For example, a user may monitor an actual robotic surgical procedure in a real operating room via a virtual reality system that interfaces with the real operating room , the user may interact with a virtual reality environment that is reflective of the conditions in the real operating room. In some variations, one or more sensors 750 may be configured to detect status of at least one robotic component , a component of a robotic surgical system, such as a robotic arm, a tool driver coupled to a robotic arm, a patient operating table to which a robotic arm is attached, a control tower, or other component of a robotic surgical operating room. Such status may indicate, for example, position, orientation, speed, velocity, operative state , on or off, power level, mode, or any other suitable status of the component. For example, one or more accelerometers may be coupled to a robotic arm link and be configured to provide information about the robotic arm link's position, orientation, and/or velocity of movement, etc. Multiple accelerometers on multiple robotic arms may be configured to provide information regarding impending and/or present collisions between robotic arms, between different links of a robotic arm, or between a robotic arm and a nearby obstacle having a known position. As another example, one or more proximity sensors , infrared sensor, capacitive sensor may be coupled to a portion of a robotic arm or other components of the robotic surgical system or surgical environment. Such proximity sensors may, for example, be configured to provide information regarding impending collisions between objects. Additionally or alternatively, contact or touch sensors may be coupled to a portion of a robotic arm or other components of the robotic surgical environment, and may be configured to provide information regarding a present collision between objects. In another example, one or more components of the robotic surgical system or surgical environment may include markers , infrared markers to facilitate optical tracking of the position, orientation, and/or velocity of various components, such as with overhead sensors monitoring the markers in the surgical environment. Similarly, the surgical environment may additionally or alternatively include cameras for scanning and/or modeling the surgical environment and its contents. Such optical tracking sensors and/or cameras may be configured to provide information regarding impending and/or present collisions between objects. As another example, one or more sensors 750 may be configured to detect a status of a patient, a surgeon, or other surgical staff. Such status may indicate, for example, position, orientation, speed, velocity, and/or biological metrics such as heart rate, blood pressure, temperature, etc. For example, a heart rate monitor, a blood pressure monitor, thermometer, and/or oxygenation sensor, etc. may be coupled to the patient and enable a user to keep track of the patient's condition. Generally, in these variations, a virtual reality processor 210 may generate a virtual robotic surgical environment similar to that described elsewhere herein. Additionally, upon receiving status information from the one or more sensors 750, the virtual reality processor 210 or other processor in the system may incorporate the detected status in any one or more suitable ways. For example, in one variation, the virtual reality processor 210 may be configured to generate a virtual reality replica or near-replica of a robotic surgical environment and/or a robotic surgical procedure performed therein. For example, the one or more sensors 750 in the robotic surgical environment may be configured to detect a status of a robotic component corresponding to a virtual robotic component in the virtual robotic surgical environment , the virtual robotic component may be substantially representative of the robotic component in visual form and/or function. In this variation, the virtual reality processor 210 may be configured to receive the detected status of the robotic component, and then modify the virtual robotic component based at least in part on the detected status such that the virtual robotic component mimics the robotic component. For example, if a surgeon moves a robotic arm during a robotic surgical procedure to a particular pose, then a virtual robotic arm in the virtual environment may move correspondingly. As another example, the virtual reality processor 210 may receive status information indicating an alarm event, such as an impending or present collision between objects, or poor patient health condition. Upon receiving such information, the virtual reality processor 210 may provide a warning or alarm to the user of the occurrence of the event, such as by displaying a visual alert , text, icon indicating collision, a view within the virtual environment depicting the collision, , audio alert, etc. As yet another example, the one or more sensors in the robotic surgical environment may be used to compare an actual surgical procedure occurring in the non-virtual robotic surgical environment with a planned surgical procedure as planned in a virtual robotic surgical environment. For example, an expected position of at least one robotic component , robotic arm may be determined during surgical preplanning, as visualized as a corresponding virtual robotic component in a virtual robotic surgical environment. During an actual surgical procedure, one or more sensors may provide information about a measured position of the actual robotic component. Any differences between the expected and measured position of the robotic component may indicate deviations from a surgical plan that was constructed in the virtual reality environment. Since such deviations may eventually result in undesired consequences , unintended collisions between robotic arms, , identification of deviations may allow the user to adjust the surgical plan accordingly , reconfigure approach to a surgical site, change surgical instruments, . User ModesGenerally, the virtual reality system may include one or more user modes enabling a user to interact with the virtual robotic surgical environment by moving and/or manipulating the handheld controllers 230. Such interactions may include, for example, moving virtual objects , virtual robotic arm, virtual tool, in the virtual environment, adding camera viewpoints to view the virtual environment simultaneously from multiple vantage points, navigate within the virtual environment without requiring the user to move the head-mounted display 220 , by walking, etc. as further described below. In some variations, the virtual reality system may include a plurality of user modes, where each user mode is associated with a respective subset of user interactions. As shown in 8, at least some of the user modes may be shown on a display , head-mounted display 220 for user selection. For example, at least some of the user modes may correspond to selectable user mode icons 812 displayed in a user mode menu 810. The user mode menu 810 may be overlaid on the display of the virtual robotic surgical environment such that a graphical representation 230 of the handheld controller or user hand, other suitable representative icon, may be maneuvered by the user to select a user mode icon, thereby activating the user mode corresponding to the selected user mode icon. As shown in 8, the user mode icons 812 may be generally arranged in a palette or circle, but may be alternatively arranged in a grid or other suitable arrangement. In some variations, a selected subset of possible user modes may be presented in the menu 810 based on, for example, user preferences , associated with a set of user login information, preferences of users similar to the current user, type of surgical procedure, etc. 14 illustrates a method of operation 1400 of an exemplary variation of a user mode menu providing selection of one or more user mode icons. To activate the user menu, the user may activate a user input method associated with the menu. For example, an input method may be activated by a user engaging with a handheld controller , handheld user interface device, such as by pressing a button or other suitable feature on the handheld controller 1410. As another example, an input method may be activated by a user engaging a pedal or other feature of a user console 1410. Voice commands and/or other devices may additionally or alternatively be used to activate an input method associated with the menu. While the input method is engaged 1412, the virtual reality system may render and display an array of user mode icons , arranged in a palette around a central origin as shown in 8A. The array of user mode icons may be generally displayed near or around a graphical representation of the handheld controller and/or at a rendered cursor that is controlled by the handheld controller. For example, in one variation in which a handheld controller includes a circular menu button and a graphical representation of the handheld controller also has a circular menu button displayed in the virtual reality environment, the array of user mode icons may be centered around and aligned with the menu button such that the normal vectors of the menu plane and menu button are substantially aligned. The circular or radial menu may include, for example, multiple different menu regions 1414 or sectors, each of which may be associated with an angle range , an arcuate segment of the circular menu and a user mode icon , as shown in 8. Each region may be toggled between a selected state and an unselected state. The method 1400 may generally include determining selection of a user mode by the user and receiving confirmation that the user would like to activate the selected user mode for the virtual reality system. To select a user mode in the user mode menu, the user may move the handheld controller 1420 to freely manipulate the graphical representation of the handheld controller and navigate through the user mode icons in the user mode menu. Generally, the position/orientation of the handheld controller and position/orientation of the graphical representation of the handheld controller which moves in correspondence with the handheld controller may be analyzed to determine whether the user has selected a particular user mode icon. For example, in variations in which the user mode icons are arranged in a generally circular palette around a central origin, the method may include determining radial distance and/or angular orientation of the graphical representation of the handheld controller relative to the central origin. For example, a test for determining user selection of a user mode icon may include one or more prongs, which may be satisfied in any suitable order. In a first prong 1422, the distance of the graphical representation of the handheld controller to the center of the user mode menu or another reference point in the user mode menu is compared to a distance threshold. The distance may be expressed in terms of absolute distance , number of pixels or ratios , percentage of distance between a center point and the user mode icons arranged around the periphery of the user mode menu, such as 80% or more. If the distance is less than the threshold, then it may be determined that no user mode icon is selected. Additionally or alternatively, selection of a user mode icon may depend on a second prong 1424. In the second prong 1424, the orientation of the graphical representation of the handheld controller is measured and correlated to a user mode icon associated with an arcuate segment of the menu. If the orientation is corresponds to a selected arcuate segment of the menu, then it may be determined that a particular user mode associated with the selected arcuate segment is selected by the user. For example, a user mode icon may be determined as selected by the user if both the distance and the angular orientation of the graphical representation of the handheld controller relative to the origin satisfy the conditions 1422 and 1424. After determining that a user has selected a particular user mode icon, the method may, in some variations, convey such selection to the user , as confirmation by visual and/or auditory indications. For example, in some variations, the method may include rendering one or more visual cues 1430 in the displayed virtual reality environment in response to determining that a user has selected a user mode icon. As shown in 14, exemplary visual cues 1432 include modifying the appearance of the selected user mode icon and/or the arcuate segment associated with the selected user mode icon with highlighting , thickened outlines, animation , wiggling lines, dancing or pulsating icon, change in size , enlargement of icon, change in apparent depth, change in color or opacity , more or less translucent, change in pattern fill of icon, change in position , move radially outward or inward from the central origin, , and/or any suitable visual modification. In some variations, indicating to the user in these or any suitable manner may inform the user which user mode will be activated, prior to the user confirming the selection of a particular user mode. For example, the method may include rendering one or more visual cues 1430 as the user navigates or scrolls through the various user mode icons in the menu. The user may confirm approval of the selected user mode icon in one or more various manners. For example, the user may release or deactivate the user input method 1440 associated with the menu , releasing a button on the handheld controller, disengaging a foot pedal, such as to indicate approval of the selected user mode. In other variations, the user may confirm selection by hovering over the selected user mode icon for at least a predetermined period of time , at least 5 seconds, double-clicking the user input method associated with the user menu , double-clicking the button, , speaking a verbal command indicating approval, etc. In some variations, upon receiving confirmation that the user approves the selected user mode, the method may include verifying which user mode icon has been selected. For example, as shown in 14, a test for verifying which user mode icon has been selected may include one or more prongs, which may be satisfied in any suitable order. For example, in variations in which the user mode icons are arranged in a generally circular palette around a central origin, the method may include determining radial distance relative to the central origin 1442 and/or angular orientation of the graphical representation of the handheld controller relative to the central origin 1446 when the user indicates approval of user mode icon selection. In some variations, prongs 1442 and 1446 may be similar to prongs 1422 and 1424 described above, respectively. If at least one of these prongs 1442 and 1444 is not satisfied, then the release of the user input method may correlated to a non-selection of a user mode icon , the user may have changed his or her mind about selecting a new user mode. Accordingly, if the graphical representation of the handheld controller fails to satisfy the distance threshold 1442 then the original or previous user mode may be retained 1444. Similarly, if the graphical representation of the handheld controller fails to correspond to an arcuate segment of the menu 1446, then the original or previous user mode may be retained 1448. If the graphical representation of the handheld controller does satisfy the distance threshold 1442 and corresponds to an arcuate segment of the menu, then the selected user mode may be activated 1450. In other variations, a user mode may additionally or alternatively be selected with other interactions, such as voice command, eye-tracking via sensors, etc. Furthermore, the system may additionally or alternatively suggest activation of one or more user modes based on criteria such as user activity within the , if the user is frequently turning his head to see detail on the edge of his field of view, the system may suggest a user mode enabling placement of a camera to provide a heads-up display window view from a desired vantage point, as described below, type of surgical procedure, etc. Object GrippingOne exemplary user mode with the virtual robotic surgical environment enables a user to grip, move, or otherwise manipulate virtual objects in the virtual environment. Examples of manipulable virtual objects include, but are not limited to, virtual representations of physical items , one or more virtual robotic arms, one or more virtual tool drivers, virtual manual laparoscopic tools, virtual patient operating table or other resting surface, virtual control tower or other equipment, virtual user console, and other virtual or graphical constructs such as portals, window display, patient imaging or other projections on a heads-up display, etc. which are further described below. At least some of the virtual objects may include or be associated with at least one virtual touchpoint or selectable feature. When the virtual touchpoint is selected by a user, the user may move , adjust position and/or orientation the virtual object associated with the selected virtual touchpoint. Furthermore, multiple virtual touchpoints may be simultaneously selected , with multiple handheld controllers 230 and their graphical representations 230 on the same virtual object or multiple separate virtual objects. The user may generally select a virtual touchpoint by moving a handheld controller 230 to correspondingly move a graphical representation 230 to the virtual touchpoint in the virtual environment, then engaging an interactive feature such as a trigger or button on the handheld controller 230 to indicate selection of the virtual touchpoint. In some variations, a virtual touchpoint may remain selected as long as the user engages the interactive feature on the handheld controller 230 , as long as the user depresses a trigger and may become unselected when the user releases the interactive feature. For example, the virtual touchpoint may enable the user to click and drag the virtual object via the virtual touchpoint. In some variations, a virtual touchpoint may be toggled between a selected state and an unselected state, in that a virtual touchpoint may remain selected after a single engagement of the interactive feature on the handheld controller until a second engagement of the interactive feature toggles the virtual touchpoint to an unselected state. In the virtual robotic surgical environment, one or both kinds of virtual touchpoints may be present. A virtual object may include at least one virtual touchpoint for direct manipulation of the virtual object. For example, a virtual robotic arm in the virtual environment may include a virtual touchpoint on one of its virtual arm links. The user may move a handheld controller 230 until the graphical representation 230 of the handheld controller is in proximity to , hovering over the virtual touchpoint, engage a trigger or other interactive feature on the handheld controller 230 to select the virtual touchpoint, then move the handheld controller 230 to manipulate the virtual robotic arm via the virtual touchpoint. Accordingly, the user may manipulate the handheld controller 230 in order to reposition the virtual robotic arm in a new pose, such as to create a more spacious workspace in the virtual environment by the patient, test range of motion of the virtual robotic arm to determine likelihood of collisions between the virtual robotic arm and other objects, etc. A virtual object may include at least one virtual touchpoint that is associated with a second virtual object, for indirect manipulation of the second virtual object. For example, a virtual control panel may include a virtual touchpoint on a virtual switch or button that is associated with a patient operating table. The virtual switch or button may, for example, control the height or angle of the virtual patient operating table in the virtual environment, similar to how a switch or button on a real control panel might electronically or mechanically modify the height or angle of a real patient operating table. The user may move a handheld controller 230 until the graphical representation 230 of the handheld controller is in proximity to , hovering over the virtual touchpoint, engage a trigger or other interactive feature on the handheld controller 230 to select the virtual touchpoint, then move the handheld controller 230 to manipulate the virtual switch or button via the virtual touchpoint. Accordingly, the user may manipulate the handheld controller 230 in order to modify the height or angle of the virtual environment, such as to improve angle of approach or access to a workspace in the virtual environment. When a virtual touchpoint is selected, the virtual reality processor may modify the virtual robotic surgical environment to indicate to the user that the virtual touchpoint is indeed selected. For example, the virtual object including the virtual touchpoint may be highlighted by being graphically rendered in a different color , blue or red and/or outlined in a different line weight or color, in order to visually contrast the affected virtual object from other virtual objects in the virtual environment. Additionally or alternatively, the virtual reality processor may provide audio feedback , a tone, beep, or verbal acknowledgment through an audio device indicating selection of the virtual touchpoint, and/or tactile feedback , a vibration through a handheld controller 230, the head-mounted display 220, or other suitable device. NavigationOther exemplary user modes with the virtual robotic surgical environment may enable a user to navigate and explore the virtual space within the virtual environment. Snap PointsIn some variations, the system may include a user mode enabling snap points, or virtual targets within a virtual environment which may be used to aid user navigation within the virtual environment. A snap point may, for example, be placed at a user-selected or default location within the virtual environment and enable a user to quickly navigate to that location upon selection of the snap point. A snap point may, in some variations, be associated with an orientation within the virtual environment and/or an apparent scale zoom level of the display of the environment from that vantage point. Snap points may, for example, be visually indicated as colored dots or other colored markers graphically displayed in the first-person perspective view. By selecting a snap point, the user may be transported to the vantage point of the selected snap point within the virtual robotic surgical environment. For example, 16 illustrates method of operation 1600 of an exemplary variation of a user mode enabling snap points. As shown in 16, a snap point may be positioned 1610 in the virtual environment by a user or as a predetermined setting. For example, a user may navigate through a user mode menu as described above, and select or grab a snap point icon from the menu with a handheld controller , indicated with a colored dot or other suitable marker and drag and drop the snap point icon to a desired location and/or orientation in the virtual environment. The snap point may, in some variations, be repositioned by the user reselecting the snap point , moving the graphical representation of the handheld controller until it intersects with the snap point or a collision volume boundary around the snap point, then engaging an input feature such as a button or trigger and dragging and dropping the snap point icon to a new desired location. In some variations, the user may set the scale or zoom level of the vantage point 1620 associated with the snap point, such by adjusting a displayed slider bar or scroll wheel, motions as described above for setting a scale level for an environmental view manipulation, etc. The snap point may, in some examples, have a default scale level associated with all or a subcategory of snap points, a scale level associated with the current vantage point of the user when the user places the snap point, or adjusted as described above. Furthermore, once a snap point is placed, the snap point may be stored 1630 in memory , local or remote storage for future access. A snap point may, in some variations, be deleted from the virtual environment and from memory. For example, a snap point may be selected in a similar manner as for repositioning of the snap point and designed for deletion by dragging it off-screen to a predetermined location , virtual trash can and/or moving it with a predetermined velocity , thrown in a direction away from the user's vantage point with a speed greater than a predetermined threshold, selection of a secondary menu option, voice command, etc. Once one or more snap points for a virtual environment are stored in memory, the user may select one of the stored snap points 1640 for use. For example, upon selection of a stored snap point, the user's vantage point may be adjusted to the position, orientation, and/or scale of the selected snap point's settings 1650, thereby allowing the user to feel as if they are teleporting to the location associated with the selected snap point. In some variations, the user's previous vantage point may be stored as a snap point 1660 to enable easy undo of the user's perceived teleportation and transition the user back to their previous vantage point. Such a snap point may be temporary , disappear after a predetermined period of time, such as after 5-10 seconds. In some examples, the user's previous vantage point may be stored as a snap point only if the user's previous location was not a pre-existing snap point. Furthermore, in some variations, a virtual trail or trajectory , line or arc may be displayed in the virtual environment connecting the user's previous vantage point to the user's new vantage point associated with the selected snap point, which may, for example, provide the user with context as to how they have teleported within the virtual environment. Such a visual indication may be removed from the display of the virtual environment after a predetermined period of time , after 5-10 seconds. Generally, in some variations, a snap point may operate in a similar manner as portals described below, except that a snap point may indicate a vantage point without providing a window preview of the virtual environment. For example, snap points may be placed at user-selected vantage points outside and/or inside the virtual patient, and may be linked into one or more trajectories, similar to portals as described above. In some variations, snap point trajectories may be set by the user in a manner similar to that described below for portals. PortalsIn some variations, the system may include a user mode that facilitates placement of one or more portals, or teleportation points, at user-selected locations in the virtual environment. Each portal may, for example, serve as a transportation gateway to a corresponding vantage point in the virtual environment, thereby allowing the user to swiftly change vantage points for viewing and navigating the virtual environment. Generally, upon selection , with one or more handheld controllers 230 of a portal, the user's apparent location may transition to the location of the selected portal, such that the user views the virtual environment from the selected portal's vantage point and has the sensation of jumping around the virtual environment. By placing one or more portals around the virtual environment, the user may have the ability to quickly move between various vantage points. Placement, adjustment, storing, and/or navigation of portals around the virtual environment may be similar to that of snap points described above. For example, as generally described above, the system may display a first-person perspective view of the virtual robotic surgical environment from a first vantage point within the virtual robotic surgical environment. The user may navigate through a menu to select a user mode that enables placement of a portal. As shown in 9A, the user may manipulate the graphical representation 230 of the handheld controller to position a portal 910 in a selected location in the virtual environment. For example, the user may engage a feature , trigger or button on the handheld controller while a portal placement-enabling user mode is activated, such that while the feature is engaged and the user moves the position and/or orientation of the handheld controller, a portal 910 may appear and be moved within the virtual environment. One or more portal placement indicators 920 , one or more arrows, a line, an arc, etc. connecting the graphical representation 230 to a prospective portal location may aid in communicating to the user the prospective location of a portal 910, such as by helping with depth perception. Size of the portal 910 may be adjusted by grabbing and stretching or shrinking the sides of the portal 910 via the handheld controllers. When the portal 910 location is confirmed , by the user releasing the engaged feature on the handheld controller, double-clicking, , the user's apparent location within the virtual environment may be updated to match the vantage point associated with the portal 910. In some variations, as described below, at least some vantage points within the virtual location may be prohibited. These prohibited vantage points may be stored in memory , local or remote storage. In these variations, if a portal 910 location is confirmed in a prohibited location , compared to and matched among a list of prohibited vantage points stored in memory, then the user's apparent location within the virtual environment may be retained with no changes. However, if a portal 910 location is confirmed as permissible , compared to and not matched among the list of prohibited vantage points, then the user's apparent location within the virtual environment may be updated as described above. In some variations, once the user has placed the portal 910 at a desired vantage point, a window view of the virtual environment from the vantage point of the placed portal 910 may be displayed within the portal 910, thereby offering a preview of the view offered by the portal 910. The user may, for example, view through the portal 910 with full parallax, such that the portal 910 behaves as a type of magnifying lens. For example, while looking through the portal 910, the user may view the virtual environment as if the user has been scaled to the inverse of the portal's scale factor which affects both the interpupillary distance and the focal distance and as if the user has been translated to the reciprocal of the portal's scale factor 1/portal scale factor of the distance from the portal 910 to the user's current location. Furthermore, the portal 910 may include an event horizon which may be a texture on a plane that is rendered, for example, using additional one or more cameras described below within the virtual environment scene positioned as described above. In these variations, when traveling through the portal 910 after selecting the portal 910 for teleportation, the user's view of the virtual environment may naturally converge with the user's apparent vantage point during the user's approach to the portal, since the user's vantage point is offset as a fraction of the distance from the portal by 1/portal scale factor. Accordingly, the user may feel as if they are smoothly and naturally stepping into viewing the virtual environment at the scale factor associated with the selected portal. As shown in 9A, in some variations, the portal 910 may be generally circular. However, in other variations, one or more portals 910 may be any suitable shape, such as elliptical, square, rectangular, irregular, etc. Furthermore, the window view of the virtual environment that is displayed in the portal may display the virtual environment at a scale factor associated with the portal, such that the view of the virtual environment displayed in different portals may be displayed at different zoom levels , 1, 1. 5, 2, 2. 5, 3, , thereby also changing the scale of the user relative to the environment. The scale factor of the window view in a portal may also indicate or correspond with the scale of the view that would be displayed if the user is transported to that portal's vantage point. For example, if a view of the virtual environment outside a virtual patient is about 1, then a window view of the virtual environment inside the virtual patient may be about 2 or more, thereby providing a user with more detail of the internal tissue of the virtual patient. The scale factor may be user-defined or predetermined by the system , based on location of the portal in the virtual environment. In some variations, the scale factor may correlate to the displayed size of the portal 910, though in other variations, the scale factor may be independent of the portal size. In some variations, a portal 910 may be placed in substantially any vantage point in the virtual environment that the user desires. For example, a portal 910 may be placed anywhere on a virtual ground surface of the virtual operating room or on a virtual object , table, chair, user console, . As another example, as shown in 9B, a portal 910 may be placed in mid-air at any suitable elevation above the virtual ground surface. As yet another example, as shown in 9C, a portal may be placed on or inside a virtual patient, such as portals 910a and 910b which are placed on the abdomen of a patient and enable views of the intestines and other internal organs of the virtual patient , simulated augmented reality. In this example, the virtual patient may be generated from medical imaging and other information for a real non-virtual patient, such that portals 910a and 910b may enable the user to have an immersive view of an accurate representation of the real patient's tissue , for viewing tumors, , and/or generated from internal virtual cameras described below placed inside the patient. In some variations, the system may limit placement of a portal 910 according to predefined guidelines , only outside the patient or only inside the patient, which may correspond, for example, to a type of simulated surgical procedure or a training level , beginner or advanced user level associated with the virtual environment. Such prohibited locations may be indicated to the user by, for example, a visual change in the portal 910 as it is being placed , changing outline color, displaying a grayed-out or opaque window view within the port 910 as it is being placed and/or auditory indications , beep, tone, verbal feedback. In yet other variations, the system may additionally or alternatively include one or more portals 910 placed in predetermined locations, such as at a virtual user console in the virtual environment, adjacent the virtual patient table, etc. Such predetermined locations may, for example, depend the type of procedure, or be saved as part of a configuration file. A portal 910 may be viewable from either side , front side and rear side of the portal. In some variations, the view from one side of the portal 910 may be different from an opposite side of the portal 910. For example, when viewed from a first side , front of the portal 910, the portal may provide a view of the virtual environment with a scale factor and parallax effects as described above, while when viewed from a second side , rear of the portal 910, the portal may provide a view of the virtual environment with a scale factor of about one. As another example, the portal may provide a view of the virtual environment with a scale factor and parallax effects when viewed from both the first side and the second side of the portal. In some variations, multiple portals 910 may be sequentially linked to include a trajectory in the virtual environment. For example, as shown in 9C, a first-person perspective view of the virtual robotic surgical environment from a first vantage point may be displayed , an immersive view. The user may place a first portal 910a in a second vantage point that is different from the first vantage point , closer to the virtual patient than the first vantage point and a first window view of the virtual robotic surgical environment from the second vantage point may be displayed in the first portal 910a. Similarly, the user may place a second portal 910b in a third vantage point , closer to the patient than the first and second vantage points, and a second window view of the virtual robotic surgical environment may be displayed in the second portal 910b. The user may provide a user input associating the first and second portals 910a and 910b , by selection with the handheld controllers, drawing a line between the first and second portals with the handheld controllers, such that the first and second portals are sequentially linked, thereby generating a trajectory between the first and second portals. In some variations, after multiple portals 910 are linked to generate a trajectory, transportation along the trajectory may not require explicit selection of each sequential portal. For example, once on the trajectory , at the second vantage point, traveling between linked portals may be accomplished by engagement of a trigger, button, touchpad, scroll wheel, other interactive feature of the handheld controller, voice command, etc. Additional portals may be linked in a similar manner. For example, two, three, or more portals may be linked in series to generate an extended trajectory. As another example, multiple portals may form branched trajectories, where at least two trajectories share at least one portal in common but otherwise each trajectory has at least one portal that is unique to that trajectory. As yet another example, multiple portals may form two or more trajectories that share no portals in common. The user may select which trajectory on which to travel, such as by using the handheld controllers and/or voice command, etc. One or more trajectories between portals may be visually indicated , with a dotted line, color coding of portals along the same trajectory, , and such visual indication of trajectories may be toggled on and off, such as based on user preference. Other portal features may facilitate easy navigation of the trajectories between portals. For example, a portal may change color when the user has entered and gone through that portal. As shown in 9C, in another example, a portal itself may be displayed with direction arrows indicating the permissible direction of the trajectory including that portal. Furthermore, travel along the trajectories may be accomplished with an undo command via handheld controllers and/or voice command, that returns the user to the previous vantage point , displays the view of the virtual environment from the previous vantage point. In some variations, a home or default vantage point may be established such as according to user preference or system settings in order to enable a user to return to that home vantage point quickly with a shortcut command, such as an interactive feature on a handheld controller or a voice command , Reset my position. For example, a home or default vantage point may be at a virtual user console or adjacent to the virtual patient table. The user mode facilitating placement and use of portals, or another separate user mode, may further facilitate deletion of one or more portals. For example, a portal may be selected for deletion with the handheld controllers. As another example, one or more portals may be selected for deletion via voice command , delete all portals or delete portal A. Free NavigationThe system may include a user mode that facilitates free navigation around the virtual robotic surgical environment. For example, as described herein, the system may be configured to detect the user's walking movements based on sensors in the head-mounted display and/or handheld controllers, and may correlate the user's movements into repositioning within a virtual operating room. In another variation, the system may include a flight mode that enables the user to quickly navigate the virtual environment in a flying manner at different elevations and/or speeds, and at different angles. For example, the user may navigate in flight mode by directing one or more handheld controllers and/or the headset in a desired direction for flight. Interactive features on the handheld controller may further control flight. For example, a directional pad or touchpad may provide control for forward, backward, strafing, etc. motions while maintaining substantially the same perspective view of the virtual environment. Translation may, in some variations, occur without acceleration, as acceleration may tend to increase the likelihood of simulator sickness. In another user setting, a directional pad or touchpad or orientation of the headset may provide control for elevation of the user's apparent location within the virtual environment. Furthermore, in some variations, similar to that described above with respect to portals, a home or default vantage point within the flight mode may be established in order to enable a user to return to that home vantage point quickly with a shortcut command. Parameters such as speed of flight in response to a user input may be adjustable by the user and/or set by the system by default. Furthermore, in flight mode, the scaling factor of the displayed view may be controlled via the handheld controllers. The scaling factor may, for example, affect apparent elevation of the user's location within the virtual environment. In some variations, the user may use the handheld controllers to pull apart two points in the displayed view to zoom out and draw closer two points in the displayed view to zoom in, or conversely pull apart two points in the displayed view to zoom in an draw closer two points in the displayed view to zoom out. Additionally or alternatively, the user may utilize voice commands , increase zoom to 2 to change the scaling factor of the displayed view. For example, 10A and 10B illustrate exemplary views of the virtual environment that are relatively zoomed in and zoomed out, respectively. Parameters such as the speed of the change in scaling factor, minimum and maximum scaling factor ranges, etc. may be adjustable by the user and/or set by the system by default. As the user freely navigates the virtual environment in flight mode, the displayed view may include features to reduce eye fatigue, nausea, etc. For example, in some variations, the system may include a comfort mode in which outer regions of the displayed view are removed as the user navigates in flight mode, which may, for example, help reduce motion sickness for the user. As shown in 10C, when in the comfort mode, the system may define a transition region 1030 between an inner transition boundary 1010 and an outer transition boundary 1020 around a focal area , center of the user's view. Inside the transition region inside the inner transition boundary 1010, a normal view of the virtual robotic surgical environment is displayed. Outside the transition region outside the outer transition boundary 1020, a neutral view or plain background , a plain, gray background is displayed. Within the transition region 1030, the displayed view may have a gradient that gradually transitions the view of the virtual environment to the neutral view. Although the transition region 1030 shown in 10C is depicted as generally circular, with generally circular inner and outer transition boundaries 1010 and 1020, in other variations the inner and outer transition boundaries 1010 and 1020 may define a transition region 1030 that is elliptical or other suitable shape. Furthermore, in some variations, various parameters of the transition region, such as size, shape, gradient, etc. may be adjustable by the user and/or set by the system by default. In some variations, as shown in 11, the user may view the virtual robotic surgical environment from a dollhouse view that allows the user to view the virtual operating room from an overhead vantage point, with a top-down perspective. In the dollhouse view, the virtual operating room may be displayed at a smaller scale factor , smaller than life-size on the display, thereby changing the scale of the user relative to the virtual operating room. The dollhouse view may provide the user with additional contextual awareness of the virtual environment, as the user may view the entire virtual operating room at once, as well as the arrangement of its contents, such as virtual equipment, virtual personnel, virtual patient, etc. Through the dollhouse view, for example, the user may rearrange virtual objects in the virtual operating room with fuller contextual awareness. The dollhouse view may, in some variations, be linked in a trajectory along with portals and/or snap points described above. Environment View RotationIn some variations, the system may include a user mode that enables the user to navigate the virtual robotic surgical environment by moving the virtual environment around his or her current vantage point. The environment view rotation mode may offer a different manner in which the user may navigate the virtual environment, such as by grasping and manipulating the environment as if it were an object. As the user navigates through the virtual environment in such a manner, a comfort mode similar to that described above may additionally be implemented to help reduce simulation-related motion sickness. For example, in an environment view rotation mode, the user may rotate a displayed scene around a current vantage point by selecting and dragging the view of the virtual environment around the user's current vantage point. In other words, in the environment view rotation mode, the user's apparent location in the virtual environment appears fixed while the virtual environment may be moved. This is contrast to other modes, such as, for example, flight mode described above, in which generally the environment may appear fixed while the user moves. Similar to the scaling factor adjustments described above for flight mode, in the environment view rotation mode, the scaling factor of the displayed view of the environment may be controlled by the handheld controllers and/or voice commands , by using the handheld controllers to select and pull apart two points in the displayed view to zoom in, . For example, as shown in 15, in one exemplary variation of a method 1500 for operating in an environment view rotation mode, the user may activate a user input method 1510 such as on a handheld controller , a button or trigger or other suitable feature or any suitable device. In some variations, one handheld controller 1520 may be detected upon activation of the user input method. The original position of the handheld controller at the time of activation may be detected and stored 1522. Thereafter, as the user moves the handheld controller , while continuing to activate the user input method, the current position of the handheld controller may be detected 1524. A vector difference between the original or previous position and the current position of the handheld controller may be calculated 1526, and position of the vantage point of the user may be adjusted 1528 based at least partially on the calculated vector difference, thereby creating an effect that makes the user feel they are grabbing and dragging the virtual environment around. In some variations, two handheld controllers 1520 may be detected upon activation of the user input method. The original positions of the handheld controllers may be detected 1522, and a centerpoint and an original vector between the original positions of the handheld controllers 1523 may be calculated and stored. Thereafter, as the user moves one or more both handheld controllers , while continuing to activate the user input method, the current positions of the handheld controllers may be detected 1524 and used to form the basis for a calculated vector difference between original and current vectors between handheld controllers 1528. The position and/or orientation of the vantage point of the user may be adjusted 1528, based on the calculated vector difference. For example, the orientation or rotation of the displayed view may be rotated around the centerpoint between the handheld controller locations, thereby creating an effect that makes the user feel they are grabbing and dragging the environment around. Similarly, the scale of the display of the virtual environment 1529 may be adjusted based on the calculated difference in distance between the two handheld controllers, thereby creating an effect that makes the user feel they are grabbing and zooming in and out of the displayed view of the virtual environment. Although the above-described user modes are described separately, it should be understood that aspects of these modes characterize exemplary ways that a user may navigate the virtual robotic surgical environment, and may be combined in a single user mode. Furthermore, some of these aspects may be seamlessly linked. For example, an overhead vantage point generally associated with flight mode may be sequentially linked with one or more portals in a trajectory. Even further, in some variations, a vantage point or displayed view of the virtual environment , as adjusted via one or more of the above user modes may be linked to at least one default vantage point , default in position, orientation, and/or scale. For example, by activating a user input , on a handheld controller, foot pedal, , a user may reset the current vantage point to a designated or predetermined vantage point in the virtual environment. The user's current vantage point may, for example, be gradually and smoothly animated in order to transition to default values of position, orientation, and/or scale. Supplemental ViewsIn some variations, an exemplary user mode or modes of the system may display one or more supplemental views of additional information to a user, such as overlaid over or inset in the primary, first-person perspective view of the virtual robotic surgical environment. For example, as shown in 12, a heads-up display 1210 HUD may provide a transparent overlay over a primary first-person perspective view of the virtual environment. The HUD 1210 may toggled on and off, thereby allowing the user to control whether to display the HUD 1210 at any particular time. Supplemental views of additional information, such as that described below, may be placed onto the HUD such that the user may observe the supplemental views without looking away from the primary view. For example, the supplemental views may stick to the HUD 1210 and move with the user's head movement such that the supplemental views are always in the user's field of view. As another example, supplemental views of additional information may be loosely fixed to the HUD 1210, in that the supplemental views may be small or at least partially hidden off-screen or in peripheral vision when the user's head is generally facing forward, but minor or slight head movement to one side may expand and/or bring one or more supplemental views into the user's field of view. The one or more supplemental views may be arranged on the HUD 1210 in a row, grid, or other suitable arrangement. In some variations, the HUD 1210 may include predetermined snap points to which the supplemental views , camera views, are positioned. For example, the user may select a supplemental view on the HUD 1210 for closer inspection, then replace the supplemental view on the HUD 1210 by dragging it generally in the direction of a snap point, whereupon the supplemental view may be drawn to and fixed at the snap point without needing to be precisely placed there by the user. As another example, in a virtual command station mode, one or more supplemental views may be displayed in a virtual space with one or more content windows or panels arranged in front of the user in the virtual space , similar to a navigable menu. For example, as shown in 13, multiple content windows , 1310a, 1310b, 1310c, and 1310d may be positioned in a semi-circular arrangement or other suitable arrangement for display to a user. The arrangement of the content windows may be adjusted by the user , using handheld controllers with their graphical representations 230 to select and drag or rotate content windows. The content windows may display, for example, an endoscope video feed, a portal view, a stadium overhead view of the virtual operating room, patient data , imaging, other camera or patient information views such as those described herein, etc. By viewing multiple panels simultaneously, the user may be able to simultaneously monitor multiple aspects of the virtual operating room and/or with the patient, thereby allowing the user to have an overarching and broader awareness of the virtual environment. For example, the user may become aware of and then respond more quickly to any adverse events in the virtual environment , simulated negative reactions of the virtual patient during a simulated surgical procedure. Furthermore, the virtual command station mode may enable a user to select any one of the content windows and become immersed in the displayed content , with a first-person perspective. Such a fully immersive mode may temporarily dismiss the other content windows, or may minimize , be relegated to a HUD overlaid over the selected immersive content. As an illustrative example, in the virtual command station mode, the system may display multiple content windows including an endoscopic camera video feed showing the inside of a virtual patient's abdomen. The user may select the endoscopic camera video feed to become fully immersed in the virtual patient's abdomen , while still manipulating robotic arms and instruments attached to the arms. Camera ViewsIn some variations, a user mode may enable the user to place a virtual camera in a selected vantage point in the virtual environment, and a window view of the virtual environment from the selected vantage point may be displayed in the HUD such that the user may simultaneously view both his first-person perspective field of view and the camera view the view provided by the virtual camera that may update in real time. A virtual camera may be placed in any suitable location in the virtual environment , inside or outside the patient, overhead the patient, overhead the virtual operating room, . For example, as shown in 12, the user may place a virtual camera 1220 , using object gripping as described above near the pelvic region of a virtual patient and facing the patient's abdomen so as to provide a virtual video feed of the patient's abdomen. Once placed, a virtual camera 1220 may be subsequently repositioned. A camera view , a circular inset view, or window of any suitable shape may be placed on the HUD as a window view showing the virtual video feed from the vantage point of the virtual camera 1220. Similarly, multiple virtual cameras may be placed in the virtual environment to enable multiple camera views to be shown on the HUD. In some variations, a predetermined arrangement of one or more virtual cameras may be loaded, such as part of a configuration file for the virtual reality processor to incorporate into the virtual environment. In some variations, the system may offer a range of different kinds of virtual cameras, which may provide different kinds of camera views. One exemplary variation of a virtual camera is a movie camera that is configured to provide a live virtual feed of the virtual environment , movie camera view 1212 in 12. Another exemplary variation of a virtual camera is an endoscopic camera which is attached to a virtual endoscope to be placed in a virtual patient. In this variation, the user may, for example, virtually perform a technique for introducing the virtual endoscope camera into the virtual patient and subsequently monitor the internal workspace inside the patient by viewing the virtual endoscopic video feed , endoscopic camera view 1214 in 12. In another exemplary variation, the virtual camera may be a wide-angle , 360-degree, panoramic, camera that is configured to provide a larger field of view of the virtual environment. In this variation, the window camera view may, for example, be displayed as a fish-eye or generally spherical display. Various aspects of the camera view may be adjusted by the user. For example, the user may adjust the location, size, scale factor, etc. of the camera view , similar to adjustments of portals as described above. As another example, the user may select one or more filters or other special imaging effects to be applied to the camera view. Exemplary filters include filters that highlight particular anatomical features , tumors or tissue characteristics , perfusion of the virtual patient. In some variations, one or more virtual cameras may be deselected or turned off , have the virtual camera and/or associated camera view selectively hidden or deleted, such as if the virtual camera or its associated camera view is obstructing the user's view of the virtual environment behind the virtual camera or camera view. In some variations, a camera view may function similarly to a portal described above to enable the user to navigate quickly around the virtual environment. For example, with reference to 12, a user may select the camera view 1212 , highlight or grab and pull the camera view 1212 toward himself to be transported to the vantage point of the camera view 1212. Patient Data Views, Etc. In some variations, a user mode may enable display of patient data and other information on the HUD or another suitable location in the display. For example, patient imaging information , ultrasound, X-ray, MRI, may be displayed in a supplemental display, overlaid over the patient , as simulated augmented reality. A user may, for example, view patient images as reference while interacting with the virtual patient. As another example, patient vitals , heartrate, blood pressure, may be displayed to the user in a supplemental view. In another variation, a user mode may enable display of other suitable information, such as training videos , exemplary surgical procedures recorded from a previous procedure, video feed from a mentor or trainer surgeon, etc. providing guidance to a user. Virtual Reality System ApplicationsGenerally, the virtual reality system may be used in any suitable scenario in which it is useful to simulate or replicate a robotic surgical environment. In some variations, the virtual reality system may be used for training purposes, such as allowing a surgeon to practice controlling a robotic surgical system and/or to practice performing a particular kind of minimally-invasive surgical procedure using a robotic surgical system. The virtual reality system may enable a user to better understand the movements of the robotic surgical system in response to user commands, both inside and outside the patient. For example, a user may don a head-mounted display under supervision of a mentor or trainer who may view the virtual reality environment alongside the user , through a second head-mounted display, through an external display, and guide the user through operations of a virtual robotic surgical system within the virtual reality environment. As another example, a user may don a head-mounted display and may view, as displayed on the immersive display , in a content window, the HUD, a training-related video such as a recording of a previously performed surgical procedure. As another example, the virtual reality system may be used for surgical planning purposes. For example, a user may operate the virtual reality system to plan surgical workflow. Configuration files of virtual objects , robotic surgical system including arms and tool drivers, user console, end effectors, other equipment, patient bed, patient, personnel, may be loaded into a virtual robotic surgical environment as representative of actual objects that will be in the actual i. e. , non-virtual, or real operating room. Within the virtual reality environment, the user may adjust features of the virtual operating room, such as positioning the user console, patient bed, and other equipment relative to one another in a desired arrangement. The user may additionally or alternatively use the virtual reality system to plan aspects of the robotic surgical system, such as selecting number and location of ports for entry of the surgical instruments, or determining optimum number and position/orientation , mounting location, arm pose, of robotic arms for a procedure, such as for minimizing potential collisions between system components during the surgical procedure. Such virtual arrangements may be based on, for example, trial-and-error, previous setups for similar surgical procedures and/or similar patients, etc. In some variations, the system may additionally or alternatively propose virtual arrangements selected based on machine learning techniques applied to datasets of previously-performed surgical procedures for various kinds of patients. As yet another example, the virtual reality system may be used for R&amp;D purposes , simulation. For example, a method for designing a robotic surgical system may include generating a virtual model of a robotic surgical system, testing the virtual model of the robotic surgical system in a virtual operating room environment, modifying the virtual model of the robotic surgical system based on the testing, and building the robotic surgical system based on the modified virtual model. Aspects of the virtual model of the robotic surgical system that may be tested in the virtual operating room environment include physical characteristics of one or more components of the robotic surgical system , diameter or length of arm links. For example, a virtual model of a particular design of a robotic arm may be built and implemented in a virtual environment, where the virtual model may be tested with respect to particular arm movements, surgical procedures, etc. , test for likelihood of collision between the robotic arm and other objects. Accordingly, a design of a robotic arm or similarly, any other component of the robotic surgical system may be at least initially tested by testing a virtual implementation of the design, rather than testing a physical prototype, thereby accelerating the R&amp;D cycle and reducing costs. Other aspects that may be tested include functionality of one or more components of the robotic surgical system , control modes of a control system. For example, as described above, a virtual operating environment application may pass status information to a kinematics application, and the kinematics application may generate and pass commands based on control algorithms, where the virtual reality processor may use the commands to cause changes in the virtual robotic surgical environment , move a virtual robotic arm in a particular way in accordance with relevant control algorithms. As such, software control algorithms may be embodied in a virtual robotic system for testing, refinement, etc. without requiring a physical prototype of the relevant robotic component, thereby conserving R&amp;D resources and accelerating the R&amp;D cycle. In another example, the virtual reality system may be used to enable multiple surgeons to collaborate in the same virtual reality environment. For example, multiple users may don head-mounted displays and interact with each other and with the same virtual robotic system, the same virtual patient, in the virtual reality environment. The users may be physically in the same room or general location, or may be remote from one another. For example, one user may be tele-mentoring the other as they collaborate to perform a surgical procedure on the virtual patient. Specific illustrative exemplary applications of the virtual reality system are described in further detail below. However, it should be understood that applications of the virtual reality system are not limited to these examples and general application scenarios described herein. Example 1Over the BedA user may use the virtual reality system to simulate an over-the-bed scenario in which he is adjacent to a patient bed or table and operating both a robotic surgical system and a manual laparoscopic tool. Such simulation may be useful for training, surgical planning, etc. For example, the user may staple tissue in a target segment of a virtual patient's intestine using both a virtual robotic tool and a virtual manual tool. In this example, the user dons a head-mounted display providing an immersive view of a virtual reality environment, and may use handheld controllers to navigate within the virtual reality environment to be adjacent to a virtual patient table on which a virtual patient lies. A proximal end of a virtual robotic arm is attached to the virtual patient table, and a distal end of the virtual robotic arm supports a virtual tool driver actuating virtual forceps that are positioned within the abdomen of the virtual patient. A virtual manual laparoscopic stapler tool is passed through a virtual cannula and having a distal end positioned within the abdomen of the virtual patient. Additionally, an endoscopic camera is positioned within the abdomen of the virtual patient, and provides a virtual camera feed showing the surgical workspace within the abdomen of the virtual patient including patient tissue, virtual robotically-controlled forceps, and a virtual manual laparoscopic stapler tool. The user continues to view the virtual environment through the immersive display in the head-mounted display, as well as the virtual endoscopic camera feed displayed in a window view in a heads-up display overlaid in the user's field of view. The user holds in one hand a handheld controller that is configured to control the robotically-driven virtual forceps. The user holds in another hand a laparoscopic hand controller that is configured to control the virtual manual laparoscopic stapler tool, with the laparoscopic hand controller passing through a cannula mounted in a mock patient body made of foam. The laparoscopic hand controller is calibrated to correspond to the virtual manual laparoscopic stapler tool. The user manipulates the handheld controller to operate the robotically-controlled forceps to manipulate the intestine of the virtual patient and uncover a target segment of the intestine. With the target segment of the intestine exposed and accessible, the user manipulates the laparoscopic hand controller to apply virtual staples to the target segment via the virtual manual laparoscopic stapler tool. Example 2Collision Resolution from User ConsoleWhen using the virtual reality system, a user may desire to resolve collisions between virtual components of the virtual robotic surgical system, even though the user may not be adjacent the colliding virtual components , the user may be seated at a distance away from the virtual patient table, such as at a virtual user console. In this example, the user dons a head-mounted display providing an immersive view provided by a virtual endoscope placed inside an abdomen of a virtual patient. Proximal ends of two virtual robotic arms are attached to separate locations on a virtual patient table, on which the virtual patient lies. Distal ends of the virtual robotic arms support respective tool drivers actuating virtual forceps that are positioned within the abdomen of the virtual patient. The user manipulates the handheld controllers to operate the two robotically-controlled virtual forceps, which manipulate virtual tissue within the virtual patient. This movement may cause a collision involving at least one of the virtual robotic arms , a virtual robotic arm may be posed so as to create a collision with itself, the virtual robotic arms may be posed so as to create a collision with each other, a virtual robotic arm may be posed so as to create a collision with the patient or nearby obstacle, . The virtual reality system detects the collision based on status information of the virtual robotic arms, and alerts the user regarding the collision. The system displays an overhead or other suitable view from a suitable vantage point of the virtual robotic surgical system, such as in a window view , picture-in-picture view. The location of the collision is highlighted in the displayed window view, such as by outlining the affected colliding components with red or another contrasting color. Alternatively, the user may detect the collision himself by monitoring a camera video feed from a virtual camera placed overhead the virtual patient table. Upon becoming aware of the collision, the user may zoom out or adjust the scale of his immersive view of the virtual reality environment. The user may engage an arm repositioning control mode that locks the position and orientation of the virtual forceps within the patient. Using the handheld controllers in an object gripping user mode, the user may grab onto virtual touchpoints on the virtual robotic arms and reposition repose the virtual robotic arms so as to resolve the collision while the control mode maintains the position and orientation of the virtual forceps during the arm repositioning. Once the virtual robotic arms are repositioned such that the collision is resolved, the user may zoom back into the previous vantage point, disengage the arm repositioning control mode, and resume using the handheld controllers to operate the virtual forceps within the virtual patient. Example 3Coordinated Relocation of Multiple Surgical Instruments from User ConsoleWhen using the virtual reality system, a user may find it useful to stay substantially in an endoscopic view and relocating multiple virtual surgical instruments , end effectors, cameras as a group rather than individually within the virtual patient, thereby saving time, as well as making it easier for the user to maintain contextual awareness of the instruments relative to the virtual patient's anatomy. In this example, the user dons a head-mounted display providing an immersive view provided by a virtual endoscope placed inside an abdomen of a virtual patient. Proximal ends of two virtual robotic arms are attached to separate locations on a virtual patient table, on which the virtual patient lies. Distal ends of the virtual robotic arm support respective tool drivers actuating virtual forceps that are positioned in the pelvic area of the virtual patient. The user may manipulate handheld controllers to operate the virtual forceps. The user may wish to move the virtual endoscope and the virtual forceps to another target region of the virtual patient's abdomen, such as the spleen. Rather than move each surgical instrument individually, the user may engage a coordinated relocation mode. Once this mode is engaged, the endoscopic camera view zooms out along the axis of the endoscope to a distance sufficient to allow the user to view the new target region spleen. A spherical indicator is displayed at the distal end of the endoscope that encapsulates the distal end of the virtual endoscope and the distal ends of the virtual forceps. The user manipulates at least one handheld controller to withdraw the virtual endoscope and the virtual forceps away from the surgical workspace , until the user can see the distal end of the virtual cannula in the virtual endoscopic view, then grab and move the spherical indicator from the pelvic area to the spleen. Once the user finalizes the new target region by moving the spherical indicator to the new target region, the virtual endoscope and virtual forceps automatically travel to the new target region and the virtual endoscopic camera view zooms to show the new target region. Throughout this relatively large-scale move, the user views the virtual environment with substantially an endoscopic view of the virtual environment, thereby enabling the user to maintain awareness of the virtual patient's anatomy instead of transferring his focus between instrument and anatomy. The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the invention. However, it will be apparent to one skilled in the art that specific details are not required in order to practice the invention. Thus, the foregoing descriptions of specific embodiments of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed; obviously, many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications, they thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated. It is intended that the following claims and their equivalents define the scope of the invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/01/706/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9915380477905273,
                    "F_SPEC_PARAMS": [
                        "reduced patient scarring, less patient pain, shorter patient recovery periods,",
                        "lower medical treatment costs"
                    ],
                    "S_SPEC_PARAMS": [
                        "expensive",
                        "time-consuming"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery."
                    ],
                    "S_SENTS": [
                        "Such cyclical prototyping and testing is generally cumulatively expensive and time-consuming."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Side Effects"
                    ],
                    "F_SIM_SCORE": 0.4541338384151459,
                    "S_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.5887263417243958,
                    "GLOBAL_SCORE": 1.7129681378602981
                },
                "sort": [
                    1.7129681
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11284955-20220329",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11284955-20220329",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-06-25",
                    "PUBLICATION_DATE": "2022-03-29",
                    "INVENTORS": [
                        "Haoran Yu",
                        "Pablo Eduardo Garcia Kilroy",
                        "Bernard Fai Kin Siu",
                        "Eric Mark Johnson"
                    ],
                    "APPLICANTS": [
                        "VERB SURGICAL INC.    ( Mountain View , US )"
                    ],
                    "INVENTION_TITLE": "Emulation of robotic arms and control thereof in a virtual reality environment",
                    "DOMAIN": "A61B 3470",
                    "ABSTRACT": "A virtual reality system providing a virtual robotic surgical environment, and methods for using the virtual reality system, are described herein. The virtual reality system may be used to expedite the R&amp;D cycle during development of a robotic surgical system, such as by allowing simulation of potential design without the time and significant expense of physical prototypes. The virtual reality system may also be used to test a control algorithm or a control mode for a robotic surgical component.",
                    "CLAIMS": "1. A virtual reality system for verifying designs of a robotic surgical system, the system comprising: one or more processors configured to generate a virtual component according to a shape and a size of a model of a component of the robotic surgical system, in a virtual surgical system that emulates the robotic surgical system, wherein the virtual component includes a virtual operating room; and a user input device communicatively coupled to the processors configured to: manipulate the virtual component for testing the model of the component of the robotic surgical system in the virtual surgical system; and modify the shape or the size of the model of the component of the robotic surgical system based on the testing, wherein modifying comprises varying a shape or dimension of the virtual operating room; wherein the processors are further configured to export the modified model of the component of the robotic surgical system for designing the component in the robotic surgical system. 2. The system of claim 1, wherein generate the virtual component comprises importing the model of the component of the robotic surgical system. 3. The system of claim 1, wherein the virtual component in the virtual surgical system comprises at least one virtual table-mounted robotic arm supporting a virtual robotic surgical instrument. 4. The system of claim 3, wherein testing the model of the component of the robotic surgical system comprises performing a virtual surgical procedure using the virtual table-mounted robotic arm and the virtual robotic surgical instrument. 5. The system of claim 3, wherein testing the model of the component of the robotic surgical system comprises detecting a collision event involving the virtual robotic arm. 6. The system of claim 5, wherein modifying the shape or size, of the model of the component of the robotic surgical system comprises varying designs of arm links and joints, number and arrangement thereof, and mounting location thereof in response to the detected collision event. 7. The system of claim 6, wherein the user input device is further configured to manipulate the virtual table-mounted robotic arm for testing the modified model in the virtual surgical system. 8. The system of claim 1, wherein the virtual component includes a plurality of virtual robotic arms mounted on a virtual operating table, a virtual user console, a virtual control tower, a virtual patient and one or more virtual staff in a virtual operating room. 9. The system of claim 8, wherein modifying the model of the component of the robotic surgical system comprises varying arrangements of the virtual operating table, the virtual user console and the virtual control tower in the virtual operating room. 10. A computer-implemented method for designing a component for a robotic surgical system including a robotic arm, the method comprising: importing a model of the component of the robotic surgical system, to generate a virtual component according to a shape and a size defined by the model, in a virtual surgical system, the virtual component including a virtual operating room; testing the model of the component by manipulating one or more virtual robotic arms in the virtual operating room; modifying the shape or the size of the model of the component of the robotic surgical system based on the testing, including varying a shape or a dimension of the virtual operating room; and exporting the modified model of the component for designing the component for the robotic surgical system. 11. The method of claim 10, wherein the virtual operating room comprises a virtual operating table, a virtual user console, a virtual control tower, a virtual patient and one or more virtual staff. 12. The method of claim 11, wherein testing the model of the component comprises performing a virtual surgical procedure on the virtual patient using the one or more virtual robotic arms. 13. The method of claim 11, wherein testing the model of the robotic arm comprises detecting a collision event involving the one or more virtual robotic arms, the virtual operating table, the virtual patient, or the virtual staff. 14. The method of claim 13, wherein modifying a shape or size of the model of the component comprises varying designs of the robotic arm, including arm links and joints, number and arrangement thereof, and mounting location thereof in response to the detected collision event. 15. A computer-implemented method for testing a control mode of a robotic arm in a robotic surgical system, the method comprising: generating a virtual surgical system corresponding to the robotic surgical system including generating a virtual component that includes a virtual operating room, according to a shape and size of a model of a component of the robotic surgical system, the virtual surgical system comprising one or more virtual robotic arms corresponding to the robotic arm in the robotic surgical system; importing a control mode for the robotic arm into the virtual surgical system the control mode including a set of actuation commands for a plurality of virtual joints; in response to a user input to move the one or more virtual robotic arms for testing the model of the component of the robotic surgical system in the virtual surgical system, moving the one virtual robotic arm in accordance with a control algorithm that is associated with the control mode by applying the set of actuation commands to virtual joints of the virtual robotic arms; modifying the shape or the size of the model of the component of the robotic surgical system based on the testing, wherein modifying comprises varying a shape or a dimension of the virtual operating room; refining the at least one control algorithm to update the control mode including modifying the set of actuation commands; and exporting the modified model of the component of the robotic surgical system for designing the component in the robotic surgical system. 16. The method of claim 15, wherein the one or more virtual robotic arm emulates a mechanical and kinematic model of the robotic arm in the robotic surgical system. 17. The method of claim 16, wherein the control mode comprises control algorithms for a joint command mode, a gravity compensation mode, a trajectory following mode, an idling mode, a setup mode and docking mode. 18. The method of claim of 17, moving the virtual robotic arm comprising generating an actuation command for at least one of a plurality of virtual joints in the virtual robotic arm based on the user input, the kinematic model, a current status of the virtual robotic arm, and at least one control algorithm of the control mode. 19. The method of claim 15, further comprising: exporting the updated control mode.",
                    "FIELD_OF_INVENTION": "This invention relates generally to the field of robotic surgery, and more specifically to new and useful systems and methods for providing virtual robotic surgical environments.",
                    "STATE_OF_THE_ART": "Minimally-invasive surgery MIS, such as laparoscopic surgery, involves techniques intended to reduce tissue damage during a surgical procedure. For example, laparoscopic procedures typically involve creating a number of small incisions in the patient , in the abdomen, and introducing one or more surgical instruments , an end effector, at least one camera, through the incisions into the patient. The surgical procedures may then be performed using the introduced surgical instruments, with the visualization aid provided by the camera. Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery. In some embodiments, MIS may be performed with robotic systems that include one or more robotic arms for manipulating surgical instruments based on commands from an operator. A robotic arm may, for example, support at its distal end various devices such as surgical end effectors, imaging devices, cannulae for providing access to the patient's body cavity and organs, etc. Robotic surgical systems are generally complex systems performing complex procedures. Accordingly, a user , surgeons generally may require significant training and experience to successfully operate a robotic surgical system. Such training and experience is advantageous to effectively plan the specifics of MIS procedures , determine optimal number, location, and orientation of robotic arms, determine optical number and location of incisions, determine optimal types and sizes of surgical instruments, determine order of actions in a procedure, . Additionally, the design process of robotic surgical systems may also be complicated. For example, improvements in hardware , robotic arms are prototyped as physical embodiments and physically tested. Improvements in software , control algorithms for robotic arms may also require physical embodiments. Such cyclical prototyping and testing is generally cumulatively expensive and time-consuming.",
                    "SUMMARY": [
                        "Generally, a virtual reality system for providing a virtual robotic surgical environment may include a virtual reality processor , a processor in a computer implementing instructions stored in memory for generating a virtual robotic surgical environment, a head-mounted display wearable by a user, and one or more handheld controllers manipulable by the user for interacting with the virtual robotic surgical environment. The virtual reality processor may, in some variations, be configured to generate a virtual robotic surgical environment based on at least one predetermined configuration file describing a virtual component , virtual robotic component in the virtual environment. The head-mounted display may include an immersive display for displaying the virtual robotic surgical environment to the user , with a first-person perspective view of the virtual environment. In some variations, the virtual reality system may additionally or alternatively include an external display for displaying the virtual robotic surgical environment. The immersive display and the external display, if both are present, may be synchronized to show the same or similar content. The virtual reality system may be configured to generate a virtual robotic surgical environment within which a user may navigate around a virtual operating room and interact with virtual objects via the head-mounted display and/or handheld controllers. The virtual reality system and variations thereof, as further described herein may serve as a useful tool with respect to robotic surgery, in applications including but not limited to training, simulation, and/or collaboration among multiple persons. In some variations, a virtual reality system may interface with a real or actual non-virtual operating room. The virtual reality system may enable visualization of a robotic surgical environment, and may include a virtual reality processor configured to generate a virtual robotic surgical environment comprising at least one virtual robotic component, and at least one sensor in a robotic surgical environment. The sensor may be in communication with the virtual reality processor and configured to detect a status of a robotic component corresponding to the virtual robotic component. The virtual reality processor is configured to receive the detected status of the robotic component and modify the virtual robotic component based at least in part on the detected status such that the virtual robotic component mimics the robotic component. For example, a user may monitor an actual robotic surgical procedure in a real operating room via a virtual reality system that interfaces with the real operating room , the user may interact with a virtual reality environment that is reflective of the conditions in the real operating room. Detected positions of robotic components during a surgical procedure may be compared with their expected positions as determined from surgical pre-planning in a virtual environment, such that deviations from the surgical plan may trigger a surgeon to perform adjustments to avoid collisions , change a pose of a robotic arm, . In some variations, the one or more sensors may be configured to detect characteristics or status of a robotic component such as position, orientation, speed, and/or velocity. As an illustrative example, the one or more sensors in the robotic surgical environment may be configured to detect position and/or orientation of a robotic component such as a robotic arm. The position and orientation of the robotic arm may be fed to the virtual reality processor, which moves or otherwise modifies a virtual robotic arm corresponding to the actual robotic arm. As such, a user viewing the virtual robotic surgical environment may visualize the adjusted virtual robotic arm. As another illustrative example, one or more sensors may be configured to detect a collision involving the robotic component in the robotic surgical environment, and the system may provide an alarm notifying the user of the occurrence of the collision. Within the virtual reality system, various user modes enable different kinds of interactions between a user and the virtual robotic surgical environment. For example, one variation of a method for facilitating navigation of a virtual robotic surgical environment includes displaying a first-person perspective view of the virtual robotic surgical environment from a first vantage point within the virtual robotic surgical environment, displaying a first window view of the virtual robotic surgical environment from a second vantage point and displaying a second window view of the virtual robotic surgical environment from a third vantage point. The first and second window views may be displayed in respective regions of the displayed first-person perspective view. Additionally, the method may include, in response to a user input associating the first and second window views, sequentially linking the first and second window views to generate a trajectory between the second and third vantage points. Window views of the virtual robotic surgical environment may be displayed at different scale factors , zoom levels, and may offer views of the virtual environment from any suitable vantage point in the virtual environment, such as inside a virtual patient, overhead the virtual patient, etc. In response to a user input indicating selection of a particular window view, the method may include displaying a new first-person perspective view of the virtual environment from the vantage point of the selected window view. In other words, the window views may, for example, operate as portals facilitating transportation between different vantage points within the virtual environment. As another example of user interaction between a user and the virtual robotic surgical environment, one variation of a method for facilitating visualization of a virtual robotic surgical environment includes displaying a first-person perspective view of the virtual robotic surgical environment from a first vantage point within the virtual robotic surgical environment, receiving a user input indicating placement of a virtual camera at a second vantage point within the virtual robotic surgical environment different from the first vantage point, generating a virtual camera perspective view of the virtual robotic surgical environment from the second vantage point, and displaying the virtual camera perspective view in a region of the displayed first-person perspective view. The camera view may, for example, provide a supplemental view of the virtual environment to the user that enables the user to monitor various aspects of the environment simultaneously while still maintaining primary focus on a main, first-person perspective view. In some variations, the method may further include receiving a user input indicating a selection of a virtual camera type , movie camera configured to be placed outside a virtual patient, an endoscopic camera configured to be placed inside a virtual patient, a 360-degree camera, and displaying a virtual model of the selected virtual camera type at the second vantage point within the virtual robotic surgical environment. Other examples of user interactions with the virtual environment are described herein. In another variation of a virtual reality system, the virtual reality system may simulate a robotic surgical environment in which a user may operate both a robotically-controlled surgical instrument using a handheld controller and a manual laparoscopic surgical instrument , while adjacent a patient table, or over the bed. For example, a virtual reality system for simulating a robotic surgical environment may include a virtual reality controller configured to generate a virtual robotic surgical environment comprising at least one virtual robotic arm and at least one virtual manual laparoscopic tool, a first handheld device communicatively coupled to the virtual reality controller for manipulating the at least one virtual robotic arm in the virtual robotic surgical environment, and a second handheld device comprising a handheld portion and a tool feature representative of at least a portion of a manual laparoscopic tool, wherein the second handheld device is communicatively coupled to the virtual reality controller for manipulating the at least one virtual manual laparoscopic tool in the virtual robotic surgical environment. For example, in some variations, the tool feature may include a tool shaft and a shaft adapter for coupling the tool shaft to the handheld portion of the second handheld device , the shaft adapter may include fasteners. The second handheld device may be a functioning manual laparoscopic tool or a mock-up , facsimile or genericized version of a manual laparoscopic tool, whose movements , in the tool feature may be mapped by the virtual reality controller to correspond to movements of the virtual manual laparoscopic tool. The second handheld device may be modular. For example, the tool feature may be removable from the handheld portion of the second handheld device, thereby enabling the second handheld device to function as a laparoscopic handheld device for controlling a virtual manual laparoscopic tool when the tool feature is attached to the handheld portion, as well as a non-laparoscopic handheld device , for controlling a robotically-controlled tool or robotic arm when the tool feature is detached from the handheld portion. In some variations, the handheld portion of the second handheld device may be substantially similar to the first handheld device. The handheld portion of the second handheld device may include an interactive feature, such as a trigger or button, which actuates a function of the virtual manual laparoscopic tool in response to engagement of the interactive feature by a user. For example, a trigger on the handheld portion of the second handheld device may be mapped to a virtual trigger on the virtual manual laparoscopic tool. As an illustrative example, in a variation in which the virtual manual laparoscopic tool is a virtual manual laparoscopic stapler, a trigger on the handheld portion may be mapped to firing a virtual staple in the virtual environment. Other aspects of the system may further approximate the virtual tool setup in the virtual environment. For example, the virtual reality system may further include a patient simulator , mock patient abdomen including a cannula configured to receive at least a portion of the tool feature of the second handheld device, to thereby further simulate the user feel of a manual laparoscopic tool. Generally, a computer-implemented method for operating a virtual robotic surgical environment may include generating a virtual robotic surgical environment using a client application, where the virtual robotic surgical environment includes at least one virtual robotic component, and passing information between two software applications in order to effect movements of the virtual robotic component. For example, in response to a user input to move the at least one virtual robotic component in the virtual robotic surgical environment, the method may include passing status information regarding the at least one virtual robotic component from the client application to a server application, generating an actuation command based on the user input and the status information using the server application, passing the actuation command from the server application to the client application, and moving the at least one virtual robotic component based on the actuation command. The client application and the server application may be run on a shared processor device, or on separate processor devices. In some variations, passing status information and/or passing the actuation command may include invoking an application programming interface API to support communication between the client and server applications. The API may include one or more definitions of data structures for virtual robotic components and other virtual components in the virtual environment. For example, the API may include a plurality of data structures for a virtual robotic arm, a virtual robotic arm segment , link, a virtual patient table, a virtual cannula, and/or a virtual surgical instrument. As another example, the API may include a data structure for a virtual touchpoint for allowing manipulation of at least one virtual robotic component , virtual robotic arm or other virtual component. For example, the method may include passing status information regarding a virtual robotic arm, such as position and orientation , pose of the virtual robotic arm. The client application may pass such status information to the server application, whereupon the server application may generate an actuation command based on kinematics associated with the virtual robotic arm. As described herein, there are various applications and uses for the virtual reality system. In one variation, the virtual reality system may be used to expedite the R&amp;D cycle during development of a robotic surgical system, such as by allowing simulation of potential design without the time and significant expense of physical prototypes. For example, a method for designing a robotic surgical system may include generating a virtual model of a robotic surgical system, testing the virtual model of the robotic surgical system in a virtual operating room environment, modifying the virtual model of the robotic surgical system based on the testing, and generating a real model of the robotic surgical system based on the modified virtual model. Testing the virtual model may, for example, involve performing a virtual surgical procedure using a virtual robotic arm and a virtual surgical instrument supported by the virtual robotic arm, such as through the client application described herein. During a test, the system may detect one or more collision events involving the virtual robotic arm, which may, for example, trigger a modification to the virtual model , modifying the virtual robotic arm in link length, diameter, in response to the detected collision event. Further testing of the modified virtual model may then be performed, to thereby confirm whether the modification reduced the likelihood of the collision event occurring during the virtual surgical procedure. Accordingly, testing and modifying robotic surgical system designs in a virtual environment may be used to identify issues before testing physical prototypes of the designs. In another variation, the virtual reality system may be used to test a control mode for a robotic surgical component. For example, a method for testing a control mode for a robotic surgical component may include generating a virtual robotic surgical environment, the virtual robotic surgical environment comprising at least one virtual robotic component corresponding to the robotic surgical component, emulating a control mode for the robotic surgical component in the virtual robotic surgical environment, and, in response to a user input to move the at least one virtual robotic component, moving the at least one virtual robotic component in accordance with the emulated control mode. In some variations, moving the virtual robotic component may include passing status information regarding the at least one virtual robotic component from a first application , virtual operating environment application to a second application , kinematics application, generating an actuation command based on the status information and the emulated control mode, passing the actuation command from the second application to the first application, and moving the at least one virtual robotic component in the virtual robotic surgical environment based on the actuation command. For example, the control mode to be tested may be a trajectory following control mode for a robotic arm. In trajectory following, movement of the robotic arm may be programmed then emulated using the virtual reality system. Accordingly, when the system is used to emulate a trajectory following control mode, the actuation command generated by a kinematics application may include generating an actuated command for each of a plurality of virtual joints in the virtual robotic arm. This set of actuated commands may be implemented by a virtual operating environment application to move the virtual robotic arm in the virtual environment, thereby allowing testing for collision, volume or workspace of movement, etc. Other variations and examples of virtual reality systems, their user modes and interactions, and applications and uses of the virtual reality systems, are described in further detail herein.",
                        "1A depicts an example of an operating room arrangement with a robotic surgical system and a surgeon console. 1B is a schematic illustration of one exemplary variation of a robotic arm manipulator, tool driver, and cannula with a surgical tool. 2A is a schematic illustration of one variation of a virtual reality system. 2B is a schematic illustration of an immersive display for displaying an immersive view of a virtual reality environment. 3 is a schematic illustration of components of a virtual reality system. 4A is an exemplary structure for a communication between a virtual reality environment application and a kinematics application for use in a virtual reality system. 4B and 4C are tables summarizing exemplary data structures and fields for an application program interface for communication between the virtual reality environment application and the kinematics application. 5A is a schematic illustration of another variation of a virtual reality system including an exemplary variation of a laparoscopic handheld controller. 5B is a schematic illustration of an immersive display for displaying an immersive view of a virtual reality environment including a virtual manual laparoscopic tool controlled by the laparoscopic handheld controller. 6A is a perspective view of an exemplary variation of a laparoscopic handheld controller. 6B is a schematic illustration of a virtual manual laparoscopic tool overlaid on part of the laparoscopic handheld controller shown in 6A. 6C-6E are a side view, a detailed partial perspective view, and a partial cross-sectional view, respectively, of the laparoscopic handheld controller shown in 6A. 7 is a schematic illustration of another variation of a virtual reality system interfacing with a robotic surgical environment. 8 is a schematic illustration of a displayed menu for selecting one or more user modes of one variation of a virtual reality system. 9A-9C are schematic illustrations of a virtual robotic surgical environment with exemplary portals. 10A and 10B are schematic illustrations of an exemplary virtual robotic surgical environment viewed in a flight mode. 10C is a schematic illustration of a transition region for modifying the view of the exemplary virtual robotic surgical environment in flight mode. 11 is a schematic illustration of a virtual robotic surgical environment viewed from a vantage point providing an exemplary dollhouse view of a virtual operating room. 12 is a schematic illustration of a view of a virtual robotic surgical environment with an exemplary heads-up display for displaying supplemental views. 13 is a schematic illustration of a display provided by one variation of a virtual reality system operating in a virtual command station mode. 14 is a flowchart of an exemplary variation of a method for operating a user mode menu for selection of user modes in a virtual reality system. 15 is a flowchart of an exemplary variation of a method for operating in an environment view rotation mode in a virtual reality system. 16 is a flowchart of an exemplary variation of a method for operating a user mode enabling snap points in a virtual environment."
                    ],
                    "DESCRIPTION": "Examples of various aspects and variations of the invention are described herein and illustrated in the accompanying drawings. The following description is not intended to limit the invention to these embodiments, but rather to enable a person skilled in the art to make and use this invention. Robotic Surgical System OverviewAn exemplary robotic surgical system and surgical environment is illustrated in 1A. As shown in 1A, a robotic surgical system 150 may include one or more robotic arms 160 located at a surgical platform , table, bed, , where end effectors or surgical tools are attached to the distal ends of the robotic arms 160 for executing a surgical procedure. For example, a robotic surgical system 150 may include, as shown in the exemplary schematic of 1B, at least one robotic arm 160 coupled to a surgical platform, and a tool driver 170 generally attached to a distal end of the robotic arm 160. A cannula 180 coupled to the end of the tool driver 170 may receive and guide a surgical instrument 190 , end effector, camera, . Furthermore, the robotic arm 160 may include a plurality of links that are actuated so as to position and orient the tool driver 170, which actuates the surgical instrument 190. The robotic surgical system may further include a control tower 152 , including a power supply, computing equipment, and/or other suitable equipment for supporting functionality of the robotic components. In some variations, a user such as a surgeon or other operator may use a user console 100 to remotely manipulate the robotic arms 160 and/or surgical instruments , tele-operation. The user console 100 may be located in the same procedure room as the robotic system 150, as shown in 1A. In other embodiments, the user console 100 may be located in an adjacent or nearby room, or tele-operated from a remote location in a different building, city, or country. In one example, the user console 100 comprises a seat 110, foot-operated controls 120, one or more handheld user interface devices 122, and at least one user display 130 configured to display, for example, a view of the surgical site inside a patient. For example, as shown in the exemplary user console shown in 1C, a user located in the seat 110 and viewing the user display 130 may manipulate the foot-operated controls 120 and/or handheld user interface devices to remotely control the robotic arms 160 and/or surgical instruments. In some variations, a user may operate the robotic surgical system 150 in an over the bed OTB mode, in which the user is at the patient's side and simultaneously manipulating a robotically-driven tool driver/end effector attached thereto , with a handheld user interface device 122 held in one hand and a manual laparoscopic tool. For example, the user's left hand may be manipulating a handheld user interface device 122 to control a robotic surgical component, while the user's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the user may perform both robotic-assisted MIS and manual laparoscopic techniques on a patient. During an exemplary procedure or surgery, the patient is prepped and draped in a sterile fashion, and anesthesia is achieved. Initial access to the surgical site may be performed manually with the robotic system 150 in a stowed configuration or withdrawn configuration to facilitate access to the surgical site. Once access is completed, initial positioning and/or preparation of the robotic system may be performed. During the surgical procedure, a surgeon or other user in the user console 100 may utilize the foot-operated controls 120 and/or user interface devices 122 to manipulate various end effectors and/or imaging systems to perform the procedure. Manual assistance may also be provided at the procedure table by sterile-gowned personnel, who may perform tasks including but not limited to retracting organs, or performing manual repositioning or tool exchange involving one or more robotic arms 160. Non-sterile personnel may also be present to assist the surgeon at the user console 100. When the procedure or surgery is completed, the robotic system 150 and/or user console 100 may be configured or set in a state to facilitate one or more post-operative procedures, including but not limited to robotic system 150 cleaning and/or sterilization, and/or healthcare record entry or printout, whether electronic or hard copy, such as via the user console 100. In 1A, the robotic arms 160 are shown with a table-mounted system, but in other embodiments, the robotic arms may be mounted in a cart, ceiling or sidewall, or other suitable support surface. The communication between the robotic system 150, the user console 100, and any other displays may be via wired and/or wireless connections. Any wired connections may be optionally built into the floor and/or walls or ceiling. The communication between the user console 100 and the robotic system 150 may be wired and/or wireless, and may be proprietary and/or performed using any of a variety of data communication protocols. In still other variations, the user console 100 does not include an integrated display 130, but may provide a video output that can be connected to output to one or more generic displays, including remote displays accessible via the internet or network. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. In other examples, additional user consoles 100 may be provided, for example to control additional surgical instruments, and/or to take control of one or more surgical instruments at a primary user console. This will permit, for example, a surgeon to take over or illustrate a technique during a surgical procedure with medical students and physicians-in-training, or to assist during complex surgeries requiring multiple surgeons acting simultaneously or in a coordinated manner. Virtual Reality SystemA virtual reality system for providing a virtual robotic surgical environment is described herein. As shown in 2A, a virtual reality system 200 may include a virtual reality processor 210 , a processor in a computer implementing instructions stored in memory for generating a virtual robotic surgical environment, a head-mounted display 220 wearable by a user U, and one or more handheld controllers 230 manipulable by the user U for interacting with the virtual robotic surgical environment. As shown in 2B, the head-mounted display 220 may include an immersive display 222 for displaying the virtual robotic surgical environment to the user U , with a first-person perspective view of the virtual environment. The immersive display may, for example, be a stereoscopic display provided by eyepiece assemblies. In some variations, the virtual reality system 200 may additionally or alternatively include an external display 240 for displaying the virtual robotic surgical environment. The immersive display 222 and the external display 240, if both are present, may be synchronized to show the same or similar content. As described in further detail herein, the virtual reality system and variations thereof, as further described herein may serve as a useful tool with respect to robotic surgery, in applications including but not limited to training, simulation, and/or collaboration among multiple persons. More specific examples of applications and uses of the virtual reality system are described herein. Generally, the virtual reality processor is configured to generate a virtual robotic surgical environment within which a user may navigate around a virtual operating room and interact with virtual objects via the head-mounted display and/or handheld controllers. For example, a virtual robotic surgical system may be integrated into a virtual operating room, with one or more virtual robotic components having three-dimensional meshes and selected characteristics , dimensions and kinematic constraints of virtual robotic arms and/or virtual surgical tools, number and arrangement thereof, . Other virtual objects, such as a virtual control towers or other virtual equipment representing equipment supporting the robotic surgical system, a virtual patient, a virtual table or other surface for the patient, virtual medical staff, a virtual user console, etc. , may also be integrated into the virtual reality operating room. In some variations, the head-mounted display 220 and/or the handheld controllers 230 may be modified versions of those included in any suitable virtual reality hardware system that is commercially available for applications including virtual and augmented reality environments , for gaming and/or military purposes and are familiar to one of ordinary skill in the art. For example, the head-mounted display 220 and/or the handheld controllers 230 may be modified to enable interaction by a user with a virtual robotic surgical environment , a handheld controller 230 may be modified as described below to operate as a laparoscopic handheld controller. The handheld controller may include, for example, a carried device , wand, remote device, and/or a garment worn on the user's hand , gloves, rings, wristbands, and including sensors and/or configured to cooperate with external sensors to thereby provide tracking of the user's hands, individual fingers, wrists, etc. Other suitable controllers may additionally or alternatively be used , sleeves configured to provide tracking of the user's arms. Generally, a user U may don the head-mounted display 220 and carry or wear at least one handheld controller 230 while he or she moves around a physical workspace, such as a training room. While wearing the head-mounted display 220, the user may view an immersive first-person perspective view of the virtual robotic surgical environment generated by the virtual reality processor 210 and displayed onto the immersive display 222. As shown in 2B, the view displayed onto the immersive display 222 may include one or more graphical representations 230 of the handheld controllers , virtual models of the handheld controllers, virtual models of human hands in place of handheld controllers or holding handheld controllers, . A similar first-person perspective view may be displayed onto an external display 240 , for assistants, mentors, or other suitable persons to view. As the user moves and navigates within the workspace, the virtual reality processor 210 may change the view of the virtual robotic surgical environment displayed on the immersive display 222 based at least in part on the location and orientation of the head-mounted display and hence the user's location and orientation, thereby allowing the user to feel as if he or she is exploring and moving within the virtual robotic surgical environment. Additionally, the user may further interact with the virtual robotic surgical environment by moving and/or manipulating the handheld controllers 230. For example, the handheld controllers 230 may include one or more buttons, triggers, touch-sensitive features, scroll wheels, switches, and/or other suitable interactive features that the user may manipulate to interact with the virtual environment. As the user moves the handheld controllers 230, the virtual reality processor 210 may move the graphical representations 230 of the handheld controllers or a cursor or other representative icon within the virtual robotic surgical environment. Furthermore, engaging one or more interactive features of the handheld controllers 230 may enable the user to manipulate aspects of the virtual environment. For example, the user may move a handheld controller 230 until the graphical representation 230 of the handheld controller is in proximity to a virtual touchpoint , selectable location on a virtual robotic arm in the environment, engage a trigger or other interactive feature on the handheld controller 230 to select the virtual touchpoint, then move the handheld controller 230 while engaging the trigger to drag or otherwise manipulate the virtual robotic arm via the virtual touchpoint. Other examples of user interactions with the virtual robotic surgical environment are described in further detail below. In some variations, the virtual reality system may engage other senses of the user. For example, the virtual reality system may include one or more audio devices , headphones for the user, speakers, for relaying audio feedback to the user. As another example, the virtual reality system may provide tactile feedback, such as vibration, in one or more of the handheld controllers 230, the head-mounted display 220, or other haptic devices contacting the user , gloves, wristbands, . Virtual Reality ProcessorThe virtual reality processor 210 may be configured to generate a virtual robotic surgical environment within which a user may navigate around a virtual operating room and interact with virtual objects. A general schematic illustrating an exemplary interaction between the virtual reality processor and at least some components of the virtual reality system is shown in 3. In some variations, the virtual reality processor 210 may be in communication with hardware components such as the head-mounted display 220, and/or handheld controllers 230. For example, the virtual reality processor 210 may receive input from sensors in the head-mounted display 220 to determine location and orientation of the user within the physical workspace, which may be used to generate a suitable, corresponding first-person perspective view of the virtual environment to display in the head-mounted display 220 to the user. As another example, the virtual reality control 210 may receive input from sensors in the handheld controllers 230 to determine location and orientation of the handheld controllers 230, which may be used to generate suitable graphical representations of the handheld controllers 230 to display in the head-mounted display 220 to the user, as well as translate user input for interacting with the virtual environment into corresponding modifications of the virtual robotic surgical environment. The virtual reality processor 210 may be coupled to an external display 240 , a monitor screen that is visible to the user in a non-immersive manner and/or to other persons such as assistants or mentors who may wish to view the user's interactions with the virtual environment. In some variations, the virtual reality processor 210 or multiple processor machines may be configured to execute one or more software applications for generating the virtual robotic surgical environment. For example, as shown in 4, the virtual reality processor 210 may utilize at least two software applications, including a virtual operating environment application 410 and a kinematics application 420. The virtual operating environment application and the kinematics application may communicate via a client-server model. For example, the virtual operating environment application may operate as a client, while the kinematics application may operate as a server. The virtual operation environment application 410 and the kinematics application 420 may be executed on the same processing machine, or on separate processing machines coupled via a computer network , the client or the server may be a remote device, or the machines may be on a local computer network. Additionally, it should be understood that in other variations, the virtual operating environment application 410 and/or the kinematics application 420 may interface with other software components. In some variations, the virtual operating environment application 410 and the kinematics application 520 may invoke one or more application program interfaces APIs, which define the manner in which the applications communicate with one another. The virtual operating environment 410 may allow for a description or definition of the virtual operating room environment , the operating room, operating table, control tower or other components, user console, robotic arms, table adapter links coupling robotic arms to the operating table, . At least some descriptions of the virtual operating room environment may be saved , in a model virtual reality component database 202 and provided to the processor as configuration files. For example, in some variations, as shown in 3, the virtual reality processor such as through the virtual operating environment application 410 described above may be in communication with a model virtual reality component database 202 , stored on a server, local or remote hard drive, or other suitable memory. The model virtual reality component database 202 may store one or more configuration files describing virtual components of the virtual robotic surgical environment. For example, the database 202 may store files describing different kinds of operating rooms , varying in room shape or room dimensions, operating tables or other surfaces on which a patient lies , varying in size, height, surfaces, material construction, , control towers , varying in size and shape, user console , varying in user seat design, robotic arms , design of arm links and joints, number and arrangement thereof, number and location of virtual touchpoints on the arm, , table adapter links coupling robotic arms to an operating table , design of table adapter links and joints, number and arrangement thereof, , patient types , varying in sex, age, weight, height, girth, and/or medical personnel , generic graphical representations of people, graphical representations of actual medical staff, . As one specific example, a configuration file in Unified Robot Description Format URDF may store a configuration of a particular robotic arm, including definitions or values for fields such as number of arm links, number of arm joints connecting the arm links, length of each arm link, diameter or girth of each arm link, mass of each arm link, type of arm joint , roll, pitch, yaw , etc. Additionally, kinematic constraints may be loaded as a wrapper over a virtual robotic component , arm to further define the kinematic behavior of the virtual robotic component. In other variations, the virtual reality processor 210 may receive any suitable descriptions of virtual components to load and generate in the virtual robotic surgical environment. Accordingly, the virtual reality processor 210 may receive and utilize different combinations of configuration files and/or other descriptions of virtual components to generate particular virtual robotic surgical environments. In some variations, as shown in 3, the virtual reality processor 210 may additionally or alternatively be in communication with a patient records database 204, which may store patient-specific information. Such patient-specific information may include, for example, patient imaging data , X-ray, MRI, CT, ultrasound, , medical histories, and/or patient metrics , age, weight, height, , though other suitable patient-specific information may additionally or alternatively be stored in the patient records database 204. When generating the virtual robotic surgical environment, the virtual reality processor 210 may receive patient-specific information from the patient records database 204 and integrate at least some of the received information into the virtual reality environment. For example, a realistic representation of the patient's body or other tissue may be generated and incorporated into the virtual reality environment , a 3D model generated from a combined stack of 2D images, such as MRI images, which may be useful, for example, for determining desirable arrangement of robotic arms around the patient, optimal port placement, etc. specific to a particular patient, as further described herein. As another example, patient imaging data may be overlaid over a portion of the user's field of view of the virtual environment , overlaying an ultrasound image of a patient's tissue over the virtual patient's tissue. In some variations, the virtual reality processor 210 may embed one or more kinematics algorithms via the kinematics application 420 to at least partially describe behavior of one or more components of the virtual robotic system in the virtual robotic surgical environment. For example, one or more algorithms may define how a virtual robotic arm responds to user interactions , moving the virtual robotic arm by selection and manipulation of a touchpoint on the virtual robotic arm, or how a virtual robotic arm operates in a selected control mode. Other kinematics algorithms, such as those defining operation of a virtual tool driver, a virtual patient table, or other virtual components, may additionally or alternatively be embedded in the virtual environment. By embedding in the virtual environment one or more kinematics algorithms that accurately describe behavior of an actual real robotic surgical system, the virtual reality processor 210 may permit the virtual robotic surgical system to function accurately or realistically compared to a physical implementation of a corresponding real robotic surgical system. For example, the virtual reality processor 210 may embed at least one control algorithm that represents or corresponds to one or more control modes defining movement of a robotic component , arm in an actual robotic surgical system. For example, the kinematics application 420 may allow for a description or definition of one or more virtual control modes, such as for the virtual robotic arms or other suitable virtual components in the virtual environment. Generally, for example, a control mode for a virtual robotic arm may correspond to a function block that enables the virtual robotic arm to perform or carry out a particular task. For example, as shown in 4, a control system 430 may include multiple virtual control modes 432, 434, 436, etc. governing actuation of at least one joint in the virtual robotic arm. The virtual control modes 432, 434, 436, etc. may include at least one primitive mode which governs the underlying behavior for actuation of at least one joint and/or at least one user mode which governs higher level, task-specific behavior and may utilize one or more primitive modes. In some variations, a user may activate a virtual touchpoint surface of a virtual robotic arm or other virtual object, thereby triggering a particular control mode , via a state machine or other controller. In some variations, a user may directly select a particular control mode through, for example, a menu displayed in the first-person perspective view of the virtual environment. Examples of primitive virtual control modes include, but are not limited to, a joint command mode which allows a user to directly actuate a single virtual joint individually, and/or multiple virtual joints collectively, a gravity compensation mode in which the virtual robotic arm holds itself in a particular pose, with particular position and orientation of the links and joints, without drifting downward due to simulated gravity, and trajectory following mode in which the virtual robotic arm may move to follow a sequence of one or more Cartesian or other trajectory commands. Examples of user modes that incorporate one or more primitive control modes include, but are not limited to, an idling mode in which the virtual robotic arm may rest in a current or default pose awaiting further commands, a setup mode in which the virtual robotic arm may transition to a default setup pose or a predetermined template pose for a particular type of surgical procedure, and a docking mode in which the robotic arm facilitates the process in which the user attaches the robotic arm to a part, such as with gravity compensation, . Generally, the virtual operating environment application 410 and the kinematics application 420 may communicate with each other via a predefined communication protocol, such as an application program interface APIs that organizes information , status or other characteristics of virtual objects and other aspects of the virtual environment. For example, the API may include data structures that specify how to communicate information about virtual objects such as a virtual robotic arm in whole and/or on a segment-by-segment basis a virtual table, a virtual table adapter connecting a virtual arm to the virtual table, a virtual cannula, a virtual tool, a virtual touchpoint for facilitating user interaction with the virtual environment, user input system, handheld controller devices, etc. Furthermore, the API may include one or more data structures that specify how to communicate information about events in the virtual environment , a collision event between two virtual entities or other aspects relating to the virtual environment , reference frame for displaying the virtual environment, control system framework, . Exemplary data structures and exemplary fields for containing their information are listed and described in 4B and 4C, though it should be understood that other variations of the API may include any suitable types, names, and numbers of data structures and exemplary field structures. In some variations, as generally illustrated schematically in 4A, the virtual operating environment application 410 passes status information to the kinematics application 420, and the kinematics application 420 passes commands to the virtual operating environment application 410 via the API, where the commands are generated based on the status information and subsequently used by the virtual reality processor 210 to generate changes in the virtual robotic surgical environment. For example, a method for embedding one or more kinematics algorithms in a virtual robotic surgical environment for control of a virtual robotic arm may include passing status information regarding at least a portion of the virtual robotic arm from the virtual operating environment application 410 to the kinematics application 420, algorithmically determining an actuation command to actuate at least one virtual joint of the virtual robotic arm, and passing the actuation command from the kinematics application 420 to virtual operating environment application 410. The virtual reality processor 210 may subsequently move the virtual robotic arm in accordance with the actuation command. As an illustrative example for controlling a virtual robotic arm, a gravity compensation control mode for a virtual robotic arm may be invoked, thereby requiring one or more virtual joint actuation commands in order to counteract simulated gravity forces on the virtual joints in the virtual robotic arm. The virtual operating environment application 410 may pass to the kinematics application 420 relevant status information regarding the virtual robotic arm , position of at least a portion of the virtual robotic arm, position of the virtual patient table to which the virtual robotic arm is mounted, position of a virtual touchpoint that the user may have manipulated to move the virtual robotic arm, joint angles between adjacent virtual arm links and status information , direction of simulated gravitational force on the virtual robotic arm. Based on the received status information from the virtual operating environment application 410 and known kinematic and/or dynamic properties of the virtual robotic arm and/or virtual tool drive attached to the virtual robotic arm , known from a configuration file, , the control system 430 may algorithmically determine what actuated force at each virtual joint is required to compensate for the simulated gravitational force acting on that virtual joint. For example, the control system 430 may utilize a forward kinematic algorithm, an inverse algorithm, or any suitable algorithm. Once the actuated force command for each relevant virtual joint of the virtual robotic arm is determined, the kinematics application 420 may send the force commands to the virtual operating environment application 410. The virtual reality processor subsequently may actuate the virtual joints of the virtual robotic arm in accordance with the force commands, thereby causing the virtual robotic arm to be visualized as maintaining its current pose despite the simulated gravitational force in the virtual environment , instead of falling down or collapsing under simulated gravitational force. Another example for controlling a virtual robotic arm is trajectory following for a robotic arm. In trajectory following, movement of the robotic arm may be programmed then emulated using the virtual reality system. Accordingly, when the system is used to emulate a trajectory planning control mode, the actuation command generated by a kinematics application may include generating an actuated command for each of a plurality of virtual joints in the virtual robotic arm. This set of actuated commands may be implemented by a virtual operating environment application to move the virtual robotic arm in the virtual environment, thereby allowing testing for collision, volume or workspace of movement, etc. Other virtual control algorithms for the virtual robotic arm and/or other virtual components , virtual table adapter links coupling the virtual robotic arm to a virtual operating table may be implemented via similar communication between the virtual operating environment application 410 and the kinematics application 420. Although the virtual reality processor 210 is generally referred to herein as a single processor, it should be understood that in some variations, multiple processors may be used to perform the processors described herein. The one or more processors may include, for example, a processor of a general purpose computer, a special purpose computer or controller, or other programmable data processing apparatus or component, etc. Generally, the one or more processors may be configured to execute instructions stored on any suitable computer readable media. The computer readable media may include, for example, magnetic media, optical media, magneto-optical media and hardware devices that are specially configured to store and execute program code, such as application-specific integrated circuits ASICs, programmable logic devices PLDs, ROM and RAM devices, flash memory, EEPROMs, optical devices , CD or DVD, hard drives, floppy drives, or any suitable device. Examples of computer program code include machine code, such as produced by a compiler, and files containing higher-level code that are executed by a computer using an interpreter. For example, one variation may be implemented using C++, JAVA, or other suitable object-oriented programming language and development tools. As another example, another variation may be implemented in hardwired circuitry in place of, or in combination with, machine-executable software instructions. Head-Mounted Display and Handheld ControllersAs shown in 2A, a user U may wear a head-mounted display 220 and/or hold one or more handheld controllers 230. The head-mounted display 220 and handheld controllers 230 may generally enable a user to navigate and/or interact with the virtual robotic surgical environment generated by the virtual reality processor 210. The head-mounted display 220 and/or handheld controllers 230 may communicate signals to the virtual reality processor 210 via a wired or wireless connection. In some variations, the head-mounted display 220 and/or the handheld controllers 230 may be modified versions of those included in any suitable virtual reality hardware system that is commercially available for applications including virtual and augmented reality environments. For example, the head-mounted display 220 and/or the handheld controllers 230 may be modified to enable interaction by a user with a virtual robotic surgical environment , a handheld controller 230 may be modified as described below to operate as a laparoscopic handheld controller. In some variations, the virtual reality system may further include one or more tracking emitters 212 that emit infrared light into a workspace for the user The tracking emitters 212 may, for example, be mounted on a wall, ceiling, fixture, or other suitable mounting surface. Sensors may be coupled to outward-facing surfaces of the head-mounted display 220 and/or handheld controllers 230 for detecting the emitted infrared light. Based on the location of any sensors that detect the emitted light and when such sensors detect the emitted light after the light is emitted, the virtual reality processor 220 may be configured to determine , through triangulation the location and orientation of the head-mounted display 220 and/or handheld controllers 230 within the workspace. In other variations, other suitable means , other sensor technologies such as accelerometers or gyroscopes, other sensor arrangements, may be used to determine location and orientation of the head-mounted display 220 and handheld controllers 230. In some variations, the head-mounted display 220 may include straps , with buckles, elastic, snaps, that facilitate mounting of the display 220 to the user's head. For example, the head-mounted display 220 may be structured similar to goggles, a headband or headset, a cap, etc. The head-mounted display 220 may include two eyepiece assemblies providing a stereoscopic immersive display, though alternatively may include any suitable display. The handheld controllers 230 may include interactive features that the user may manipulate to interact with the virtual robotic surgical environment. For example, the handheld controllers 230 may include one or more buttons, triggers, touch-sensitive features, scroll wheels, switches, and/or other suitable interactive features. Additionally, the handheld controllers 230 may have any of various form factors, such as a wand, pinchers, generally round shapes , ball or egg-shaped, etc. In some variations, the graphical representations 230 displayed on the head-mounted display 220 and/or external display 240 may generally mimic the form factor of the actual, real handheld controllers 230. In some variations, the handheld controller may include a carried device , wand, remote device, and/or a garment worn on the user's hand , gloves, rings, wristbands, and including sensors and/or configured to cooperate with external sensors to thereby provide tracking of the user's hands, individual fingers, wrists, etc. Other suitable controllers may additionally or alternatively be used , sleeves configured to provide tracking of the user's arms. Laparoscopic Handheld ControllerIn some variations, as shown in the schematic of 5A, the handheld controller 230 may further include at least one tool feature 232 that is representative of at least a portion of a manual laparoscopic tool, thereby forming a laparoscopic handheld controller 234 that may be used to control a virtual manual laparoscopic tool. Generally, for example, the tool feature 232 may function to adapt the handheld controller 230 into a controller substantially similar in form , user feel and touch to a manual laparoscopic tool. The laparoscopic handheld controller 234 may be communicatively coupled to the virtual reality processor 210 for manipulating a virtual manual laparoscopic tool in the virtual robotic surgical environment, and may help enable the user to feel as if he or she is using an actual manual laparoscopic tool while interacting with the virtual robotic surgical environment. In some variations the laparoscopic handheld device may be a mock-up , facsimile or genericized version of a manual laparoscopic tool, while in other variations the laparoscopic handheld device may be a functioning manual laparoscopic tool. Movements of at least a portion of the laparoscopic handheld controller may be mapped by the virtual reality controller to correspond to movements of the virtual manual laparoscopic tool. Thus, in some variations, the virtual reality system may simulate use of a manual laparoscopic tool for manual MIAs shown in 5A, the laparoscopic handheld controller 234 may be used with a mock patient setup to further simulate the feel of a virtual manual laparoscopic tool. For example, the laparoscopic handheld controller 234 may be inserted into a cannula 250 , an actual cannula used in MIS procedures to provide realistic feel of a manual tool within a cannula, or a suitable representation thereof, such as a tube with a lumen for receiving a tool shaft portion of the laparoscopic handheld controller 234. The cannula 250 may be placed in a mock patient abdomen 260, such as a foam body with one or more insertion sites or ports for receiving the cannula 250. Alternatively, other suitable mock patient setups may be used, such as a cavity providing resistance , with fluid, with similar feel as an actual patient abdomen. Additionally, as shown in 5B, the virtual reality processor may generate a virtual robotic surgical environment including a virtual manual laparoscopic tool 236 and/or a virtual cannula 250 relative to a virtual patient , the graphical representation 250 of the cannula depicted as inserted in the virtual patient. As such, the virtual environment with the virtual manual laparoscopic tool 236 and virtual cannula 250 may be displayed on the immersive display provided by the head-mounted display 220, and/or the external display 240. A calibration procedure may be performed to map the laparoscopic handheld controller 234 to the virtual manual laparoscopic tool 236 within the virtual environment. Accordingly, as the user moves and manipulates the laparoscopic handheld controller 234, the combination of the at least one tool feature 234 and the mock patient setup may allow the user to tactilely feel as if he or she is using a manual laparoscopic tool in the virtual robotic surgical environment. Likewise, as the user moves and manipulates the laparoscopic handheld controller 234, the corresponding movements of the virtual manual laparoscopic tool 236 may allow the user to visualize the simulation that he or she is using a manual laparoscopic tool in the virtual robotic surgical environment. In some variations, the calibration procedure for the laparoscopic handheld controller generally maps the laparoscopic handheld controller 234 to the virtual manual laparoscopic tool 236. For example, generally, the calibration procedure may zero its position relative to a reference point within the virtual environment. In an exemplary calibration procedure, the user may insert the laparoscopic handheld controller through a cannula 250 into a mock patient abdomen 260, which may be placed on a table in front of the user , at a height that is representative of the height of a real operating patient table. The user may continue inserting the laparoscopic handheld controller into the mock patient abdomen 260 into a suitable depth representative of depth achieved during a real laparoscopic procedure. Once the laparoscopic handheld controller is suitably placed in the mock patient abdomen 260, the user may provide an input , squeeze a trigger or push a button on the laparoscopic handheld controller, by voice command, to confirm and orient the virtual patient to the location and height of the mock patient abdomen 260. Additionally, other aspects of the virtual environment may be calibrated to align with the real, tangible aspects of the system, such as by depicting virtual components adjustably movable to target locations and allowing user input to confirm new alignment of the virtual component with target locations , by squeezing a trigger or pushing a button on the laparoscopic handheld controller, voice command, . Orientation of virtual components , rotational orientation of a shaft may be adjusted with a touchpad, trackball, or other suitable input on the laparoscopic handheld controller or other device. For example, the virtual operating room may be aligned with the real room in which the user is standing, a distal end of the virtual cannula or trocar may be aligned with the real entry location in the mock patient abdomen, etc. Furthermore, in some variations, a virtual end effector , endocutter, clipper may be located and oriented via the laparoscopic handheld controller to a new target location and orientation in similar manners. In some variations, as shown in 5B, the system may include both a handheld controller 230 and a laparoscopic handheld controller 234. Accordingly, the virtual reality processor may generate a virtual environment including both a graphical representation 230 of a handheld controller 230 with no laparoscopic attachment and a virtual manual laparoscopic tool 236 as described above. The handheld controller 230 may be communicatively coupled to the virtual reality processor 210 for manipulating at least one virtual robotic arm, and the laparoscopic handheld controller 234 may be communicatively coupled to the virtual reality processor 210 for manipulating a virtual manual laparoscopic tool 236. Thus, in some variations, the virtual reality system may simulate an over the bed mode of using a robotic surgical system, in which an operator is at the patient's side and manipulating both a robotic arm , with one hand providing robotic-assisted MIS, and a manual laparoscopic tool providing manual MIThe tool feature 232 may include any suitable feature generally approximating or representing a portion of a manual laparoscopic tool. For example, the tool feature 232 may generally approximate a laparoscopic tool shaft , include an elongated member extending from a handheld portion of the controller. As another example, the tool feature 232 may include a trigger, button, or other laparoscopic interactive feature similar to that present on a manual laparoscopic tool that engages an interactive feature on the handheld controller 230 but provides a realistic form factor mimicking the feel of a manual laparoscopic tool , the tool feature 232 may include a larger trigger having a realistic form factor that is overlaid with and engages a generic interactive feature on the handheld controller 230. As yet another example, the tool feature 232 may include selected materials and/or masses to create a laparoscopic handheld controller 234 having a weight distribution that is similar to a particular kind of manual laparoscopic tool. In some variations, the tool feature 232 may include plastic , polycarbonate, acrylonitrile butadiene styrene ABS, nylon, that is injection molded, machined, 3D-printed, or other suitable material shaped in any suitable fashion. In other variations, the tool feature 232 may include metal or other suitable material that is machined, casted, etc. In some variations, the tool feature 236 may be an adapter or other attachment that is formed separately from the handheld controller 230 and coupled to the handheld controller 230 via fasteners , screws, magnets, , interlocking features , threads or snap fit features such as tabs and slots, , epoxy, welding , ultrasonic welding, etc. The tool feature 236 may be reversibly coupled to the handheld controller 230. For example, the tool feature 236 may be selectively attached to the handheld controller 230 in order to adapt a handheld controller 230 when a laparoscopic-style handheld controller 230 is desired, while the tool feature 236 may be selectively detached from the handheld controller 230 when a laparoscopic-style handheld controller 230 is not desired. Alternatively, the tool feature 236 may be permanently coupled to the handheld portion 234, such as during manufacturing. Furthermore, in some variations, the handheld portion 234 and the tool feature 236 may be integrally formed , injection molded together as a single piece. One exemplary variation of a laparoscopic handheld controller is shown in 6A. The laparoscopic handheld controller 600 may include a handheld portion 610 , similar to handheld controller 230 described above, a tool shaft 630, and a shaft adapter 620 for coupling the tool shaft 630 to the handheld portion 610. As shown in 6B, the laparoscopic handheld controller 600 may generally be used to control a virtual manual laparoscopic stapler tool 600, though the laparoscopic handheld controller 600 may be used to control other kinds of virtual manual laparoscopic tools , scissors, dissectors, graspers, needleholders, probes, forceps, biopsy tools, etc. For example, the handheld portion 610 may be associated with a virtual handle 610 of the virtual manual laparoscopic stapler tool 600 having a stapler end effector 640, such that the user's manipulation of the handheld portion 610 is mapped to manipulation of the virtual handle 610. Similarly, the tool shaft 630 may correspond to a virtual tool shaft 630 of the virtual manual laparoscopic stapler tool 600. The tool shaft 630 and the virtual tool shaft 630 may be inserted into a cannula and a virtual cannula, respectively, such that movement of the tool shaft 630 relative to the cannula is mapped to movement of the virtual tool shaft 630 within the virtual cannula in the virtual robotic surgical environment. The handheld portion 610 may include one or more interactive features, such as finger trigger 612 and/or button 614, which may receive user input from the user's fingers, palms, etc. and be communicatively coupled to a virtual reality processor. In this exemplary embodiment, the finger trigger 612 may be mapped to a virtual trigger 612 on the virtual manual laparoscopic stapler tool 600. The virtual trigger 612 may be visualized as actuating the virtual end effector 640 , causing the virtual members of the virtual end effector 640 to close and fire staplers for stapling virtual tissue in the virtual environment. Accordingly, when the user actuates the finger trigger 612 on the laparoscopic handheld controller, the signal from finger trigger 612 may be communicated to the virtual reality processor, which modifies the virtual manual laparoscopic stapler tool 600 to interact within the virtual environment in simulation of an actual manual laparoscopic stapler tool. In another variation, a trigger attachment may physically resemble , in shape and form the virtual trigger 612 on the virtual manual laparoscopic stapler tool 600 and may be coupled to the finger trigger 612, which may enable the laparoscopic handheld controller 600 to even more closely mimic the user feel of the virtual manual laparoscopic stapler tool 600. As shown in 6C-6E, the shaft adapter 620 may generally function to couple the tool shaft 630 to the handheld portion 610, which may, for example, adapt a handheld controller similar to handheld controller 210 described above into a laparoscopic handheld controller. The shaft adapter 620 may generally include a first end for coupling to the handheld portion 610 and a second end for coupling to the tool shaft 630. As shown best in 6E, the first end of the shaft adapter 620 may include a proximal portion 620a and distal portion 620b configured to clamp on a feature of the handheld portion 610. For example, the handheld portion 610 may include generally ring-like portion defining a central space 614 which receives the proximal portion 620a and the distal portion 620b. The proximal portion 620a and the distal portion 620b may clamp on either side of the ring-like portion at its inner diameter, and be fixed to the ring-like portion via fasteners not shown passing through fastener holes 622, thereby securing the shaft adapter 620 to the handheld portion 610. Additionally or alternatively, the shaft adapter 620 may couple to the handheld portion 610 in any suitable fashion, such as an interference fit, epoxy, interlocking features , between the proximal portion 620a and the distal portion 620b, etc. As also shown in 6E, the second end of the shaft adapter 620 may include a recess for receiving the tool shaft 620. For example, the recess may be generally cylindrical for receiving a generally cylindrical end of a tool shaft portion 630, such as through a press fit, friction fit, or other interference fit. Additionally or alternatively, the tool shaft 620 may be coupled to the shaft adapter 620 with fasteners , screws, bolts, epoxy, ultrasonic welding, . The tool shaft 630 may be any suitable size , length, diameter for mimicking or representing a manual laparoscopic tool. In some variations, the shaft adapter 620 may be selectively removable from the handheld portion 610 for permitting selective use of the handheld portion 610 both as a standalone handheld controller , handheld controller 210 and as a laparoscopic handheld controller 600. Additionally or alternatively, the tool shaft 630 may be selectively removable from the shaft adapter 620 , although the shaft adapter 620 may be intentionally fixed to the handheld portion 610, the tool shaft 620 may be selectively removable from the shaft adapter 620 to convert the laparoscopic handheld control 600 to a standalone handheld controller 210. Generally, the tool feature of the laparoscopic handheld controller 600, such as the shaft adapter 620 and the tool shaft 630, may be made of a rigid or semi-rigid plastic or metal, and may be formed through any suitable manufacturing process, such as 3D printing, injection molding, milling, turning, etc. The tool feature may include multiple kinds of materials, and/or weights or other masses to further simulate the user feel of a particular manual laparoscopic tool. System VariationsOne or more aspects of the virtual reality system described above may be incorporated into other variations of systems. For example, in some variations, a virtual reality system for providing a virtual robotic surgical environment may interface with one or more features of a real robotic surgical environment. For example, as shown in 3, a system 700 may include one or more processors , a virtual reality processor 210 configured to generate a virtual robotic surgical environment, and one or more sensors 750 in a robotic surgical environment, where the one or more sensors 750 is in communication with the one or more processors. Sensor information from the robotic surgical environment may be configured to detect status of an aspect of the robotic surgical environment, such for mimicking or replicating features of the robotic surgical environment in the virtual robotic surgical environment. For example, a user may monitor an actual robotic surgical procedure in a real operating room via a virtual reality system that interfaces with the real operating room , the user may interact with a virtual reality environment that is reflective of the conditions in the real operating room. In some variations, one or more sensors 750 may be configured to detect status of at least one robotic component , a component of a robotic surgical system, such as a robotic arm, a tool driver coupled to a robotic arm, a patient operating table to which a robotic arm is attached, a control tower, or other component of a robotic surgical operating room. Such status may indicate, for example, position, orientation, speed, velocity, operative state , on or off, power level, mode, or any other suitable status of the component. For example, one or more accelerometers may be coupled to a robotic arm link and be configured to provide information about the robotic arm link's position, orientation, and/or velocity of movement, etc. Multiple accelerometers on multiple robotic arms may be configured to provide information regarding impending and/or present collisions between robotic arms, between different links of a robotic arm, or between a robotic arm and a nearby obstacle having a known position. As another example, one or more proximity sensors , infrared sensor, capacitive sensor may be coupled to a portion of a robotic arm or other components of the robotic surgical system or surgical environment. Such proximity sensors may, for example, be configured to provide information regarding impending collisions between objects. Additionally or alternatively, contact or touch sensors may be coupled to a portion of a robotic arm or other components of the robotic surgical environment, and may be configured to provide information regarding a present collision between objects. In another example, one or more components of the robotic surgical system or surgical environment may include markers , infrared markers to facilitate optical tracking of the position, orientation, and/or velocity of various components, such as with overhead sensors monitoring the markers in the surgical environment. Similarly, the surgical environment may additionally or alternatively include cameras for scanning and/or modeling the surgical environment and its contents. Such optical tracking sensors and/or cameras may be configured to provide information regarding impending and/or present collisions between objects. As another example, one or more sensors 750 may be configured to detect a status of a patient, a surgeon, or other surgical staff. Such status may indicate, for example, position, orientation, speed, velocity, and/or biological metrics such as heart rate, blood pressure, temperature, etc. For example, a heart rate monitor, a blood pressure monitor, thermometer, and/or oxygenation sensor, etc. may be coupled to the patient and enable a user to keep track of the patient's condition. Generally, in these variations, a virtual reality processor 210 may generate a virtual robotic surgical environment similar to that described elsewhere herein. Additionally, upon receiving status information from the one or more sensors 750, the virtual reality processor 210 or other processor in the system may incorporate the detected status in any one or more suitable ways. For example, in one variation, the virtual reality processor 210 may be configured to generate a virtual reality replica or near-replica of a robotic surgical environment and/or a robotic surgical procedure performed therein. For example, the one or more sensors 750 in the robotic surgical environment may be configured to detect a status of a robotic component corresponding to a virtual robotic component in the virtual robotic surgical environment , the virtual robotic component may be substantially representative of the robotic component in visual form and/or function. In this variation, the virtual reality processor 210 may be configured to receive the detected status of the robotic component, and then modify the virtual robotic component based at least in part on the detected status such that the virtual robotic component mimics the robotic component. For example, if a surgeon moves a robotic arm during a robotic surgical procedure to a particular pose, then a virtual robotic arm in the virtual environment may move correspondingly. As another example, the virtual reality processor 210 may receive status information indicating an alarm event, such as an impending or present collision between objects, or poor patient health condition. Upon receiving such information, the virtual reality processor 210 may provide a warning or alarm to the user of the occurrence of the event, such as by displaying a visual alert , text, icon indicating collision, a view within the virtual environment depicting the collision, , audio alert, etc. As yet another example, the one or more sensors in the robotic surgical environment may be used to compare an actual surgical procedure occurring in the non-virtual robotic surgical environment with a planned surgical procedure as planned in a virtual robotic surgical environment. For example, an expected position of at least one robotic component , robotic arm may be determined during surgical preplanning, as visualized as a corresponding virtual robotic component in a virtual robotic surgical environment. During an actual surgical procedure, one or more sensors may provide information about a measured position of the actual robotic component. Any differences between the expected and measured position of the robotic component may indicate deviations from a surgical plan that was constructed in the virtual reality environment. Since such deviations may eventually result in undesired consequences , unintended collisions between robotic arms, , identification of deviations may allow the user to adjust the surgical plan accordingly , reconfigure approach to a surgical site, change surgical instruments, . User ModesGenerally, the virtual reality system may include one or more user modes enabling a user to interact with the virtual robotic surgical environment by moving and/or manipulating the handheld controllers 230. Such interactions may include, for example, moving virtual objects , virtual robotic arm, virtual tool, in the virtual environment, adding camera viewpoints to view the virtual environment simultaneously from multiple vantage points, navigate within the virtual environment without requiring the user to move the head-mounted display 220 , by walking, etc. as further described below. In some variations, the virtual reality system may include a plurality of user modes, where each user mode is associated with a respective subset of user interactions. As shown in 8, at least some of the user modes may be shown on a display , head-mounted display 220 for user selection. For example, at least some of the user modes may correspond to selectable user mode icons 812 displayed in a user mode menu 810. The user mode menu 810 may be overlaid on the display of the virtual robotic surgical environment such that a graphical representation 230 of the handheld controller or user hand, other suitable representative icon, may be maneuvered by the user to select a user mode icon, thereby activating the user mode corresponding to the selected user mode icon. As shown in 8, the user mode icons 812 may be generally arranged in a palette or circle, but may be alternatively arranged in a grid or other suitable arrangement. In some variations, a selected subset of possible user modes may be presented in the menu 810 based on, for example, user preferences , associated with a set of user login information, preferences of users similar to the current user, type of surgical procedure, etc. 14 illustrates a method of operation 1400 of an exemplary variation of a user mode menu providing selection of one or more user mode icons. To activate the user menu, the user may activate a user input method associated with the menu. For example, an input method may be activated by a user engaging with a handheld controller , handheld user interface device, such as by pressing a button or other suitable feature on the handheld controller 1410. As another example, an input method may be activated by a user engaging a pedal or other feature of a user console 1410. Voice commands and/or other devices may additionally or alternatively be used to activate an input method associated with the menu. While the input method is engaged 1412, the virtual reality system may render and display an array of user mode icons , arranged in a palette around a central origin as shown in 8A. The array of user mode icons may be generally displayed near or around a graphical representation of the handheld controller and/or at a rendered cursor that is controlled by the handheld controller. For example, in one variation in which a handheld controller includes a circular menu button and a graphical representation of the handheld controller also has a circular menu button displayed in the virtual reality environment, the array of user mode icons may be centered around and aligned with the menu button such that the normal vectors of the menu plane and menu button are substantially aligned. The circular or radial menu may include, for example, multiple different menu regions 1414 or sectors, each of which may be associated with an angle range , an arcuate segment of the circular menu and a user mode icon , as shown in 8. Each region may be toggled between a selected state and an unselected state. The method 1400 may generally include determining selection of a user mode by the user and receiving confirmation that the user would like to activate the selected user mode for the virtual reality system. To select a user mode in the user mode menu, the user may move the handheld controller 1420 to freely manipulate the graphical representation of the handheld controller and navigate through the user mode icons in the user mode menu. Generally, the position/orientation of the handheld controller and position/orientation of the graphical representation of the handheld controller which moves in correspondence with the handheld controller may be analyzed to determine whether the user has selected a particular user mode icon. For example, in variations in which the user mode icons are arranged in a generally circular palette around a central origin, the method may include determining radial distance and/or angular orientation of the graphical representation of the handheld controller relative to the central origin. For example, a test for determining user selection of a user mode icon may include one or more prongs, which may be satisfied in any suitable order. In a first prong 1422, the distance of the graphical representation of the handheld controller to the center of the user mode menu or another reference point in the user mode menu is compared to a distance threshold. The distance may be expressed in terms of absolute distance , number of pixels or ratios , percentage of distance between a center point and the user mode icons arranged around the periphery of the user mode menu, such as 80% or more. If the distance is less than the threshold, then it may be determined that no user mode icon is selected. Additionally or alternatively, selection of a user mode icon may depend on a second prong 1424. In the second prong 1424, the orientation of the graphical representation of the handheld controller is measured and correlated to a user mode icon associated with an arcuate segment of the menu. If the orientation is corresponds to a selected arcuate segment of the menu, then it may be determined that a particular user mode associated with the selected arcuate segment is selected by the user. For example, a user mode icon may be determined as selected by the user if both the distance and the angular orientation of the graphical representation of the handheld controller relative to the origin satisfy the conditions 1422 and 1424. After determining that a user has selected a particular user mode icon, the method may, in some variations, convey such selection to the user , as confirmation by visual and/or auditory indications. For example, in some variations, the method may include rendering one or more visual cues 1430 in the displayed virtual reality environment in response to determining that a user has selected a user mode icon. As shown in 14, exemplary visual cues 1432 include modifying the appearance of the selected user mode icon and/or the arcuate segment associated with the selected user mode icon with highlighting , thickened outlines, animation , wiggling lines, dancing or pulsating icon, change in size , enlargement of icon, change in apparent depth, change in color or opacity , more or less translucent, change in pattern fill of icon, change in position , move radially outward or inward from the central origin, , and/or any suitable visual modification. In some variations, indicating to the user in these or any suitable manner may inform the user which user mode will be activated, prior to the user confirming the selection of a particular user mode. For example, the method may include rendering one or more visual cues 1430 as the user navigates or scrolls through the various user mode icons in the menu. The user may confirm approval of the selected user mode icon in one or more various manners. For example, the user may release or deactivate the user input method 1440 associated with the menu , releasing a button on the handheld controller, disengaging a foot pedal, such as to indicate approval of the selected user mode. In other variations, the user may confirm selection by hovering over the selected user mode icon for at least a predetermined period of time , at least 5 seconds, double-clicking the user input method associated with the user menu , double-clicking the button, , speaking a verbal command indicating approval, etc. In some variations, upon receiving confirmation that the user approves the selected user mode, the method may include verifying which user mode icon has been selected. For example, as shown in 14, a test for verifying which user mode icon has been selected may include one or more prongs, which may be satisfied in any suitable order. For example, in variations in which the user mode icons are arranged in a generally circular palette around a central origin, the method may include determining radial distance relative to the central origin 1442 and/or angular orientation of the graphical representation of the handheld controller relative to the central origin 1446 when the user indicates approval of user mode icon selection. In some variations, prongs 1442 and 1446 may be similar to prongs 1422 and 1424 described above, respectively. If at least one of these prongs 1442 and 1444 is not satisfied, then the release of the user input method may correlated to a non-selection of a user mode icon , the user may have changed his or her mind about selecting a new user mode. Accordingly, if the graphical representation of the handheld controller fails to satisfy the distance threshold 1442 then the original or previous user mode may be retained 1444. Similarly, if the graphical representation of the handheld controller fails to correspond to an arcuate segment of the menu 1446, then the original or previous user mode may be retained 1448. If the graphical representation of the handheld controller does satisfy the distance threshold 1442 and corresponds to an arcuate segment of the menu, then the selected user mode may be activated 1450. In other variations, a user mode may additionally or alternatively be selected with other interactions, such as voice command, eye-tracking via sensors, etc. Furthermore, the system may additionally or alternatively suggest activation of one or more user modes based on criteria such as user activity within the , if the user is frequently turning his head to see detail on the edge of his field of view, the system may suggest a user mode enabling placement of a camera to provide a heads-up display window view from a desired vantage point, as described below, type of surgical procedure, etc. Object GrippingOne exemplary user mode with the virtual robotic surgical environment enables a user to grip, move, or otherwise manipulate virtual objects in the virtual environment. Examples of manipulable virtual objects include, but are not limited to, virtual representations of physical items , one or more virtual robotic arms, one or more virtual tool drivers, virtual manual laparoscopic tools, virtual patient operating table or other resting surface, virtual control tower or other equipment, virtual user console, and other virtual or graphical constructs such as portals, window display, patient imaging or other projections on a heads-up display, etc. which are further described below. At least some of the virtual objects may include or be associated with at least one virtual touchpoint or selectable feature. When the virtual touchpoint is selected by a user, the user may move , adjust position and/or orientation the virtual object associated with the selected virtual touchpoint. Furthermore, multiple virtual touchpoints may be simultaneously selected , with multiple handheld controllers 230 and their graphical representations 230 on the same virtual object or multiple separate virtual objects. The user may generally select a virtual touchpoint by moving a handheld controller 230 to correspondingly move a graphical representation 230 to the virtual touchpoint in the virtual environment, then engaging an interactive feature such as a trigger or button on the handheld controller 230 to indicate selection of the virtual touchpoint. In some variations, a virtual touchpoint may remain selected as long as the user engages the interactive feature on the handheld controller 230 , as long as the user depresses a trigger and may become unselected when the user releases the interactive feature. For example, the virtual touchpoint may enable the user to click and drag the virtual object via the virtual touchpoint. In some variations, a virtual touchpoint may be toggled between a selected state and an unselected state, in that a virtual touchpoint may remain selected after a single engagement of the interactive feature on the handheld controller until a second engagement of the interactive feature toggles the virtual touchpoint to an unselected state. In the virtual robotic surgical environment, one or both kinds of virtual touchpoints may be present. A virtual object may include at least one virtual touchpoint for direct manipulation of the virtual object. For example, a virtual robotic arm in the virtual environment may include a virtual touchpoint on one of its virtual arm links. The user may move a handheld controller 230 until the graphical representation 230 of the handheld controller is in proximity to , hovering over the virtual touchpoint, engage a trigger or other interactive feature on the handheld controller 230 to select the virtual touchpoint, then move the handheld controller 230 to manipulate the virtual robotic arm via the virtual touchpoint. Accordingly, the user may manipulate the handheld controller 230 in order to reposition the virtual robotic arm in a new pose, such as to create a more spacious workspace in the virtual environment by the patient, test range of motion of the virtual robotic arm to determine likelihood of collisions between the virtual robotic arm and other objects, etc. A virtual object may include at least one virtual touchpoint that is associated with a second virtual object, for indirect manipulation of the second virtual object. For example, a virtual control panel may include a virtual touchpoint on a virtual switch or button that is associated with a patient operating table. The virtual switch or button may, for example, control the height or angle of the virtual patient operating table in the virtual environment, similar to how a switch or button on a real control panel might electronically or mechanically modify the height or angle of a real patient operating table. The user may move a handheld controller 230 until the graphical representation 230 of the handheld controller is in proximity to , hovering over the virtual touchpoint, engage a trigger or other interactive feature on the handheld controller 230 to select the virtual touchpoint, then move the handheld controller 230 to manipulate the virtual switch or button via the virtual touchpoint. Accordingly, the user may manipulate the handheld controller 230 in order to modify the height or angle of the virtual environment, such as to improve angle of approach or access to a workspace in the virtual environment. When a virtual touchpoint is selected, the virtual reality processor may modify the virtual robotic surgical environment to indicate to the user that the virtual touchpoint is indeed selected. For example, the virtual object including the virtual touchpoint may be highlighted by being graphically rendered in a different color , blue or red and/or outlined in a different line weight or color, in order to visually contrast the affected virtual object from other virtual objects in the virtual environment. Additionally or alternatively, the virtual reality processor may provide audio feedback , a tone, beep, or verbal acknowledgment through an audio device indicating selection of the virtual touchpoint, and/or tactile feedback , a vibration through a handheld controller 230, the head-mounted display 220, or other suitable device. NavigationOther exemplary user modes with the virtual robotic surgical environment may enable a user to navigate and explore the virtual space within the virtual environment. Snap PointsIn some variations, the system may include a user mode enabling snap points, or virtual targets within a virtual environment which may be used to aid user navigation within the virtual environment. A snap point may, for example, be placed at a user-selected or default location within the virtual environment and enable a user to quickly navigate to that location upon selection of the snap point. A snap point may, in some variations, be associated with an orientation within the virtual environment and/or an apparent scale zoom level of the display of the environment from that vantage point. Snap points may, for example, be visually indicated as colored dots or other colored markers graphically displayed in the first-person perspective view. By selecting a snap point, the user may be transported to the vantage point of the selected snap point within the virtual robotic surgical environment. For example, 16 illustrates method of operation 1600 of an exemplary variation of a user mode enabling snap points. As shown in 16, a snap point may be positioned 1610 in the virtual environment by a user or as a predetermined setting. For example, a user may navigate through a user mode menu as described above, and select or grab a snap point icon from the menu with a handheld controller , indicated with a colored dot or other suitable marker and drag and drop the snap point icon to a desired location and/or orientation in the virtual environment. The snap point may, in some variations, be repositioned by the user reselecting the snap point , moving the graphical representation of the handheld controller until it intersects with the snap point or a collision volume boundary around the snap point, then engaging an input feature such as a button or trigger and dragging and dropping the snap point icon to a new desired location. In some variations, the user may set the scale or zoom level of the vantage point 1620 associated with the snap point, such by adjusting a displayed slider bar or scroll wheel, motions as described above for setting a scale level for an environmental view manipulation, etc. The snap point may, in some examples, have a default scale level associated with all or a subcategory of snap points, a scale level associated with the current vantage point of the user when the user places the snap point, or adjusted as described above. Furthermore, once a snap point is placed, the snap point may be stored 1630 in memory , local or remote storage for future access. A snap point may, in some variations, be deleted from the virtual environment and from memory. For example, a snap point may be selected in a similar manner as for repositioning of the snap point and designed for deletion by dragging it off-screen to a predetermined location , virtual trash can and/or moving it with a predetermined velocity , thrown in a direction away from the user's vantage point with a speed greater than a predetermined threshold, selection of a secondary menu option, voice command, etc. Once one or more snap points for a virtual environment are stored in memory, the user may select one of the stored snap points 1640 for use. For example, upon selection of a stored snap point, the user's vantage point may be adjusted to the position, orientation, and/or scale of the selected snap point's settings 1650, thereby allowing the user to feel as if they are teleporting to the location associated with the selected snap point. In some variations, the user's previous vantage point may be stored as a snap point 1660 to enable easy undo of the user's perceived teleportation and transition the user back to their previous vantage point. Such a snap point may be temporary , disappear after a predetermined period of time, such as after 5-10 seconds. In some examples, the user's previous vantage point may be stored as a snap point only if the user's previous location was not a pre-existing snap point. Furthermore, in some variations, a virtual trail or trajectory , line or arc may be displayed in the virtual environment connecting the user's previous vantage point to the user's new vantage point associated with the selected snap point, which may, for example, provide the user with context as to how they have teleported within the virtual environment. Such a visual indication may be removed from the display of the virtual environment after a predetermined period of time , after 5-10 seconds. Generally, in some variations, a snap point may operate in a similar manner as portals described below, except that a snap point may indicate a vantage point without providing a window preview of the virtual environment. For example, snap points may be placed at user-selected vantage points outside and/or inside the virtual patient, and may be linked into one or more trajectories, similar to portals as described above. In some variations, snap point trajectories may be set by the user in a manner similar to that described below for portals. PortalsIn some variations, the system may include a user mode that facilitates placement of one or more portals, or teleportation points, at user-selected locations in the virtual environment. Each portal may, for example, serve as a transportation gateway to a corresponding vantage point in the virtual environment, thereby allowing the user to swiftly change vantage points for viewing and navigating the virtual environment. Generally, upon selection , with one or more handheld controllers 230 of a portal, the user's apparent location may transition to the location of the selected portal, such that the user views the virtual environment from the selected portal's vantage point and has the sensation of jumping around the virtual environment. By placing one or more portals around the virtual environment, the user may have the ability to quickly move between various vantage points. Placement, adjustment, storing, and/or navigation of portals around the virtual environment may be similar to that of snap points described above. For example, as generally described above, the system may display a first-person perspective view of the virtual robotic surgical environment from a first vantage point within the virtual robotic surgical environment. The user may navigate through a menu to select a user mode that enables placement of a portal. As shown in 9A, the user may manipulate the graphical representation 230 of the handheld controller to position a portal 910 in a selected location in the virtual environment. For example, the user may engage a feature , trigger or button on the handheld controller while a portal placement-enabling user mode is activated, such that while the feature is engaged and the user moves the position and/or orientation of the handheld controller, a portal 910 may appear and be moved within the virtual environment. One or more portal placement indicators 920 , one or more arrows, a line, an arc, etc. connecting the graphical representation 230 to a prospective portal location may aid in communicating to the user the prospective location of a portal 910, such as by helping with depth perception. Size of the portal 910 may be adjusted by grabbing and stretching or shrinking the sides of the portal 910 via the handheld controllers. When the portal 910 location is confirmed , by the user releasing the engaged feature on the handheld controller, double-clicking, , the user's apparent location within the virtual environment may be updated to match the vantage point associated with the portal 910. In some variations, as described below, at least some vantage points within the virtual location may be prohibited. These prohibited vantage points may be stored in memory , local or remote storage. In these variations, if a portal 910 location is confirmed in a prohibited location , compared to and matched among a list of prohibited vantage points stored in memory, then the user's apparent location within the virtual environment may be retained with no changes. However, if a portal 910 location is confirmed as permissible , compared to and not matched among the list of prohibited vantage points, then the user's apparent location within the virtual environment may be updated as described above. In some variations, once the user has placed the portal 910 at a desired vantage point, a window view of the virtual environment from the vantage point of the placed portal 910 may be displayed within the portal 910, thereby offering a preview of the view offered by the portal 910. The user may, for example, view through the portal 910 with full parallax, such that the portal 910 behaves as a type of magnifying lens. For example, while looking through the portal 910, the user may view the virtual environment as if the user has been scaled to the inverse of the portal's scale factor which affects both the interpupillary distance and the focal distance and as if the user has been translated to the reciprocal of the portal's scale factor 1/portal scale factor of the distance from the portal 910 to the user's current location. Furthermore, the portal 910 may include an event horizon which may be a texture on a plane that is rendered, for example, using additional one or more cameras described below within the virtual environment scene positioned as described above. In these variations, when traveling through the portal 910 after selecting the portal 910 for teleportation, the user's view of the virtual environment may naturally converge with the user's apparent vantage point during the user's approach to the portal, since the user's vantage point is offset as a fraction of the distance from the portal by 1/portal scale factor. Accordingly, the user may feel as if they are smoothly and naturally stepping into viewing the virtual environment at the scale factor associated with the selected portal. As shown in 9A, in some variations, the portal 910 may be generally circular. However, in other variations, one or more portals 910 may be any suitable shape, such as elliptical, square, rectangular, irregular, etc. Furthermore, the window view of the virtual environment that is displayed in the portal may display the virtual environment at a scale factor associated with the portal, such that the view of the virtual environment displayed in different portals may be displayed at different zoom levels , 1, 1. 5, 2, 2. 5, 3, , thereby also changing the scale of the user relative to the environment. The scale factor of the window view in a portal may also indicate or correspond with the scale of the view that would be displayed if the user is transported to that portal's vantage point. For example, if a view of the virtual environment outside a virtual patient is about 1, then a window view of the virtual environment inside the virtual patient may be about 2 or more, thereby providing a user with more detail of the internal tissue of the virtual patient. The scale factor may be user-defined or predetermined by the system , based on location of the portal in the virtual environment. In some variations, the scale factor may correlate to the displayed size of the portal 910, though in other variations, the scale factor may be independent of the portal size. In some variations, a portal 910 may be placed in substantially any vantage point in the virtual environment that the user desires. For example, a portal 910 may be placed anywhere on a virtual ground surface of the virtual operating room or on a virtual object , table, chair, user console, . As another example, as shown in 9B, a portal 910 may be placed in mid-air at any suitable elevation above the virtual ground surface. As yet another example, as shown in 9C, a portal may be placed on or inside a virtual patient, such as portals 910a and 910b which are placed on the abdomen of a patient and enable views of the intestines and other internal organs of the virtual patient , simulated augmented reality. In this example, the virtual patient may be generated from medical imaging and other information for a real non-virtual patient, such that portals 910a and 910b may enable the user to have an immersive view of an accurate representation of the real patient's tissue , for viewing tumors, , and/or generated from internal virtual cameras described below placed inside the patient. In some variations, the system may limit placement of a portal 910 according to predefined guidelines , only outside the patient or only inside the patient, which may correspond, for example, to a type of simulated surgical procedure or a training level , beginner or advanced user level associated with the virtual environment. Such prohibited locations may be indicated to the user by, for example, a visual change in the portal 910 as it is being placed , changing outline color, displaying a grayed-out or opaque window view within the port 910 as it is being placed and/or auditory indications , beep, tone, verbal feedback. In yet other variations, the system may additionally or alternatively include one or more portals 910 placed in predetermined locations, such as at a virtual user console in the virtual environment, adjacent the virtual patient table, etc. Such predetermined locations may, for example, depend the type of procedure, or be saved as part of a configuration file. A portal 910 may be viewable from either side , front side and rear side of the portal. In some variations, the view from one side of the portal 910 may be different from an opposite side of the portal 910. For example, when viewed from a first side , front of the portal 910, the portal may provide a view of the virtual environment with a scale factor and parallax effects as described above, while when viewed from a second side , rear of the portal 910, the portal may provide a view of the virtual environment with a scale factor of about one. As another example, the portal may provide a view of the virtual environment with a scale factor and parallax effects when viewed from both the first side and the second side of the portal. In some variations, multiple portals 910 may be sequentially linked to include a trajectory in the virtual environment. For example, as shown in 9C, a first-person perspective view of the virtual robotic surgical environment from a first vantage point may be displayed , an immersive view. The user may place a first portal 910a in a second vantage point that is different from the first vantage point , closer to the virtual patient than the first vantage point and a first window view of the virtual robotic surgical environment from the second vantage point may be displayed in the first portal 910a. Similarly, the user may place a second portal 910b in a third vantage point , closer to the patient than the first and second vantage points, and a second window view of the virtual robotic surgical environment may be displayed in the second portal 910b. The user may provide a user input associating the first and second portals 910a and 910b , by selection with the handheld controllers, drawing a line between the first and second portals with the handheld controllers, such that the first and second portals are sequentially linked, thereby generating a trajectory between the first and second portals. In some variations, after multiple portals 910 are linked to generate a trajectory, transportation along the trajectory may not require explicit selection of each sequential portal. For example, once on the trajectory , at the second vantage point, traveling between linked portals may be accomplished by engagement of a trigger, button, touchpad, scroll wheel, other interactive feature of the handheld controller, voice command, etc. Additional portals may be linked in a similar manner. For example, two, three, or more portals may be linked in series to generate an extended trajectory. As another example, multiple portals may form branched trajectories, where at least two trajectories share at least one portal in common but otherwise each trajectory has at least one portal that is unique to that trajectory. As yet another example, multiple portals may form two or more trajectories that share no portals in common. The user may select which trajectory on which to travel, such as by using the handheld controllers and/or voice command, etc. One or more trajectories between portals may be visually indicated , with a dotted line, color coding of portals along the same trajectory, , and such visual indication of trajectories may be toggled on and off, such as based on user preference. Other portal features may facilitate easy navigation of the trajectories between portals. For example, a portal may change color when the user has entered and gone through that portal. As shown in 9C, in another example, a portal itself may be displayed with direction arrows indicating the permissible direction of the trajectory including that portal. Furthermore, travel along the trajectories may be accomplished with an undo command via handheld controllers and/or voice command, that returns the user to the previous vantage point , displays the view of the virtual environment from the previous vantage point. In some variations, a home or default vantage point may be established such as according to user preference or system settings in order to enable a user to return to that home vantage point quickly with a shortcut command, such as an interactive feature on a handheld controller or a voice command , Reset my position. For example, a home or default vantage point may be at a virtual user console or adjacent to the virtual patient table. The user mode facilitating placement and use of portals, or another separate user mode, may further facilitate deletion of one or more portals. For example, a portal may be selected for deletion with the handheld controllers. As another example, one or more portals may be selected for deletion via voice command , delete all portals or delete portal A. Free NavigationThe system may include a user mode that facilitates free navigation around the virtual robotic surgical environment. For example, as described herein, the system may be configured to detect the user's walking movements based on sensors in the head-mounted display and/or handheld controllers, and may correlate the user's movements into repositioning within a virtual operating room. In another variation, the system may include a flight mode that enables the user to quickly navigate the virtual environment in a flying manner at different elevations and/or speeds, and at different angles. For example, the user may navigate in flight mode by directing one or more handheld controllers and/or the headset in a desired direction for flight. Interactive features on the handheld controller may further control flight. For example, a directional pad or touchpad may provide control for forward, backward, strafing, etc. motions while maintaining substantially the same perspective view of the virtual environment. Translation may, in some variations, occur without acceleration, as acceleration may tend to increase the likelihood of simulator sickness. In another user setting, a directional pad or touchpad or orientation of the headset may provide control for elevation of the user's apparent location within the virtual environment. Furthermore, in some variations, similar to that described above with respect to portals, a home or default vantage point within the flight mode may be established in order to enable a user to return to that home vantage point quickly with a shortcut command. Parameters such as speed of flight in response to a user input may be adjustable by the user and/or set by the system by default. Furthermore, in flight mode, the scaling factor of the displayed view may be controlled via the handheld controllers. The scaling factor may, for example, affect apparent elevation of the user's location within the virtual environment. In some variations, the user may use the handheld controllers to pull apart two points in the displayed view to zoom out and draw closer two points in the displayed view to zoom in, or conversely pull apart two points in the displayed view to zoom in an draw closer two points in the displayed view to zoom out. Additionally or alternatively, the user may utilize voice commands , increase zoom to 2 to change the scaling factor of the displayed view. For example, 10A and 10B illustrate exemplary views of the virtual environment that are relatively zoomed in and zoomed out, respectively. Parameters such as the speed of the change in scaling factor, minimum and maximum scaling factor ranges, etc. may be adjustable by the user and/or set by the system by default. As the user freely navigates the virtual environment in flight mode, the displayed view may include features to reduce eye fatigue, nausea, etc. For example, in some variations, the system may include a comfort mode in which outer regions of the displayed view are removed as the user navigates in flight mode, which may, for example, help reduce motion sickness for the user. As shown in 10C, when in the comfort mode, the system may define a transition region 1030 between an inner transition boundary 1010 and an outer transition boundary 1020 around a focal area , center of the user's view. Inside the transition region inside the inner transition boundary 1010, a normal view of the virtual robotic surgical environment is displayed. Outside the transition region outside the outer transition boundary 1020, a neutral view or plain background , a plain, gray background is displayed. Within the transition region 1030, the displayed view may have a gradient that gradually transitions the view of the virtual environment to the neutral view. Although the transition region 1030 shown in 10C is depicted as generally circular, with generally circular inner and outer transition boundaries 1010 and 1020, in other variations the inner and outer transition boundaries 1010 and 1020 may define a transition region 1030 that is elliptical or other suitable shape. Furthermore, in some variations, various parameters of the transition region, such as size, shape, gradient, etc. may be adjustable by the user and/or set by the system by default. In some variations, as shown in 11, the user may view the virtual robotic surgical environment from a dollhouse view that allows the user to view the virtual operating room from an overhead vantage point, with a top-down perspective. In the dollhouse view, the virtual operating room may be displayed at a smaller scale factor , smaller than life-size on the display, thereby changing the scale of the user relative to the virtual operating room. The dollhouse view may provide the user with additional contextual awareness of the virtual environment, as the user may view the entire virtual operating room at once, as well as the arrangement of its contents, such as virtual equipment, virtual personnel, virtual patient, etc. Through the dollhouse view, for example, the user may rearrange virtual objects in the virtual operating room with fuller contextual awareness. The dollhouse view may, in some variations, be linked in a trajectory along with portals and/or snap points described above. Environment View RotationIn some variations, the system may include a user mode that enables the user to navigate the virtual robotic surgical environment by moving the virtual environment around his or her current vantage point. The environment view rotation mode may offer a different manner in which the user may navigate the virtual environment, such as by grasping and manipulating the environment as if it were an object. As the user navigates through the virtual environment in such a manner, a comfort mode similar to that described above may additionally be implemented to help reduce simulation-related motion sickness. For example, in an environment view rotation mode, the user may rotate a displayed scene around a current vantage point by selecting and dragging the view of the virtual environment around the user's current vantage point. In other words, in the environment view rotation mode, the user's apparent location in the virtual environment appears fixed while the virtual environment may be moved. This is contrast to other modes, such as, for example, flight mode described above, in which generally the environment may appear fixed while the user moves. Similar to the scaling factor adjustments described above for flight mode, in the environment view rotation mode, the scaling factor of the displayed view of the environment may be controlled by the handheld controllers and/or voice commands , by using the handheld controllers to select and pull apart two points in the displayed view to zoom in, . For example, as shown in 15, in one exemplary variation of a method 1500 for operating in an environment view rotation mode, the user may activate a user input method 1510 such as on a handheld controller , a button or trigger or other suitable feature or any suitable device. In some variations, one handheld controller 1520 may be detected upon activation of the user input method. The original position of the handheld controller at the time of activation may be detected and stored 1522. Thereafter, as the user moves the handheld controller , while continuing to activate the user input method, the current position of the handheld controller may be detected 1524. A vector difference between the original or previous position and the current position of the handheld controller may be calculated 1526, and position of the vantage point of the user may be adjusted 1528 based at least partially on the calculated vector difference, thereby creating an effect that makes the user feel they are grabbing and dragging the virtual environment around. In some variations, two handheld controllers 1520 may be detected upon activation of the user input method. The original positions of the handheld controllers may be detected 1522, and a centerpoint and an original vector between the original positions of the handheld controllers 1523 may be calculated and stored. Thereafter, as the user moves one or more both handheld controllers , while continuing to activate the user input method, the current positions of the handheld controllers may be detected 1524 and used to form the basis for a calculated vector difference between original and current vectors between handheld controllers 1528. The position and/or orientation of the vantage point of the user may be adjusted 1528, based on the calculated vector difference. For example, the orientation or rotation of the displayed view may be rotated around the centerpoint between the handheld controller locations, thereby creating an effect that makes the user feel they are grabbing and dragging the environment around. Similarly, the scale of the display of the virtual environment 1529 may be adjusted based on the calculated difference in distance between the two handheld controllers, thereby creating an effect that makes the user feel they are grabbing and zooming in and out of the displayed view of the virtual environment. Although the above-described user modes are described separately, it should be understood that aspects of these modes characterize exemplary ways that a user may navigate the virtual robotic surgical environment, and may be combined in a single user mode. Furthermore, some of these aspects may be seamlessly linked. For example, an overhead vantage point generally associated with flight mode may be sequentially linked with one or more portals in a trajectory. Even further, in some variations, a vantage point or displayed view of the virtual environment , as adjusted via one or more of the above user modes may be linked to at least one default vantage point , default in position, orientation, and/or scale. For example, by activating a user input , on a handheld controller, foot pedal, , a user may reset the current vantage point to a designated or predetermined vantage point in the virtual environment. The user's current vantage point may, for example, be gradually and smoothly animated in order to transition to default values of position, orientation, and/or scale. Supplemental ViewsIn some variations, an exemplary user mode or modes of the system may display one or more supplemental views of additional information to a user, such as overlaid over or inset in the primary, first-person perspective view of the virtual robotic surgical environment. For example, as shown in 12, a heads-up display 1210 HUD may provide a transparent overlay over a primary first-person perspective view of the virtual environment. The HUD 1210 may toggled on and off, thereby allowing the user to control whether to display the HUD 1210 at any particular time. Supplemental views of additional information, such as that described below, may be placed onto the HUD such that the user may observe the supplemental views without looking away from the primary view. For example, the supplemental views may stick to the HUD 1210 and move with the user's head movement such that the supplemental views are always in the user's field of view. As another example, supplemental views of additional information may be loosely fixed to the HUD 1210, in that the supplemental views may be small or at least partially hidden off-screen or in peripheral vision when the user's head is generally facing forward, but minor or slight head movement to one side may expand and/or bring one or more supplemental views into the user's field of view. The one or more supplemental views may be arranged on the HUD 1210 in a row, grid, or other suitable arrangement. In some variations, the HUD 1210 may include predetermined snap points to which the supplemental views , camera views, are positioned. For example, the user may select a supplemental view on the HUD 1210 for closer inspection, then replace the supplemental view on the HUD 1210 by dragging it generally in the direction of a snap point, whereupon the supplemental view may be drawn to and fixed at the snap point without needing to be precisely placed there by the user. As another example, in a virtual command station mode, one or more supplemental views may be displayed in a virtual space with one or more content windows or panels arranged in front of the user in the virtual space , similar to a navigable menu. For example, as shown in 13, multiple content windows , 1310a, 1310b, 1310c, and 1310d may be positioned in a semi-circular arrangement or other suitable arrangement for display to a user. The arrangement of the content windows may be adjusted by the user , using handheld controllers with their graphical representations 230 to select and drag or rotate content windows. The content windows may display, for example, an endoscope video feed, a portal view, a stadium overhead view of the virtual operating room, patient data , imaging, other camera or patient information views such as those described herein, etc. By viewing multiple panels simultaneously, the user may be able to simultaneously monitor multiple aspects of the virtual operating room and/or with the patient, thereby allowing the user to have an overarching and broader awareness of the virtual environment. For example, the user may become aware of and then respond more quickly to any adverse events in the virtual environment , simulated negative reactions of the virtual patient during a simulated surgical procedure. Furthermore, the virtual command station mode may enable a user to select any one of the content windows and become immersed in the displayed content , with a first-person perspective. Such a fully immersive mode may temporarily dismiss the other content windows, or may minimize , be relegated to a HUD overlaid over the selected immersive content. As an illustrative example, in the virtual command station mode, the system may display multiple content windows including an endoscopic camera video feed showing the inside of a virtual patient's abdomen. The user may select the endoscopic camera video feed to become fully immersed in the virtual patient's abdomen , while still manipulating robotic arms and instruments attached to the arms. Camera ViewsIn some variations, a user mode may enable the user to place a virtual camera in a selected vantage point in the virtual environment, and a window view of the virtual environment from the selected vantage point may be displayed in the HUD such that the user may simultaneously view both his first-person perspective field of view and the camera view the view provided by the virtual camera that may update in real time. A virtual camera may be placed in any suitable location in the virtual environment , inside or outside the patient, overhead the patient, overhead the virtual operating room, . For example, as shown in 12, the user may place a virtual camera 1220 , using object gripping as described above near the pelvic region of a virtual patient and facing the patient's abdomen so as to provide a virtual video feed of the patient's abdomen. Once placed, a virtual camera 1220 may be subsequently repositioned. A camera view , a circular inset view, or window of any suitable shape may be placed on the HUD as a window view showing the virtual video feed from the vantage point of the virtual camera 1220. Similarly, multiple virtual cameras may be placed in the virtual environment to enable multiple camera views to be shown on the HUD. In some variations, a predetermined arrangement of one or more virtual cameras may be loaded, such as part of a configuration file for the virtual reality processor to incorporate into the virtual environment. In some variations, the system may offer a range of different kinds of virtual cameras, which may provide different kinds of camera views. One exemplary variation of a virtual camera is a movie camera that is configured to provide a live virtual feed of the virtual environment , movie camera view 1212 in 12. Another exemplary variation of a virtual camera is an endoscopic camera which is attached to a virtual endoscope to be placed in a virtual patient. In this variation, the user may, for example, virtually perform a technique for introducing the virtual endoscope camera into the virtual patient and subsequently monitor the internal workspace inside the patient by viewing the virtual endoscopic video feed , endoscopic camera view 1214 in 12. In another exemplary variation, the virtual camera may be a wide-angle , 360-degree, panoramic, camera that is configured to provide a larger field of view of the virtual environment. In this variation, the window camera view may, for example, be displayed as a fish-eye or generally spherical display. Various aspects of the camera view may be adjusted by the user. For example, the user may adjust the location, size, scale factor, etc. of the camera view , similar to adjustments of portals as described above. As another example, the user may select one or more filters or other special imaging effects to be applied to the camera view. Exemplary filters include filters that highlight particular anatomical features , tumors or tissue characteristics , perfusion of the virtual patient. In some variations, one or more virtual cameras may be deselected or turned off , have the virtual camera and/or associated camera view selectively hidden or deleted, such as if the virtual camera or its associated camera view is obstructing the user's view of the virtual environment behind the virtual camera or camera view. In some variations, a camera view may function similarly to a portal described above to enable the user to navigate quickly around the virtual environment. For example, with reference to 12, a user may select the camera view 1212 , highlight or grab and pull the camera view 1212 toward himself to be transported to the vantage point of the camera view 1212. Patient Data Views, Etc. In some variations, a user mode may enable display of patient data and other information on the HUD or another suitable location in the display. For example, patient imaging information , ultrasound, X-ray, MRI, may be displayed in a supplemental display, overlaid over the patient , as simulated augmented reality. A user may, for example, view patient images as reference while interacting with the virtual patient. As another example, patient vitals , heartrate, blood pressure, may be displayed to the user in a supplemental view. In another variation, a user mode may enable display of other suitable information, such as training videos , exemplary surgical procedures recorded from a previous procedure, video feed from a mentor or trainer surgeon, etc. providing guidance to a user. Virtual Reality System ApplicationsGenerally, the virtual reality system may be used in any suitable scenario in which it is useful to simulate or replicate a robotic surgical environment. In some variations, the virtual reality system may be used for training purposes, such as allowing a surgeon to practice controlling a robotic surgical system and/or to practice performing a particular kind of minimally-invasive surgical procedure using a robotic surgical system. The virtual reality system may enable a user to better understand the movements of the robotic surgical system in response to user commands, both inside and outside the patient. For example, a user may don a head-mounted display under supervision of a mentor or trainer who may view the virtual reality environment alongside the user , through a second head-mounted display, through an external display, and guide the user through operations of a virtual robotic surgical system within the virtual reality environment. As another example, a user may don a head-mounted display and may view, as displayed on the immersive display , in a content window, the HUD, a training-related video such as a recording of a previously performed surgical procedure. As another example, the virtual reality system may be used for surgical planning purposes. For example, a user may operate the virtual reality system to plan surgical workflow. Configuration files of virtual objects , robotic surgical system including arms and tool drivers, user console, end effectors, other equipment, patient bed, patient, personnel, may be loaded into a virtual robotic surgical environment as representative of actual objects that will be in the actual i. e. , non-virtual, or real operating room. Within the virtual reality environment, the user may adjust features of the virtual operating room, such as positioning the user console, patient bed, and other equipment relative to one another in a desired arrangement. The user may additionally or alternatively use the virtual reality system to plan aspects of the robotic surgical system, such as selecting number and location of ports for entry of the surgical instruments, or determining optimum number and position/orientation , mounting location, arm pose, of robotic arms for a procedure, such as for minimizing potential collisions between system components during the surgical procedure. Such virtual arrangements may be based on, for example, trial-and-error, previous setups for similar surgical procedures and/or similar patients, etc. In some variations, the system may additionally or alternatively propose virtual arrangements selected based on machine learning techniques applied to datasets of previously-performed surgical procedures for various kinds of patients. As yet another example, the virtual reality system may be used for R&amp;D purposes simulation. For example, a method for designing a robotic surgical system may include generating a virtual model of a robotic surgical system, testing the virtual model of the robotic surgical system in a virtual operating room environment, modifying the virtual model of the robotic surgical system based on the testing, and building the robotic surgical system based on the modified virtual model. Aspects of the virtual model of the robotic surgical system that may be tested in the virtual operating room environment include physical characteristics of one or more components of the robotic surgical system , diameter or length of arm links. For example, a virtual model of a particular design of a robotic arm may be built and implemented in a virtual environment, where the virtual model may be tested with respect to particular arm movements, surgical procedures, etc. , test for likelihood of collision between the robotic arm and other objects. Accordingly, a design of a robotic arm or similarly, any other component of the robotic surgical system may be at least initially tested by testing a virtual implementation of the design, rather than testing a physical prototype, thereby accelerating the R&amp;D cycle and reducing costs. Other aspects that may be tested include functionality of one or more components of the robotic surgical system , control modes of a control system. For example, as described above, a virtual operating environment application may pass status information to a kinematics application, and the kinematics application may generate and pass commands based on control algorithms, where the virtual reality processor may use the commands to cause changes in the virtual robotic surgical environment , move a virtual robotic arm in a particular way in accordance with relevant control algorithms. As such, software control algorithms may be embodied in a virtual robotic system for testing, refinement, etc. without requiring a physical prototype of the relevant robotic component, thereby conserving R&amp;D resources and accelerating the R&amp;D cycle. In another example, the virtual reality system may be used to enable multiple surgeons to collaborate in the same virtual reality environment. For example, multiple users may don head-mounted displays and interact with each other and with the same virtual robotic system, the same virtual patient, in the virtual reality environment. The users may be physically in the same room or general location, or may be remote from one another. For example, one user may be tele-mentoring the other as they collaborate to perform a surgical procedure on the virtual patient. Specific illustrative exemplary applications of the virtual reality system are described in further detail below. However, it should be understood that applications of the virtual reality system are not limited to these examples and general application scenarios described herein. Example 1Over the BedA user may use the virtual reality system to simulate an over-the-bed scenario in which he is adjacent to a patient bed or table and operating both a robotic surgical system and a manual laparoscopic tool. Such simulation may be useful for training, surgical planning, etc. For example, the user may staple tissue in a target segment of a virtual patient's intestine using both a virtual robotic tool and a virtual manual tool. In this example, the user dons a head-mounted display providing an immersive view of a virtual reality environment, and may use handheld controllers to navigate within the virtual reality environment to be adjacent to a virtual patient table on which a virtual patient lies. A proximal end of a virtual robotic arm is attached to the virtual patient table, and a distal end of the virtual robotic arm supports a virtual tool driver actuating virtual forceps that are positioned within the abdomen of the virtual patient. A virtual manual laparoscopic stapler tool is passed through a virtual cannula and having a distal end positioned within the abdomen of the virtual patient. Additionally, an endoscopic camera is positioned within the abdomen of the virtual patient, and provides a virtual camera feed showing the surgical workspace within the abdomen of the virtual patient including patient tissue, virtual robotically-controlled forceps, and a virtual manual laparoscopic stapler tool. The user continues to view the virtual environment through the immersive display in the head-mounted display, as well as the virtual endoscopic camera feed displayed in a window view in a heads-up display overlaid in the user's field of view. The user holds in one hand a handheld controller that is configured to control the robotically-driven virtual forceps. The user holds in another hand a laparoscopic hand controller that is configured to control the virtual manual laparoscopic stapler tool, with the laparoscopic hand controller passing through a cannula mounted in a mock patient body made of foam. The laparoscopic hand controller is calibrated to correspond to the virtual manual laparoscopic stapler tool. The user manipulates the handheld controller to operate the robotically-controlled forceps to manipulate the intestine of the virtual patient and uncover a target segment of the intestine. With the target segment of the intestine exposed and accessible, the user manipulates the laparoscopic hand controller to apply virtual staples to the target segment via the virtual manual laparoscopic stapler tool. Example 2Collision Resolution from User ConsoleWhen using the virtual reality system, a user may desire to resolve collisions between virtual components of the virtual robotic surgical system, even though the user may not be adjacent the colliding virtual components , the user may be seated at a distance away from the virtual patient table, such as at a virtual user console. In this example, the user dons a head-mounted display providing an immersive view provided by a virtual endoscope placed inside an abdomen of a virtual patient. Proximal ends of two virtual robotic arms are attached to separate locations on a virtual patient table, on which the virtual patient lies. Distal ends of the virtual robotic arms support respective tool drivers actuating virtual forceps that are positioned within the abdomen of the virtual patient. The user manipulates the handheld controllers to operate the two robotically-controlled virtual forceps, which manipulate virtual tissue within the virtual patient. This movement may cause a collision involving at least one of the virtual robotic arms , a virtual robotic arm may be posed so as to create a collision with itself, the virtual robotic arms may be posed so as to create a collision with each other, a virtual robotic arm may be posed so as to create a collision with the patient or nearby obstacle, . The virtual reality system detects the collision based on status information of the virtual robotic arms, and alerts the user regarding the collision. The system displays an overhead or other suitable view from a suitable vantage point of the virtual robotic surgical system, such as in a window view , picture-in-picture view. The location of the collision is highlighted in the displayed window view, such as by outlining the affected colliding components with red or another contrasting color. Alternatively, the user may detect the collision himself by monitoring a camera video feed from a virtual camera placed overhead the virtual patient table. Upon becoming aware of the collision, the user may zoom out or adjust the scale of his immersive view of the virtual reality environment. The user may engage an arm repositioning control mode that locks the position and orientation of the virtual forceps within the patient. Using the handheld controllers in an object gripping user mode, the user may grab onto virtual touchpoints on the virtual robotic arms and reposition repose the virtual robotic arms so as to resolve the collision while the control mode maintains the position and orientation of the virtual forceps during the arm repositioning. Once the virtual robotic arms are repositioned such that the collision is resolved, the user may zoom back into the previous vantage point, disengage the arm repositioning control mode, and resume using the handheld controllers to operate the virtual forceps within the virtual patient. Example 3Coordinated Relocation of Multiple Surgical Instruments from User ConsoleWhen using the virtual reality system, a user may find it useful to stay substantially in an endoscopic view and relocating multiple virtual surgical instruments , end effectors, cameras as a group rather than individually within the virtual patient, thereby saving time, as well as making it easier for the user to maintain contextual awareness of the instruments relative to the virtual patient's anatomy. In this example, the user dons a head-mounted display providing an immersive view provided by a virtual endoscope placed inside an abdomen of a virtual patient. Proximal ends of two virtual robotic arms are attached to separate locations on a virtual patient table, on which the virtual patient lies. Distal ends of the virtual robotic arm support respective tool drivers actuating virtual forceps that are positioned in the pelvic area of the virtual patient. The user may manipulate handheld controllers to operate the virtual forceps. The user may wish to move the virtual endoscope and the virtual forceps to another target region of the virtual patient's abdomen, such as the spleen. Rather than move each surgical instrument individually, the user may engage a coordinated relocation mode. Once this mode is engaged, the endoscopic camera view zooms out along the axis of the endoscope to a distance sufficient to allow the user to view the new target region spleen. A spherical indicator is displayed at the distal end of the endoscope that encapsulates the distal end of the virtual endoscope and the distal ends of the virtual forceps. The user manipulates at least one handheld controller to withdraw the virtual endoscope and the virtual forceps away from the surgical workspace , until the user can see the distal end of the virtual cannula in the virtual endoscopic view, then grab and move the spherical indicator from the pelvic area to the spleen. Once the user finalizes the new target region by moving the spherical indicator to the new target region, the virtual endoscope and virtual forceps automatically travel to the new target region and the virtual endoscopic camera view zooms to show the new target region. Throughout this relatively large-scale move, the user views the virtual environment with substantially an endoscopic view of the virtual environment, thereby enabling the user to maintain awareness of the virtual patient's anatomy instead of transferring his focus between instrument and anatomy. The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the invention. However, it will be apparent to one skilled in the art that specific details are not required in order to practice the invention. Thus, the foregoing descriptions of specific embodiments of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed; obviously, many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications, they thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated. It is intended that the following claims and their equivalents define the scope of the invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/55/849/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9915380477905273,
                    "F_SPEC_PARAMS": [
                        "reduced patient scarring, less patient pain, shorter patient recovery periods,",
                        "lower medical treatment costs"
                    ],
                    "S_SPEC_PARAMS": [
                        "expensive",
                        "time-consuming"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery."
                    ],
                    "S_SENTS": [
                        "Such cyclical prototyping and testing is generally cumulatively expensive and time-consuming."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Side Effects"
                    ],
                    "F_SIM_SCORE": 0.4541338384151459,
                    "S_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.5887263417243958,
                    "GLOBAL_SCORE": 1.7129681378602981
                },
                "sort": [
                    1.7129681
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11247337-20220215",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11247337-20220215",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-05-20",
                    "PUBLICATION_DATE": "2022-02-15",
                    "INVENTORS": [
                        "Martin A. Szarski",
                        "David Michael Bain"
                    ],
                    "APPLICANTS": [
                        "The Boeing Company    ( Chicago , US )"
                    ],
                    "INVENTION_TITLE": "Robots, robotic systems, and related methods",
                    "DOMAIN": "B25J 91687",
                    "ABSTRACT": "Robots for moving relative to a surface, robotic systems including the same, and associated methods are disclosed. A robot includes a body, at least two legs, and at least two feet. Each leg has a proximal end region operatively coupled to the body at a respective body joint with one rotational degree of freedom and a distal end region operatively coupled to a respective foot at a respective foot joint comprising two rotational degrees of freedom. Each foot is configured to be translated relative to the surface with two degrees of translational freedom. Robotic systems include one or more robots and a surface along which the one or more robots are positioned to move. Methods of operating robots and of operating robotic systems include translating at least one foot of a robot to operatively move the body of the robot with six degrees of freedom.",
                    "CLAIMS": "1. A method of operating a robot comprising a body; at least two legs, wherein each leg has a proximal end region and a distal end region, and wherein the proximal end region of each leg is operatively coupled to the body at a respective body joint with one rotational degree of freedom; and at least two feet, wherein each foot is operatively coupled to the distal end region of a respective leg of the at least two legs at a respective foot joint comprising two rotational degrees of freedom, wherein each foot is configured to be selectively, independently, and motively translated relative to a surface with two degrees of translational freedom, and wherein each foot includes a drive means for selectively, independently, and motively translating the foot relative to the surface; the method comprising: working on a work piece with the robot; wherein the working on the work piece with the robot includes assembling the work piece with the robot; and wherein the assembling the work piece with the robot includes selectively, independently, and motively translating at least one foot of the at least two feet relative to the surface. 2. The method of claim 1, wherein the assembling the work piece further includes gripping a part with the robot, and wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes selectively, independently, and motively translating at least one foot of the at least two feet to rotate the part relative to the work piece and translate the part toward the work piece. 3. The method of claim 2, wherein the selectively, independently, and motively translating at least one foot of the at least two feet results in the part becoming threadingly coupled to the work piece. 4. The method of claim 2, wherein the robot comprises an end effector operatively coupled to the body, and wherein the gripping the part with the robot includes gripping the part with the end effector. 5. The method of claim 1, wherein each foot of the at least two feet comprises a planar motor. 6. The method of claim 1, wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes providing a cushion of air between the at least one foot of the at least two feet and the surface. 7. The method of claim 1, wherein the assembling the work piece further includes selectively fixing at least one foot of the at least two feet to the surface at a respective location. 8. The method of claim 1, wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes translating only one foot of the at least two feet. 9. The method of claim 1, wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes translating two or more feet of the at least two feet. 10. The method of claim 1, wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes translating the at least two feet sequentially. 11. The method of claim 1, wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes translating the at least two feet simultaneously. 12. The method of claim 11, wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes coordinating the simultaneous movement of the at least two feet to bring the at least two feet toward each other while also revolving the at least two feet around a point on the surface such that the part translates away from the surface and toward the work piece. 13. The method of claim 1, wherein the robot includes a sensor configured to detect one or more of position in three-dimensional space, orientation in three-dimensional space, and acceleration, and wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes translating each foot of the at least two feet responsive to information collected by the sensor. 14. The method of claim 13, wherein the sensor is configured to measure a position of the body; wherein the method further comprises comparing the position of the body to a target position; and wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes bringing the body to the target position. 15. The method of claim 13, wherein the sensor is configured to measure a rotational orientation of the body; wherein the method further comprises comparing the rotational orientation of the body to a target rotational orientation; and wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes bringing the body to the target rotational orientation. 16. The method of claim 1, wherein the selectively, independently, and motively translating at least one foot of the at least two feet includes monitoring and controlling, with a system controller, motion of the robot to avoid the robot colliding with its surroundings. 17. The method of claim 1, wherein the assembling the work piece with the robot includes assembling a part to the work piece, wherein the work piece is at least a portion of an aircraft, and wherein the part is a component of the aircraft. 18. A robot, comprising: a body; an end effector operatively coupled to the body; at least two legs, wherein each leg has a proximal end region and a distal end region, and wherein the proximal end region of each leg is operatively coupled to the body at a respective body joint with one rotational degree of freedom; and at least two feet, wherein each foot is operatively coupled to the distal end region of a respective leg at a respective foot joint comprising two rotational degrees of freedom, wherein each foot is configured to be selectively, independently, and motively translated relative to a surface with two degrees of translational freedom, and wherein each foot includes a drive means for selectively, independently, and motively translating the foot relative to the surface; wherein the robot is configured to assemble a work piece. 19. A robotic system, comprising: the robot of claim 18; and a system controller configured to selectively translate at least one foot of the at least two feet of the robot relative to the surface to assemble the work piece. 20. The robotic system of claim 19, wherein the end effector is configured to grip a part, and wherein the system controller is configured to selectively translate the at least one foot of the at least two feet to rotate the part relative to the work piece and translate the part toward the work piece to assemble the part to the work piece.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to robots, robotic systems, and related methods.",
                    "STATE_OF_THE_ART": "Modern automated manufacturing facilities commonly utilize kinematic robots to transport, manipulate, and/or assemble work pieces and/or components thereof. Such a robot may be characterized by a number of degrees of freedom DOF through which a component of the robot may be moved. For example, a 6 degree of freedom 6 DOF robot may be capable of moving an end effector mounted on the robot through three translational degrees of freedom , X, Y, and Z as well as through three rotational degrees of freedom , roll, pitch, and yaw. In addition, a robot may be characterized by a work envelope that describes a set of all locations and orientations accessible by the robot. It is generally desirable that a kinematic robot be capable of achieving full 6 DOF motion over a large work envelope while limiting the total size and/or footprint of the robot. Serial robots generally include a plurality of independently controllable link elements connected in series. While serial robots may allow for motion with up to 6 DOF as well as a large work envelope, their speed and precision are limited. In particular, as a consequence of mounting the link elements in series, the errors of the individual links are compounded, requiring large link elements with extremely fine calibration to achieve end effector accuracy. Consequently, the large mass of the link elements limits the speed with which the serial robot may be manipulated. Alternatively, parallel robots generally include a plurality of independently controllable link elements connected in parallel, such that the errors of each link element are averaged rather than compounded. However, current designs for parallel robots generally require a large footprint relative to their work envelope and/or achieve full 6 DOF motion only when they include heavy wrist elements added in series with the parallel link elements.",
                    "SUMMARY": [
                        "Parallel kinematic robots for moving relative to a surface, robotic systems including the same, and associated methods are disclosed. A robot includes a body, at least two legs, and at least two feet. Each leg of the at least two legs has a proximal end region and a distal end region, wherein the proximal end region of each leg is operatively coupled to the body at a respective body joint with one rotational degree of freedom. Each foot of the at least two feet is operatively coupled to the distal end region of a respective leg of the at least two legs at a respective foot joint comprising two rotational degrees of freedom. Each foot is configured to be selectively, independently, and motively translated relative to the surface with two degrees of translational freedom. A method of operating a robot includes selectively, independently, and motively translating at least one foot of the at least two feet of the robot to operatively move the body of the robot with six degrees of freedom. A robotic system includes one or more robots and a surface along which the one or more robots are positioned to move. A method of operating a robotic system includes selectively, independently, and motively translating at least one foot of the one or more robots to operatively move the respective bodies with six degrees of freedom.",
                        "1 is a schematic illustration representing robots according to the present disclosure. 2 is a perspective view of an example robot according to the present disclosure. 3 is another perspective view of the example robot of 2. 4 is another perspective view of the example robot of 2. 5 is a schematic illustration representing robotic systems according to the present disclosure. 6 is another schematic illustration representing robotic systems according to the present disclosure. 7 is a schematic illustration representing a robotic system according to the present disclosure. 8 is a schematic illustration representing a robotic system according to the present disclosure. 9 is a schematic illustration representing a robotic system according to the present disclosure. 10 is a schematic illustration representing a robotic system according to the present disclosure. 11 is a flowchart schematically representing methods of operating robots and robotic systems according to the present disclosure."
                    ],
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/37/473/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9616420269012451,
                    "F_SPEC_PARAMS": [
                        "speed",
                        "precision"
                    ],
                    "S_SPEC_PARAMS": [
                        "errors of the individual links are compounded,",
                        "fine calibration",
                        "end effector accuracy",
                        "speed",
                        "require a large footprint",
                        "achieve full 6 DOF motion"
                    ],
                    "A_PARAMS": [
                        "large mass of the link elements"
                    ],
                    "F_SENTS": [
                        "While serial robots may allow for motion with up to 6 DOF as well as a large work envelope, their speed and precision are limited."
                    ],
                    "S_SENTS": [
                        "In particular, as a consequence of mounting the link elements in series, the errors of the individual links are compounded, requiring large link elements with extremely fine calibration to achieve end effector accuracy.",
                        "Consequently, the large mass of the link elements limits the speed with which the serial robot may be manipulated.",
                        "However, current designs for parallel robots generally require a large footprint relative to their work envelope and/or achieve full 6 DOF motion only when they include heavy wrist elements added in series with the parallel link elements."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Speed",
                        "Accuracy of Measurement"
                    ],
                    "F_SIM_SCORE": 0.8230582475662231,
                    "S_TRIZ_PARAMS": [
                        "Reliability",
                        "Accuracy of Measurement",
                        "Speed",
                        "Productivity",
                        "Length of Moving Object"
                    ],
                    "S_SIM_SCORE": 0.5640796422958374,
                    "GLOBAL_SCORE": 1.7123538289751326
                },
                "sort": [
                    1.7123538
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11285618-20220329",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11285618-20220329",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-09-13",
                    "PUBLICATION_DATE": "2022-03-29",
                    "INVENTORS": [
                        "Donghyeong Lee"
                    ],
                    "APPLICANTS": [
                        "LG ELECTRONICS INC.    ( Seoul , KR )"
                    ],
                    "INVENTION_TITLE": "Grip apparatus and robot having the same",
                    "DOMAIN": "B25J 150023",
                    "ABSTRACT": "A grip apparatus is provided. The grip apparatus includes: a housing formed therein with an accommodation space opened downward to accommodate an object; an elastic membrane provided in the accommodation space and surrounding the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastening member.",
                    "CLAIMS": "1. A grip apparatus comprising: a housing opened downward and formed therein with an accommodation space to accommodate an object; an elastic membrane provided in the accommodation space to surround the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastening member, wherein the fastening member includes a belt or a wire. 2. The grip apparatus according to claim 1, wherein the housing is formed therein with a through-hole through which the fastening member passes, in which the through-hole is connected to a space between the inner periphery of the housing and the elastic membrane. 3. The grip apparatus according to claim 2, wherein the through-hole includes a pair of through-holes spaced apart from each other in a horizontal direction, and a distance between the pair of through-holes is shorter than a diameter of the accommodation space. 4. A grip apparatus comprising; a housing opened downward and formed therein with an accommodation space to accommodate an obi eel; an elastic membrane provided in the accommodation space to surround the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastening member, wherein the winder includes: a roller rotated about a vertical axis and wound by the fastening member; and a motor configured to rotate the roller. 5. The grip apparatus of claim 4, further comprising a torque sensor configured to sense a torque load of the motor, wherein the motor is stopped when a sensed value of the torque sensor reaches a predetermined set torque. 6. A grip apparatus comprising: a housing opened downward and formed therein with an accommodation space to accommodate an object; an elastic membrane provided in the accommodation space to surround the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastenin member, wherein the elastic membrane includes: an upper fixing portion fixed to the housing; a lower fixing portion fixed to the housing and spaced downward from the upper fixing portion; and an elastic portion positioned between the upper fixing portion and the lower fixing portion, and wherein the fastening member faces the elastic portion with respect to a horizontal direction. 7. A grip apparatus comprising: a housing opened downward and formed therein with an accommodation space to accommodate an object; an elastic membrane provided in the accommodation space to surround the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastening member, wherein a plurality of protrusions are formed on an inner surface of the elastic membrane. 8. A robot comprising: a manipulator; a housing provided in the manipulator and formed therein with an accommodation space opened downward to accommodate an object; an elastic membrane provided in the accommodation space to surround the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastening member, wherein the robot further comprises a controller configured to: control the manipulator so that the housing descends while the accommodation space is positioned above the object, and control the winder so that the fastening member is wound while the object is accommodated in the accommodation space. 9. The robot according to claim 8, wherein the fastening member includes a belt or a wire. 10. The robot according to claim 8, wherein the winder includes: a roller rotated about a vertical axis and wound b the fastening member; and a motor configured to rotate the roller. 11. The robot according to claim 10, further comprising a torque sensor configured to sense a torque load of the motor, wherein the controller stops the motor when a sensed value of the torque sensor reaches a predetermined set torque. 12. The robot according to claim 8, wherein the elastic membrane includes: an upper fixing portion fixed to the housing; a lower fixing portion fixed to the housing and spaced downward from the upper fixing portion; and an elastic portion positioned between the upper fixing portion and the lower fixing portion, and wherein the fastening member faces the elastic portion with respect to a horizontal direction. 13. The robot according to claim 8, wherein the housing is formed therein with a through-hole through which the fastening member passes, in which the through-hole is connected to a space between the inner periphery of the housing and the elastic membrane. 14. The robot according to claim 13, wherein the through-hole includes a pair of through-holes spaced apart from each other in a horizontal direction, and a distance between the pair of through-holes is shorter than a diameter of the accommodation space. 15. The robot according to claim 8, wherein a plurality of protrusions are formed on an inner surface of the elastic membrane. 16. A robot comprising: a manipulator; a housing provided in the manipulator and formed therein with an accommodation space opened downward to accommodate an object; an airbag provided at an inner periphery of the housing to surround the object; and a pneumatic unit configured to inject air into the airbag, wherein the robot further comprises a controller configured to: control the manipulator so that the housing descends while the accommodation space is positioned above the object, and control the pneumatic unit so that the airbag inflates while the object is accommodated in the accommodation space. 17. The robot according to claim 16, wherein the pneumatic unit includes: an air pump; and an air channel configured to connect the air pump to the airbag.",
                    "STATE_OF_THE_ART": "The present disclosure relates to a grip apparatus and a robot having the same to grip an object quickly and accurately. Robots are generally used to perform various tasks such as welding, assembly or painting at industrial manufacturing sites. In addition, the robots have been gradually expanding an application area all over industries and service fields including a personal service area to provide general services around human life, and a specialized service area to provide specialized services such as medical services. In particular, a picking robot for picking up an object has been actively developed. The picking robot requires a gripper capable of quickly and accurately picking atypical objects, that is, objects having various shapes, sizes, and materials. The gripper is classified into a mechanical gripper having a plurality of fingers driven by hydraulic or pneumatic pressure to pick up an object mechanically, and a vacuum gripper capable of picking up the object by generating a vacuum on an interface with the object. In addition, an electro-adhesive gripper is also known in which the object adheres thereto using an electrostatic force generated when a current flows through a conductor. However, the conventional mechanical gripper and vacuum gripper are inefficient to be used for picking up atypical objects due to imitations in picking up the objects, which have various sizes and shapes, at proper pressure without damage. In addition, the conventional electro-adhesive gripper requires a large contact area to pick up heavy objects and has to be applied with a large voltage, so there is a limitation in picking up atypical objects.",
                    "SUMMARY": [
                        "Embodiments provide a grip apparatus and a robot having the same to quickly and accurately grip objects having various sizes and shapes without damage. The gripper according to the embodiments includes: a housing formed therein with an accommodation space opened downward to accommodate an object; an elastic membrane provided in the accommodation space and surrounding the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastening member. The fastening member may be a belt or a wire. The winder may include: a roller rotated about a vertical axis and wound by the fastening member; and a motor configured to rotate the roller. The gripper according to the embodiments may further include a torque sensor configured to sense a torque load of the motor. The motor may be stopped when a sensed value of the torque sensor reaches a predetermined set torque. The elastic membrane may include: an upper fixing portion fixed to the housing; a lower fixing portion fixed to the housing and spaced downward from the upper fixing portion; and an elastic portion positioned between the upper fixing portion and the lower fixing portion. The fastening member may face the elastic portion with respect to a horizontal direction. The housing may be formed therein with a through-hole through which the fastening member passes, in which the through-hole is connected to a space between the inner periphery of the housing and the elastic membrane. The through-hole may include a pair of through-holes spaced apart from each other in a horizontal direction, and a distance between the pair of through-holes may be shorter than a diameter of the accommodation space. A plurality of protrusions may be formed on an inner surface of the elastic membrane. The robot according to the embodiments includes: a manipulator; a housing provided in the manipulator and formed therein with an accommodation space opened downward to accommodate an object; an elastic membrane provided in the accommodation space and surrounding the object; a fastening member at least partially positioned between an inner periphery of the housing and the elastic membrane to fasten the elastic membrane; and a winder configured to pull the fastening member by winding the fastening member. The fastening member may be a belt or a wire. The robot according to the embodiments may further include a controller configured to control the manipulator so that the housing descends while the accommodation space is positioned above the object, and control the winder so that the fastening member is wound while the object is accommodated in the accommodation space. The winder may include: a roller rotated about a vertical axis and wound by the fastening member; and a motor configured to rotate the roller. The robot according to the embodiments may further include a torque sensor configured to sense a torque load of the motor. The controller may stop the motor when a sensed value of the torque sensor reaches a predetermined set torque. The elastic membrane may include: an upper fixing portion fixed to the housing; a lower fixing portion fixed to the housing and spaced downward from the upper fixing portion; and an elastic portion positioned between the upper fixing portion and the lower fixing portion. The fastening member may face the elastic portion with respect to a horizontal direction. The housing may be formed therein with a through-hole through which the fastening member passes, in which the through-hole is connected to a space between the inner periphery of the housing and the elastic membrane. The through-hole may include a pair of through-holes spaced apart from each other in a horizontal direction, and a distance between the pair of through-holes may be shorter than a diameter of the accommodation space. A plurality of protrusions may be formed on an inner surface of the elastic membrane. The robot according to the embodiments includes: a manipulator; a housing provided in the manipulator and formed therein with an accommodation space opened downward to accommodate an object; an airbag provided at an inner periphery of the housing and surrounding the object; and a pneumatic unit configured to inject air into the airbag. The robot according to the embodiments may further include a controller configured to control the manipulator so that the housing descends while the accommodation space is positioned above the object, and control the pneumatic so that the airbag inflates while the object is accommodated in the accommodation space. The pneumatic unit may include: an air pump; and an air channel configured to connect the air pump to the airbag. The details of one or more embodiments are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.",
                        "1 illustrates an AI device 100 including a robot according to an embodiment of the present invention. 2 illustrates an AI server 200 connected to a robot according to an embodiment of the present invention. 3 illustrates an AI system 1 according to an embodiment of the present invention. 4a and 4b are views illustrating a general configuration and operation of the robot according to the embodiments. 5 is a view showing a gripper according to one embodiment. 6a and 6b are sectional views taken along line A-A of the gripper shown in 5. 7a and 7b are sectional views taken along line B-B of the gripper shown in 5. 8a and 8b are sectional views showing a gripper according to another embodiment."
                    ],
                    "DESCRIPTION": "Hereinafter, specific embodiments will be described in detail with reference to the accompanying drawings. &lt;Robot&gt;A robot may refer to a machine that automatically processes or operates a given task by its own ability. In particular, a robot having a function of recognizing an environment and performing a self-determination operation may be referred to as an intelligent robot. Robots may be classified into industrial robots, medical robots, home robots, military robots, and the like according to the use purpose or field. The robot includes a driving unit may include an actuator or a motor and may perform various physical operations such as moving a robot joint. In addition, a movable robot may include a wheel, a brake, a propeller, and the like in a driving unit, and may travel on the ground through the driving unit or fly in the air. &lt;Artificial Intelligence AI&gt;Artificial intelligence refers to the field of studying artificial intelligence or methodology for making artificial intelligence, and machine learning refers to the field of defining various issues dealt with in the field of artificial intelligence and studying methodology for solving the various issues. Machine learning is defined as an algorithm that enhances the performance of a certain task through a steady experience with the certain task. An artificial neural network ANN is a model used in machine learning and may mean a whole model of problem-solving ability which is composed of artificial neurons nodes that form a network by synaptic connections. The artificial neural network can be defined by a connection pattern between neurons in different layers, a learning process for updating model parameters, and an activation function for generating an output value. The artificial neural network may include an input layer, an output layer, and optionally one or more hidden layers. Each layer includes one or more neurons, and the artificial neural network may include a synapse that links neurons to neurons. In the artificial neural network, each neuron may output the function value of the activation function for input signals, weights, and deflections input through the synapse. Model parameters refer to parameters determined through learning and include a weight value of synaptic connection and deflection of neurons. A hyperparameter means a parameter to be set in the machine learning algorithm before learning, and includes a learning rate, a repetition number, a mini batch size, and an initialization function. The purpose of the learning of the artificial neural network may be to determine the model parameters that minimize a loss function. The loss function may be used as an index to determine optimal model parameters in the learning process of the artificial neural network. Machine learning may be classified into supervised learning, unsupervised learning, and reinforcement learning according to a learning method. The supervised learning may refer to a method of learning an artificial neural network in a state in which a label for learning data is given, and the label may mean the correct answer or result value that the artificial neural network must infer when the learning data is input to the artificial neural network. The unsupervised learning may refer to a method of learning an artificial neural network in a state in which a label for learning data is not given. The reinforcement learning may refer to a learning method in which an agent defined in a certain environment learns to select a behavior or a behavior sequence that maximizes cumulative compensation in each state. Machine learning, which is implemented as a deep neural network DNN including a plurality of hidden layers among artificial neural networks, is also referred to as deep learning, and the deep learning is part of machine learning. In the following, machine learning is used to mean deep learning. 1 illustrates an AI device 100 including a robot according to an embodiment of the present invention. The AI device 100 may be implemented by a stationary device or a mobile device, such as a TV, a projector, a mobile phone, a smartphone, a desktop computer, a notebook, a digital broadcasting terminal, a personal digital assistant PDA, a portable multimedia player PMP, a navigation device, a tablet PC, a wearable device, a set-top box STB, a DMB receiver, a radio, a washing machine, a refrigerator, a desktop computer, a digital signage, a robot, a vehicle, and the like. Referring to 1, the AI device 100 may include a communication unit 110, an input unit 120, a learning processor 130, a sensing unit 140, an output unit 150, a memory 170, and a processor 180. The communication unit 110 may transmit and receive data to and from external devices such as other AI devices 100a to 100e and the AI server 200 by using wire/wireless communication technology. For example, the communication unit 110 may transmit and receive sensor information, a user input, a learning model, and a control signal to and from external devices. The communication technology used by the communication unit 110 includes GSM Global System for Mobile communication, CDMA Code Division Multi Access, LTE Long Term Evolution, 5G, WLAN Wireless LAN, Wi-Fi Wireless-Fidelity, Bluetooth, RFID Radio Frequency Identification, Infrared Data Association IrDA, ZigBee, NFC Near Field Communication, and the like. The input unit 120 may acquire various kinds of data. At this time, the input unit 120 may include a camera for inputting a video signal, a microphone for receiving an audio signal, and a user input unit for receiving information from a user. The camera or the microphone may be treated as a sensor, and the signal acquired from the camera or the microphone may be referred to as sensing data or sensor information. The input unit 120 may acquire a learning data for model learning and an input data to be used when an output is acquired by using learning model. The input unit 120 may acquire raw input data. In this case, the processor 180 or the learning processor 130 may extract an input feature by preprocessing the input data. The learning processor 130 may learn a model composed of an artificial neural network by using learning data. The learned artificial neural network may be referred to as a learning model. The learning model may be used to an infer result value for new input data rather than learning data, and the inferred value may be used as a basis for determination to perform a certain operation. At this time, the learning processor 130 may perform AI processing together with the learning processor 240 of the AI server 200. At this time, the learning processor 130 may include a memory integrated or implemented in the AI device 100. Alternatively, the learning processor 130 may be implemented by using the memory 170, an external memory directly connected to the AI device 100, or a memory held in an external device. The sensing unit 140 may acquire at least one of internal information about the AI device 100, ambient environment information about the AI device 100, and user information by using various sensors. Examples of the sensors included in the sensing unit 140 may include a proximity sensor, an illuminance sensor, an acceleration sensor, a magnetic sensor, a gyro sensor, an inertial sensor, an RGB sensor, an IR sensor, a fingerprint recognition sensor, an ultrasonic sensor, an optical sensor, a microphone, a lidar, and a radar. The output unit 150 may generate an output related to a visual sense, an auditory sense, or a haptic sense. At this time, the output unit 150 may include a display unit for outputting time information, a speaker for outputting auditory information, and a haptic module for outputting haptic information. The memory 170 may store data that supports various functions of the AI device 100. For example, the memory 170 may store input data acquired by the input unit 120, learning data, a learning model, a learning history, and the like. The processor 180 may determine at least one executable operation of the AI device 100 based on information determined or generated by using a data analysis algorithm or a machine learning algorithm. The processor 180 may control the components of the AI device 100 to execute the determined operation. To this end, the processor 180 may request, search, receive, or utilize data of the learning processor 130 or the memory 170. The processor 180 may control the components of the AI device 100 to execute the predicted operation or the operation determined to be desirable among the at least one executable operation. When the connection of an external device is required to perform the determined operation, the processor 180 may generate a control signal for controlling the external device and may transmit the generated control signal to the external device. The processor 180 may acquire intention information for the user input and may determine the user's requirements based on the acquired intention information. The processor 180 may acquire the intention information corresponding to the user input by using at least one of a speech to text STT engine for converting speech input into a text string or a natural language processing NLP engine for acquiring intention information of a natural language. At least one of the STT engine or the NLP engine may be configured as an artificial neural network, at least part of which is learned according to the machine learning algorithm. At least one of the STT engine or the NLP engine may be learned by the learning processor 130, may be learned by the learning processor 240 of the AI server 200, or may be learned by their distributed processing. The processor 180 may collect history information including the operation contents of the AI apparatus 100 or the user's feedback on the operation and may store the collected history information in the memory 170 or the learning processor 130 or transmit the collected history information to the external device such as the AI server 200. The collected history information may be used to update the learning model. The processor 180 may control at least part of the components of AI device 100 so as to drive an application program stored in memory 170. Furthermore, the processor 180 may operate two or more of the components included in the AI device 100 in combination so as to drive the application program. 2 illustrates an AI server 200 connected to a robot according to an embodiment of the present invention. Referring to 2, the AI server 200 may refer to a device that learns an artificial neural network by using a machine learning algorithm or uses a learned artificial neural network. The AI server 200 may include a plurality of servers to perform distributed processing, or may be defined as a 5G network. At this time, the AI server 200 may be included as a partial configuration of the AI device 100, and may perform at least part of the AI processing together. The AI server 200 may include a communication unit 210, a memory 230, a learning processor 240, a processor 260, and the like. The communication unit 210 can transmit and receive data to and from an external device such as the AI device 100. The memory 230 may include a model storage unit 231. The model storage unit 231 may store a learning or learned model or an artificial neural network 231a through the learning processor 240. The learning processor 240 may learn the artificial neural network 231a by using the learning data. The learning model may be used in a state of being mounted on the AI server 200 of the artificial neural network, or may be used in a state of being mounted on an external device such as the AI device 100. The learning model may be implemented in hardware, software, or a combination of hardware and software. If all or part of the learning models are implemented in software, one or more instructions that constitute the learning model may be stored in memory 230. The processor 260 may infer the result value for new input data by using the learning model and may generate a response or a control command based on the inferred result value. 3 illustrates an AI system 1 according to an embodiment of the present invention. Referring to 3, in the AI system 1, at least one of an AI server 200, a robot 100a, a self-driving vehicle 100b, an XR device 100c, a smartphone 100d, or a home appliance 100e is connected to a cloud network 10. The robot 100a, the self-driving vehicle 100b, the XR device 100c, the smartphone 100d, or the home appliance 100e, to which the AI technology is applied, may be referred to as AI devices 100a to 100e. The cloud network 10 may refer to a network that forms part of a cloud computing infrastructure or exists in a cloud computing infrastructure. The cloud network 10 may be configured by using a 3G network, a 4G or LTE network, or a 5G network. That is, the devices 100a to 100e and 200 configuring the AI system 1 may be connected to each other through the cloud network 10. In particular, each of the devices 100a to 100e and 200 may communicate with each other through a base station, but may directly communicate with each other without using a base station. The AI server 200 may include a server that performs AI processing and a server that performs operations on big data. The AI server 200 may be connected to at least one of the AI devices constituting the AI system 1, that is, the robot 100a, the self-driving vehicle 100b, the XR device 100c, the smartphone 100d, or the home appliance 100e through the cloud network 10, and may assist at least part of AI processing of the connected AI devices 100a to 100e. At this time, the AI server 200 may learn the artificial neural network according to the machine learning algorithm instead of the AI devices 100a to 100e, and may directly store the learning model or transmit the learning model to the AI devices 100a to 100e. At this time, the AI server 200 may receive input data from the AI devices 100a to 100e, may infer the result value for the received input data by using the learning model, may generate a response or a control command based on the inferred result value, and may transmit the response or the control command to the AI devices 100a to 100e. Alternatively, the AI devices 100a to 100e may infer the result value for the input data by directly using the learning model, and may generate the response or the control command based on the inference result. Hereinafter, various embodiments of the AI devices 100a to 100e to which the above-described technology is applied will be described. The AI devices 100a to 100e illustrated in 3 may be regarded as a specific embodiment of the AI device 100 illustrated in 1. &lt;AI+Robot&gt;The robot 100a, to which the AI technology is applied, may be implemented as a guide robot, a carrying robot, a cleaning robot, a wearable robot, an entertainment robot, a pet robot, an unmanned flying robot, or the like. The robot 100a may include a robot control module for controlling the operation, and the robot control module may refer to a software module or a chip implementing the software module by hardware. The robot 100a may acquire state information about the robot 100a by using sensor information acquired from various kinds of sensors, may detect recognize surrounding environment and objects, may generate map data, may determine the route and the travel plan, may determine the response to user interaction, or may determine the operation. The robot 100a may use the sensor information acquired from at least one sensor among the lidar, the radar, and the camera so as to determine the travel route and the travel plan. The robot 100a may perform the above-described operations by using the learning model composed of at least one artificial neural network. For example, the robot 100a may recognize the surrounding environment and the objects by using the learning model, and may determine the operation by using the recognized surrounding information or object information. The learning model may be learned directly from the robot 100a or may be learned from an external device such as the AI server 200. At this time, the robot 100a may perform the operation by generating the result by directly using the learning model, but the sensor information may be transmitted to the external device such as the AI server 200 and the generated result may be received to perform the operation. The robot 100a may use at least one of the map data, the object information detected from the sensor information, or the object information acquired from the external apparatus to determine the travel route and the travel plan, and may control the driving unit such that the robot 100a travels along the determined travel route and travel plan. The map data may include object identification information about various objects arranged in the space in which the robot 100a moves. For example, the map data may include object identification information about fixed objects such as walls and doors and movable objects such as pollen and desks. The object identification information may include a name, a type, a distance, and a position. In addition, the robot 100a may perform the operation or travel by controlling the driving unit based on the control/interaction of the user. At this time, the robot 100a may acquire the intention information of the interaction due to the user's operation or speech utterance, and may determine the response based on the acquired intention information, and may perform the operation. 4a and 4b are views illustrating general configurations and operations of the robot according to the embodiments. The robot according to the embodiments may denote the robot 100a described above. In addition, the robot according to the embodiments may be a scara robot. The robot according to the embodiments may include a base 3, a manipulator M, a gripper 20, and a controller C. The base 3 may be mounted on a floor or a structure to entirely support the robot. The manipulator M may be connected to the base 3, and perform a task within a predetermined radial range around the base 3. The manipulator M may include a plurality of arms and a plurality of actuators configured to actuate the arms. The configurations and types of the manipulator M are not limited. For example, the manipulator M may include a first arm M1 connected to the base 3 and rotated about a vertical axis, a second arm M2 connected to an end of the first arm M1 and rotated about the vertical axis, and an elevating member M3 provided at an end of the second arm M2 and moving up and down. A gripper 20 may be provided at a lower end of the manipulator M, more specifically the elevating member M3. Accordingly, the manipulator M may enable the gripper 20 to move up and down or move in the horizontal direction. The gripper 20 may grip an object D. More specifically, the gripper 20 may descend from above the object D toward the object D to grip the object. As shown in 4a, the object D may be conveyed on a conveyor belt 2 at a constant speed. As shown in 4b, when the object D reaches a predetermined position on the conveyor belt 2, the manipulator M may grip the object D by controlling the gripper 20. Then, the manipulator M may move the gripper 20 gripping the object D to a predetermined target point. When the gripper 20 reaches the target position, the gripper 20 may release the object D. An appearance of the gripper 20 may be formed by an outer housing 21, and the outer housing 21 may accommodate a housing 30, an elastic membrane 40, a fastening member 50, and a winder 60 described later. 5 is a view showing the gripper according to one embodiment. 6a and 6b are sectional views taken along line A-A of the gripper shown in 5. 7a and 7b are sectional views taken along line B-B of the gripper shown in 5. The gripper 20 according to the embodiments may include a housing 30, an elastic membrane 40, a fastening member 50, and a winder 60. The housing 30 may have a circular hollow cylinder shape. More specifically, the housing 30 may be formed with an accommodation space 31 for accommodating the object D, and the accommodation space 31 may have a hollow shape. In other words, the accommodation space 31 may be defined by an inner circumference 32 of the housing 30, and may be opened upward and downward. However, the embodiments are not limited thereto, and the accommodating space 31 may have a shape opened upward only and closed upward. The object D may be accommodated in the accommodation space 31. More specifically, the manipulator M see 4a and 4b may move the gripper 20 downward from above the object D, so that the object D may be accommodated in the accommodation space 31. The accommodation space 31 may be formed to have a diameter and a height sufficient to accommodate the object D to be gripped. For example, the object D to be gripped may be any one of a plurality of types having various sizes and shapes, and the accommodation space 31 may be formed to have a diameter and a height that can accommodate the object D having the largest size among the types. The diameter of the accommodation space 31 may denote an inner diameter of the housing 30. The housing 30 may have a through-hole 35 through which the fastening member 50 described later passes. The through-hole 35 may be a long hole elongated vertically. The through-hole 35 may be provided with a pair spaced apart in the horizontal direction. The pair of through-holes 35 may be parallel to each other, but are not limited thereto. The through-hole 35 may be formed through from an outer circumference of the housing 30 to the inner circumference 32. In other words, the through-hole 35 may communicate with the accommodation space 31. The elastic membrane 40 may be provided in the accommodation space 31 and surround the object D. The elastic membrane 40 may be provided on the inner circumference 32 of the housing 30. The elastic membrane 40 may cover the inner circumference 32 of the housing 30. An outer surface of the elastic membrane 40 may face the inner circumference 32 of the housing 31, and an inner surface of the elastic membrane 40 may face the object D or an upper side portion of the object D. A horizontal section of the elastic membrane 40 may have a ring shape. The elastic membrane 40 may have a predetermined height. The elastic membrane 40 may include a material having elasticity. The elastic membrane 40 may be elastically deformed in all directions. The elastic membrane 40 may be fastened by the fastening member 50 described later to wrap the object D, and thus the object D can be gripped. On the contrary, when the fastening member 50 loosens a force for fastening the elastic membrane 40, the elastic membrane 40 may be restored into the original shape by a restoring force, and thus the object D may be released. More specifically, the elastic membrane 40 may include an upper fixing portion 41, a lower fixing portion 42, and an elastic portion 43. The upper fixing portion 41 and the lower fixing portion 42 may be fixed to the housing 30. More specifically, the upper fixing portion 41 may be fixed to an upper portion of the inner circumference 32 of the housing 30 and/or a top surface of the housing 30. The lower fixing portion 42 may be fixed to a lower portion of the inner circumference 32 of the housing 30 and/or a bottom surface of the housing 30. In particular, the lower fixing portion 42 may be fixed to a lower end of the inner circumference 32 of the housing 30 and/or the bottom surface of the housing 30. Accordingly, the elastic membrane 40 can easily grip the object D even when the height of the object D is low. The upper fixing portion 41 and the lower fixing portion 42 may be elongated in the circumferential direction. The upper fixing portion 41 may be positioned above the lower fixing portion 42. The upper fixing portion 41 and the lower fixing portion 42 may be spaced vertically from each other. The upper fixing portion 41 may include an upper end of the elastic membrane 40 or be adjacent to the upper end of the elastic membrane 40. The lower fixing portion 42 may include a lower end of the elastic membrane 40 or be adjacent to the lower end of the elastic membrane 40. The elastic portion 43 may be positioned between the upper fixing portion 41 and the lower fixing portion 42. The elastic portion 43 may be positioned inside the fastening member 50, and may be stretched by fastening of the fastening member 50 so as to wrap and grip the object D. A plurality of protrusions 44 may be formed on the inner surface of the elastic membrane 40. More specifically, the protrusions 44 may be formed on an inner surface of the elastic portion 43. The protrusions 44 may include the same elastic material as the elastic membrane 40, and may be elastically deformed when pressed by the object D. The protrusions 44 may assist the grip of the object D by the elastic membrane 40. In other words, the protrusions 44 may enable the object D to be gripped more reliably. The fastening member 50 may fasten the elastic membrane 40. The fastening member 50 may be a belt or a wire. The fastening member 50 may be a single member or include a plurality of members disposed vertically. At least a part of the fastening member 50 may be positioned between the inner circumference 32 of the housing 30 and the elastic membrane 40. More specifically, at least a part of the fastening member 50 may be positioned between the inner circumference 32 of the housing 30 and an outer surface of the elastic portion 43 in the horizontal direction, and positioned between the upper fixing portion 41 and the lower fixing portion in the vertical direction. The fastening member 50 may be maintained in contact with the inner circumference 32 of the housing 30 at a normal state. When the fastening member 50 is wound by the winder 60 described later, the fastening member 50 may be spaced apart from the inner circumference 32 of the housing 30, and press and fasten the elastic membrane 40 inward. The fastening member 50 may pass through the through-hole 35 formed in the housing 30, and enter between the inner circumference 32 of the housing 30 and the elastic membrane 40. In other words, the through-hole 35 may be connected to a space between the inner circumference 32 of the housing 30 and the elastic membrane 40. More specifically, the fastening member 50 may pass through one of the pair of through-holes 35 from the outside of the housing 30, enter between the inner circumference 32 of the housing 30 and the elastic membrane 40, pass through the other through-hole 35 while wrapping more than a half of an outer circumference of the elastic membrane 40, and come out of the housing 30. In other words, both ends of the fastening member 50 may be positioned outside the housing 30. A distance L between the pair of through-holes 35 may be shorter than a diameter Dl of the accommodation space 31. Preferably, the distance L between the pair of through-holes 35 may be less than a half of the diameter Dl of the accommodation space 31. Accordingly, the fastening member 50 passing through the pair of through-holes 35 may sufficiently surround the outer circumference of the elastic membrane 40. The winder 60 may pull the fastening member 50 by winding the fastening member 50. The winder 60 may wind or loosen the fastening member 50. The winder 60 may be positioned outside the housing 30. More specifically, the winder 60 may include a roller on which the fastening member 50 is wound, and a motor 62 configured to rotate the roller 61The roller 61 may be elongated vertically. The roller 61 may be rotated about the vertical axis. When the roller 61 is rotated in one direction, the fastening member 50 may be wound on the roller 61, and when the roller 61 is rotated in the other direction, the fastening member 50 may be released from the roller 61. The roller 61 may include a pair of rollers, and both ends of the fastening member 50 may be connected to the pair of rollers 61, respectively. Rotation directions in the pair of rollers 61 may be the same or different from each other to wind the fastening member 50. The pair of rollers 61 may simultaneously wind or unwind the fastening member 50. Accordingly, the object D may be quickly gripped or released. However, the embodiments are not limited thereto, and one end of the fastening member 50 may be fixed to the housing 30 and only the other end may be connected to the roller 61. In this case, only a single through-hole 35 may be formed in the housing 30. The motor 62, as shown in 5, may be directly connected to the roller 61. However, the embodiments are not limited thereto, and the connection may be configured such that a rotational force of the motor 62 is transmitted to the roller 61 by a power transmission member such as a gear. A pair of motor 62 may be provided to rotate the pair of rollers 61, respectively. The motor 62 may be provided with a torque sensor 63 configured to sense a torque load of the motor 62. When a sensed value of the torque sensor 63 reaches a predetermined set torque, the motor 62 may be stopped. The set torque may denote a torque load applied to the motor 62 when the elastic membrane 40 stably grips the object D. More specifically, when the fastening member 50 fastens the elastic membrane 40, the elastic membrane 40 may come into contact with the object D and press the object D. The torque load applied on the motor 62 may become greater. Thus, the sensed value of the torque sensor 63 may be gradually increased. When the sensed value becomes greater sufficiently, it may be determined that the elastic membrane 40 stably grips the object D. Accordingly, the torque sensor 63 may easily and reliably determine whether the object D is gripped. Meanwhile, the motor 62 and the torque sensor 63 may be electrically connected to and communicate with the controller C see 4a and 4B described above. Accordingly, the controller C may control the gripper 20 to grip or release the object D by controlling the motor 62. In addition, the controller C may stop the motor 62 when the sensed value of the torque sensor 63 reaches the predetermined set torque. Hereinafter, operations of the gripper 20 and the robot according to the embodiments will be described. As shown in 4b, the controller C may control the manipulator M such that the housing 30 descends in a state where the accommodation space 31 of the housing 30 is positioned above the object D. Accordingly, as shown in 6a and 7a, the object D may be accommodated in the accommodation space 31. The controller C may control the winder 60 such that the fastening member 50 is wound in a state where the object D is accommodated in the accommodation space 31. Accordingly, as shown in 6b and 7b, the fastening member 50 may fasten the elastic membrane 40. Accordingly, the elastic membrane 40 may come into contact with the object D and press the object D. The elastic membrane 40 may be stretched according to a shape of the object D, and smoothly surround a surface of the object D. The fastening member 50 may fasten the elastic membrane 40 until the elastic membrane 40 stably grips the object D. The controller C may stop the motor 62 when the sensed value of the torque sensor 63 reaches the predetermined set torque. Thus, the fastening member 50 may maintain a state of fastening the elastic membrane 40 without being further wound on the roller 61. The elastic membrane 40 may also maintain the state of gripping the object D. Thus, the gripper 20 may complete gripping the object D. Then, the controller C may control the manipulator M to move the gripper 20 gripping the object D to a target position. When the gripper 20 reaches the target position, the controller C may control the winder 60 to loosen the fastening member 50. Accordingly, the fastening member 50 may stop fastening the elastic membrane 40, and the elastic membrane 40 may be unfastened by a restoring force. Accordingly, the elastic membrane 40 may be spaced apart from the object D, so that the object D may be released. 8a and 8b are sectional views showing a gripper according to another embodiment. The gripper 20 according to the embodiments may include a housing 30, an air bag 70, and a pneumatic unit 80. Descriptions of the housing 30 and the accommodation space 31 follow the above descriptions. The airbag 70 may be provided in the accommodation space 31, and surround the object D. The airbag 70 may be provided at the inner circumference 32 of the housing 30. The airbag 70 may cover the inner circumference 32 of the housing 30. The airbag 70 may be inflated by the pneumatic unit 80 described later so as to wrap the object D, thereby gripping the object D. On the contrary, the airbag 70 may be shrunk by the pneumatic unit 80 and thus the object D may be released. The airbag 70 may include a fixing part 70a fixed to the housing 30. The pneumatic unit 80 may adjust internal pressure of the airbag 70. The pneumatic unit 80 may inflate the airbag 70 by injecting air into the airbag 70, or deflate the air bag 70 by discharging the air from the airbag 70. More specifically, the pneumatic unit 80 may include an air pump 81 and an air channel 82 configured to connect the air pump 81 to the air bag 70. The air pump 81 may pump air in both directions. The air pump 81 may be positioned outside the housing 30. The air channel 82 may communicate with an inside of the airbag 70. The air channel 82 may be connected to the airbag through a through-hole formed through from the outer circumference to the inner circumference of the housing 30. Alternatively, the air channel 82 may be connected to the airbag 70 through an open top or bottom surface of the accommodation space 31. The air injected from the air pump 81 may be guided to the airbag 70 through the air channel 82. Alternatively, the air exhausted from the airbag 70 may be guided to the air pump 81 through the air channel 82. In other words, the air channel 82 may be a bi-directional channel through which the air flows bi-directionally. The air pump 81 may be electrically connected to and communicate with the controller C see 4a and 4b described above. Accordingly, the controller C may control the air pump 81 to allow the gripper 20 to grip or release the object D. Hereinafter, operations of the gripper 20 and the robot according to the embodiments will be described. As shown in 4b, the controller C may control the manipulator M such that the housing 30 descends in a state where the accommodation space 31 of the housing 30 is positioned above the object D. Thus, as shown in 8a, the object D may be accommodated in the accommodation space 31. The controller may control the pneumatic unit 80 to inject air into the airbag 70 in a state where the object D is accommodated in the accommodation space 31. Accordingly, as shown in 8b, the airbag 70 may be inflated. Accordingly, the airbag 70 may come into contact with the object D and press the object D. The pneumatic unit 80 may inject the air into the airbag 70 until the airbag 70 stably grips the object D. When the controller C stops the operation of the air pump 81, the airbag 70 may maintain a state of gripping the object D. Then, the controller C may control the manipulator M to move the gripper 20 gripping the object D to a target position. When the gripper 20 reaches the target position, the controller C may control the pneumatic unit 80 to discharge the air from the airbag 70. Accordingly, the airbag 70 may be deflated. Accordingly, the airbag 70 may be spaced apart from the object D, so that the object D may be released. According to the embodiments objects having various sizes and shapes can be gripped quickly and accurately without damage. In addition, the torque sensor can easily and reliably determine whether the object is gripped. In addition, a distance between the pair of through-holes may be shorter than a diameter of the accommodation space, so that the fastening member passing through the through-hole can sufficiently surround an outer circumference of the elastic membrane. In addition, a plurality of protrusions may be formed on the inner surface of the elastic membrane, so that the elastic membrane can grip the object more stably. The above descriptions are merely illustrative of the technical idea of the present disclosure, and it will be apparent that a person having ordinary skill in the art may carry out various deformations and modifications within the scope without departing from inherent features of the present disclosure. Accordingly, the embodiments disclosed in the present disclosure are intended to not limit but illustrate the technical idea of the present disclosure, so the scope of the technical idea of the present disclosure is not limited by those embodiments. The scope of the present disclosure should be understood according to the following claims, and all technical ideas within the scope of equivalents should be construed as falling within the scope of the present disclosure.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/18/856/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9957767128944397,
                    "F_SPEC_PARAMS": [
                        "quickly",
                        "accurately picking atypical objects,",
                        "sizes,"
                    ],
                    "S_SPEC_PARAMS": [
                        "inefficient",
                        "damage"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "The picking robot requires a gripper capable of quickly and accurately picking atypical objects, that is, objects having various shapes, sizes, and materials."
                    ],
                    "S_SENTS": [
                        "However, the conventional mechanical gripper and vacuum gripper are inefficient to be used for picking up atypical objects due to imitations in picking up the objects, which have various sizes and shapes, at proper pressure without damage."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Speed",
                        "Shape"
                    ],
                    "F_SIM_SCORE": 0.613994836807251,
                    "S_TRIZ_PARAMS": [
                        "Waste of Energy",
                        "Harmful Factors Acting on Object"
                    ],
                    "S_SIM_SCORE": 0.6167685389518738,
                    "GLOBAL_SCORE": 1.7111584007740022
                },
                "sort": [
                    1.7111584
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11234883-20220201",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11234883-20220201",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-08-13",
                    "PUBLICATION_DATE": "2022-02-01",
                    "INVENTORS": [
                        "Dwight Meglan",
                        "Samuel Cordero"
                    ],
                    "APPLICANTS": [
                        "Covidien LP    ( Mansfield , US )"
                    ],
                    "INVENTION_TITLE": "Operating table for robotic surgical systems",
                    "DOMAIN": "A61G 1308",
                    "ABSTRACT": "An actuating system including at least one actuation section that includes a plurality of actuating supports. Each of the actuating supports includes a shaft assembly and configured to be actuated individually. The shaft assembly of each of the plurality of actuating supports is configured to translate between a first position and a second position. In the first position an actuated actuating support is aligned with the remaining of the plurality of actuating supports and in the second position the actuated actuating support is misaligned with the remaining of the plurality of actuating supports.",
                    "CLAIMS": "1. A method of positioning a patient during a medical procedure comprising: determining a desired initial position of the patient upon an actuating operating table system based upon parameters of the patient; positioning the patient upon the actuating operating table system; actuating an actuating system of the actuating operating table system to position the patient in the initial position; monitoring the initial position of the patient in relation to a surgical instrument; and actuating the actuating system to maintain the initial position of the patient in relation to the surgical instrument. 2. The method of positioning a patient during a medical procedure of claim 1, wherein actuating the actuating system of the operating table includes actuating at least one actuating support of the actuating system. 3. The method of positioning a patient during a medical procedure of claim 2, further comprising transitioning the at least one actuating support of the actuating system between a first position and a second position. 4. The method of positioning a patient during a medical procedure of claim 1, further comprising simultaneously moving the surgical instrument relative to the patient and actuating the actuating system. 5. The method of positioning a patient during a medical procedure of claim 1, further comprising monitoring the actuating system. 6. A method of positioning a patient during a medical procedure comprising: determining a desired initial position of the patient upon an actuating operating table system based upon parameters of the patient, the actuating operating table system including: at least one actuating section; and a plurality of actuating supports, wherein each of the plurality of supports includes a shaft assembly; wherein each of the plurality of actuating supports is configured to be activated individually, the shaft assembly of each of the plurality of actuating supports is configured to translate between a first position and a second position, wherein when an activated actuating support of the plurality of actuating supports is in the first position, the shaft assembly of the activated actuating support is aligned with the remaining of the plurality of actuating supports, and when an activated actuating support of the plurality of actuating supports is in the second position, the shaft assembly of the activated actuating support is misaligned with the remaining of the plurality of actuating supports; positioning the patient upon the actuating operating table system; actuating a select number of the plurality of actuating supports to position the patient in the initial position; monitoring the initial position of the patient in relation to a surgical instrument; and actuating the plurality of actuating supports of the actuating system to maintain the initial position of the patient in relation to the surgical instrument. 7. The method of positioning a patient during a medical procedure of claim 6, further comprising: actuating a select number of the plurality of actuating supports to maintain the initial position of the patient in relation to the surgical instrument. 8. The method of positioning a patient during a medical procedure of claim 6, wherein actuating the actuating operating table system includes actuating at least one actuating support of the actuating system. 9. The method of positioning a patient during a medical procedure of claim 8, further comprising transitioning the at least one actuating support of the actuating operating table system between a first position and a second position. 10. The method of positioning a patient during a medical procedure of claim 6, further comprising simultaneously moving the surgical instrument, and moving the patient by actuating the plurality of actuating supports of the actuating operating table system. 11. The method of positioning a patient during a medical procedure of claim 6, further comprising monitoring the actuating operating table system. 12. A method of positioning a patient during a medical procedure comprising: positioning the patient upon an actuating operating table system; actuating an actuating system of the actuating operating table system to position the patient in an initial position; monitoring the initial position of the patient in relation to a surgical instrument; and actuating the actuating system to maintain the initial position of the patient in relation to the surgical instrument. 13. The method of positioning a patient during a medical procedure of claim 12, wherein actuating the actuating system of the operating table includes actuating at least one actuating support of the actuating system. 14. The method of positioning a patient during a medical procedure of claim 12, further comprising transitioning the at least one actuating support of the actuating system between a first position and a second position. 15. The method of positioning a patient during a medical procedure of claim 12, further comprising simultaneously moving the surgical instrument, and moving the patient by actuating the plurality of actuating supports of the actuating system. 16. The method of positioning a patient during a medical procedure of claim 12, further comprising monitoring the actuating system.",
                    "STATE_OF_THE_ART": "Traditionally, the mobility of a patient during a medical procedure is limited, if not completely restricted, due to current designs of operating tables. This limitation can cause unnecessary hiccups throughout the medical procedure and potentially limit the range of use for any employed surgical instrument. Additionally, current operating tables prevent the patient from becoming an integral component of the medical procedure. Having an operating table with the capability of constant relative movement of the patient towards the employed surgical device and/or general repositioning would allow the patient to become an integral component of the medical procedure and streamline the entire procedure for the medical staff. Currently, operating tables can generally be divided into general operating tables and specialty operating tables. Typically, general operating tables provide limited adjustable support to a patient, for example the height and slope of the table may be adjusted by a clinician. This requires manual adjustment of a patient in situations where a targeted area needs to be raised above the general operating table. In these situations, a member of the medical staff will typically position an object, such as a pillow, beneath the targeted area of the patient. However, this is not an ideal method of adjustment. In comparison, specialty operating tables provide a larger range of adjustments. For example, individual sections of a specialty operating table may be adjusted, that is, entire sections of the specialty operating table are adjustable in relation thereto. Although specialty operating tables provide a larger range of possible adjustments, these tables still lack the ability to constantly move the patient relative to the employed surgical instrument. Thus, a need still remains for an operating table that provides constant movement of the patient relative to the surgical instrument.",
                    "SUMMARY": [
                        "The present disclosure relates generally to a surgical system. More particularly, the present disclosure relates to a robotic surgical system including an actuating operating table system. An actuating system including at least one actuating section that includes a plurality of actuating supports. Each of the actuating supports includes a shaft assembly and configured to be activated individually. The shaft assembly of each of the plurality of actuating supports is configured to translate between a first position and a second position. In the first position an activated actuating support is aligned with the remaining of the plurality of actuating supports and in the second position the activated actuating support is misaligned with the remaining of the plurality of actuating supports. The shaft assembly of each of the plurality of actuating supports includes a head. In the first position the head of the activated actuating support is aligned with the remaining of the plurality of actuating supports and in the second position the head of the activated actuating support is misaligned with the remaining of the plurality of actuating supports. Each shaft assembly also includes an inflatable chamber, wherein the inflatable chamber inflates and deflates moving the shaft assembly between the first position and second position. Each actuating support of the actuating system includes a base. Each base includes a sensor, a channel configured to receive fluid, and a valve positioned within the channel and electrically coupled to the sensor. The sensor is configured to receive a signal from an operating system which opens and closes the valve to permit ingress and egress of fluid into/from the channel. The actuating system is configured to simultaneously activate more than one of the plurality of actuating supports. The plurality of actuating supports is configured to reposition a patient relative to a surgical instrument. In one embodiment, each shaft assembly of the plurality of actuating supports is configured to translate to a third position, wherein the third position is in between the first position and the second position. In another embodiment, the at least one actuating section is configured to be coupled to an operating table. A method of positioning a patient during a medical procedure includes determining a desired initial position of the patient upon an actuating operating table system, positioning the patient upon the actuating operating table system, actuating an actuating system of the actuating operating table system to position the patient in the initial position, monitoring the initial position of the patient in relation to a surgical instrument, and actuating the actuating system to maintain the initial position of the patient in relation to the surgical instrument. The desired initial position of the patient is based upon the parameters of the patient. Actuating the actuating system of the actuating operating table system includes actuating at least one actuating support of the actuating system. The at least one actuating support of the actuating system is transitioned between a first position and a second position. The method also includes simultaneously moving the surgical device relative to the patient and actuating the actuating system. Additionally, the method further includes monitoring the actuating system.",
                        "The surgical system will be more fully appreciated as the same becomes better understood from the following detailed description when considered in connection with the following drawings, in which: 1 is a schematic illustration of a robotic surgical system including an actuating operating table in accordance with the present disclosure; 2A is a top view of the actuating operating table of 1; 2B is a side view of the actuating operating table of 1; 3 is a schematic diagram of an operating system which forms part of the robotic surgical system of 1 in accordance with the present disclosure; 3A is an illustration of a user interface presenting a view showing a section of the actuating operating table in accordance with the present disclosure; 4 is an perspective view of an actuating system of the actuating operating table of 2A; 4A is a cross-section of an actuating support of the actuating system of 4 as taken along section line 4A-4A in 4; 4B is a top view of an actuating support of the actuating system of 4; 5A is a side view of an actuating operating table with inactivated actuating supports in accordance with the present disclosure; and 5B is a side view of the actuating operating table of 5A with at least one activated actuating support."
                    ],
                    "DESCRIPTION": "Various embodiments will be described in detail with reference to the drawings, wherein like reference numerals identify similar or identical elements. As commonly known, the term clinician refers to a doctor, a nurse, or any other care provider and may include support personnel. Additionally, directional terms such as front, rear, upper, lower, top, bottom, and the like are used simply for convenience of description and are not intended to limit the disclosure attached hereto. In the following description, well-known functions or constructions are not described in detail to avoid obscuring the present disclosure in unnecessary detail. In general, the present disclosure relates to a robotic surgical system including an actuating operating table system to facilitate constant movement of a patient's body relative to a surgical instrument during a medical procedure, such that the patient becomes an integral component of the medical procedure. For example, the actuation of the actuating operating table system may manipulate a targeted area relative to a surgical instrument, such that a clinician gains improved access to the targeted area. Referring initially to 1, a surgical system, such as, for example, a robotic surgical system 1, generally includes one or more surgical robotic arms 2, 3, a control device 4, an operating console 5, and an actuating operating table system 100. Any of the surgical robotic arms 2, 3 may have a robotic surgical assembly 10 and an electromechanical surgical instrument 20 coupled thereto. In some embodiments, the robotic surgical assembly 10 may be removably attached to a slide rail 40 of one of the surgical robotic arms 2, 3. In certain embodiments, the robotic surgical assembly system 100 may be fixedly attached to the slide rail 40 of one of the surgical robotic arms 2, 3. The operating console 5 includes a display device 6, which is set up to display three-dimensional images; and manual input devices 7, 8, by means of which the clinician not shown, is able to telemanipulate the robotic arms 2, 3 in a first operating mode, as known in principle to a person skilled in the art. Each of the robotic arms 2, 3 may be composed of any number of members, which may be connected through joints. The robotic arms 2, 3 may be driven by electric drives not shown that are connected to the control device 4. The control device 4 , a computer is set up to activate the drives, for example, by means of a computer program, in such a way that the robotic arms 2, 3, the attached robotic surgical assembly 10, and thus the electromechanical surgical instrument 20 including the electromechanical end effector, not shown execute a desired movement according to a movement defined by means of the manual input device 7, 8. The control device 4 may also be set up in such a way that it regulates the movement of the robotic arms 2, 3 and/or of the drives. The robotic surgical system 1 is configured for use on a patient P positioned , lying on the actuating operating table system 100 to be treated in a minimally invasive manner by means of a surgical instrument, , the electromechanical surgical instruments 20. The robotic surgical system 1 may also include more than two robotic arms 2, 3, the additional robotic arms likewise connected to the control device 4 and telemanipulatable by means of the operating console 5. A surgical instrument, for example, the electromechanical surgical instrument 20 including the electromechanical end effector thereof, may also be attached to any additional robotic arms. For a detailed discussion of the construction and operation of a robotic surgical system, reference may be made to 8,828,023, filed on Nov. 3, 2011, entitled Medical Workstation, the entire content of which is incorporated by reference herein. Turning now to 2A, 2B, 3, 3A, 4, 4A, and 4B an embodiment of an actuating operating table system 100 includes an operating system 160, an actuating system 110, and an operating table 102. The operating system 160 of the actuating operating table system 100 provides a clinician with the ability to control the actuating system 110 of the actuating operating table system 100. The operating system 160 is a real-time operating system that permits the clinician to actuate the actuating system 110 of the actuating operating table system 100 throughout the entire medical procedure. Additionally, the operating system 160 provides visual representation of the actuating system 110 before, during, and after the medical procedure. The operating system 160 may be incorporated in the operating console 5 of the surgical system 1 or may be an individual unit. In embodiments where the operating system 160 is an individual unit, the operating system 160 may be a mobile hand-held device. The operating system 160 may include a memory 162, an application 164, a processor 166, a display 168, a network interface 170, an input device 172, and/or an output module 174. The memory 162 includes any non-transitory computer-readable storage media for storing data and/or software that is executable by the processor 166 and which controls the operation of the operating system 160. In an embodiment, the memory 162 may include one or more solid-state storage devices such as flash memory chips. The memory 162 may store the application 164. The application 164 provides the clinician the ability to actuate the actuating system 110 of the actuating operating table system 100. The application 164 may be one or more software programs stored in the memory 162 and executable by the processor 166 of the operating system 160. The application 164 is configured to send and receive information from the actuating system 110 of the actuating operating table system 100. Also, the application 164 may, when executed by the processor 166, cause the display 168 to present user interfaces, such as the user interface 180 illustrated in 3A. The processor 166 may be a general purpose processor, a specialized graphics processing unit GPU configured to perform specific graphics processing tasks while freeing up the general processor to perform other tasks, and/or number or combinations of such processors. The display 168 may be touch sensitive, voice activated, and/or motion activated, enabling the display 168 to serve as both an input and output device. The display 168 visually displays different parameters of the actuating system 110, which will be discussed in further detail below. The network interface 170 may be configured to connect to a network such as a local area network LAN consisting of a wired network and/or a wireless network, a wide area network WAN, a wireless mobile network, a Bluetooth network, and/or the internet. The operating system 160 may receive updates to its software, for example, the application 164, via the network interface 170. The input device 172 may be any device by means of which the clinician may interact with the operating system 160, such as, for example, a mouse, keyboard, foot pedal, touch screen, and/or voice interface. The output module 174 may include any connectivity port or bus, such as, for example, parallel ports, serial ports, universal serial busses USB, or any other similar connectivity port known to those skilled in the art. With continued reference to 2A, 4, 4A and 4B, the actuating system 110 includes actuating sections 112, and actuating supports 120. The actuating system 110 may include a first actuating section 112A, a second actuating section 112B, and a third actuating section 112C, for example. The first section 112A may be positioned proximate to the top or head of the operating table 102, while the third section 112C may be positioned proximate to the bottom or foot of the operating table 102. The second section 112B may be positioned between the first and second sections 112A, 112B. Each of the actuating section 112 of the actuating system 110 may take any appropriate shape or profile, for example, rectangular, circular, or freeform. Additionally, each of the actuating section 112 of the actuating system 110 may have similar dimensions to one another. The length of each of the actuating section 112 may range from 6 inches to 12 inches, and the width of each of the actuating section 112 may range from 2 inches to 6 inches. Each of the actuating sections 112 includes an upper layer 114. The upper layer 114 is configured to translate at least between a neutral position A 5A and to a raised positioned B 5B in reaction to the actuating supports 120. The clinician may select the dimensions of each actuating section 112 of the actuating system 110 and/or the number of actuating sections 112 based on the parameters of the patient and the medical procedure. In some embodiments, the actuating system 110 may include fewer actuating sections 112, for example a single actuating section not illustrated, and/or the actuating system 110 may include more actuating sections 112, for example, more than three actuating sections not illustrated. The clinician may considered a number of different patient parameters, for example the height and the weight of the patient. The patient's parameters and the parameters of the medical procedure may be uploaded and stored within the operating system 160 and displayed via user interface 180 3A. The clinician can access the patient's parameters and the parameters of the medical procedure throughout the entire duration of the medical procedure. In some embodiments, each actuating section 112 of the actuating system 110 may be integrally formed with the operating table 102. In other embodiments, each actuating section 112 of the actuating system 110 may be individually movable and attachable to a top surface of the operating table 102. Each actuating section 112 supports actuation points or plugs 118, as seen in 2A. The actuation points 118 are positioned upon the upper surface 114 of each actuating section 112 of the actuating system 110. The number of the actuation points 118 of each actuating section 112 and the layout of the actuation points 118 of each actuating section 112 may be selected based on the clinician's needs. The number and layout of the actuation points 118 correspond to the number of actuating supports 120, as described below. Each actuation point 118 may take any appropriate form, for example, a circle, an oval, and/or a square. Specifically referring to 4, 4A, and 4B, the actuating supports 120 each include a base 122 and a shaft assembly 134. Each base 122 includes a connecting protrusion 124 4B, a receiving groove 126 4B, a transceiver 128 4A housed within the base 122, a sensor 129 4A, a channel 130 defined therethrough, a valve 132 4A, a valve sensor 133 4A. Each base 122 is configured to connect with one another via the receiving groove 126 and the connecting protrusion 124. The receiving groove 126 is configured and adapted to receive the connecting protrusion 124. In one embodiment, the connecting protrusion 124 may be positioned within the receiving groove 126 by vertically aligning the connecting protrusion 124 of one of the base 122 with the receiving groove 126 of another one of the base 122 and vertically translating the connecting protrusion 124 of one of the base 122 in relations to the receiving groove 126 of the other base 122 , in the manner of a dovetail connection. The transceiver 128 housed within each base 122 is capable of receiving a signal from the operating system 160, and transmitting that signal to the valve sensor 133. The signal received by the transceiver 128 and sent to valve sensor 133, will actuate the valve sensor 133 thereby actuating support 120. The transceiver 128 may be any transceiver configured to receive a signal from the operating system 160 and transmit that signal to the valve sensor 133. The channel 130 is defined through each base 122 transverse to a vertical axis X of shaft assembly 134. The channel 130 is configured to receive a fluid, such as air, via a pneumatic pump not illustrated that may be connected to the base 122. The channel 130 is positioned relative to the inflatable chamber 146 of the shaft assembly 134, such that upon receiving a signal from the operating system 160, the fluid passing through the channel 130 will enter into the inflatable chamber 146 of the shaft assembly 134. The fluid passing through the channel 130 may enter within the inflatable chamber 146 of the shaft assembly 134 via the valve 132. The valve 132 is electrically coupled to the transceiver 128 via valve sensor 133. When the surgeon selects which actuating support 120 to actuate/activate, the operating system 160 will send a signal to the transceiver 128 of the selected actuating support 120 which in turn will send the signal to the valve sensor 133 opening and/or closing the valve 132 to allow the desired amount of fluid to enter within the inflatable chamber 146 of the shaft assembly 134, thus actuating the selected actuating support 120. The sensor 129 housed within each base 122 is capable of measuring the height of the shaft assembly 134. In one embodiment, sensor 129 is an ultrasound sensor capable of receiving reflected light or reflected laser not illustrated. Sensor 129 continually measures the height of the shaft assembly 134. Sensor 129 may be any sensor capable of measuring the fluctuating height of shaft assembly 134. Sensor 129 is configured to transmit the fluctuating height measurement of shaft assembly 134 to the operating system 160. Each base 122 is securable to the operating table 102 of the actuating operating table system 100. In some embodiments, each base 122 is integrally formed with the operating table 102 of the actuating operating table system 100. In other embodiments, the base 122 is releasably secured to the operating table 102, such that each base 122 may be rearranged or removed from the operating table 102. Each shaft assembly 134 includes a shaft 136 including compression rings 138 forming a sleeve or bellows, an inflatable chamber 146, a rigid member 140, and a head 148. The compression rings 138 may be integrally formed with the shaft 136. The compression rings 138 of the shaft 136 are spaced apart from one another, which allow each compression ring 138 to be compressed in relation to the other compression rings 138 of the shaft 136. The inflatable chamber 146 may be positioned at the bottom of each shaft assembly 134. The inflatable chamber 146 is configured to receive fluid from the pneumatic pump not illustrated via the channel 130 and the valve 132 of the base 122. Depending on amount of fluid received by the inflatable chamber 146 will either inflate or deflate. The rigid member 140 includes a first cylinder member 142 and a second cylinder member 144. The second cylinder member 144 is configured and dimensioned to fit within the first cylinder member 142, such that an outer diameter of the second cylinder member 144 is comparable to an inner diameter of the first cylinder member 142. The rigid member 140 is positioned on top of the inflatable chamber 146 so that the second cylinder member 144 transitions between a first position and a second position in response to the inflation and deflation of the inflatable chamber 146. The rigid member 140 provides limited support to the shaft assembly 134; rather the rigid member 140 provides guidance for the expansion of the shaft assembly 134. The head 148 of each shaft assembly 134 extends from the shaft 136 and may be integrally formed therewith. Each head 148 may be positioned adjacent to a lower surface 116 of each actuating section 112 of the actuating system 110. Each head 148 of the shaft assembly 134 may include at least a partially flat surface 150. The partially flat surface 150 may define a dent 152, which may be centrally defined therein. The dent 152 may be configured and dimensioned to receive a respective actuation point 118 therein. In this manner, each actuating support 120 will be centrally aligned with a corresponding actuation point 118. Additionally, the actuation point 118 secures the actuating sections 112 to the heads 148 of the shaft assemblies 134, for example, by a threaded connection or the like. Each shaft assembly 134 is configured to translate between at least a first position and a second position. In the first position, the compression rings 138, the rigid member 140, and the inflatable chamber 146 of the shaft assembly 134 are fully compressed/deflated and the actuation point 118 is positioned in a neutral position. Also in the first position, the head 148 of the shaft assembly 134 remains in contact with the lower surface 116 of the actuating section 112, with the upper surface 114 of the actuating section 112 remaining in a neutral position A 5A. As each shaft assembly 134 transitions between the first and the second positions, the compression rings 138, the rigid member 138, and the inflatable chamber 146 of the shaft assembly 134 become decompressed/deflated and/or extended/inflated. This decompression and/or extension in turn translates each actuation point 118 between the neutral position and a raised position. In the second position, the compression rings 138, the rigid member 140, and the inflatable chamber 146 will be fully decompressed/inflated and actuation point 118 positioned in a fully raised position. Also, the head 142 of the shaft assembly 134 remains in contact with the lower surface 116 of the actuating section 112. In this manner, the head 148 of the shaft assembly 134 raises the upper surface 114 of the actuating section 112, including the actuation point 118, to a raised positioned B 4B. Each actuating support 120 of the actuating system 110 may be individually actuated, such that the clinician may individual select which the actuating support 120 to actuate via the operating system 160. The operating table 102 is mounted to a base 104 2B. The base 104 of the operating table 102 is configured to adjust the height and slope of the operating table 102. Each actuating section 112 may be formed of any suitable biocompatible material. For example, the actuating support 120 may be stainless steel, cobalt alloys, and titanium alloys. The upper surface 114 of each actuating section 112 may be flexible biocompatible material. Moving to 5A and 5B, an embodiment of the actuating operating table system 100, shown in use, is illustrated. Initially, the patient's parameters will be gathered and entered into the operating system 160. With this information, the operating system 160 will calculate and provide instruction for an optimal placement, or a suggested placement, of the actuating operating table system 100 in relation to the remaining components of the robotic surgical system 1. Further, the operating system 160 will indicate an optimal or suggested initial position of the patient upon the actuating operating table system 100. Prior to positioning the patient upon the actuating operating table system 100, each actuating support 120 may be unactuated and positioned in the first positioned, , position A 5A. The clinician may reference the operating system 160 when positioning the patient upon the actuating operating table system 100 to ensure optimal initial positioning. The operating system 160 may provide a signal to the clinician when the patient is optimally positioned upon the actuating operating table system 100. The operating system 160 may also identify parameters of each actuating support 120 and/or the actuating system 110 as a whole throughout the entire procedure. The parameters displayed on the user interface 180 of the operating system 160 may include the percentage of inflation of each actuated/activate actuating support 120, which of the actuating support 120 is actuated, the height of each actuated actuating support 120, or any other parameter of the actuating system 110. After the patient is positioned upon the actuating operating table system 100, the clinician may actuate any of the actuating supports 120 5B. The clinician may select which the actuating support 120 to actuate based upon the patient's parameter, the specific medical procedure being performed, the surgical instrument being used and/or any combination thereof. The clinician can continually actuate each actuating support 120, translating the actuating supports 120 between the first position and second position or any position therebetween, , a third position based on the clinician's needs and/or progression of the surgical procedure and in relation with the surgical instrument 20. The clinician may monitor forces acting on the actuating supports 120 and adjust the position of the actuating supports 120 heights thereof to maintain a location or adjust a location of a patient or body part thereof, as needed or desired. The ability for constant readjustment of the patient's position allows improved access to the patient throughout the entire medical procedure. Further, it enhances the robotic surgical system 1 performance and allows the patient to become an integral component of the medical procedure. Upon completion of the medical procedure, the clinician may elect to keep the actuating supports 120 actuated and/or may elect to return the actuating supports 120 to the first position. Persons skilled in the art will understand that the structures and methods specifically described herein and shown in the accompanying figures are non-limiting exemplary embodiments, and that the description, disclosure, and figures should be construed merely as exemplary of particular embodiments. It is to be understood, therefore, that the present disclosure is not limited to the precise embodiments described, and that varies other changes and modifications may be effected by one skilled in the art without departing from the scope or spirit of the disclosure. Additionally, the elements and features shown or described in connection with certain embodiments may be combined with the elements and features of certain other embodiments without departing from the scope of the present disclosure, and that such modification and variations are also included within the scope of the present disclosure. Accordingly, the subject matter of the present disclosure is not limited by what has been particularly shown and described.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/83/348/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9826743006706238,
                    "F_SPEC_PARAMS": [
                        "mobility",
                        "unnecessary hiccups",
                        "range of use"
                    ],
                    "S_SPEC_PARAMS": [
                        "larger range of possible adjustments,",
                        "ability to constantly move the patient"
                    ],
                    "A_PARAMS": [
                        "designs of operating tables"
                    ],
                    "F_SENTS": [
                        "<s> Traditionally, the mobility of a patient during a medical procedure is limited, if not completely restricted, due to current designs of operating tables.",
                        "This limitation can cause unnecessary hiccups throughout the medical procedure and potentially limit the range of use for any employed surgical instrument.",
                        "Additionally, current operating tables prevent the patient from becoming an integral component of the medical procedure."
                    ],
                    "S_SENTS": [
                        "Although specialty operating tables provide a larger range of possible adjustments, these tables still lack the ability to constantly move the patient relative to the employed surgical instrument."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Speed",
                        "Convenience of Use"
                    ],
                    "F_SIM_SCORE": 0.6205787658691406,
                    "S_TRIZ_PARAMS": [
                        "Adaptability"
                    ],
                    "S_SIM_SCORE": 0.5605514049530029,
                    "GLOBAL_SCORE": 1.7065727194150289
                },
                "sort": [
                    1.7065728
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10932409-20210302",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10932409-20210302",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-11-07",
                    "PUBLICATION_DATE": "2021-03-02",
                    "INVENTORS": [
                        "Ryan Douglas Ingvalson",
                        "Jason Thomas Kraft",
                        "Alexander Steven Frick"
                    ],
                    "APPLICANTS": [
                        "THE TORO COMPANY    ( Bloomington , US )"
                    ],
                    "INVENTION_TITLE": "System and method for operating an autonomous robotic working machine within a travelling containment zone",
                    "DOMAIN": "A01D 34008",
                    "ABSTRACT": "Apparatus, systems, and methods for directing an autonomous robotic vehicle such as a lawn mower relative to a work region. In some embodiments, the vehicle travels in a random pattern within a travelling containment zone of a lesser size than the work region. The travelling containment zone may move or travel across the work region such that, over time, the travelling containment zone travels over most all of a working surface of the work region.",
                    "CLAIMS": "1. A method of operating an autonomous working vehicle within a predefined work region, the method comprising: defining, with an electronic controller associated with the working vehicle, a travelling containment zone that lies at least partially within the work region, the travelling containment zone defining a zone area that is less than an area of the work region; autonomously operating the working vehicle within the travelling containment zone; constraining a position of the working vehicle to be within the travelling containment zone; and moving the travelling containment zone across the work region while the working vehicle operates within the travelling containment zone, wherein moving the travelling containment zone comprises: adding a portion of the work region located adjacent a leading edge of the travelling containment zone to the zone area; and removing a portion of the zone area located adjacent a trailing edge of the travelling containment zone from the zone area. 2. The method of claim 1, wherein the working vehicle is a lawn mower. 3. The method of claim 1, wherein the work region comprises a grass surface of a property. 4. The method of claim 1, further comprising varying a shape of the travelling containment zone as the travelling containment zone moves across the work region. 5. The method of claim 1, further comprising moving the working vehicle in a random manner within the travelling containment zone. 6. The method of claim 1, further comprising controlling a steering angle and a ground speed of the working vehicle with the controller. 7. The method of claim 1, further comprising maintaining an initial position of the travelling containment zone for a period of time before moving the travelling containment zone across the work region. 8. The method of claim 1, further comprising estimating, with the controller, a time at which the working vehicle will have travelled over an entirety of the work region. 9. The method of claim 1, further comprising either: maintaining the zone area of the travelling containment zone constant as the travelling containment zone moves across the work region; or varying the zone area of the travelling containment zone as the travelling containment zone moves across the work region. 10. The method of claim 1, further comprising either: maintaining a speed of the working vehicle while operating the working vehicle within the travelling containment zone; or varying the speed of the working vehicle while operating the working vehicle within the travelling containment zone. 11. The method of claim 1, wherein moving the travelling containment zone comprises moving the travelling containment zone at either a constant rate or a variable rate. 12. A mowing system comprising an autonomously operating mower configured to cut grass within a work region as the mower travels about the work region, the system comprising: a chassis supported upon a grass surface by ground support members, wherein one or more of the ground support members comprises a drive member; a grass cutting element carried by the chassis; one or more motors configured to power the cutting element and the drive member; and an electronic controller configured to control an operation of the cutting element and a speed and a direction of the mower; wherein the controller is further configured to: define a travelling containment zone located at least partially within the work region, the travelling containment zone comprising a zone area that is less than an area of the work region; constrain operation of the mower to be within the travelling containment zone; and, while the mower is operating therein, move the travelling containment zone across the work region by advancing a leading edge and a trailing edge of the travelling containment zone. 13. The system of claim 12, wherein the controller maintains an initial position of the travelling containment zone for a period of time before the travelling containment zone travels across the work region. 14. The system of claim 12, wherein the mower further comprises a positioning system configured to estimate a position of the mower within the work region, the positioning system operatively connected to the controller. 15. The system of claim 12, wherein the system further comprises a base station located in or near the work region. 16. A method of operating an autonomous working vehicle within a predefined work region, the method comprising: defining, with an electronic controller associated with the working vehicle, a travelling containment zone that lies within the work region, wherein the work region bounds a first plurality of grid cells and the travelling containment zone bounds a second plurality of grid cells, the second plurality of grid cells being less than, and a subset of, the first plurality of grid cells; autonomously operating the working vehicle within the travelling containment zone; constraining a position of the working vehicle to be within the travelling containment zone; deciding, with the controller, a direction in which to advance a leading edge of the travelling containment zone; and moving the travelling containment zone across the work region while the working vehicle operates within the travelling containment zone, wherein moving the travelling containment zone comprises: adding grid cells located outside of, and adjacent to, the leading edge of the travelling containment zone to the travelling containment zone; and advancing a trailing edge of the travelling containment zone by removing grid cells located inside of the travelling containment zone adjacent to the trailing edge from the travelling containment zone. 17. The method of claim 16, wherein deciding the direction to advance the leading edge of the travelling containment zone comprises scoring two or more grid cells of the first plurality of grid cells, the two or more grid cells being outside of, and adjacent to, the leading edge of the travelling containment zone. 18. The method of claim 17, wherein scoring the two or more grid cells comprises evaluating a wavefront grid value of each cell of the two or more grid cells. 19. The method of claim 17, wherein scoring the two or more grid cells comprises comparing a distance from each cell of the two or more grid cells to a centroid of the travelling containment zone. 20. The method of claim 16, further comprising detecting bifurcation of the leading edge into at least a first segment and a second segment upon contact of the leading edge with an exclusion zone contained within the work region. 21. The method of claim 20, further comprising replacing the leading edge with either the first segment or the second segment.",
                    "STATE_OF_THE_ART": "Lawn and garden machines are known for performing a variety of tasks. For instance, powered lawn mowers are used by both homeowners and professionals alike to maintain grass areas within a property or yard. Robotic lawn mowers that autonomously perform the grass cutting function are also known. Robotic lawn mowers are typically battery-powered and are often limited to cutting only a portion of the property before requiring re-charging, which typically requires the mower to return to a charging base station. Robotic lawnmowers also generally cut grass in a random travel pattern within a fixed property boundary, wherein the boundary is defined by a continuous boundary marker, , an energized wire laying on or buried beneath the lawn at the property boundary. Such boundary wires may also extend into the interior of the yard to demarcate obstacles , trees, flower beds, or other exclusion zones. The mower may then move randomly within the areas delineated by the boundary wire. While effective, the random pattern of the mower combined with the variability of the boundary i. e. , shape of property lines, shape and size of obstacles, can create problems. For example, the mower may sometimes mow the same areas longer than needed, while missing areas not yet mowed. This occurs when the layout of the yard , the boundaries and excluded areas/obstacles, combined with the random pattern motion of the mower, leads to increased difficulty accessing certain areas , those areas of the yard having narrow entry passages. If the mower is unable to access these areas, such areas may be missed, reducing the perceived quality of cut. Conversely, when the mower is able to reach these areas, it may get unduly hung-up, potentially extending the time it takes to complete the mowing task before requiring recharging.",
                    "SUMMARY": [
                        "Embodiments described herein may provide a method of operating an autonomous working vehicle within a predefined work region, the method comprising: defining, with a controller associated with the working vehicle, a travelling containment zone that lies at least partially within the work region, the travelling containment zone defining a zone area that is less that an area of the work region; autonomously operating the working vehicle within the travelling containment zone; constraining a position of the working vehicle to be within the travelling containment zone; and moving the travelling containment zone across the work region while the working vehicle operates within the travelling containment zone. In another embodiment, a method of operating an autonomous working vehicle within a predefined work region is provided, the method comprising: defining, with a controller associated with the working vehicle, a travelling containment zone that lies at least partially within the work region, the travelling containment zone defining a zone area that is less that an area of the work region; autonomously transporting the working vehicle from a location beyond the travelling containment zone to a location within the travelling containment zone; autonomously operating an implement attached to the working vehicle within the travelling containment zone; and moving the travelling containment zone across the work region while the working vehicle operates within the travelling containment zone. In yet another embodiment, a mowing system is provided that includes an autonomously operating mower adapted to cut grass growing in a work region as the mower travels about the work region. The mower includes: a chassis supported upon a grass surface by ground support members, wherein one or more of the ground support members comprises a drive member; a grass cutting element attached to the chassis; a motor adapted to power the cutting element and the drive member; and a controller adapted to control the cutting element and a speed and direction of the drive member. The controller is further adapted to: identify a travelling containment zone at least partially within the work region, the travelling containment zone comprising a zone area that is less than an area of the work region; and constrain operation of the mower to be within both the travelling containment zone and the work region as the travelling containment zone travels across the work region. In yet another embodiment, a method of operating an autonomous working vehicle within a predefined work region is provided, wherein the method includes defining, with a controller associated with the working vehicle, a travelling containment zone that lies at least partially within the work region, wherein the work region bounds a first plurality of grid cells and the travelling containment zone bounds a lesser second plurality of grid cells. The method further includes: autonomously operating the working vehicle within the travelling containment zone; constraining a position of the working vehicle to be within the travelling containment zone; moving the travelling containment zone across the work region while the working vehicle operates within the travelling containment zone; and deciding, with the controller, a direction in which to advance the travelling containment zone. In still another embodiment, a method of operating an autonomous working vehicle within a predefined work region is provided. The method may include one or more of: defining, with an electronic controller associated with the working vehicle, a travelling containment zone that lies at least partially within the work region, the travelling containment zone defining a zone area that is less that an area of the work region; autonomously operating the working vehicle within the travelling containment zone; constraining a position of the working vehicle to be within the travelling containment zone; and moving the travelling containment zone across the work region while the working vehicle operates within the travelling containment zone. One or more aspects may be additionally included, in any combination, to produce yet additional embodiments. For example, the working vehicle may be configured as a lawn mower. In another aspect, the work region may comprise a grass surface of a property. In another aspect, a shape of the travelling containment zone may be varied as the travelling containment zone moves across the work region. In still another aspect, the method may further comprise moving the working vehicle in a random manner within the travelling containment zone. In yet another aspect, the method includes controlling a steering angle and a ground speed of the working vehicle with the controller. In yet another aspect, the method further includes maintaining an initial position of the travelling containment zone for a period of time before moving the travelling containment zone across the work region. In still yet another aspect, the method includes estimating, with the controller, a time at which operation of the working vehicle over the entire work region will be complete. In still another aspect, the method includes either: maintaining the zone area of the travelling containment zone constant as the travelling containment zone moves across the work region; or varying the zone area of the travelling containment zone as the travelling containment zone moves across the work region. In yet another aspect, the method includes either: maintaining a speed of the working vehicle while operating the working vehicle within the travelling containment zone; or varying a speed of the working vehicle while operating the working vehicle within the travelling containment zone. In yet another aspect, moving the travelling containment zone comprises moving the travelling containment zone at either: a constant rate or a variable rate. In another embodiment, a mowing system is provided comprising an autonomously operating mower adapted to cut grass within a work region as the mower travels about the work region. In one aspect, the system may comprise one or more of: a chassis supported upon a grass surface by ground support members, wherein one or more of the ground support members comprises a drive member; a grass cutting element carried by the chassis; one or more motors adapted to power the cutting element and the drive member; and an electronic controller adapted to control operation of the cutting element and a speed and direction of the mower. The controller is adapted to: identify a travelling containment zone at least partially within the work region, the travelling containment zone comprising a zone area that is less than an area of the work region; and constrain operation of the mower to be within both the travelling containment zone and the work region as the travelling containment zone travels across the work region. One or more aspects may be additionally included, in any combination, to produce yet additional embodiments. For example, in one aspect, the controller may maintain an initial position of the travelling containment zone for a period of time before moving the travelling containment zone across the work region. In another aspect, the mower further comprises a positioning system adapted to estimate a position of the mower within the work region, wherein the positioning system is operatively connected to the controller. In yet another aspect, the system further comprises a base station located in or near the work region. In still yet another embodiment, a method of operating an autonomous working vehicle within a predefined work region is provided. In one aspect, the method may comprise one or more of: defining, with an electronic controller associated with the working vehicle, a travelling containment zone that lies at least partially within the work region, wherein the work region bounds a first plurality of grid cells and the travelling containment zone bounds a lesser second plurality of grid cells; autonomously operating the working vehicle within the travelling containment zone; constraining a position of the working vehicle to be within the travelling containment zone; deciding, with the controller, a direction in which to advance a leading edge of the travelling containment zone; and moving the travelling containment zone across the work region while the working vehicle operates within the travelling containment zone. One or more aspects may be additionally included, in any combination, to produce yet additional embodiments. For example, in one aspect, deciding the direction to advance the travelling containment zone comprises scoring two or more grid cells, the two or more grid cells being externally adjacent to a boundary of the travelling containment zone. In another aspect, the scoring the two or more grid cells comprises evaluating a wavefront grid value of each cell of the two or more grid cells. In another aspect, scoring the two or more grid cells comprises comparing a distance from each cell of the two or more grid cells to a centroid of the travelling containment zone. In yet another aspect, the method further includes detecting bifurcation of the leading edge into at least a first segment and a second segment upon contact of the leading edge with an exclusion zone contained within the work area. In still another aspect, the method further comprises replacing the leading edge with either the first segment or the second segment. In still yet another aspect, moving the travelling containment zone comprises advancing a trailing edge of the travelling containment zone, while in another aspect, moving the travelling containment zone comprises adding grid cells from the first plurality of grid cells to the travelling containment zone. In yet another aspect, moving the travelling containment zone comprises removing grid cells from the travelling containment zone. The above summary is not intended to describe each embodiment or every implementation. Rather, a more complete understanding of illustrative embodiments will become apparent and appreciated by reference to the following Detailed Description of Exemplary Embodiments and claims in view of the accompanying figures of the drawing.",
                        "Exemplary embodiments will be further described with reference to the figures of the drawing, wherein: 1A illustrates an exemplary property defining work regions, exclusion zones, and transit zones in accordance with embodiments of the present disclosure; 1B is a diagrammatic view of an autonomous working vehicle, , robotic lawn mower, in accordance with embodiments of the present disclosure; 2 illustrates an exemplary method of operating the mower within a travelling containment zone of a work region; 3A-3K illustrate an exemplary method of operating a mower in another, more complex, work region, wherein: 3A illustrate an initial travelling containment zone at a time t=t1; 3B illustrates the travelling containment zone at a time t=t2; 3C illustrates the travelling containment zone at a time t=t3; 3D illustrates the travelling containment zone at a time t=t4; 3E illustrates the travelling containment zone at a time t=t5, wherein the travelling containment zone is shown moving around an exclusion zone to prevent bisecting the travelling containment zone; 3F illustrates the travelling containment zone at a time t=t6; 3G illustrates the travelling containment zone at a time t=t7; 3H illustrates the travelling containment zone at a time t=t8; 3I illustrates the travelling containment zone at a time t=t9; 3J illustrates the travelling containment zone at a time t=t10; and 3K illustrates the travelling containment zone at a time t=t11; 4 illustrates a map of yet another exemplary work region defined by a boundary and containing multiple exclusion zones, the work region laid over an x-y grid, the grid defining a plurality of grid cells; 5 is a visual depiction of an exemplary wavefront grid of the work region map of 4; 6A-6H illustrate an exemplary computer-simulated method of operating a mower within the work region shown in the map of 4, wherein: 6A illustrates a travelling containment zone at time t=t1; 6B illustrates the travelling containment zone at time t=t2, illustrating a leading edge of the travelling containment zone as it encounters an exclusion zone; 6C illustrates the travelling containment zone at time t=t3; 6D illustrates travelling containment zone at time t=t4; 6E illustrates the travelling containment zone at time t=t5; 6F illustrates the travelling containment zone at time t=t6 after the mower finished in the area shown in 6E and has relocated to a new starting area; and 6G illustrates the travelling containment zone at time t=t7 after the mower has again relocated to a new starting area; and 6H is an enlarged view of the travelling containment zone of 6B; and 7 illustrates an exemplary process that assists in preventing bifurcation or splitting of the travelling containment zone. The figures are rendered primarily for clarity and, as a result, are not necessarily drawn to scale. Moreover, various structure/components, including but not limited to fasteners, electrical components wiring, cables, , and the like, may be shown diagrammatically or removed from some or all of the views to better illustrate aspects of the depicted embodiments, or where inclusion of such structure/components is not necessary to an understanding of the various exemplary embodiments described herein. The lack of illustration/description of such structure/components in a particular figure is, however, not to be interpreted as limiting the scope of the various embodiments in any way."
                    ],
                    "DESCRIPTION": "In the following detailed description of illustrative embodiments, reference is made to the accompanying figures of the drawing which form a part hereof. It is to be understood that other embodiments, which may not be described and/or illustrated herein, are certainly contemplated. All headings provided herein are for the convenience of the reader and should not be used to limit the meaning of any text that follows the heading, unless so specified. Moreover, unless otherwise indicated, all numbers expressing quantities, and all terms expressing direction/orientation , vertical, horizontal, parallel, perpendicular, in the specification and claims are to be understood as being modified in all instances by the term about. Further, the term and/or if used means one or all of the listed elements or a combination of any two or more of the listed elements. Still further, i. e. may be used herein as an abbreviation for id est, and means that is, while , may be used as an abbreviation for exempli gratia, and means for example. Embodiments of the present disclosure provide autonomous robotic working vehicles and methods of operating the same within a predefined work region to achieve improved vehicle coverage , with an implement associated with the vehicle of the work region during operation. For example, the vehicle may be an autonomous robotic mower adapted to cut grass, using an associated cutting member, on a working surface located within a work region , a turf grass surface of a residential or commercial property as the mower travels across the work region. By implementing methods like those described and illustrated herein, such a mower may be able to achieve more efficient cutting coverage than may otherwise be provided with known random-travel coverage methods. As used herein, property is defined as a geographic region such as a yard circumscribed by a fixed boundary within which the vehicle may perform work , mow grass. For example, 1A illustrates an exemplary property or yard 50 defined by a boundary 59. Work region see areas labeled as 51 in 1A is used herein to refer to those areas contained or mostly contained within the property boundary 59 within which the vehicle will perform work. For example, work regions could be defined by grass surfaces of the property or yard 50 upon which an autonomous lawn mower will operate. As further shown in 1A, a property may contain one or more work regions 51 including, for example, a front yard area and a back yard area, or two yard areas separated by a sidewalk or driveway 52. Exclusion zone is defined herein as an area contained within a work region in which the vehicle is not intended to operate , not intended to mow grass. Examples of exclusion zones include landscaped areas and gardens such as areas 53 shown in 1A, pools, buildings, driveways see, , driveway 52, and other yard features. Transit zones see transit zones 55 in 1A may be used herein to refer to paths the vehicle may take when travelling between different work regions of the property. Typically, the vehicle will not perform work when moving through a transit zone. While described herein as a robotic mower, such a configuration is exemplary only as systems and methods described herein also have application to other autonomously operated machines/vehicles including, for example, commercial mowing products, other ground working vehicles , debris blowers/vacuums, aerators, material spreaders, snow throwers, as well as indoor working vehicles such as vacuums and floor scrubbers/cleaners. It is noted that the terms comprises and variations thereof do not have a limiting meaning where these terms appear in the accompanying description and claims. Further, a, an, the, at least one, and one or more are used interchangeably herein. Moreover, relative terms such as left, right, front, fore, forward, rear, aft, rearward, top, bottom, side, upper, lower, above, below, horizontal, vertical, and the like may be used herein and, if so, are from the perspective shown in the particular figure, or while the vehicle 100 is in an operating configuration , while the vehicle 100 is positioned such that wheels 106 and 108 rest upon a generally horizontal ground surface 103 as shown in 1B. These terms are used only to simplify the description, however, and not to limit the interpretation of any embodiment described. 1B illustrates an autonomously operating working vehicle , a robotic lawn mower 100 of a lawn mowing system, the mower constructed in accordance with exemplary embodiments of the present disclosure for simplicity of description, the mower 100 is illustrated schematically. As shown in this view, the mower 100 may include a frame or chassis 102 that carries and/or encloses various components of the mower as described below. The mower may further include ground support members, , one or more rear wheels 106 and one or more front wheels 108, that support the chassis 102 upon a ground grass surface 103. One or more of the ground support members may include a drive member. For example. one or both of the rear wheels 106 may be driven , by one or more electric wheel motors 104 to propel the mower 100 over the ground surface 103. In some embodiments, the front wheels 108 may freely caster relative to the chassis 102 , about vertical axes. In such a configuration, mower direction may be controlled via differential rotation of the two rear wheels 106 in a manner similar to a conventional zero-turn-radius ZTR riding mower. That is to say, a separate wheel motor 104 may be provided for each of a left and right rear wheel 106 so that speed and direction of each rear wheel may be independently controlled. In addition or alternatively, the front wheels 108 could be actively steerable , using one or more steer motors 105 to assist with control of mower 100 direction, and/or could be driven i. e. , to provide a front-wheel or all-wheel-drive mower. A grass cutting element , blade 110 may be carried by the chassis. For example, the cutting element may be coupled to a cutting motor 112 that is itself attached to the chassis 102. When the motors 112 and 104 are energized, the mower 100 may be propelled over the ground surface 103 such that vegetation , grass over which the mower passes is cut by the blade 110. While illustrated herein using only a single blade 110/motor 112, mowers incorporating multiple blades, powered by single or multiple motors, are contemplated within the scope of this disclosure. Moreover, while described herein in the context of one or more conventional blades, other cutting elements including, for example, nylon string or line elements, knives, cutting reels, etc. , are certainly possible without departing from the scope of this disclosure. Still further, embodiments combining various cutting elements, , a rotary blade with an edge-mounted string trimmer, are also contemplated. The exemplary mower 100 may further include a power source, which in one embodiment, is a battery 114 having a lithium-based chemistry , lithium-ion. Other embodiments may utilize batteries of other chemistries, or other power source technologies , solar power, fuel cell, internal combustion engines altogether, without departing from the scope of this disclosure. It is further noted that, while shown as using independent blade and wheel motors, such a configuration is exemplary only as embodiments wherein blade and wheel power is provided by a single prime mover are also contemplated. The mower 100 may further include one or more sensors to provide location data. For instance, some embodiments may include a positioning system , global positioning system GPS receiver 116 and/or other position system that may provide similar data adapted to estimate a position of the mower 100 within a work region and provide such information to a controller 120 described below. For example, one or more of the wheels 106, 108 , both rear wheels 106 may include encoders 118 that provide wheel rotation/speed information that may be used to estimate mower position , based upon an initial start position within a given work region. Other sensors , infrared, radio detection and ranging radar, light detection and ranging lidar, now known or later developed may also be incorporated into the mower 100. The mower 100 may further include a sensor 115 adapted to detect a boundary wire when the latter is used to define a boundary of the work region. The mower 100 may also include a controller 120 adapted to monitor and control various mower functions. The exemplary controller 120 may include a processor 122 that receives various inputs and executes one or more computer programs or applications stored in memory 124. The memory 124 may include computer-readable instructions or applications that, when executed, , by the processor 122, cause the controller 120 to perform various calculations and/or issue commands. That is to say, the processor 122 and memory 124 may together define a computing apparatus operable to process input data and generate the desired output to one or more components/devices. For example, the controller may be operatively connected to the positioning system such that the processor 122 may receive various input data including positional data from the GPS receiver 116 and/or encoders 118, and generate speed and steering angle commands to the drive wheel motors 104 to cause the drive wheels 106 to rotate at the same or different speeds and in the same or different directions. In other words, the controller 120 may control the steering angle and ground speed the speed and direction of the mower 100, as well as operation of the cutting blade 110 including blade actuation/de-actuation and blade rotation speed. In view of the above, it will be readily apparent that the functionality of the controller 120 may be implemented in any manner known to one skilled in the art. For instance, the memory 124 may include any volatile, non-volatile, magnetic, optical, and/or electrical media, such as a random-access memory RAM, read-only memory ROM, non-volatile RAM NVRAM, electrically-erasable programmable ROM EEPROM, flash memory, and/or any other digital media. While shown as both being incorporated into the controller 120, the memory 124 and the processor 122 could be contained in separate modules. The processor 122 may include any one or more of a microprocessor, a controller, a digital signal processor DSP, an application specific integrated circuit ASIC, a field-programmable gate array FPGA, and/or equivalent discrete or integrated logic circuitry. In some embodiments, the processor 122 may include multiple components, such as any combination of one or more microprocessors, one or more controllers, one or more DSPs, one or more ASICs, and/or one or more FPGAs, as well as other discrete or integrated logic circuitry. The functions attributed to the controller 120/processor 122 herein may be embodied as software, firmware, hardware, or any combination thereof. In 1B, schematic connections are generally shown between the controller 120 and the battery 114, wheel motors 104, blade motor 112, optional boundary wire sensor 115, wireless radio 117 which may communicate with, for example, a remote computer such as a mobile phone 119, and GPS receiver 116. This interconnection is exemplary only as the various subsystems of the mower 100 could be connected in most any manner, , directly to one another, wirelessly, via a bus architecture , controller area network CAN bus, or any other connection configuration that permits data and/or power to pass between the various components of the mower. Exemplary methods of using the mower 100 will now be described, initially with reference to 2. This figure illustrates a generic rectangular work region 200 , residential yard in which the mower 100 may operate. For purposes of describing operation of the exemplary mower, it is assumed that a peripheral boundary 250 of the work region 200 as well as any exclusion zones if present contained within the work region see, , 3A is known to the mower 100. For instance, the boundary 250 of the work region and any exclusion zone may be marked by an energized boundary wire detectable by a sensor see sensor 115 in 1B provided on the mower during a training phase. In addition or alternatively, the boundary 250 may be detectable or known via other techniques without departing from the scope of the disclosure. For instance, the mower 100 may detect machine-visible markings located on the property, or it may map and store boundary information during a training phase of the mower. In 2, the boundary 250 of the yard encloses, and defines, the work region 200. As stated above, the exemplary work region 200 is generally rectangular in shape and lacks any exclusion zones , flower beds, trees, . However, such a shape is exemplary only as embodiments of the present disclosure may find application to work regions , yards of most any size and shape and having any number of exclusion zones see, , work region 300 shown in 3A-3K. The hatched portion of 2 illustrates what will be referred to herein as a travelling containment zone 202 see exemplary zones 202t1 and 202t2. The travelling containment zone 202 represents a dynamically changing , travelling, expanding, contracting, or otherwise moving subarea of the work region 200 i. e. , the travelling containment zone lies at least partially within the work region and defines a zone area that is less than an area of the work region. A position of the mower 100 may, at any time during operation, be constrained to being within both the travelling containment zone 202 and the work region as the travelling containment zone travels across the work region for reasons that are further described below. In accordance with embodiments of the present disclosure, at least some portions of the travelling containment zone 202 may be delineated by a virtual boundary electronically defined and recognized with the controller 120, i. e. , some or all portions of the travelling containment zone 202 may not be delineated by any physical demarcation or sensor-detected border. It is further noted that, while illustrated as rectangular, the travelling containment zone 202 may vary in shape as the travelling containment zone moves across the work region. Unlike the generally fixed work region 200 though, the travelling containment zone 202 may be designed to move or travel across the work region 200 while the mower autonomously operates within the travelling containment zone so that all of the working surface grass surface of the work region 200 is eventually enveloped within the travelling containment zone. For example, when mower operation is initiated, the controller 120 see 1B may , based on control algorithms, the known shape of the work region, and previous cutting history, define or otherwise identify an initial travelling containment zone 202t1 as shown in 2. Once again, to identify the travelling containment zone 202, the controller 120 may utilize not only the physical boundary 250 of the work region 200 , that boundary defined by a boundary wire, but also one or more virtual boundaries 252 , one or more boundaries ultimately extending between physical boundaries of the work area created by the controller 120. In some embodiments, the travelling containment zone 202 will, although moving, remain a singular enclosed region , 202t1, 202t2 during the entire mowing operation. The travelling containment zone 202 is able to provide generally even coverage of the entire work region 200. However, such even coverage may not always be optimal. For instance, it is not uncommon for the grass growth rate of some areas of the work region 200 to be different than others. As a result, the controller 120 may maintain historical information regarding the cutting load on the mower at most any given position within the work region 300. Cutting load, as used herein, refers to the work required by the mower to cut grass. The cutting load may thus be a function of both: power drawn by/supplied to the cutting motor 112 see 1B; and the propulsion or ground speed of the mower which is proportional to power drawn by/supplied to the wheel motors 104. Accordingly, if the controller 120 detects that the cutting load is higher than a predetermined limit, it may slow the speed at which the travelling containment zone 202 moves to ensure that the mower 100 has adequate opportunity to cut the grass within the travelling containment zone 202. Conversely, the controller 120 may increase the rate of movement of the travelling containment zone 202 when cutting load is lower than a predetermined limit. In addition to, or instead of, changing the speed of travelling containment zone 202 movement, the travel speed of the mower 100 could also be increased or decreased. That is, the controller may maintain a speed of the mower 100 while autonomously operating the mower within the travelling containment zone which may move at a constant or variable rate of speed, or alternatively vary a speed of the mower while operating the mower within the travelling containment zone. In some embodiments, the mower 100 may move autonomously and in a random manner within the travelling containment zone 202 during operation. Because the travelling containment zone 202 is, by definition, a geographic subset of the work region 200, the travelling containment zone at any given time is likely less complex , has fewer or no narrow passages or bottlenecks that may cause the mower to get hung-up and more geographically uniform , have consistent turf quality than the work region as a whole. Moreover, the travelling containment zone is also likely to have fewer obstacles than a larger area i. e. , fewer obstacles than contained within the entire work region 200. As a result, the mower 100 is more likely during random movement to evenly cover all areas of the travelling containment zone 202 and thus eventually evenly cover the entire work region 200 than if the randomly-moving mower was only limited to the larger physical boundary of the work region 200. To ensure the entire work region 200 is covered cut by the mower 100, the travelling containment zone 202 may move or travel across the work region, , from left to right in 2, as the mowing operation progresses. Moving of the travelling containment zone may occur at either a constant rate or a variable rate. As the travelling containment zone 202 moves within the work region 200 depicted in 2, area is both added to the zone 202 , by advancement of the virtual leading edge 252L of the virtual boundary 252 and subtracted from the zone 202 by advancement of the trailing edge 252T of the virtual boundary 252. For example, at some time after mower operation begins, the travelling containment zone 202 will have moved from the initial travelling containment zone depicted as travelling containment zone 202t1 in 2 to a new position depicted as travelling containment zone 202t2. In the illustrated embodiment, the area of 202t1 and 202t2 may remain constant i. e. , the controller may maintain a zone area of the travelling containment zone constant as the travelling containment zone moves across the work region. However, in other embodiments, the area of the travelling containment zone 202 may change i. e. , the controller may vary the zone area of the travelling containment zone as the travelling containment zone moves across the work region. Note that the suffixes t1, t2, etc. in relation to the containment zone 202 are used only to indicate a different point in time of mower operation. That is to say, the travelling containment zones 202 represented in 2 illustrate the cutting area at separate instantaneous points in time. In reality, the area added to and removed from the travelling containment zone 202 via advancement of the leading and trailing edges 252L, 252T may, at least in some embodiments, be set at a rate constant or variable selected to ensure adequate cutting of all portions of the travelling containment zone. While dynamic movement of the travelling containment zone 202 may allow efficient coverage of the entire work region 200, it is contemplated that the controller 120 may, occasionally, slow or stop movement of the travelling containment zone for a given period of time. For example, it may be advantageous to maintain an initial position of the travelling containment zone 202t1 for a period of time before the travelling containment zone travels across the work region to ensure that the mower 100 has adequate opportunity to cover the portions of the lawn adjacent the work region boundary the left-most boundary 250 in 2. Whether to maintain the position of the travelling containment zone 200 fixed, and how long it remains fixed, may be determined by the controller 120 based upon, , the size of the travelling containment zone, the size and speed of the mower 100, cutting load, and statistical and/or historical coverage information for the mower 100 that may be retained in the memory 124 see 1B. In other embodiments, the initial travelling containment zone 202t1 may, instead of being held stationary, actually start with its trailing edge 252T outside , to the left in 2 of the physical boundary 250 of the property i. e. , the travelling containment zone may extend beyond the work region. Although the trailing edge 252T is located beyond the boundary 250, the mower 100 may still be restricted from travelling beyond the work region 200, i. e. , the mower may remain within the work region at all times during mowing. As a result, as the travelling containment zone 202 travels during mower operation, the leftmost boundary of the property , along a boundary 250 will remain in the travelling containment zone for some period of time, i. e. , until the trailing edge 252T of the containment zone 202 moves through to the right of the yard boundary 250. To practically implement this functionality, the controller 120 could set the initial travelling containment zone equal to a small area, then move the leading edge 252L until the area of the travelling containment zone is equal or filled to the desired area, at which point the trailing edge 252T is introduced and starts advancing in relation to the leading edge 252L to ensure the desired area remains constant. This process is illustrated in more detail below see, , 6A-6G. As mower 100 operation continues, the travelling containment zone 202 may reach the last portions of the work region as shown by work region 202tn in 2. Again, the controller 120 may maintain the position of the travelling containment zone 202tn for a fixed period of time to ensure portions the right-most boundary 250 of this containment zone are adequately covered, or it may continue to move the containment zone to the right, effectively shrinking or emptying the area of the travelling containment zone as the trailing edge 252T of the travelling containment zone approaches the right-most boundary 250 of the work region 200. Of course, the controller 120 may prevent the travelling containment zone from becoming so small that mower operation is compromised, , the travelling containment zone may avoid contracting beyond a minimum size that would interfere with the mower's ability to move. By controlling the mower 100 as described above, the mower is, at any given time, limited to operating within a subarea i. e. , a travelling containment zone 202 of the overall work region 200. Given the mower's random travel pattern, it is thus more likely that the mower will cover the entire work region evenly , avoid leaving uncut sections of lawn than a mower that is not so restricted. Further, by containing the mower 100 to a smaller travelling containment zone, it is more likely that isolated areas of the lawn , narrow turf passages between exclusion zones; described with reference to 3A-3K below will be adequately covered, while also minimizing the chance that the mower may get hung-up or trapped for an extended period within such isolated areas. The actual movement of the travelling containment zone 202 may occur in increments defined by the controller 120. For example, once the work region 200 is mapped by the mower , during a teaching phase, the controller 120 may overlay a virtual grid on the work region see, , exemplary grid cells or elements 251 in 2 as further described below. The grid element size may be small, effectively allowing the travelling containment zone 202 to be defined with high resolution boundaries at the expense of increased computational loading of the controller 120. Alternatively, if the grid element size is large, the travelling containment zone 202 may move in relatively larger, discrete steps resulting in the travelling containment zone 202 having lower resolution , courser boundaries. In practice, the grid element size is selected to be at least as small as the physical footprint of the mower 100, thereby ensuring that all areas of the work region 200 in which the mower can fit will eventually be covered by the travelling containment zone. To adequately provide coverage along various , curved boundaries see boundary 350 in 3A of the work region and exclusion zones see zones 354 and 355 described below, however, the grid cell size may be set even smaller than the mower footprint. Moreover, while shown as 4-sided elements, other grid cells may be of any polygonal shape , any shape having three or more sides , triangular, pentagonal, hexagonal, octagonal, etc. -shaped elements without departing from the scope of this disclosure. The speed at which the travelling containment zone moves, however, may be independent of, and unaffected by, grid size. Rather, at least in some embodiments, movement of the travelling containment zone may be defined by the following equation: A=B*tEquation 1wherein:A=area added to, and subtracted from, the travelling containment zone, , meter m2;B=rate at which area is covered by the mower in unit-area per unit-time, , m2/second; andt=time interval, , seconds sec. Accordingly, the area added to and removed from the travelling containment zone may be independent of actual containment zone size. Rather, the area , grid cells added/removed AA is dependent on parameter B. The nominal or initial value of B may be directly dependent upon the average speed and cutting width of the mower 100. For instance, a given robotic mower 100 may cover ground area at some unit-area-per-time rate referred to herein as Bo. During normal operation, the controller 120 may set B=Bo. However, when the controller 120 determines that more or less cutting is desired, the parameter B may be adjusted. For example, B may be increased or decreased by an incremental value AB i. e. , B=Bo+AB. Finally, the time interval t may be dependent upon both AA and B. As an example, AA may be set equal to one or a multiple thereof grid cell 251 as shown in 2 , AA may be equal to 8 grid cells 251 such that 8 cells are simultaneously added to and subtracted from the travelling containment zone 202. The parameter t is thus the time interval at which grid cells 251 are added and removed from the travelling containment zone it may thus be constructive to view AA and B as dependent variables and t as independent in Equation 1 above. In addition to potentially improving coverage efficiency, a dynamically travelling containment zone 202 may also allow the controller 120 to better predict time-to-completion of the mowing operation. That is, by knowing the rate of movement of the travelling containment zone and the size and shape of the yard which may be known from initial training of the mower and/or historical mowing information, the controller may be able to accurately estimate the time at which the mower will complete mowing of the entire property , estimate the time at which operation of the mower over the entire work region over the working surface of the work region will be complete. This estimate may improve over time as the controller 120 learns how to optimize travelling containment zone movement. Such estimated information may be provided to a homeowner or operator via any number of methods. For example, the mower may include a display that provides, among other data, a time-to-completion estimate. In other embodiments, the mower may include a wireless radio , IEEE 802. 11 Wi-Fi radio 117 shown in 1B that may communicate over a local area or wide area network with a mobile device , cellular phone 119. In the case of the latter, the mower may provide the time-to-completion estimate via an application running on the mobile device, or via periodic notifications , text messages provided to the mobile device. While 2 illustrates a basic methodology in accordance with embodiments of the present disclosure, most properties present a more complicated geometry than the rectangular shape shown in 2. 3A-3K illustrate operation of the mower 100 in accordance with embodiments of the present disclosure over such an exemplary property. 3A illustrate a property or work region 300 having an irregular boundary 350 detectable and/or known by the mower 100. The work region 300 may further include exclusion zones 354, 355 that may also be identified by the controller 120 see 1B as boundaries which boundaries could again be physical , a wire or virtual , an electronic map of the work region 300 known, or otherwise determined, by the controller. Unlike the work region 200 shown in 2, the work region 300 of 3A presents a more complicated shape that requires the controller 120 of the mower 100 see 1B to make correspondingly more sophisticated decisions as to how to cover the work region. For purposes of describing 3A-3K, the instantaneous or current travelling containment zone 302 see initial zone 302t1 is represented by hatched lines, the portions of the work region that have been covered by the mower are represented by the double-hatched lines, and the uncut portions of the work region are shown unmarked. As shown in 3A, the controller 120 may select an initial travelling containment zone 302t1 various containment zones illustrated in 3A-3K may be referred to generically as 302 and control the mower's drive wheels until the mower 100 is located at some position within the zone 302t1. That is to say, if the mower 100 is not within the travelling containment zone selected by the controller, the mower may be autonomously transported under control of the controller from a location beyond the travelling containment zone to a location within the desired containment zone before autonomous mowing begins. For example, the controller 120 may chart a path from its current position to a position somewhere within the travelling containment zone, such path being within the work region 300 and outside of any exclusion zones , zones 354, 355. Once the path is identified, the controller 120 may control the drive wheels 106 see 1B to transport the mower to the desired position. The initial travelling containment zone may be selected based on any number of factors including, for example, proximity of the mower 100 to the zone 302t1 at the time mower operation begins, and how long it has been since the zone 302t1 was last mowed. In some embodiments, the controller 120 may utilize a wavefront grid further described below, wherein the controller calculates a linear distance from each grid cell to a base position which may be the location of the mower's base station see, , 220 in 2. Such distances may be calculated based on a path that obeys all boundaries work region and exclusion zones. The initial travelling containment zone may then be determined to be a zone containing cells farthest from the base position, i. e. , the location identified by the greatest value in the wavefront grid. Once positioned within the travelling containment zone 302t1, the mower 100 may activate its cutting motor 112 see 1B and begin cutting grass as the mower moves randomly within the confines of the travelling containment zone 302t1. As described above, the travelling containment zones 302t1 may be fixed for some period of time to ensure the physical boundary of the work region the left-most edge of the work region boundary 350 in 3A is adequately covered. Alternatively, the travelling containment zone could grow or fill outwardly , toward the right from an initial location at or near the physical boundary 350 of the work region until the containment zone reaches the desired size, at which time a trailing edge of the containment zone would also advance. In either scenario, virtual boundary 352 , leading edge 352L thereof may begin to travel , to the right in 3A as time progresses. As shown in 3B, as the leading edge 352L advances, a trailing edge 352T of the virtual boundary 352 may ultimately follow. In some embodiments, the leading edge is thus a set of grid cells that have been iteratively scored by the controller and added to the travelling containment zone although not all grid cells along the leading edge are populated, thus avoiding expansion of the travelling containment zone into unintended open areas. Scoring may be performed based upon the wavefront grid values described elsewhere herein. In this way, the controller 120 may better identify direction of movement of the travelling containment zone, and ensure adequate coverage of the containment zone boundaries. Of course, other aspects may also be taken into consideration when populating leading edge grid cells , maintaining shape quality of the travelling containment zone. In some embodiments, the boundaries of the travelling containment zone 302 may behave somewhat like a virtual, continuous rope that can morph to accommodate most any boundary shape. As a result, the travelling containment zone 302 may accommodate the irregular shaped left boundary of the work region 300 shown in 3A, and then, once the trailing edge 352T has moved beyond , to the right of the left-most work region boundary, the trailing edge 352T may assume another , linear shape as shown by the travelling containment zone 302t2 in 3B. While shown as transitioning to a linear trailing edge 352T to allow efficient mower operation, such a transition is only exemplary as most any shape of travelling containment zone is possible without departing from the scope of this disclosure. In fact, when scoring algorithms like those described herein are utilized to advance the travelling containment zone, the leading edge and trailing edges of the travelling containment zone as well as other edges of the containment zone may likely have non-linear shapes as illustrated in, for example, the simulations shown in 6A-6G. Again, while the travelling containment zone 302 is illustrated as a distinct zone at any given time in 3A-3K as well as in 6A-6G, such depiction is exemplary as the travelling containment zone may move generally continuously , via small discrete steps or periodically , via larger discrete steps across the work region 300 during mower operation. Thus, the travelling containment zones illustrated in the figures are intended to illustrate only the size and shape of the travelling containment zones at different arbitrary but discrete points in time. In 3C, the leading edge 352L of the travelling containment zone 302 zone 302t3 is shown immediately prior to encountering the first exclusion zone 354. Once zone 354 is encountered by the leading edge 352L, the controller 120 decides which segment of the leading edge, i. e. , the upper segment 353-1 above the exclusion zone 354, or the lower segment 353-2 below the exclusion zone 354, in alternate cases, three or more potential leading edges could exist will become the effective new leading edge 352L. Stated alternatively, the controller 120 may decide at this time whether to move the travelling containment zone to either side of , along the top or the bottom of the exclusion zone 354 by selecting either the upper segment 353-1 or the lower segment 353-2 as the new leading edge 352L. As shown in 3D, the controller 120 decides to select segment 353-2 as the new leading edge and thus the travelling containment zone 302 slides or morphs along a lower side of the exclusion zone 354 as illustrated by travelling containment zone 302t4. By utilizing algorithms that ensure the boundary of the travelling containment zone remains continuous, the active cutting area may move along the lower boundary of the exclusion zone 354 and avoid being bisected by the exclusion zone while the controller 120 logs that an area 356 above the exclusion zone 354 remains uncut. In some embodiments, when the leading edge 352L splits , as shown herein upon encountering the exclusion zone 354, the two potential leading edges , segments 353-1 and 353-2 may be scored against one another by the controller 120 to assist the controller in selecting one of the potential segments to become the new leading edge , using the wavefront grid values, most desirable zone shapes, most efficient mowing patterns, . Once the most optimal leading edge segment is selected, the controller 120 may then populate grid cells along that selected segment/leading edge path. This process continues until the leading edge 352L again encounters a boundary. Such boundary encounters include: encountering another boundary that causes the leading edge to again split into two or more segments see, , encounter with exclusion zone 355 in 3F; and encountering a dead-end either by hitting a boundary of the work region see, , encounter with boundary 350 as shown in 3K; or by hitting the boundary of a previously cut portion of the work region see, , encounter with previously mowed section as shown in 3J. In the case of the exclusion zone encounter, the potential leading edges are evaluated as indicated above and operation continues as described. However, when a dead-end is encountered, the trailing edges 352T may move toward the leading edge 352L until the area of the travelling containment zone is effectively zero in practice, the zone may stay of a sufficient size to avoid constraining movement of the mower. If areas of the work region still remain uncut, the controller 120 may then move the mower 100 to these uncut areas in a manner similar to that described above with regard to moving the mower to the initial travelling containment zone. An example of the latter occurring is shown in 3J and 3K and described below. In practice, the controller 120 may also need to apply algorithms that generally prevent the travelling containment zone from becoming too restrictive for effective mower navigation. For example, as the travelling containment zone progresses , from the zone 302t4 shown in 3D toward the zone 302t5 shown in 3E, the controller 120 encounters various decision points, such as reaching the exclusion zone 354. Upon reaching the exclusion zone 354, the travelling containment zone 302 may, as described above, slide down along the lower side of the exclusion zone as shown in 3D and 3E. However, while the travelling containment zone continues to move, the controller 120 must also apply some constraints on the shape of the zone. For instance, the controller 120 may prevent the trailing edge 352T of 3E from moving so far to the right that the area indicated by reference numeral 357 becomes too narrow, making mower navigation in the area 357 problematic. Rather, the controller 120 may, as shown in 3E, limit the minimum width of the area 357 and instead begin drawing an upper trailing edge 352Tu downwardly as the travelling containment zone 302t5 expands toward the right. In this way, the travelling containment zone 302 may add area by moving the leading edge 352L as indicated in 3E while subtracting equivalent area by sliding the upper trailing edge 352Tu downwardly, all while maintaining a position of the left trailing edge 352T to minimize mower navigation issues. As the travelling containment zone 352 continues to travel across the work region 300, the leading edge 352L of the virtual boundary may reach the second exclusion zone 355 as indicated in 3F. When this occurs, the controller 120 may again determinebased upon grid cell scoring or random selectionto pursue a course , a segment above the exclusion zone 355 , in zone 358 between zones 354 and 355, or a route that first takes the mower 100 below the exclusion zone 355 , between the zone 355 and the work region boundary, the latter being illustrated by travelling containment zone 302t6 in 3F. Once the travelling containment zone travels past the exclusion zone 355, the travelling containment zone may expand both toward the right-most boundary 350 in 3G of the work region 300, as well as upwardly as represented by the travelling containment zone 302t7 and virtual boundary leading edge 352L. As the travelling containment zone 302 moves above the exclusion zone 355, it may expand into the previously uncut zone 358 between the two exclusion zones 354, 355 as represented by the travelling containment zone 302t8 in 3H. As the travelling containment zone 302 continues to move , upwardly in 3I it again encounters the exclusion zone 354 and may begin to wrap around the right and top side of the exclusion zone to cover the previously uncut area 356 as represented by travelling containment zone 302t9 in 3I. Once the virtual leading edge 352L encounters the boundary 350 as shown in 3I, the controller 120, knowing that two isolated areas 356 and 359 of the work region 300 remain to be cut, decides whether to move left , into area 356 or right , into area 359. In the embodiment illustrated in 3J, the controller 120 has decided to move the mower 100 left and mow the area 356 as indicated by the travelling containment zone 302t10. Once the mower 100 completes mowing of the area 356, the controller 120 may recognize that area 359 remains uncut and may command the mower to proceed to that area. Once within that area, the mower 100 may re-activate the cutting unit and continue to mow the final uncut area represented by travelling containment zone 302t11 as shown in 3K. As indicated in 3A-3K, the controller 120 may make decisions as to how the travelling containment zone travels to ensure adequate coverage of the work region 300 in an efficient manner. As the mower 100 completes multiple mowing sessions, the controller 120 may also learn which decisions resulted in the most efficient operation. For example, the controller 120 may determine that it would be more efficient to first transition the travelling containment zone above the exclusion zone 354 rather than below the exclusion zone as shown in 3D. Once the controller has gathered data for both paths, it may be able to accurately determine which path results in the most efficient operation and pursue that path in the future. Such decisions may be required at various points during the mowing process , when the exclusion zone 355 see 3F or a dead-end see 3J is encountered. After comparing historical data from previous mowing sessions, the controller 120 may achieve a travelling containment zone travel path that seeks to optimize mower efficiency as well as permits an accurate estimation of job time-to-completion. While the controller 120 may learn and provide the most efficient cutting path for most any work region 300, algorithms may also be provided that permit the mower to pursue alternative paths during different mowing sessions where such alternative paths may be beneficial , to prevent undesirable effects such as lawn burn-in when some boundaries of the travelling containment zone are repeatedly in the same location. Such alternative paths could be provided by storing multiple wavefront grids i. e. , emanating from different base cells on the work area such that the mower may begin mowing operation at different initial locations and correspondingly make potentially different decisions upon encountering the various boundaries and exclusion zones. 4, 5, and 6A-6H illustrate computer-simulated mowing coverage of another work area 400 in accordance with embodiments of the present disclosure. As with the work area 300, the work area 400 may be circumscribed by a boundary 450 and includes various exclusion zones 454-1, 454-2, and 454-3 collectively 454 contained therein. Data defining the boundary/exclusion zones may be stored in memory 124, , after a training process, or otherwise known by the controller 120. Values shown on the axes in these figures represent units of length , meters. As shown in 4, an x-y grid may be superimposed over the work area 400 with an initial position , origin at coordinate 0,0 at work area vertex 451. The initial position may be a location of the system base station 220 used to house and re-charge the mower when not in use, the base station typically being located in or near the work region. While the grid is generally illustrated as two-dimensional in 4, it is certainly adaptable to work areas having elevational variation without departing from the scope of this disclosure. Initial wavefront grid values may be calculated , with the controller 120, propagating from this initial position. For general information regarding wavefront grids, see, , E. Galceran, M. Carreras, A Survey on Coverage Path Planning for Robotics, Robotics and Autonomous Systems 61 2013 1258-1276. An exemplary wavefront grid 500 for the work area 400 is visually represented in 5. In this figure, a distance measured along a path that avoids all exclusion zones from the initial cell to any other cell within the work area 400 is represented by the vertical or z-axis. As further shown in this view, the wavefront grid 500 also accounts for both the boundary 450 of the work area 400, as well as the exclusion zones 454. Again, while a single wavefront grid 500 is illustrated herein, mowers and systems in accordance with embodiments of the present disclosure may map and store multiple wavefront grids, , emanating from different initial cells. With the data from this map, the controller 120 may compute the shortest travel distance from the base cell i. e. , cell from which the wavefront grid propagates, which is located at coordinates 0,0 in the example of 5 to any other cell on the grid , within the work area 400, considering all boundaries including boundaries of the work area and the exclusion zones. Accordingly, the controller 120 may identify efficient mower routes , to return to base station, to travel to new, uncut portion of the work area, as needed during operation. For instance, if the mower 100 were to require re-charging when at the location shown in 4, the controller 120 could determine, based upon the wavefront grid, that route 370 is shorter than route 372 and select the former travel path to return to the base station 220. With the wavefront grid values computed and stored in memory 124, the controller 120 may command the mower to first move, , from the initial cell proximate base station 220 to the cell farthest from this initial position i. e. , the cell having the highest wavefront grid value and commence mowing. In the exemplary workspace 400, the mower 100 would thus move to, and commence mowing at, the cells nearest vertex 460 of the boundary 450 as shown in 6A. The controller 120 may establish an initial travelling containment zone 402t1 containment zones are darkly hatched in 6A-6G, while cut portions are double-hatched and unmowed areas are unhatched and grow that zone outwardly from vertex 460. In order to determine the expansion directions of the travelling containment zone 402, the controller 120 may perform a scanning function wherein it analyzes and records , continuously or periodically data regarding each of the grid cells externally adjacent to a virtual perimeter of the travelling containment zone 402. For example, initially the travelling containment zone 402 may expand in multiple directions , generally radially as shown in 6A. However, as shown in 6B, the leading edge of the expanding travelling containment zone 402 may eventually contact a side of the exclusion zone 454-1 and split as indicated by the segments 452L-1 and 452L-2 of the zone 402t2 as used herein, the term segment is used to identify a virtual boundary that is delimited by a containment boundary , boundary 450 and/or an exclusion zone , exclusion zone 454-1. In this example simulation, the controller 120 may, upon contact of the leading edge with an exclusion zone, detect impending bifurcation of the leading edge into first and second segments 452L-1 and 452L-2. That is to say, once the advancing virtual perimeter of the travelling containment zone 402 contacts the exclusion zone 454-1, the controller 120, in order to avoid bifurcating the travelling containment zone 402/leading edge 452, performs a decision-making function, wherein it decides a direction in which to advance the leading edge of the travelling containment zone. In the illustrated example, the controller may decide to either: expand the travelling containment zone above the exclusion zone 454-1 pursue mower movement by replacing the leading edge with the first segment 452L-1; or expand the travelling containment zone downwardly along the side of the exclusion zone 454-1 pursue mower movement by replacing the leading edge with the second segment 452L-2. To make this decision, the controller 120 may identify grid cells that are adjacent each of these segments but not within the travelling containment zone 402t2 or that were not previously visited by the travelling containment zone. These adjacent grid cells are then scored based upon mean values previously calculated in the wavefront grid in some embodiments, other parameters of the cells may also be scored. Information concerning each segment score as well as other information such as identifiers for the boundaries spanned by each segment , the border 450 may have one boundary ID, and each exclusion zone 454 may have its own unique boundary ID, and geographic location of the segment may then be passed to a decision-making algorithm executed by the controller 120. In the example work area 400 shown in the simulation of 6B, the wavefront grid values are higher along segment 452L-2 see, , 5. As a result, the controller 120 may direct the travelling containment zone 402 to move forward with the segment 452L-2 becoming the new leading edge of the travelling containment zone. Once the decision is made regarding which segment will be pursued, the controller 120 may execute a population function, wherein it adds cells to the travelling containment zone , immediately forward of the new leading edge, and, optionally, a depopulation function, wherein it removes cells , by advancing the trailing edge of the travelling containment zone. In this way, the travelling containment zone may move across the work region while the mower operates therein. The travelling containment zone 402 may continue to travel downwardly , with leading edge 452L-2 in 6B, populating cells forward of the leading edge and ultimately depopulating cells along the trailing edge away from the segment 452L-1 shown in 6B. This process continues when the leading edge 452L-2 reaches the lower vertex 461 of the exclusion zone 454-1 see 6C. At this point, the leading edge of the travelling containment zone 402 can continue to move both downwardly between the right-hand boundary 450 and the containment zone 454-2, as well as move to the left between the containment zones 454-1 and 454-2. However, once the leading edge of the travelling containment zone contacts the upper right vertex 462 of the exclusion zone 454-2 in 6C, the leading edge again splits into two segments 452L-3 and 452L-4. To again avoid bifurcation of the travelling containment zone/leading edge, the controller 120 may receive cell scores from the scanning algorithm regarding the grid cells externally adjacent each of these segments based at least in part upon the cell values in the wavefront grid, and provide those scores to the decision-making algorithm. As evident in 5, the cells along the segment 452L-3 have a higher wavefront grid value than the cells along the segment 452L-4. As a result and as shown in 6C, the decision-making algorithm thus decides to move the travelling containment zone 402 402t3 downwardly by advancing the segment 452L-3 as the new leading edge. At some point, the segment 452L-4 may then become a trailing edge following the travelling containment zone downwardly, depopulating cells in the process. This process of scanning and making decisions that prevent bifurcation of the travelling containment zone/leading edge may continue throughout the work area 400. Typically, the decision-making algorithm will pursue population of cells that score higher, i. e. , those having a higher wavefront grid value. However, scoring may also consider other parameters as further described below. As shown in 6D, the travelling containment zone 402 402t4 may travel down and beneath the exclusion zone 454-2 and then, based on leading edge scoring, travel up the left side of the exclusion zone 454-2. As the controller 120 tracks those cells already visited, the travelling containment zone may ultimately cover the uncut area between the exclusion zones 454-1 and 454-2 abutting the segment 452L-4 in 6C. As shown in 6E, the travelling containment zone 402 402t5 may eventually dead-end i. e. , at the segment 452L-1 also shown in 6B. When this occurs, the travelling containment zone 402 may collapse to a minimally navigable size to ensure adequate coverage of the dead-end area. Once covered, the mower 100 may disable its cutting blades , de-energize the motor 112 1B and then travel to a portion of the work area 400 that is yet to be cut. This uncut area again may be determined by evaluating the wavefront grid values of the remaining uncut cells of the work area 400 and determining which cells have the highest scores. In the simulation shown in 6A-6G see 5, the controller 120 may thus command the mower 100 to move to the portion of the work area 400 in and around the cells adjacent the vertex 463 as shown in 6F. The cutting blades may then again engage, and the temporary cutting zone 402 402t6 may grow/move to cover the portion of the work area in the upper left quadrant of 6F. The travelling containment zone 402 , zone 402t6 in 6F may then advance until it dead-ends against the previously cut portion of the work zone 400. At this point, the mower 100 may again disable its cutting blades and travel to another portion of the work area 400 that is yet to be cut. Again, determination of the next starting point may be selected based upon the remaining cells having the highest wavefront grid value, , the portion of the work area in and around the cells near vertex 464 in 6G. The travelling containment zone 402 402t7 may then expand and travel to cover the remaining uncut portions of the work area 400, , those portions above, to the left of, and below the exclusion zone 454-3 in 6G. Once the entire work area 400 has been covered by the mower 100, the mower may de-energize its blades and return to the base station 220 for re-charging. The dead-end situation illustrated in 6E is not the result of encountering a physical boundary i. e. , encountering the boundary 450 or an exclusion zone 454, but rather the result of the leading edge segmentation algorithm implemented in the illustrated embodiments. That is, the dead-end encountered in 6E is really a virtual dead-end created by the previously established segment 452L-1 see 6B. Examples of hard dead-end scenarios that occur independent of work area segmentation are shown in, for example, encountering the boundary 350 in 3K, and encountering the right-most boundary 250 in 2. While not illustrated, methods and systems in accordance with embodiments of this disclosure may avoid at least some of these virtual dead-ends. For example, in the situation illustrated in 6E, the controller could alternatively recognize that the previous segment 452L-1 will ultimately result in a dead-end. It could then evaluate other segments of the travelling containment zone , the left-most edge 452L-5 in 6E to determine a travel path that could avoid such a dead-end result. For instance, instead of collapsing into the dead-end in 6E, the travelling containment zone 402t5 could instead, at or before the time represented in 6E, begin travelling to the left using the leading edge 452L-5. In this scenario, the static segment 452L-1 would eventually begin travelling to the left as well, forming the trailing edge of the travelling containment zone. The travelling containment zone could then continue to advance toward the left-most boundary 450 in the upper left quadrant of the work area 400, where it would then dead-end against the physical boundary 450. As evident from the description above regarding 6A-6G, travelling containment zone systems and methods in accordance with embodiments of the present disclosure may involve at least four processing functions: scanning scoring externally adjacent cells; decision-making deciding where to direct the travelling containment zone; populating cells adding cells along the leading edge of the travelling containment zone; and depopulating cells i. e. , removing cells along the trailing edge of the travelling containment zone. As used herein, externally adjacent refers, at any given time, to cells that are outside of the travelling containment zone, but are located adjacent to a boundary of the containment zone. 6H is an enlarged portion of the zone 402t2 shown in 6B. The cells illustrated in this view are enlarged for clarity. As stated above, actual grid cell size may be smaller than that illustrated , to provide a higher resolution grid. The work region itself may bound a first plurality of grid cells only some of these cells peripheral or externally adjacent to the edges 452L-1 and 452L-2 are illustrated in this figure, while the travelling containment zone bounds a lesser, second plurality of grid cells the second plurality being a subset of the first plurality of grid cells. At the particular point in time represented in 6H, the first processing function scanning may score two or more cells, wherein one of the two or more cells is selected from the cells 453 externally adjacent to the edge 452L-1; and another of the two or more cells is selected from the cells 455 externally adjacent to the edge 452L-2. Such scoring may again be based on parameters including, for example: evaluating or comparing wavefront grid values of each of these two or more grid cells; and comparing a distance from each cell of these two or more grid cells to a centroid 457 of the travelling containment zone. Based upon this analysis, the controller may execute the second decision-making function and select one of these edges to form the new leading edge of the travelling containment zone. The controller may then populate cells , add cells 455 , from the first plurality of grid cells to the travelling containment zone along the new leading edge 452L-2 third function and depopulate cells , remove cells 459 from the travelling containment zone along what will now be the trailing edge of the travelling containment zone fourth function. 7 is a flow chart illustrating a decision-making process 700 of the travelling containment zone , executed by the controller 120 in accordance with embodiments of the present disclosure. As described above, scoring values of relevant cells in the grid by the scanning function would be known before entry into this algorithm. The process is entered at 701. At 702, it is determined, based upon the scanning function described elsewhere herein, whether the travelling containment zone is dead-ended. If the answer is yes, the process proceeds to 704, wherein no further cells are populated, after which the process exits at 706. If the travelling containment zone is not dead-ended at 702, the process may next determine whether there is a previously selected leading edge of the travelling containment zone at 708. If the answer is no, the process may select the highest scoring segment in which to establish the leading edge of the travelling containment zone at 710 after which the process proceeds to 716. At 716, the process evaluates whether the leading edge is dead-ended. If the answer is yes, the process declares a dead-end at 718 and returns to 702. If, however, the leading edge is not determined to be dead-ended at 716, the process proceeds to populate the leading edge at 720, after which the process ends. If, on the other hand, the answer is yes at 708, the process may next determine if the leading edge has bifurcated or split at 712. If the answer at 712 is no, the process may process directly to 716. If, however, the answer at 712 is yes, the process may first select the highest scoring segment from the split at 714 before proceeding to 716. The populator function/algorithm may only be active during times that the travelling containment zone is travelling both leading edge and trailing edge are moving or filling a stage where the travelling containment zone is initially growing to the desired size as shown, for example, in 6A and 6B. In some embodiments, the score of a relevant cell as determined by the scanning function may be an aggregate score that depends on different factors. For example, one sub-score value may be based upon the wavefront grid value as described above, while a secondary sub-score value may be based upon, for example, a distance of the particular cell from a centroid or other geometric feature of the current travelling containment zone. These sub-scores may then be summed ensuring that units or weights of the respective sub-scores are consistent to yield the total cell score used in the decision-making algorithm. Moreover, while the entire leading edge may be scored by the scanning function, the controller 120 may only populate , add cells to the travelling containment zone a portion, for example, the top 20%-50%, 30% in one embodiment, of the cells along the leading edge. Like the populator function, the depopulator is only active during certain times, i. e. , when the travelling containment zone is travelling both leading and trailing edges are moving or emptying a stage where the travelling containment zone is contracting such as at a dead-end as indicated in 6E. In some embodiments, the depopulator function may simply analyze the entire border of the travelling containment zone and remove a portion of the oldest cells therein, , the oldest 40%-60%. However, in other embodiments, the depopulator function could also evaluate individual segments in a manner similar to the populator function in an effort to more efficiently remove cells from the travelling containment zone. Moreover, as with the populator function, the depopulator function could utilize a scoring algorithm , evaluated during the scanning function/phase to calculate various cell sub-scores. For example, in addition to cell age within the travelling containment zone, the scoring algorithm could sub-score each cell on a distance of the cell from the centroid or other geometric feature of the current travelling containment zone. Once again, the sub-scores may be added to obtain an overall cell score for depopulation. The concept of depopulation is an iterative process that seeks to remove a target number of cells equivalent to those added by the populator function when the travelling containment zone is moving, and remove an arbitrary target number of cells when the travelling containment zone is emptying when the travelling containment zone is filling, the depopulator function may be inactive until the zone reaches the desired size, and which point the depopulator may actively remove cells. While described herein in the context of a single mowing session, the mower 100 may be unable to complete mowing of the entire work region in a single session without exhausting its battery. Accordingly, the controller 120 may log positional data at all times and store such data regarding which areas were mowed and what time they were mowed. As a result, the mower may suspend cutting when the battery needs recharging, and then subsequently resume cutting after charging in the same area where cutting was previously suspended. As shown in 2, 4, and 6A, the mower system may also include the base station 220 that may be located in or near the work region. The base station may house the mower 100 between mowing sessions and permit the mower battery to recharge. In some embodiments, some aspects of the mower controller could be incorporated into the base station. For example, the base station 220 may, in addition or alternatively, incorporate the controller 120. In this instance, the base station 220 may be able to wirelessly and bidirectionally communicate with the mower , via Wi-Fi to receive data from, and provide data to, the mower. The complete disclosures of the patent documents and other various publications cited herein are incorporated by reference in their entirety as if each were individually incorporated. In the event that any inconsistency exists between the disclosure of the present application and the disclosures of any document incorporated herein by reference, the disclosure of the present application shall govern. Illustrative embodiments are described and reference has been made to possible variations of the same. These and other variations, combinations, and modifications will be apparent to those skilled in the art, and it should be understood that the claims are not limited to the illustrative embodiments set forth herein.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/09/324/109/0.pdf",
                    "CONTRADICTION_SCORE": 0.985042929649353,
                    "F_SPEC_PARAMS": [
                        "battery-powered"
                    ],
                    "S_SPEC_PARAMS": [
                        "difficulty accessing certain areas",
                        "unduly hung-up,",
                        "extending the time it takes to complete the mowing task before requiring recharging"
                    ],
                    "A_PARAMS": [
                        "layout of the yard",
                        "random pattern motion of the mower,"
                    ],
                    "F_SENTS": [
                        "Robotic lawn mowers are typically battery-powered and are often limited to cutting only a portion of the property before requiring re-charging, which typically requires the mower to return to a charging base station."
                    ],
                    "S_SENTS": [
                        "This occurs when the layout of the yard , the boundaries and excluded areas/obstacles, combined with the random pattern motion of the mower, leads to increased difficulty accessing certain areas , those areas of the yard having narrow entry passages.",
                        "Conversely, when the mower is able to reach these areas, it may get unduly hung-up, potentially extending the time it takes to complete the mowing task before requiring recharging."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Power"
                    ],
                    "F_SIM_SCORE": 0.4940493404865265,
                    "S_TRIZ_PARAMS": [
                        "Waste of Time"
                    ],
                    "S_SIM_SCORE": 0.5253323912620544,
                    "GLOBAL_SCORE": 1.6947337955236434
                },
                "sort": [
                    1.6947337
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11369445-20220628",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11369445-20220628",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-06-12",
                    "PUBLICATION_DATE": "2022-06-28",
                    "INVENTORS": [
                        "Gregory K. Toth",
                        "Nitish Swarup",
                        "Thomas R. Nixon",
                        "David Q. Larkin",
                        "Steven J. Colton"
                    ],
                    "APPLICANTS": [
                        "Intuitive Surgical Operations, Inc.    ( Sunnyvale , US )"
                    ],
                    "INVENTION_TITLE": "Tool memory-based software upgrades for robotic surgery",
                    "DOMAIN": "A61B 3432",
                    "ABSTRACT": "Robotic devices, systems, and methods for use in robotic surgery and other robotic applications, and/or medical instrument devices, systems, and methods include both a reusable processor and a limited-use robotic tool or medical treatment probe. A memory on the limited-use component includes machine readable code with data and/or programming instructions to be implemented by the processor. Programming of the processor is updated by shipping of new data. Once downloaded by the processor from a component, subsequent components take advantage of the updated processor without repeated downloading.",
                    "CLAIMS": "1. A surgical robotic tool removably couplable to a robotic system having a plug-and-play capability, the robotic system comprising a tool holder configured to receive and move the tool in a plurality of degrees of freedom and a signal interface for reading tool data from the tool, the tool comprising: an elongate shaft having distal and proximal ends; an end effector supported at the distal end of the elongate shaft by a distal joint so as to facilitate orienting the end effector within an internal surgical workspace; a proximal housing coupled to the proximal end of the elongate shaft, the housing adapted to be supported by the tool holder of the robotic system; and a memory coupled to the proximal housing, the memory adapted to electrically couple to the signal interface of the robotic system, the memory storing tool data comprising: an indicator allowing a robotic system to identify that the tool includes additional tool information; and a listing of tool parameters sufficient for the robotic system to direct movement of the tool in the surgical procedure and to update programming of the robotic system. 2. The tool of claim 1, the listing of tool parameters comprising more than 100 parameters. 3. The tool of claim 2, the listing of tool parameters comprising more than 1000 parameters. 4. The tool of claim 1, the tool data comprising legacy tool data. 5. The tool of claim 1, the tool data further comprising a most recent system processor software revision. 6. The tool of claim 1, the listing of tool parameters comprising at least one of lengths, parametric maximum torque information, and parametric range of motion information. 7. The tool of claim 1, the tool data further comprising a subroutine allowing the robotic system to drive a new type of kinematic assembly on the tool. 8. The tool of claim 1, the tool data comprising parametric values for at least one of tool tip length, grip open torque, grip close torque, and grip angle. 9. The tool of claim 1, the end effector comprising at least one of DeBakey forceps, microforceps, Potts scissors, clip applier including first and second end effector elements which pivot relative to each other so as to define a pair of end effector jaws, a scalpel, and an electrocautery probe having a single end effector element. 10. The tool of claim 1, the tool data further comprising tool use restrictions comprising at least one of limitations to training/demonstrations only, to engineering or prototype development, to clinical medical trials, or to use in surgery on humans. 11. The tool of claim 1, the tool comprising an ultrasonic imaging probe or a cardiac ablation catheter. 12. A method for driving a surgical robotic tool removably couplable to a robotic system having a plug-and-play capability, the robotic system comprising a tool holder configured to receive and move the tool in a plurality of degrees of freedom and a signal interface for reading tool data from the tool, the tool comprising an elongate shaft having distal and proximal ends, an end effector supported at the distal end of the elongate shaft by a distal joint so as to facilitate orienting the end effector within an internal surgical workspace, a proximal housing coupled to the proximal end of the elongate shaft, the housing adapted to be supported by the tool holder of the robotic system, and a memory coupled to the proximal housing, the memory adapted to electrically couple to the signal interface of the robotic system, the memory storing tool data, the method comprising: transmitting, from the memory to the signal interface, an indicator allowing a robotic system to identify that the tool includes additional tool information; and transmitting, from the memory to the signal interface, a listing of tool parameters sufficient for the robotic system to direct movement of the tool in the surgical procedure and to update programming of the robotic system. 13. The method of claim 2, the listing of tool parameters comprising more than 100 parameters. 14. The method of claim 13, the listing of tool parameters comprising more than 1000 parameters. 15. The method of claim 13, the tool data comprising legacy tool data. 16. The method of claim 13, the tool data further comprising a most recent system processor software revision. 17. The method of claim 13, the listing of tool parameters comprising at least one of link lengths, parametric maximum torque information, and parametric range of motion information. 18. The method of claim 13, the tool data further comprising a subroutine allowing the robotic system to drive a new type of kinematic assembly on the tool. 19. The method of claim 13, the tool data comprising parametric values for at least one of tool tip length, grip open torque, grip close torque, and grip angle. 20. The method of claim 13, the end effector comprising at least one of DeBakey forceps, microforceps, Potts scissors, clip applier including first and second end effector elements which pivot relative to each other so as to define a pair of end effector jaws, a scalpel, and an electrocautery probe having a single end effector element.",
                    "STATE_OF_THE_ART": "The present invention is generally related to medical and/or robotic devices, systems, and methods. For example, the invention allows the system software of robotic systems used in surgery or other applications to be revised by including update data in a memory of a tool which can be mounted on a robotic arm. The software update may facilitate the use of tools which were not available when the robotic system software was installed. In another example, a medical device system having a disposable component and a reusable processor may have the processor software updated by use of a disposable instrument, thereby allowing the instrument to be used for both treating tissues and updating system programming. Minimally invasive medical techniques are intended to reduce the amount of extraneous tissue that is damaged during diagnostic or surgical procedures, thereby reducing patient recovery time, discomfort, and deleterious side effects. While many of the surgeries performed each year in the US could potentially be performed in a minimally invasive manner, only a portion of current surgeries use these advantageous techniques due to limitations in minimally invasive surgical instruments and the additional surgical training involved in mastering them. Minimally invasive telesurgical systems for use in surgery have been developed to increase a surgeon's dexterity and avoid some of the limitations on traditional minimally invasive techniques. In telesurgery, the surgeon uses some form of remote control such as a servomechanism or the like to manipulate surgical instrument movements, rather than directly holding and moving the instruments by hand. In telesurgery systems, the surgeon can be provided with an image of the surgical site at the surgical workstation. While viewing a two or three dimensional image of the surgical site on a display, the surgeon performs the surgical procedures on the patient by manipulating master control devices, which in turn control the motion of servomechanically operated instrumentsThe servomechanism used for telesurgery will often accept input from two master controllers one for each of the surgeon's hands and may include two or more robotic arms. It will often be advantageous to change the position of the image capturing device so as to enable the surgeon to view the surgical site from another position. Mapping of the hand movements to the image displayed from the image capture device can help the surgeon provide more direct control over movement of the surgical instruments. While the new telesurgical systems and devices have proven highly effective and advantageous, still further improvements would be desirable. For example, a wider variety of surgical instruments may be adapted or developed for mounting to these new robotic arms for performing existing and new minimally invasive procedures. By having the flexibility of attaching a wide variety of new instruments having new surgical end effectors to existing telesurgical systems, surgeons may be able to perform more and more surgical procedures using minimally invasive techniques. The existing systems, including their software, processors, and manipulator actuation structures, are already being deployed in surgical rooms throughout the country and throughout the world. Unfortunately, as new robotic surgical instruments are developed, updating this base of existing capital equipment can be slow, expensive, and difficult to implement uniformly, particularly for the robotic surgical systems already located in other countries. For the reasons outlined above, it would be advantageous to provide improved devices, systems, and methods for robotic surgery. It would also be advantageous to provide improvements for other robotic applications. Similarly, as medical instrument systems having both reusable capital equipment with reprogrammable processors and disposable components proliferate, it may be advantageous to provide improved devices, systems, and methods for updating the capital equipment, as well as providing business methods for commercializing systems for telesurgery, robotics, medical instruments, and a variety of other fields.",
                    "SUMMARY": [
                        "The present invention generally provides improved robotic devices, systems, and methods for use in robotic surgery and other robotic applications. The present invention also provides improved medical instrument devices, systems, and methods for using these improved medical systems, particularly for medical systems including both a reprogrammable processor and a limited-use medical treatment probe. Business methods may also be provided by the present invention for commercializing capital equipment having related peripherals and limited-use components. In some embodiments, the invention may incorporate a memory into a limited-use component such as a robotic surgical tool. Machine readable code cant be stored in that tool memory with data and/or programming instructions to be implemented by the system processor. This can allow the programming of the system processor to be effectively updated by shipping of new surgical robotic tools or other disposables. Optionally, once the update data or programming instructions have been downloaded by the processor from a tool, subsequent tools can take advantage of the updated processor programming without requiring repeated downloading of the new software. This can avoid delays and expense, for example, when surgical instruments are repeatedly removed and attached to a robotic arm during one or more surgical procedures. In a first aspect, the invention provides a robotic system comprising a robotic arm having a tool holder and a signal interface. A plurality of robotic tools are each receivable by the holder for manipulation by the robotic arm. A processor has a memory and is coupled to the robotic arm. The processor memory comprises robotic tool data associated with the plurality of tools. The processor directs movement of a robotic tool received by the tool holder using associated tool data from the processor memory. A first additional tool has a memory with additional tool data or code. The memory of the processor may lack this additional tool data or code prior to coupling of the first additional tool to the robotic arm. The first tool transmits the additional tool data or code to the processor via the tool signal interface. The processor stores the additional data or code in the processor memory for use in directing movement of the robotic arm after the first additional tool is removed from the tool holder. The additional data or code may also be used while the first additional tool is still mounted to the arm. In the exemplary embodiments, the tools comprise surgical robotic tools having a plurality of differing surgical end effectors, so that the robotic system comprises a surgical robotic system. The first additional tool may have a tool type identifier. A second additional tool may also have the same tool type identifier. The processor may store the additional tool data from the first additional tool in the memory, and may, in response to the tool type identifier of the second additional tool, direct movement to the robotic arm using the additional tool data from the processor memory when the second additional tool is received by the tool holder. By making use of the additional tool data from the memory rather than downloading this new data each time a new tool is attached the system may avoid system delays during tool changes. The processor may store the additional tool data in an update list of the processor memory. The update list may comprise data associated with a plurality of additional tools of differing types. The processor memory may also comprise a native list of data associated with the plurality of tools. The processor may give priority to the additional tool data of the update list over the tool data of the native list when similar tool identifiers are included on both so as to allow effective revisions to the tool data of known tools. The memory may also include a reject tool list, with the processor inhibiting use of an additional tool having a rejected tool identifier included on the reject tool list. The processor may comprise machine readable code for directing movement of the tools. The machine readable code may embody a software capability identifier, often in the form of a software revision identifier, and the additional tool data may specify a minimum software capability and/or revision. The processor may inhibit use of the first additional tool if the software revision is older than the minimum software revision. This allows, for example, a distributor of the tools to specify a minimum processor software capability if a new tool is to be used, for example, so that a tool requiring a particular subroutine will only be used on systems having processors that include that subroutine. A second tool may include machine readable code embodying tool data, including tool-movement directing data and a maximum software revision. The processor may direct movement of the second tool using tool data from the memory of the processor when the software revision identifier of the processor is older than the maximum specified software revision of the second tool. This allows the processor to selectively make use of resident tool data in a memory of the processor rather than taking time to download that same information from an additional tool which has been sitting for several months or years, long after the processor software has already been updated, and possibly after the data in the tool memory has been updated. At least some of the plurality of tools may comprise legacy tools lacking some or all of the additional tool data. The additional tool data may include an indicator or flag so as to alert the processor that it should make use of the additional tool data. This indicator or flag may be turned off or negated in the legacy tools. Optionally, an input device may be coupled to the processor for receiving input movements, with the processor effecting an output movement of a tool received by the tool holder corresponding to the input movement so that the robotic system comprises a master-slave robotic system. A first configuration time may correspond to configuring the processor for directing movement using the tool data from the memory, and a second configuration time may correspond to configuring the processor for directing movement using additional tool data downloaded from the mounted tool. The second configuration time may be longer than the first configuration time. The processor may be adapted to limit cumulative configuration time by storing the additional tool data from the first tool in the memory, and using that stored additional data for the first additional tool after removal and attachment, a second additional tool, and/or the like. In another aspect, the invention provides a medical instrument system comprising a plurality of medical instruments. Each instrument has a tissue treatment delivery surface and an interface. A processor has an interface sequentially couplable with the instrument interfaces. The processor has a memory storing code or data, and controls treatment delivered by the tissue treatment delivery surfaces of the instruments using the stored code or data. A first additional instrument has a tissue treatment delivery surface, an instrument memory, and an interface coupled to the instrument memory so as to provide communication between the instrument memory and the processor when the additional instrument interface is coupled to the processor interface. The processor is configured to store update code or data from the instrument memory in the memory of the processor. A second additional tool has a tissue treatment delivery surface and an interface. The processor controls treatment delivered by the tissue treatment delivery surface of the second additional tool using the update code or data communicated from the instrument memory of the first additional instrument. In many embodiments, the plurality of medical instruments will comprise a plurality of medical instrument types. Each medical instrument may have an instrument memory coupled to the interface. The memory may comprise a type identifier, each instrument type having associated instrument type data embodied by the machine readable code. The processor may direct treatment via the tissue treatment surface of an instrument coupled with the processor using associated instrument data in response to the type identifier of the coupled instrument. The second additional instrument may have the same type identifier as the first additional instrument. In another aspect, the invention provides a robotic method comprising sequentially mounting a plurality of robotic tools of different types to a robotic arm. Movement of each sequentially mounted robotic tool is directed with a processor using data or code associated with the mounted robotic tool type. The data or code for the plurality of tools is stored in the memory of the processor. A first additional robotic tool is mounted to the robotic arm. Update tool data or code is transmitted from a memory of the first tool to the processor. Movement of the robotic arm is directed by the processor using the update tool data or code. The tools may comprise surgical robotic tools having a plurality of different surgical end effectors. The movement of the surgical end effectors may be directed by a surgeon moving an input device so as to perform a surgical procedure. Hence, the robotic arm and input device may be components of a master-slave surgical robotic system. There will often be a configuration time or a download delay involved when the processor downloads the additional tool data from a mounted tool. This configuration time may be substantially greater than the time involved in configuring the processor for directing movement of a tool when the desired data is already resident in a memory of the processor. To limit the overall reconfiguration time and avoid delays during tool changes, the processor may store the additional tool data downloaded from a mounted tool for use with other tools of a similar type. The additional tool data may be stored in an update list of the memory, while the tool data associated with the known tool types may be stored in a native list of the memory of the processor. When a tool identifier is listed in both the native memory so that the tool was known when the processor software revision was loaded into the processor, for example but the tool data is later updated for example, to lower a grip strength limit to increase safety margins based on experience when using the tool in the field the same tool type may be included in both lists. Under such circumstances, it may be advantageous to give priority to the update list, for example, by checking the update list first, and if a tool identifier is found therein, making use of that information and not going on to check the native list. This allows tool data to be effectively updated by distributing appropriate additional tools having revised tool data. Additional options include a reject list in the processor memory, with the processor inhibiting use of a tool having a tool identifier included on the reject tool list. The invention can facilitate the distribution of tools which are intended for use with existing robotic systems and other capital equipment, but which tools have been developed after the systems have been installed in the field. However, not all previously installed systems may have the capability of utilizing newly-developed tools. For example, an installed robotic surgery system may have software that is not configured to accommodate a sliding joint at the distal end effector. If a new tool is developed which makes use of such a sliding joint, it may be easier to update the software of an installed robotic surgery system using magnetic or optical recording media, a network, or other conventional means, rather than taking the time to download an extensive software revision via a surgical robotic tool coupled to a robotic surgery system in the middle of a surgical procedure. Similarly, if a new tool is developed which requires hardware that is not installed on some robotic systems, such as an ultrasound driver for an ultrasound tool. Advantageously, the processor memory may store system capability data, and the additional tool data may identify a minimum system capability. The processor can inhibit use of the first additional tool if the system capability data is incompatible with the minimum system capability of the first additional tool. For example, the processor in the memory may store a software revision identifier and the additional tool data may include a minimum software revision. After a tool has been distributed to systems in the field for an extended period of time, software revisions may be implemented on the installed systems for a variety of reasons. If the programming of the processor is to be updated, it will often be advantageous to avoid any time delay involved in downloading tool data to the processor memory by including the additional tool data with a system software revision. The memory of the processor may store a software revision identifier, and a second tool may have a tool memory with tool data including a maximum software revision. Although the tool memory may also include data for use in directing movement to the tool, the processor may direct movement of the second tool using tool data from the processor memory in response to the software revision identifier of the processor being older than the maximum software revision of the second tool. This may avoid, for example, the use of obsolete tool data stored on a memory of an old tool. At least some of the plurality of the tools may comprise legacy tools. Such legacy tools need not necessarily include a tool memory with tool data sufficient for using the tool if the tool is not known to the robotic system. Nonetheless, a tool type identifier may be transmitted from each legacy tool mounted on the arm to the processor. The processor may identify the tool data in the processor memory using the transmitted tool type identifier. Optionally, the processor may differentiate between legacy tools and tools having full downloadable additional tool data based on data transmitted from the tool. Ideally, systems sold after legacy tools will still be compatible with the earlier-distributed legacy tools, while many newly-distributed tools having downloadable tool data will still be compatible with older systems, even if the older systems cannot fully take advantage of the downloadable data. In another aspect, the invention provides a method comprising mounting a first robotic tool to a robotic arm. Movement of the mounted first tool is directed with a processor using first tool data. The first tool data is stored in a memory of the processor prior to mounting the first tool. A second robotic tool is mounted to the robotic arm. The second robotic tool has a memory with data associated with the second robotic tool. The second tool data is transmitted from the second tool to the processor. Movement of the mounted second tool is directed with the processor using the transmitted second tool data. The second tool data need not be stored in the memory of the processor prior to mounting the second tool. A third robotic tool may be mounted to the robotic arm. The third robotic tool may have a tool identifier corresponding to a tool identifier of the second tool. Movement of the third tool may be directed by the processor using the transmitted second tool data. This can avoid repeated delays that might be imposed by downloading similar tool data from tools of the same type into the processor. The third tool may be mounted to the robotic arm prior to mounting the second tool. Movement of the third tool may be directed using third tool data prior to mounting of the second tool. The second tool data may supercede the third tool data for tools having the same tool type identifier as the second tool after the second tool is mounted. In another aspect, the invention provides a medical instrument system revision method comprising sequentially coupling a plurality of medical instruments to a processor, each instrument having a tissue treatment delivery surface and an interface. Treatment delivery by the tissue treatment delivery surfaces of the sequentially coupled instruments may be controlled using data stored in a memory of the processor. A first additional instrument may be coupled to the processor. The first additional instrument may have a tissue treatment delivery surface and an instrument memory. Update data may be transmitted from the memory of the additional instrument to the processor. The transmitted update data may be stored in the memory of the processor, and treatment delivery from a tissue treatment delivery surface of a second additional tool may be controlled using the stored update data. The update data may optionally define a machine readable code embodying program instructions for implementing method steps for treating tissue using the tissue treatment delivery surface of a tool. Optionally, the update data may comprise data representing physical parameters of a treatment tool type. For example, when used with a surgical robotic system, the update data may represent geometric joint locations, tool strength limits, offsets, and the like. The additional instrument may be decoupled from the processor and later recoupled back with the processor. The treatment delivery may be controlled with the recoupled additional instrument using the stored update data so as to avoid repeated downloading of the update data from the memory of the additional instrument. In another aspect, the invention provides a method comprising providing a plurality of systems. Each system has a processor, and the systems are provided to a plurality of customers. A plurality of peripheral or limited-use devices are distributed to the customers. Each device can be coupled to a system so as to perform a primary function. Programming code or data of the processors are updated using memories of the coupled devices so as to alter the performance of the primary function after the devices are decoupled from the systems.",
                        "1A and 1B are perspective views illustrating a master surgeon console for inputting a surgical procedure and a robotic patient-side cart for robotically moving surgical instruments at a surgical site, respectively. 2 is a side view showing an exemplary input device for use with the master control station of 1A. 3 is a perspective view of an exemplary robotic surgical instrument or tool having a memory and a data interface. 4 shows a block diagram representing control steps followed by the control system of a minimally invasive surgical robotic apparatus in effecting movement of the end effector of the tool of 3 in response to movement of the input device of 2. 5 is a functional block diagram schematically illustrating components of a robotic surgical system according to the principles of the present invention. 6A and 6B schematically illustrate additional or new tool data, and legacy tool data, respectively, as stored in memories of robotic surgical tools for use in the system of 5. 7 is a flow chart illustrating program software embodying a method for using the system of 5, in which tools or surgical instruments can be used to update the software of a robotic surgical system. 8 is a flow chart illustrating program software embodying an alternative method for use of the system in 5. 8A-8F are perspective views of a variety of different end effectors for tools of different types."
                    ],
                    "DESCRIPTION": "The present invention generally provides robotic and/or medical instrument systems, devices, and methods. The invention is particularly useful for updating reconfigurable control systems for robotic and medical instrument processors, often allowing upgrades to be implemented in systems having processors that were programmed for use with a variety of instruments or tools. By including a memory with a new instrument or tool, and by including a data interface in the coupling arrangement between the instrument or tool and the system processor, the instrument or tools may be used as a software update distribution mechanism. This allows a system manufacturer to increase the software capabilities of robotic or medical instrument systems sold to customers throughout the US and/or throughout the world, without requiring that support personnel personally visit the installed instruments, and without having to rely entirely on telecommunications links such as the Internet. While the most immediate application for the present invention may include robotic surgical systems, the inventions described herein may also find applications in other robotic systems. Along with master-slave telerobotic systems, the invention may find applications in more conventional autonomous industrial robots and the like. The invention may also find advantageous applications for use in other medical instrument systems, particularly those having both reprogrammable processors and disposable or other limited-use treatment probes. The invention may find applications in computing and telecommunication systems to allow programming to be updated using peripheral or disposable elements having some other primary purpose, including updating of the software of a printer or computer using a memory of an ink-jet or other printing cartridge, or the like. A tool or instrument used to reprogram the system software is sometimes referred to as an update tool or instrument, an additional tool or instrument, and/or a new tool or instrument. As used herein, the term tool encompasses robotic tools that have robotic end effectors for coupling to robotic systems. The term instrument encompasses medical instruments including those having medical treatment surfaces such as surgical end effectors in the form of graspers, scalpels, electrosurgical probes, ultrasound probes, and the like. In the exemplary embodiment, a robotic surgical system makes use of any of a series of removable and replaceable end effectors supported by a robotic arm, so that the end effector assembly is both a tool and an instrument. In robotic embodiments used in non-surgical applications, the end effector assembly may comprise a robotic tool that is not a medical instrument. Similarly, in non-robotic medical instrument embodiments, the instrument assembly having the treatment surface need not be an articulable robotic tool. Nonetheless, those skilled in the art should understand that non-medical robotic systems and non-robotic medical instrument systems may include many of the components, programming, and interactions described herein. The data, reprogrammable software, program method steps, and method steps described herein may be embodied in a machine readable code and stored as a tangible medium in a wide variety of differing configurations, including random access memory, non-volatile memory, write once memory, magnetic recording media, optical recording media, and the like. Hence, the term code can encompass both programming instructions and data. Along with software, at least some of the programming and data may be embodied in the form of hardware or firmware. The term additional data encompasses revised, corrected, or different parameters or other information, as well as new parameters and the like. Referring to 1A of the drawings, an operator workstation or surgeon's console of a minimally invasive telesurgical system is generally indicated by reference numeral 200. The workstation 200 includes a viewer 202 where an image of a surgical site is displayed in use. A support tool 4 is provided on which an operator, typically a surgeon, can rest his or her forearms while gripping two master controls see 2, one in each hand. The master controls or input devices are positioned in a space 206 inwardly beyond the support 204. When using the control workstation 200, the surgeon typically sits in a chair in front of the control station, positions his or her eyes in front of the viewer 202 and grips the master controls, one in each hand, while resting his or her forearms on the support 204. In 1B of the drawings, a cart or surgical station of the telesurgical system is generally indicated by reference numeral 300. In use, the cart 300 is positioned close to a patient for which surgery is planned, and the base of the cart is then maintained at a stationary position until a surgical procedure has been completed. Cart 300 typically has wheels or castors to render it mobile. The workstation 200 is typically positioned at some distance from the cart 300, optionally being separated by a few feet within an operating room, although cart 300 and workstation 200 may alternatively be separated by a significant distance. The cart 300 typically carries three robotic arm assemblies. One of the robotic arm assemblies, indicated by reference numeral 302, is arranged to hold an image capturing device 304, , an endoscope, or the like. Each of the two other arm assemblies 10 includes a surgical instrument 14. The endoscope 304 has a viewing end 306 at a distal end of an elongate shaft. Endoscope 304 has an elongate shaft to permit viewing end 306 to be inserted through an entry port into an internal surgical site of a patient's body. The endoscope 304 is operatively connected to the viewer 202 to display an image captured at its viewing end 306 on the viewer. Each robotic arm assembly 10 is normally operatively connected to one of the master controls. Thus, the movement of the robotic arm assemblies 10 is controlled by manipulation of the master controls. The instruments 14 of the robotic arm assemblies 10 have end effectors mounted on wrist members, which are in turn pivotally mounted on distal ends of elongate shafts of the instruments 14. Instruments 14 have elongate shafts to permit the end effectors to also be inserted through entry ports into the internal surgical site of a patient's body. Movement of the end effectors relative to the ends of the shafts of the instruments 14 is also controlled by the master controls. The robotic arms 10, 302 are mounted on a carriage 97 by means of setup joint linkages 95. The carriage 97 can be adjusted selectively to vary its height relative to a base 99 of the cart 300, as indicated by arrows K. The setup joint linkages 95 are arranged to enable the lateral positions and orientations of the arms 10, 302 to be varied relative to a vertically extending column 93 of cart 300. Accordingly, the positions, orientations and heights of the arms 10, 302 can be adjusted to facilitate passing the elongate shafts of the instruments 14 and the endoscope 304 through the entry ports to desired positions relative to the surgical site. When the surgical instruments 14 and endoscope 304 are so positioned, the setup joint arms 95 and carriage 97 are typically locked in position. Workstation 200 and cart 300 are described in more detail in 6,424,885, the full disclosure of which is incorporated herein by reference. An exemplary input device 220 and surgical instrument 14 are illustrated in 2 and 3, respectively. Input device 220 includes an arm 222 and a wrist 224 which allow positional and orientational movement of an input handle 226 relative to the structure of workstation 200 see 1A. Handle 226 will generally move with a plurality of degrees of freedom relative to the workstation structure, the exemplary input device 220 providing six degrees of freedom for movement of handle 226. The linkage supporting the handle may include more or less than six degrees of freedom. Referring now to 3, surgical instrument 14 generally includes a surgical end effector 50 supported relative to a housing 53 by an elongate shaft 14. 1. End effector 50 may be supported relative to the shaft by a distal joint or wrist so as to facilitate orienting the end effector within an internal surgical workspace. Proximal housing 53 will typically be adapted to be supported by a holder of a robotic arm. As described in more detail in 6,331,181, the full disclosure of which is incorporated herein by reference, instrument 14 will often include a memory 230, with the memory typically being electrically coupled to a data interface of a holder engaging surface 232 of housing 53. This allows data communication between memory 232 and a robotic surgical processor of workstation 200. More specifically, as can be understood by reference to 1A through 4. Workstation 200 shown in 1A will typically include a processor 210. Processor 210 effects corresponding movement of a surgical movement 14 mounted to robotic arm 10 in response to movement of an input handle 226. In the exemplary embodiment, processor 210 comprises software embodying a control logic 212 illustrated schematically in 4. This control logic effects movement of an end effector 50 within an internal surgical site by pivoting an instrument shaft 14. 1 about a minimally invasive insertion point. The control logic 212 employed by processor 210 generates motor drive signals in response to an input handle movement. These motor drive signals are transmitted to the robot arms, and cause to effect movement at the end effector that correspond to movement at the input handle, as is described in more detail in the '885 patent, previously incorporated herein by reference. As can be understood by reference to the diagram of the control logic illustrated in 4, the joint kinematics of both the robotic arm 10 and the surgical instrument 14 are simulated within the controller. A wide variety of robotic surgery instrument types are described in the patent literature. These different robotic too types are often removed and replaced during a robotic procedure, so as to allow the surgeon to perform differing functions. For example, a scissor structure may be removed and replaced with an electrosurgical scalpel. Such different tool types may have wrists or other tool joints with different geometries, such as differing separation distances between joint axes. Different tool types may also have differing ranges of motions about each axis, different joint binding positions or singularities, and/or other differences in their joint geometries, as can be understood with reference to 8A-8F. Additionally, two different surgical instrument end effector structures will often have differing strengths, different inertias, different effective gearing ratios between motion about their axes and movement of associated driving elements, and the like. Still further differences between different tool types may include the presence or absence of an electrosurgical capability, the useful life of a tool, the ability to replace end effector elements, or the like. Logic 212 of processor 210 can accommodate these differing tool kinematics and properties when information regarding the tool type currently mounted to a robotic arm 10 is made available to the processor. Referring now to 5, a robotic surgical system 500 is schematically illustrated as including a master controller 502 coupled to a slave robotic arm 504 so as to move a robotic tool 506. Tool 506 is held by a tool holder 508 of robotic arm 504. A plurality of alternative tools 510 are available to sequentially replace tool 506 by detaching tool 506 from the holder 508, and instead engaging a selected tool from among the plurality of tools on the robot arm. To allow the logic of controller processor 508 to adjust for the tool currently coupled to the slave 504 the holder 508, data from the mounted tool is transmitted via the holder to the slave and on to the processor 502. The transmitted tool data typically includes a tool type identifier. The plurality of tools 510 may include two different groupings of tool types: known or legacy tools 512 and new or additional tools 514. Known tools 512 may include tool types which were developed and known when master controller processor 502 was programmed, when the last software revision downloaded to processor 502 was written, or the like. Known tools 512 include tools that have tool type identifiers sometimes referred to as Tool Unique Identifiers TUID which are included in a memory 516 of processor 502. In some embodiments, it may be desirable to distribute concurrently both plug-and-play or system updating tools and non-plug-and-play tools. This may decrease average tool costs while still allowing many of the capabilities described herein. Advantageously, when a known tool such as tool type 110 is mounted to slave robotic arm 504, transmission of the tool type identifier TUID 110 to the master controller 502 via tool holder 508 provides sufficient information for the master controller processor to reconfigure its software to properly control movement of the slave, instrument, and end effector. More specifically, the tool type identifier is sufficient for processor 502 to look up the tool kinematic data from the native list 518 of processor memory 516. This can be done quite quickly, without delaying use of the robotic tool by the surgeon. In contrast, new tools 514 may include end effectors, driving systems, tool strengths, or other tool characteristics that have been developed or revised since the programming of processor 502. To allow such a new tool type such as tool TUID 306 to be controlled by processor 502, the processor may determine from the tool type identifier that the desired tool kinematic data is not available in the memory 516 of the processor. The processor may then download the desired tool kinematic information via holder 508 or some alternative data interface and store this new information in an update list 520 of the processor memory 516. This effectively allows the processor to be reprogrammed to accommodate the new tool. The information included in tool memory 230 of a new tool type is schematically illustrated in 6A. Legacy tool data 232 resident in a memory of a known tool is schematically illustrated in 6B. As used herein, the term plug-and-play and the initials PNP encompass the capability of a robotic or medical instrument and/or system to update the system programming from a memory of a tool or instrument. In the exemplary embodiment, legacy tool data 232 may be embodied in memory 230 of instrument 14 see 3 and 6B, with the exemplary memory comprising Dallas part DS 2505. Circuitry coupling the memory of the instrument to the instrument interface, and details regarding the instrument interface, can be found in 6,331,181. Alternative instruments, circuitry, and interfaces are also known, including those employed by the Zeus robotic surgical system previously commercialized by COMPUTER MOTION, Inc. of Santa Barbara, Calif. The exemplary legacy tool data includes legacy information with a tool identifier that can be used to identify that particular tool, along with tool type identifier data. A unique Dallas chip number may be used as the tool identifier, and the tool identifier allows tracking of the use of that particular tool over its lifetime throughout numerous different surgical procedures on a plurality of different robotic arms and/or robotic systems. Some portion of the legacy information that is consistent throughout all the legacy tools may be used as a negative PNP flag so that robotic system 500 can determine that the legacy tool does not include additional tool data. Referring now to 6A, additional or PNP tool data 230 includes legacy information analogous to that of legacy data 232 in 6B. This facilitates reverse compatibility and use of a new tool with systems 500 that do not have plug-and-play capability. However, the legacy information of new tool data 230 will include a PNP flag. This allows a system having plug-and-play capability to identify that the new tool includes the additional tool information and to make use of that information. Along with legacy information, new tool data 230 will generally include tool update information 234 with a listing of tool parameters sufficient for system 500 to safely and effectively direct movement of the new tool in the surgical procedure. Several tool parameters may be listed, often comprising more than 10 parameters, and optionally comprising more than 100 or even more than 1000 parameters. In exemplary embodiments, new tool data 230 may comprise 2K, 8K, or 64K of data or more, exemplary memories on which the additional tool data 230 is stored comprising DALLAS SEMICONDUCTOR part numbers DS2505, DS2505V, or DS2505P. In addition to both legacy information and tool update information 234, new tool data 230 may include limited PNP info so as to allow a limited plug-and-play capability for older robotic surgical systems. In this exemplary embodiment, as will be described with reference to 8 below, the information may include a limited number of parameters, with each parameter corresponding to an associated tool parameter included in the full new tool data so that these tool parameters are repeated in the tool memory. An older robotic surgical system may download any parameter listed in the limited PNP data from the tool to the memory 516 of processor 500, optionally for use only with the mounted tool. Referring now to 5, memory 516 of processor 502 may store the tool data in a variety of alternative tangible media, including magnetic recording media, optical recording media, RAM, ROM, and the like. In the exemplary embodiment, tool data may be stored, at least in part, in NVRAM for the update list and in a flash memory for the native and reject lists. Tool data may also be stored at least in part in a random access memory of the master controller processor 502 during use. While a tool is mounted it may be beneficial to avoid altering the system behavior for that tool, even if an update tool of the same tool type is mounted to a different arm of the system. Once a mounted tool is removed and re-mounted, the system may take advantage of new data. Use of tool kinematic and strength information is more fully described in 6,424,885, previously incorporated herein by reference. As used herein, tool data is stored in a memory of processor 502 when it is embodied in tangible media of system 500 which remains coupled to the system processor when a mounted tool is removed and replaced by any of the plurality 510 of alternative tools. Hence, data which is downloaded from the tool and recorded on magnetic or optical recording media, embodied in an EEPROM, or temporarily stored in a RAM of processor 502 or any other peripheral storage device of processor 502 is considered stored in a memory of the processor. Processor 502 will typically comprise software and/or hardware capable of implementing program steps embodied in machine readable code so as to effect the methods of the present invention. In the exemplary embodiment, processor 502 comprises a processor 210 of controller workstation 200 see 1A, ideally comprising a board having a plurality of commercially available processors, including Share processors available from ANALOG DEVICES, Power PC processors available from MOTOROLLA, as well as memory and the like. The exemplary programming code is primarily written in C programming language, but a wide variety of other languages could also be used. Processor 502 may also comprise software and/or hardware distributed throughout the robotic surgical system, with arms 10, cart 300, and even holder 508 [see 5 and 1B] often having local processing capability in any of a wide variety of distributed processing arrangements. Alternative processor architectures which may be adapted to make use of the methods and devices of the invention include those of the Zeus robotic surgery system previously commercialized by COMPUTER MOTION, INC. Referring now to 7, a method 600 embodying the present invention starts when a tool is loaded 602 or mounted onto a robotic arm of the robotic system. Legacy information is downloaded from the memory of the tool, allowing the system to determine whether the tool memory has updated tool information 604. The data initially downloaded from the tool may include more than just the legacy data. In fact, if only legacy information is available the remainder of the initially downloaded information may be blank, which may be used as an indicator that the tool is a legacy tool. Downloading of this initial information from the tool to the processor of the system may be performed during mechanical engagement confirmation movements of the tool through the tool holder or interface, and thereby does not necessarily delay a surgical procedure. The downloaded legacy information will be sufficient to also allow the system to identify the tool type 606. In the exemplary system, the tool type identifier is calculated from one or more legacy fields in the legacy data. Alternative systems may simply read the tool type identifier from the tool memory. First assuming that the mounted tool is a legacy tool which does not include the additional tool data, after the processor calculates the tool type identifier 606 the processor determines whether the tool is of a type that has previously been updated 608 for example, by previously loading a plug-and-play tool of the same type as that which has been mounted onto the robotic arm. The processor may determine whether the tool type data has already been updated by comparing the tool type to update list 520 in the memory 516 of processor 502 see 5. If the tool type is indeed included in the update list, processor 502 may control movement of the robotic arm and mounted tool using data already resident in the processor memory 516, thereby supporting the mounted instrument 610 without delaying for downloading of additional tool data. Supporting of the instrument may comprise, for example, reconfiguring software of the processor using the parametric tool data from memory 516 for the tool type identifier of the tool. If the tool type is not listed in the update list, processor 502 may then determine if the tool type identifier is already included in the native list 518 of the processor memory 516 at step 612. If the tool identifier is included in native list 518, the processor can again support the mounted instrument 614 without downloading the additional data from the tool. Specifically, processor 502 is reconfigured using native list tool parametric information that is again already present in tool memory 516 when the tool is mounted to the robotic arm. Hence, once again downloading delays can be avoided. Processor 502 effectively gives priority to the update list over the native list, so that tool parametric information may be effectively revised by distributing tools of an existing tool type having a tool identifier of a known tool. For example, if a known tool type is found to perform more reliably if a prior strength of an infector torque is reduced, this allows updating of how these existing tools are used in the field. Optionally, the manufacturer may elect not to update tool types of previously-sold tools so as to avoid having a single tool or two tools at the same time react differently before and after an update or plug-and-play tool is used on a system. As we have assumed above that the mounted tool is not plug-and-play capable at step 604, if the desired tool parametric information for supporting the instrument is not available in processor memory 516 in neither the native list 518 nor the update list 520, the system may reject the mounted instrument 616 so as to avoid attempting control movement of the tool without sufficient tool data. Note that along the path of the flow chart that assumes that the instrument is not plug-and-play capable, processor 502 first checks for the tool information on the update list 520 before checking for the tool identifier on the native list 518. The preceding discussion assumed that the mounted tool was not plug-and-play capable at step 604, that is, that the mounted tool was a legacy tool. If we instead assume that the tool mounted 602 on a robotic arm has additional tool data that may be used to update a system processor, the system processor may determine from the legacy data downloaded from the tool that the tool has plug-and-play capabilities at step 604. Despite the presence of this additional tool information in a memory in the tool, it may be advantageous to avoid downloading some or all of the additional tool data so as to limit delays during the tool swap. In method 600, the processor determines whether the mounted plug-and-play capable tool is included on a reject list 618. Rejected tools may, for example, comprise tools of a type which are not supported by the robotic system, tools of a type which are no longer supported by any robotic system, or specific tools which have been identified, after distribution, as being unsuitable for use. Hence, tool rejection may be based on comparison of a tool type identifier or specific tool identifier with a reject list 522 in memory 516 of processor 502. Any tools or tool types included on reject list 522 will be rejected 620, with the system optionally providing an indication to the system operator and/or surgeon, the system inhibiting movement of the tool or insertion of the tool end effector into a patient, and the like. If the loaded plug-and-play tool is not on the reject list, processor 502 may determine whether the system software of processor 502 is more up to date than the tool memory at step 622. This may be implemented by storing in the tool memory the most recent system processor software revision at the time a new tool is first ready for distribution. Any subsequent software revision will include the data for this new tool in the native list of the processor memory, so any system software revision that is more recent than the system revision information stored on the tool memory does not need to take advantage of the additional tool data stored on the tool memory. Effectively, this means that despite the tool memory and tool plug-and-play capabilities, the tool is treated like a known tool 512, and the additional tool data is obsolete. In method 600, if the processor determines that the additional tool data in the tool memory is obsolete at step 622, the tool is treated as a tool without plug-and-play capabilities. Note that the tool parametric information included in a subsequent system software revision may be different than and more recent than the additional tool data stored on the tool itself so this avoids both delay in downloading additional tool information and use of obsolete tool information from the tool itself. If the software revision of the processor does not post-date the additional tool data in the tool memory, the processor then checks whether the system has sufficient capabilities for using the tool in step 624. In some embodiments, this may be implemented by determining whether the current software revision of the processor is equal to or more recent than a minimum software revision specified in the tool memory. If not, the tool may have been designed to be used only by systems having, for example, software subroutines that are not currently available to the system processor. The additional tool data will often comprise parametric data such as parametric link lengths, parametric maximum torque information, parametric range of motion information, and the like. If an entirely new type of kinematic assembly is included in a new tool, such parametric updating of the processor software may be insufficient absent a new subroutine. While it may be possible to load such new subroutines in a tool memory, limiting the additional tool data to parametric data limits the download time and hence tool swap delays. In other embodiments, at step 624 the system may determine whether hardware capabilities of the system are sufficient to support the tool based on a comparison of minimum system capability information from the tool memory and actual system information for robotic system 502. For example, if an ultrasound surgical tool is intended to be mounted on robotic arms having ultrasound drivers, the incompatibility of the tool with the system capabilities may be identified in step 624. If the mounted tool is found to be incompatible with the system capabilities, method 600 determines whether the tool type is included in either the update list or the native list possibly indicating that the incompatibility information in the tool memory is outdated, and if the tool type is not listed in either, the mounted tool is rejected in step 616. Assuming that the processor of the system determines that the current software revision is not more recent than the information in the tool memory, and that the system has all capabilities required for proper use of the tool, the processor then determines whether the tool type of the mounted tool is already included on the update list in step 626. If the tool type is on the update list 520 of processor memory 516, the processor can use this existing additional tool data in its memory to support the instrument 610 without delaying for downloading of the additional tool data from the tool itself. Note that the update list of the processor memory may include additional tool data that is the same as the additional tool data on the tool, or may include more current additional tool data from a subsequently distributed tool of the same type having plug-and-play capabilities. If the update list does not include the tool type of the loaded tool, or if the data of the loaded tool is more recent than the update list data for that tool type, the system processor goes on to check whether the native list 518 includes data which is more recent than or as current as the additional tool data from the tool memory in step 628. If the native list includes information for the tool type that is at least as current as the additional tool data from the tool memory, the system supports the mounted tool using the native list data. If current additional tool information is not already available in either the update list or the native list, the system processor directs downloading of the additional tool data into the update list 630. In the exemplary embodiment, making use of tool data resident in the memory of processor 502 is the most rapid way to reconfigure the processor for a tool swap to a different tool type. Downloading of additional tool data from the memory of the tool will typically involve a delay of greater than one second, in some embodiments require a delay of greater than 10 seconds, and in at least one embodiment a delay of about 14 seconds depending on system state as compared to making use of information already stored in a memory of the processor. Movement of a mounted tool is directed using six Sharc processors sold by ANALOG DEVICES of Massachusetts, ideally using a controller transform processor CTP. Method 600 illustrating 7 is largely implemented using Middleman code, with the method being initiated by the Supervisor code data. Transferred data from a mounted tool is optionally preprocessed at circuitry of the slave robot arm. A wide variety of specific implementations may be employed within the scope of the present invention. Referring now to 8, some processors may not include all the capabilities of or be employed in systems having all the need for every aspect described above regarding method 600. In a simplified plug-and-play method 700, a tool is loaded onto a robotic arm and transmission of at least some of the data from a memory of the tool to a system processor is initiated so as to allow the processor to determine whether the tool type is included on a known tool list 704. If the tool type and/or tool identification is included on the known list, and if the tool is not plug-and-play capable 706, the tool can be supported using native data in the system processor 708. If the processor determines 704 that the tool type is not on the known list, the processor may simply reject the instrument 710. Hence, the methods of the present invention may not necessarily be used to allow the use of new tool types. Instead, if the system processor determines that the tool is a known type but that the tool has plug-and-play capabilities, the processor may support the tool by downloading at least a portion of the additional tool information and using that downloaded information to configure the processor 712 if certain conditions are met. The exemplary system downloads limited PNP parameters of tool data 230, as seen in 6A. The processor may check that the downloaded data integrity appears acceptable by verifying a header stream including the limited PNP data in step 714. The processor may then verify that the current software revision of the processor is compatible with the tool per the downloaded additional tool data in step 716. If the tool integrity does not appear sufficient or the current software revision is not compatible, the tool is rejected. If the data integrity is acceptable and the software is compatible, the downloaded additional tool data from the memory of the tool is used to direct movement of the tool. In simplified plug-and-play method 700, not all available additional tool information on the tool memory need necessarily be downloaded and/or used by the system. As mentioned above regarding 6A, the additional tool information may include limited PNP information 230. In the exemplary embodiment, the limited PNP information comprises a series of parameters. If the system has limited rather than full or no plug-n-play capabilities and a parameter is listed among the limited PNP data, the system will use the parameter from the tool update data rather than that of the native list by reconfiguring the processor software using that updated parameter. In the exemplary embodiment, the limited PNP data comprises parametric values for tool tip length, grip open torque, grip close torque, and grip angle. A variety of alternative end effectors for alternative tools are illustrated in 8A through 8F. Several of these end effectors, including DEBAKEY forceps 56i, microforceps 56ii, Potts scissors 56iii, and clip applier 56iv include first and second end effector elements 56a, 56b which pivot relative to each other so as to define a pair of end effector jaws. Other end effectors, including scalpel 56v and electrocautery probe 56vi have a single end effector element. The tool data may optionally correlate to a limitation on an acceptable use for a specific mounted tool and/or a tool type. Tool and/or tool type identifiers may be stored on one or more tool use lists in the system memory, and the processor may alter or limit use of the tool in response to a comparison of a use list to the downloaded tool or tool type identifier. In some embodiments, the tool use data may be stored in the memory of the tool. Regardless, tool use restrictions may limit tools to training/demonstrations only, to engineering or prototype development, to clinical medical trials, or may allow use in surgery on humans. Tool life may vary with these different uses, and allowed use of a tool may change by revising the tool use data. Tool restrictions may be imposed by rejecting a non-allowed tool, by red or other appropriate icons shown in a display of the system, or the like. While the exemplary embodiments have been described in detail, for clarity of understanding and by way of example, a variety of changes, adaptations, and modifications will be obvious to those of skill in the art. For example, while the exemplary embodiment is described with reference to changing robotic surgical tool kinematics and the like, alternative embodiments may facilitate the use of updated ultrasonic imaging probes with existing ultrasound system, alternative cardiac ablation catheters with a RF ablation system, or new robotic tools with an existing autonomous industrial robot. Hence, the scope of the present invention is limited solely by the appended claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/45/694/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9847962856292725,
                    "F_SPEC_PARAMS": [
                        "reduce the amount of extraneous tissue that is damaged",
                        "reducing patient recovery time, discomfort,",
                        "deleterious side effects"
                    ],
                    "S_SPEC_PARAMS": [
                        "slow, expensive,",
                        "difficult to implement uniformly,",
                        "effective"
                    ],
                    "A_PARAMS": [
                        "updating this base of existing capital equipment",
                        "telesurgical systems and devices"
                    ],
                    "F_SENTS": [
                        "Minimally invasive medical techniques are intended to reduce the amount of extraneous tissue that is damaged during diagnostic or surgical procedures, thereby reducing patient recovery time, discomfort, and deleterious side effects."
                    ],
                    "S_SENTS": [
                        "Unfortunately, as new robotic surgical instruments are developed, updating this base of existing capital equipment can be slow, expensive, and difficult to implement uniformly, particularly for the robotic surgical systems already located in other countries.",
                        "While the new telesurgical systems and devices have proven highly effective and advantageous, still further improvements would be desirable."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object",
                        "Harmful Side Effects"
                    ],
                    "F_SIM_SCORE": 0.6598782241344452,
                    "S_TRIZ_PARAMS": [
                        "Speed",
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.5579940378665924,
                    "GLOBAL_SCORE": 1.6937324166297913
                },
                "sort": [
                    1.6937324
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11259884-20220301",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11259884-20220301",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2017-11-02",
                    "PUBLICATION_DATE": "2022-03-01",
                    "INVENTORS": [
                        "William A. Burbank"
                    ],
                    "APPLICANTS": [
                        "Intuitive Surgical Operations, Inc.    ( Sunnyvale , US )"
                    ],
                    "INVENTION_TITLE": "Robotic surgical stapler assembly configured to use stapler reload",
                    "DOMAIN": "A61B 3435",
                    "ABSTRACT": "A robotic surgical assembly includes a drive assembly and a shaft assembly. The drive assembly is detachably mountable to a surgical robot and includes roll, pitch, and clamp/fire inputs configured to drivingly couple with respective outputs of the surgical robot. The shaft assembly is mounted to the drive assembly and configured to detachably couple with a stapler reload assembly that includes a reload roll shaft, a reload pitch shaft, and a reload clamp/fire shaft. The shaft assembly includes a roll shaft, a pitch shaft, and a clamp/fire shaft. The roll shaft is drivingly coupled with the roll input and configured to detachably couple to the reload roll shaft. The pitch shaft is drivingly coupled with the pitch input and configured to detachably couple to the reload pitch shaft. The clamp/fire shaft is drivingly coupled with the clamp/fire input and configured to detachably couple to the reload clamp/fire shaft.",
                    "CLAIMS": "1. A robotic surgical assembly, comprising: a drive assembly detachably mountable to a surgical robot, the drive assembly comprising: a chassis; a roll input configured to drivingly couple with a roll output of the surgical robot; a pitch input configured to drivingly couple with a pitch output of the surgical robot; and a clamp/fire input configured to drivingly couple with a clamp/fire output of the surgical robot; and a shaft assembly mounted to the drive assembly and configured to detachably couple with a stapler reload assembly, wherein the shaft assembly comprises: a shaft axis, the shaft assembly being elongated along the shaft axis; a roll shaft; a roll output gear attached to the roll shaft and constrained to rotate around the shaft axis, the roll output gear drivingly coupled with the roll input so that rotation of the roll input rotates the roll shaft around the shaft axis; a pitch shaft; a pitch output gear drivingly coupled with the pitch shaft via a first screw thread interface configured to convert rotation of the pitch output gear into translation of the pitch shaft along the shaft axis, the pitch output gear constrained to rotate about the shaft axis and drivingly coupled with the pitch input so that rotation of the pitch input translates the pitch shaft along the shaft axis; a clamp/fire shaft; and a clamp/fire output gear drivingly coupled with the clamp/fire shaft via a second screw thread interface configured to convert rotation of the clamp/fire output gear into translation of the clamp/fire shaft along the shaft axis, the clamp/fire output gear constrained to rotate about the shaft axis and drivingly coupled with the clamp/fire input so that rotation of the clamp/fire input translates the clamp/fire shaft along the shaft axis; wherein in a coupled state of the shaft assembly with the stapler reload assembly: the roll shaft is configured to engage with a reload roll shaft of the stapler reload assembly to which an end effector of the stapler reload assembly is pivotably mounted, the pitch shaft is configured to engage with a reload pitch shaft of the stapler reload assembly translatable along the shaft axis to reorient the end effector relative to the reload roll shaft, and the clamp/fire shaft is engaged with a reload clamp/fire shaft of the stapler reload assembly translatable along the shaft axis to actuate the end effector to clamp and staple tissue. 2. The robotic surgical assembly of claim 1, wherein: the roll shaft has a roll shaft lumen extending along the shaft axis; the pitch shaft is accommodated within the roll shaft lumen and constrained to rotate with the roll shaft around the shaft axis; the pitch shaft has a pitch shaft lumen extending along the shaft axis; and the clamp/fire shaft is accommodated within the pitch shaft lumen and constrained to rotate with the roll shaft around the shaft axis. 3. The robotic surgical assembly of claim 1, wherein the drive assembly comprises: a shaft bearing having an outer race interfaced with the chassis and an inner race interfaced with the roll output gear, the shaft bearing constraining the roll output gear to rotation around the shaft axis; a roll output gear bearing having an outer race interfaced with the roll output gear and an inner race interfaced with the pitch output gear, the roll output gear bearing constraining the pitch output gear to rotation around the shaft axis; and a pitch output gear bearing having an outer race interfaced with the pitch output gear and an inner race interfaced with the clamp/fire output gear, the pitch output gear bearing constraining the clamp/fire output gear to rotation around the shaft axis. 4. The robotic surgical assembly of claim 3, wherein the drive assembly comprises: an upper chassis supported by the chassis; and a clamp/fire output gear bearing having an inner race interfaced with the upper chassis and an outer race interfaced with the clamp/fire output gear. 5. The robotic surgical assembly of claim 4, wherein the upper chassis is detachably mountable to the chassis. 6. The robotic surgical assembly of claim 1, wherein the drive assembly comprises: a clamp/fire input shaft rotationally coupled with the clamp/fire input; a clamp/fire input gear rotationally coupled with the clamp/fire input shaft, the clamp/fire input gear having external gear teeth drivingly engaging external gear teeth of the clamp/fire output gear; and clamp/fire input shaft bearings, each of the clamp/fire input shaft bearings having an outer race interfaced with the chassis and an inner race interfaced with the clamp/fire input shaft. 7. The robotic surgical assembly of claim 1, wherein the drive assembly comprises: a pitch input shaft rotationally coupled with the pitch input; a pitch input gear rotationally coupled with the pitch input shaft, the pitch input gear having external gear teeth drivingly engaging external gear teeth of the pitch output gear; and pitch input shaft bearings, each of the pitch input shaft bearings having an outer race interfaced with the chassis and an inner race interfaced with the pitch input shaft. 8. The robotic surgical assembly of claim 1, wherein the drive assembly comprises: a roll input shaft rotationally coupled with the roll input; roll input shaft bearings, each of the roll input shaft bearings having an outer race interfaced with the chassis and an inner race interfaced with the roll input shaft; a roll input gear rotationally coupled with the roll input shaft; and an idler gear having external teeth drivingly engaged by external teeth of the roll input gear and drivingly engaging external teeth of the roll output gear. 9. A surgical system, comprising: a drive assembly detachably mountable to a manipulator, the drive assembly comprising: a chassis; a roll input rotatably coupled to the chassis and configured to drivingly couple with a roll output of the manipulator; a pitch input rotatably coupled to the chassis and configured to drivingly couple with a pitch output of the manipulator; and a clamp/fire input rotatably coupled to the chassis and configured to drivingly couple with a clamp/fire output of the manipulator; and a shaft assembly extending along a shaft axis and coupled to the drive assembly, the shaft assembly comprising: a roll shaft; a roll output gear attached to the roll shaft and constrained to rotate about the shaft axis, the roll output gear drivingly coupled with the roll input so that rotation of the roll input rotates the roll shaft around the shaft axis; a pitch shaft; a pitch output gear drivingly coupled with the pitch shaft via a first screw thread interface configured to convert rotation of the pitch output gear into translation of the pitch shaft along the shaft axis, the pitch output gear constrained to rotate about the shaft axis and drivingly coupled with the pitch input so that rotation of the pitch input translates the pitch shaft along the shaft axis; a clamp/fire shaft; and a clamp/fire output gear drivingly coupled with the clamp/fire shaft via a second screw thread interface configured to convert rotation of the clamp/fire output gear into translation of the clamp/fire shaft along the shaft axis, the clamp/fire output gear constrained to rotate about the shaft axis and drivingly coupled with the clamp/fire input so that rotation of the clamp/fire input translates the clamp/fire shaft along the shaft axis; wherein the roll shaft, the pitch shaft, and the clamp/fire shaft are concentric about the shaft axis. 10. The system of claim 9, wherein: the roll shaft comprises a tubular shaft defining a roll shaft lumen; and the pitch shaft is received within the roll shaft lumen. 11. The system of claim 9, wherein: the pitch shaft comprises a tubular shaft defining a pitch shaft lumen; and the clamp/fire shaft is received within the pitch shaft lumen. 12. The system of claim 9, wherein the pitch shaft and the clamp/fire shaft are constrained to rotate with rotation of the roll shaft. 13. The system of claim 9, wherein the drive assembly further comprises: a clamp/fire input shaft rotationally coupled with the clamp/fire input; and a clamp/fire input gear rotationally coupled with the clamp/fire input shaft, the clamp/fire input gear having external gear teeth drivingly engaging external gear teeth of the clamp/fire output gear. 14. The system of claim 9, wherein the drive assembly further comprises: a pitch input shaft rotationally coupled with the pitch input; and a pitch input gear rotationally coupled with the pitch input shaft, the pitch input gear having external gear teeth drivingly engaging external gear teeth of the pitch output gear. 15. The system of claim 9, wherein the drive assembly further comprises: a roll input shaft rotationally coupled with the roll input; a roll input gear rotationally coupled with the roll input shaft; and an idler gear having external teeth drivingly engaged by external teeth of the roll input gear and drivingly engaging external teeth of the roll output gear.",
                    "STATE_OF_THE_ART": "Minimally invasive surgical techniques are aimed at reducing the amount of extraneous tissue that is damaged during diagnostic or surgical procedures, thereby reducing patient recovery time, discomfort, and deleterious side effects. As a consequence, the average length of a hospital stay for standard surgery may be shortened significantly using minimally invasive surgical techniques. Also, patient recovery times, patient discomfort, surgical side effects, and time away from work may also be reduced with minimally invasive surgery. A common form of minimally invasive surgery is endoscopy, and a common form of endoscopy is laparoscopy, which is minimally invasive inspection and surgery inside the abdominal cavity. In standard laparoscopic surgery, a patient's abdomen is insufflated with gas, and cannula sleeves are passed through small approximately one-half inch or less incisions to provide entry ports for laparoscopic instruments. Laparoscopic surgical instruments generally include an endoscope , laparoscope for viewing the surgical field and tools for working at the surgical site. The working tools are typically similar to those used in conventional open surgery, except that the working end or end effector of each tool is separated from its handle by an extension tube also known as, , an instrument shaft or a main shaft. The end effector can include, for example, a clamp, grasper, scissor, stapler, cautery tool, linear cutter, or needle holder. To perform surgical procedures, the surgeon passes working tools through cannula sleeves to an internal surgical site and manipulates them from outside the abdomen. The surgeon views the procedure from a monitor that displays an image of the surgical site taken from the endoscope. Similar endoscopic techniques are employed in, for example, arthroscopy, retroperitoneoscopy, pelviscopy, nephroscopy, cystoscopy, cisternoscopy, sinoscopy, hysteroscopy, urethroscopy, and the like. Minimally invasive telesurgical robotic systems are being developed to increase a surgeon's dexterity when working on an internal surgical site, as well as to allow a surgeon to operate on a patient from a remote location outside the sterile field. In a telesurgery system, the surgeon is often provided with an image of the surgical site at a control console. While viewing a three dimensional image of the surgical site on a suitable viewer or display, the surgeon performs the surgical procedures on the patient by manipulating master input or control devices of the control console. Each of the master input devices controls the motion of a servo-mechanically actuated/articulated surgical instrument. During the surgical procedure, the telesurgical system can provide mechanical actuation and control of a variety of surgical instruments or tools having end effectors that perform various functions for the surgeon, for example, holding or driving a needle, grasping a blood vessel, dissecting tissue, or the like, in response to manipulation of the master input devices. Surgical clamping and cutting instruments , non-robotic linear clamping, stapling, and cutting devices, also known as surgical staplers; and electrosurgical vessel sealing devices have been employed in many different surgical procedures. For example, a surgical stapler can be used to resect a cancerous or anomalous tissue from a gastro-intestinal tract. Many known surgical clamping and cutting devices, including known surgical staplers, have opposing jaws that clamp tissue and an articulated knife to cut the clamped tissue. Surgical staplers are often configured to use a replaceable single use stapler cartridge. A hospital can have a number of different types , different makes and models of surgical staplers, including different hand-held surgical staplers and/or telesurgical robotic system surgical staplers. Each type of surgical stapler, however, may be configured to use a corresponding type of stapler cartridge, thereby resulting in a hospital having to stock different types of replaceable stapler cartridges. Stocking different types of replaceable stapler cartridges results in additional expense in terms of additional inventory and resources to ensure that suitable numbers of each type of stapler cartridge are stocked.",
                    "SUMMARY": [
                        "Robotic surgical assemblies detachably mountable to a surgical robot are configured to use a replaceable stapler cartridge of a common hand-held surgical stapler. The ability to use the same replaceable stapler cartridge as the common hand-held surgical stapler can help to reduce the number and types of stapler cartridges that are stocked by a hospital, thereby reducing related expense. The use of the same stapler cartridge as the common hand-held surgical stapler can also reduce training expense and help to reduce surgical error related to having to use different types of replaceable stapler cartridges. Thus, in one aspect, a robotic surgical assembly includes a drive assembly detachably mountable to a surgical robot and a shaft assembly mounted to the drive assembly. The drive assembly includes a chassis, a roll input configured to drivingly couple with a roll output of the surgical robot, a pitch input configured to drivingly couple with a pitch output of the surgical robot, and a clamp/fire input configured to drivingly couple with a clamp/fire output of the surgical robot. The shaft assembly is elongated along a shaft axis and configured to detachably couple with a stapler reload assembly that includes an end effector operable to clamp and staple tissue, a reload roll shaft to which the end effector is pivotally mounted, a reload pitch shaft translatable along the shaft axis to reorient the end effector relative to the reload roll shaft, and a reload clamp/fire shaft drivingly coupled with the end effector and translatable along the shaft axis to actuate the end effector to clamp and staple tissue. The shaft assembly includes a roll shaft, a pitch shaft, and a clamp/fire shaft. The roll shaft is drivingly coupled with the drive assembly so that rotation of the roll input rotates the roll shaft around the shaft axis. The roll shaft has a proximal portion configured to detachably couple to the reload roll shaft. The pitch shaft is drivingly coupled with the drive assembly so that rotation of the pitch input translates the pitch shaft along the shaft axis. The pitch shaft has a proximal portion configured to detachably couple to the reload pitch shaft. The clamp/fire shaft is drivingly coupled with the drive assembly so that rotation of the clamp/fire input translates the clamp/fire shaft along the shaft axis. The clamp/fire shaft has a proximal portion configured to detachably couple to the reload clamp/fire shaft. In many embodiments of the robotic surgical assembly, the roll shaft, the pitch shaft, and the clamp/fire shaft are coaxial with the shaft axis. For example, the roll shaft can have a roll shaft lumen extending along the shaft axis. The pitch shaft can be accommodated within the roll shaft lumen and constrained to rotate with the roll shaft around the shaft axis. The pitch shaft can have a pitch shaft lumen extending along the shaft axis. The clamp/fire shaft can be accommodated within the pitch shaft lumen and constrained to rotate with the roll shaft around the shaft axis. In many embodiments of the robotic surgical assembly, the drive assembly includes output gears that are constrained to rotate around the shaft axis. For example, the drive assembly can include a roll output gear drivingly coupled with the roll input. The roll output gear can be attached to the roll shaft and constrained to rotate around the shaft axis. The drive assembly can include a pitch output gear drivingly coupled with the pitch input and constrained to rotate around the shaft axis. The pitch output gear can be drivingly coupled with the pitch shaft via a screw thread interface that converts rotation of the pitch output gear into translation of the pitch shaft along the shaft axis. The drive assembly can include a clamp/fire output gear drivingly coupled with the clamp/fire input and constrained to rotate around the shaft axis. The clamp/fire output gear can be drivingly coupled with the clamp/fire shaft via a screw thread interface that converts rotation of the clamp/fire output gear into translation of the clamp/fire shaft along the shaft axis. In many embodiments of the robotic surgical assembly, the drive assembly includes output bearings configured to constrain the output gears to rotate around the shaft axis. For example, the drive assembly can include a shaft bearing having an outer race interfaced with the chassis and an inner race interfaced with the roll output gear. The shaft bearing can be configured to constrain the roll output gear to rotation around the shaft axis. The drive assembly can include a roll output gear bearing having an outer race interfaced with the roll output gear and an inner race interfaced the pitch output gear. The roll output gear bearing can be configured to constrain the pitch output gear to rotation around the shaft axis. The drive assembly can include a pitch output gear bearing having an outer race interfaced with the pitch output gear and an inner race interfaced with the clamp/fire output gear. The pitch output gear roller bearing can be configured to constrain the clamp/fire output gear to rotation around the shaft axis. The drive assembly can include an upper chassis supported by the chassis and a clamp/fire output gear bearing having an inner race interfaced with the upper chassis and an outer race interfaced with the clamp/fire output gear. The upper chassis can be detachably mountable to the chassis. In embodiments of the robotic surgical assembly that include output gears, the output gears are drivingly coupled with the roll input, the pitch input, and the clamp/fire input. For example, the drive assembly can include: a a clamp/fire input shaft rotationally coupled with the clamp/fire input; b a clamp/fire input gear rotationally coupled with the clamp/fire input shaft, the clamp/fire input gear having external gear teeth drivingly engaging external gear teeth of the clamp/fire output gear; and c clamp/fire input shaft bearings, each of the clamp/fire input shaft bearings having an outer race interfaced with the chassis and an inner race interfaced with the clamp/fire input shaft. The drive assembly can include: a a pitch input shaft rotationally coupled with the pitch input; b a pitch input gear rotationally coupled with the pitch input shaft, the pitch input gear having external gear teeth drivingly engaging external gear teeth of the pitch output gear; and c pitch input shaft bearings, each of the pitch input shaft bearings having an outer race interfaced with the chassis and an inner race interfaced with the pitch input shaft. The drive assembly can include: a a roll input shaft rotationally coupled with the roll input; b roll input shaft bearings, each of the roll input shaft bearings having an outer race interfaced with the chassis and an inner race interfaced with the roll input shaft; c a roll input gear rotationally coupled with the roll input shaft; and d an idler gear having external teeth drivingly engaged by external teeth of the roll input gear and drivingly engaging external teeth of the roll output gear. In another aspect, a robotic surgical method of actuating a stapler reload assembly is provided. The method includes detachably mounting a drive assembly of a robotic surgical assembly to a surgical robot so as to: a interface a roll output of the surgical robot with a roll input of the drive assembly; b interface a pitch output of the surgical robot with a pitch input of the drive assembly; and c interface a clamp/fire output of the surgical robot with a clamp/fire input of the drive assembly. The method further includes detachably mounting a stapler reload assembly to a distal end of a shaft assembly of the robotic surgical assembly so as to: a interface a roll shaft of the robotic surgical assembly with a roll shaft of the stapler reload assembly; b interface a pitch shaft of the robotic surgical assembly with a pitch shaft of the stapler reload assembly; and c interface a clamp/fire shaft of the robotic surgical assembly with a clamp/fire shaft of the stapler reload assembly. The roll shaft of the shaft assembly is rotated by actuating a roll shaft drive mechanism drivingly coupling the roll shaft of the shaft assembly with the roll input of the drive assembly by rotating the roll output of the surgical robot. The pitch shaft of the shaft assembly is translated relative to the stapler reload assembly by actuating a pitch shaft drive mechanism drivingly coupling the pitch shaft of the shaft assembly with the pitch input of the drive assembly by rotating the pitch output of the surgical robot. The clamp/fire shaft of the shaft assembly is translated relative to the stapler reload assembly by actuating a pitch shaft drive mechanism drivingly coupling the clamp/fire shaft of the shaft assembly with the pitch input of the drive assembly by rotating the pitch output of the surgical robot. In many embodiments of the robotic surgical method of actuating a stapler reload assembly, rotating the roll shaft of the shaft assembly includes rotating a roll output gear drivingly coupled with the roll input and attached to the roll shaft. For example, rotating the roll output gear can include: a rotating a roll input shaft rotationally coupled with the roll input, b rotating a roll input gear rotationally coupled with the roll input shaft, c interfacing external gear teeth of the roll input gear with external gear teeth of the roll idler gear, and d interfacing external gear teeth of the roll idler gear with external gear teeth of the roll output gear. Rotating the roll output gear can include: a interfacing an outer race of a shaft bearing with a chassis of the drive assembly, b interfacing an inner race of the shaft bearing with the roll output gear, and c rotating the inner race of the shaft bearing with the roll output gear. In many embodiments of the robotic surgical method of actuating a stapler reload assembly, translating the pitch shaft of the shaft assembly includes: a rotating a pitch output gear drivingly coupled with the pitch input, and b generating translation of the pitch shaft from rotation of the pitch output gear via a screw thread interface between the pitch shaft and the pitch output gear. For example, rotating the pitch output gear can include: a rotating a pitch input shaft rotationally coupled with the pitch input, b rotating a pitch input gear rotationally coupled with the pitch input shaft, and c interfacing external gear teeth of the pitch input gear with external gear teeth of the pitch output gear. Rotating the pitch output gear can include: a interfacing an outer race of a roll output gear bearing with the roll output gear, b interfacing an inner race of the roll output gear bearing with the pitch output gear, c rotating the inner race of the roll output gear bearing with the pitch output gear, and d rotating the outer race of the roll output gear bearing with the roll output gear. In many embodiments of the robotic surgical method of actuating a stapler reload assembly, translating the clamp/fire shaft of the shaft assembly includes: rotating a clamp/fire output gear drivingly coupled with the clamp/fire input, and b generating translation of the clamp/fire shaft from rotation of the clamp/fire output gear via a screw thread interface between the clamp/fire shaft and the clamp/fire output gear. For example, rotating the clamp/fire output gear can include: a rotating a clamp/fire input shaft rotationally coupled with the clamp/fire input, b rotating a clamp/fire input gear rotationally coupled with the clamp/fire input shaft, and c interfacing external gear teeth of the clamp/fire input gear with external gear teeth of the clamp/fire output gear. Rotating the clamp/fire output gear can include: a interfacing an outer race of a pitch output gear bearing with the pitch output gear, b interfacing an inner race of the pitch output gear bearing with the clamp/fire output gear, c rotating the inner race of the pitch output gear bearing with the clamp/fire output gear, and d rotating the outer race of the pitch output gear bearing with the pitch output gear. Rotating the clamp/fire output gear can include: a interfacing an outer race of a clamp/fire output gear bearing with the clamp/fire output gear, b interfacing an inner race of the clamp/fire output gear bearing with an upper chassis of the drive assembly supported by the chassis of the drive assembly, and c rotating the outer race of the clamp/fire output gear bearing with the clamp/fire output gear. For a fuller understanding of the nature and advantages of the present invention, reference should be made to the ensuing detailed description and accompanying drawings. Other aspects, objects and advantages of the invention will be apparent from the drawings and detailed description that follows.",
                        "1 is a plan view of a minimally invasive robotic surgery system being used to perform a surgery, in accordance with many embodiments. 2 is a view of a surgeon's control console for a robotic surgery system, in accordance with many embodiments. 3 is a view of a robotic surgery system electronics cart, in accordance with many embodiments. 4 diagrammatically illustrates a robotic surgery system, in accordance with many embodiments. 5 is a front view of a patient side cart surgical robot of a robotic surgery system, in accordance with many embodiments. 6 is a view of a robotic surgery tool that includes a demountable stapler reload portion, in accordance with many embodiments. 7 is a simplified schematic diagram of a minimally invasive robotic surgery system including a surgical robot, a surgical stapler base assembly mounted to the surgical robot, and a surgical stapler reload mounted to the surgical stapler base assembly, in accordance with many embodiments. 8 is a side view of a surgical stapler base assembly mounted to a surgical robot, and a surgical stapler reload mounted to the surgical stapler base assembly, in accordance with many embodiments. 9 is a side view of the surgical stapler base assembly and the surgical stapler reload of 8 showing the surgical stapler reload separated from the surgical stapler base assembly, in accordance with many embodiments. 10 is a simplified schematic side view of a distal end of the surgical stapler base assembly and a proximal end of the surgical stapler reload of 8 showing the surgical stapler reload separated from the surgical stapler base assembly, in accordance with many embodiments. 11 and 12 illustrate components of a drive assembly of the surgical stapler base assembly of 8, in accordance with many embodiments. 13A and 13B illustrate components of the drive assembly of the surgical stapler base assembly of 8 for generating translation of a clamp/fire shaft, in accordance with many embodiments. 14A and 14B illustrate components of the drive assembly of the surgical stapler base assembly of 8 for generating translation of a pitch shaft, in accordance with many embodiments. 15A, 15B, and 16 illustrate components of the drive assembly of the surgical stapler base assembly of 8 for generating rotation of a roll shaft, in accordance with many embodiments. 17 is a cross-sectional view through an output gear assembly of the surgical stapler base of 8, in accordance with many embodiments."
                    ],
                    "DESCRIPTION": "In the following description, various embodiments of the present invention will be described. For purposes of explanation, specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However, it will also be apparent to one skilled in the art that the present invention may be practiced without the specific details. Furthermore, well-known features may be omitted or simplified in order not to obscure the embodiment being described. Minimally Invasive Robotic SurgeryReferring now to the drawings, in which like reference numerals represent like parts throughout the several views, 1 is a plan view illustration of a Minimally Invasive Robotic Surgical MIRS system 10, typically used for performing a minimally invasive diagnostic or surgical procedure on a Patient 12 who is lying down on an Operating table 14. The system can include a Surgeon's Console 16 for use by a Surgeon 18 during the procedure. One or more Assistants 20 may also participate in the procedure. The MIRS system 10 can further include a Patient Side Cart 22 surgical robot and an Electronics Cart 24. The Patient Side Cart 22 can manipulate at least one removably coupled tool assembly 26 hereinafter simply referred to as a tool through a minimally invasive incision in the body of the Patient 12 while the Surgeon 18 views the surgical site through the Console 16. An image of the surgical site can be obtained by an endoscope 28, such as a stereoscopic endoscope, which can be manipulated by the Patient Side Cart 22 to orient the endoscope 28. The Electronics Cart 24 can be used to process the images of the surgical site for subsequent display to the Surgeon 18 through the Surgeon's Console 16. The number of surgical tools 26 used at one time will generally depend on the diagnostic or surgical procedure and the space constraints within the operating room among other factors. If it is necessary to change one or more of the tools 26 being used during a procedure, an Assistant 20 may remove the tool 26 from the Patient Side Cart 22, and replace it with another tool 26 from a tray 30 in the operating room. 2 is a perspective view of the Surgeon's Console 16. The Surgeon's Console 16 includes a left eye display 32 and a right eye display 34 for presenting the Surgeon 18 with a coordinated stereo view of the surgical site that enables depth perception. The Console 16 further includes one or more input control devices 36, which in turn cause the Patient Side Cart 22 shown in 1 to manipulate one or more tools. The input control devices 36 can provide the same degrees of freedom as their associated tools 26 shown in 1 to provide the Surgeon with telepresence, or the perception that the input control devices 36 are integral with the tools 26 so that the Surgeon has a strong sense of directly controlling the tools 26. To this end, position, force, and tactile feedback sensors not shown may be employed to transmit position, force, and tactile sensations from the tools 26 back to the Surgeon's hands through the input control devices 36. The Surgeon's Console 16 is usually located in the same room as the patient so that the Surgeon may directly monitor the procedure, be physically present if necessary, and speak to an Assistant directly rather than over the telephone or other communication medium. However, the Surgeon can be located in a different room, a completely different building, or other remote location from the Patient allowing for remote surgical procedures. 3 is a perspective view of the Electronics Cart 24. The Electronics Cart 24 can be coupled with the endoscope 28 and can include a processor to process captured images for subsequent display, such as to a Surgeon on the Surgeon's Console, or on another suitable display located locally and/or remotely. For example, where a stereoscopic endoscope is used, the Electronics Cart 24 can process the captured images to present the Surgeon with coordinated stereo images of the surgical site. Such coordination can include alignment between the opposing images and can include adjusting the stereo working distance of the stereoscopic endoscope. As another example, image processing can include the use of previously determined camera calibration parameters to compensate for imaging errors of the image capture device, such as optical aberrations. 4 diagrammatically illustrates the robotic surgery system 10. As discussed above, the Surgeon's Console 16 can be used by a Surgeon to control the Patient Side Cart 22 Surgical Robot during a minimally invasive procedure. The Patient Side Cart 22 can use an imaging device, such as a stereoscopic endoscope, to capture images of the procedure site and output the captured images to the Electronics Cart 24. As discussed above, the Electronics Cart 24 can process the captured images in a variety of ways prior to any subsequent display. For example, the Electronics Cart 24 can overlay the captured images with a virtual control interface prior to displaying the combined images to the Surgeon via the Surgeon's Console 16. The Patient Side Cart 22 can output the captured images for processing outside the Electronics Cart 24. For example, the Patient Side Cart 22 can output the captured images to a processor 38, which can be used to process the captured images. The images can also be processed by a combination the Electronics Cart 24 and the processor 38, which can be coupled together to process the captured images jointly, sequentially, and/or combinations thereof. One or more separate displays 40 can also be coupled with the processor 38 and/or the Electronics Cart 24 for local and/or remote display of images, such as images of the procedure site, or other related images. 5 and 6 show a Patient Side Cart 22 and a surgical tool 100, respectively. The surgical tool 100 is an example of the surgical tools 26. The Patient Side Cart 22 shown provides for the manipulation of three surgical tools 26 and an imaging device 28, such as a stereoscopic endoscope used for the capture of images of the site of the procedure. Manipulation is provided by robotic mechanisms having a number of robotic joints. The imaging device 28 and the surgical tools 26 can be positioned and manipulated through incisions in the patient so that a kinematic remote center is maintained at the incision to minimize the size of the incision. Images of the surgical site can include images of the distal ends of the surgical tools 26 when they are positioned within the field-of-view of the imaging device 28. Robotic Surgical Tool Configured to Use Hand-Held Stapler ReloadThe surgical tool 100 includes a robotic surgical assembly 102 and a replaceable stapler reload 200 that is detachably coupled with the surgical assembly 102. In many embodiments, the stapler reload 200 is configured for use with an existing hand-held surgical stapler. The robotic surgical assembly 102 is configured to be detachably mountable to a surgical robot , the patient side cart 22 for use in clamping, stapling, and cutting tissue with the stapler reload 200. The surgical assembly 102 is configured to mount to an instrument holder of the patient side cart 22. The surgical assembly 102 includes a proximal drive assembly 104 and a shaft assembly 106 mounted to the drive assembly 104 and extending distally from the drive assembly 104 along a shaft axis 108. The drive assembly includes a chassis 110, a roll input member 112 mounted to the chassis 110 and rotatable relative to the chassis 110 by a respective roll output member of the patient side cart 22, a pitch input member 114 mounted to the chassis 110 and rotatable relative to the chassis 110 by a respective pitch output member of the patient side cart 22, and a clamp/fire input member 116 mounted to the chassis 110 and rotatable relative to the chassis 110 by a respective clamp/fire output member of the patient side cart 22. The stapler reload 200 is a single use replaceable assembly that is detachably mountable to the shaft assembly 106. The stapler reload 200 includes a proximal shaft assembly 202 and a stapler end effector 204. The proximal shaft assembly 202 is detachably mountable to a distal end of the shaft assembly 106. The end effector 204 is pivotally attached to a distal end of the proximal shaft assembly 202. The end effector 204 includes an upper jaw 206 and a stapler cartridge assembly 208. The stapler cartridge assembly 208 is pivotally mounted relative to the upper jaw 206 and can be actuated from an open position to a closed position to clamp tissue between the stapler cartridge assembly 208 and the upper jaw 206. The stapler cartridge assembly 208 includes an actuation input that is translatable distally to close the stapler cartridge 208 to clamp tissue between the stapler cartridge 208 and the upper jaw 206, to deploy staples from the stapler cartridge assembly 208, and to cut the stapled tissue. The deployed staples penetrate through tissue clamped between the stapler cartridge 208 and the upper jaw 206 and into contact with an anvil surface of the upper jaw 206 that forms ends of the staples to retain the staples in the stapled tissue. 7 is a simplified schematic diagram of components of an embodiment of the Minimally Invasive Robotic Surgical MIRS system 10. The illustrated components of the robotic surgical system 10 include includes the surgical robot 22, the surgical stapler base assembly 102 mounted to the surgical robot 22, a surgical stapler reload 200 mounted to the surgical stapler base assembly 102, the electronics cart 24, and the surgical console 16. The surgical robot 22 includes a motor unit and instrument holder 42 to which the surgical stapler base assembly 102 is detachably mountable. The motor unit and instrument holder 42 is operable to controllably actuate the surgical stapler base assembly 102 to controllably actuate the surgical stapler reload 200. The motor unit and instrument holder 42 includes a roll motor 44, a roll output 46 drivingly coupled with the roll motor 44, a pitch motor 48, a pitch output 50 drivingly coupled with the pitch motor 48, a clamp/fire motor 52, and a clamp/fire output 54 drivingly coupled with the clamp/fire motor 52. The roll motor 44 can be controllably operated to controllably rotate the roll output 46 relative to the motor unit and instrument holder 42. Similarly, the pitch motor 48 can be controllably operated to controllably rotate the pitch output 50 relative to the motor unit and instrument holder 42 and the clamp/fire motor 52 can be controllably operated to controllably rotate the clamp/fire output 54 relative to the motor unit and instrument holder 42. The surgical stapler base assembly 102 is configured to actuate the surgical stapler reload 200 in response to actuation of the surgical stapler base assembly 102 by the motor unit and instrument holder 42. The base assembly 102 includes the roll input 112, a roll drive assembly 118, a base roll shaft 120, the pitch input 114, a pitch drive assembly 122, a base pitch shaft 124, the clamp/fire input 116, a clamp/fire drive assembly 126, and a base clamp/fire shaft 128. When the base assembly 102 is mounted to the motor unit and instrument holder 42, the roll input 112 is drivingly coupled with the roll output 46 so that rotation of the roll output 46 generates matching rotation of the roll input 112, the pitch input 114 is drivingly coupled with the pitch output 50 so that rotation of the pitch output 50 generates matching rotation of the pitch input 114, and the clamp/fire input 116 is drivingly coupled with the clamp/fire output 54 so that rotation of the clamp/fire output 54 generates matching rotation of the clamp/fire input 116. The base roll shaft 120 is drivingly coupled with the roll input 112 via the roll drive assembly 118 so that rotation of the roll input 112 generates a corresponding rotation of the base roll shaft 120 around the shaft axis 108. The base pitch shaft 124 is drivingly coupled with the pitch input 114 via the pitch drive assembly 122 so that rotation of the pitch input 114 generates a corresponding translation of the base pitch shaft 124 along the shaft axis 108. The base clamp/fire shaft 128 is drivingly coupled with the clamp/fire input 116 via the clamp/fire drive assembly 126 so that rotation of the clamp/fire input 116 generates a corresponding translation of the base clamp/fire shaft 128 along the shaft axis 108. The surgical stapler reload 200 includes a reload roll shaft 210, a reload pitch shaft 212, a reload pitch drive assembly 214, a reload clamp/fire shaft 216, a reload clamp/fire drive assembly 218, and the stapler end effector 204. When the stapler reload 200 is mounted to the base assembly 102, the reload roll shaft 210 is coupled to the base roll shaft 120 so that the reload roll shaft 210 rotates with the base roll shaft 120 around the shaft axis 108, the reload pitch shaft 212 is coupled with the base pitch shaft 124 so that the reload pitch shaft 212 translates with the base pitch shaft 124 along the shaft axis 108, and the reload clamp/fire shaft 216 is coupled with the base clamp/fire shaft 128 so that the reload clamp/fire shaft 216 translates with the base clamp/fire shaft 128 along the shaft axis 108. The stapler end effector 204 can be pivotally mounted to the reload roll shaft 210. The stapler end effector 204 is drivingly coupled with the reload pitch shaft 212 via the reload pitch drive assembly 214 so that translation of the reload pitch shaft 212 along the shaft axis 108 reorients the stapler end effector 204 relative to the reload roll shaft 210. The stapler end effector 204 is also drivingly coupled with the reload clamp/fire shaft 216 via the reload clamp/fire assembly 218 so that translation of the reload clamp/fire shaft 216 along the shaft axis 108 can be used to actuate the stapler cartridge assembly 208 to clamp tissue between the stapler cartridge assembly 208 and the upper jaw 206, to deploy staples from the stapler cartridge assembly 208, and to cut the stapled tissue. 8 shows a side view of the surgical tool 100 with the drive assembly 104 mounted to the motor unit and instrument holder 42. In the illustrated configuration, the motor unit and instrument holder 42 is operable to controllably rotate the roll output 46, the pitch output 50, and the clamp/fire output 54 to controllably actuate the surgical base assembly 102, which thereby controllably articulates/actuate the stapler reload 200. 9 is a side view of the surgical stapler base assembly 102 and the surgical stapler reload 200 showing the surgical stapler reload 200 separated from the surgical stapler base assembly 102. As discussed in more detail with respect to 10, the base assembly 102 includes a distal portion 130 configured to receive and drivingly couple with a proximal portion 220 of the stapler reload 200. 10 is a simplified schematic side view of the distal portion 130 of the surgical stapler base assembly 102 and the proximal portion 220 of the stapler reload 200. The distal portion 130 of the base assembly 102 includes a distal portion 132 of the base roll shaft 120, a distal portion 134 of the base pitch shaft 124, and a distal portion 136 of the base clamp/fire shaft 128. The proximal portion 220 of the stapler reload 200 includes a proximal portion 222 of the reload roll shaft 210, a proximal portion 224 of the reload pitch shaft 212, and a proximal portion 226 of the reload clamp/fire shaft 216. In the illustrated embodiment, the proximal portion 132 of the base roll shaft 120 includes protruding features 138 configured to be inserted into slots 228 in the reload roll shaft 210 to rotationally tie the reload roll shaft 210 to the base roll shaft 120. In many embodiments, a retention mechanism prevents inadvertent separation of the stapler reload 200 from the base assembly 102. In the illustrated embodiment, the distal portion 134 of the base pitch shaft 124 includes a spring-loaded coupling feature 140 configured to be received and engage a shaped slot 230 in the proximal portion 224 of the reload pitch shaft 212 to constrain the reload pitch shaft 212 to translate with the base pitch shaft 124 along the shaft axis 108. In a similar manner, in the illustrated embodiment the distal portion 136 of the base clamp/fire shaft 128 includes a coupling feature 142 configured to engage a coupling feature 232 of the reload clamp/fire shaft 216 to constrain the reload clamp/fire shaft 216 to translate with the base clamp/fire shaft 128 along the shaft axis 108. 11 and 12 illustrate components of the drive assembly 100, which is used drivingly couple the base roll shaft 120 with the roll input 112, the base pitch shaft 124 with the pitch input 114, and the base clamp/fire shaft 128 with the clamp/fire input 116. The drive assembly 104 includes the chassis 110, a roll output gear 144, a pitch output gear 146, a clamp/fire output gear 148, a roll input shaft 150, a roll input gear 152, a roll idler gear 154, a pitch input shaft 156, a pitch input gear 158, a clamp/fire input shaft 160, and a clamp/fire input gear 162. As described in more detail herein, the roll output gear 144, the pitch output gear 146, and the clamp/fire output gear 148 are coaxially mounted to rotate about the shaft axis 108. The roll output gear 144 is rotationally tied to the base roll shaft 120 so that the base roll shaft 120 is rotated by rotating the roll output gear 144. The base pitch shaft 124 is interfaced with the pitch output gear 146 via a screw thread so that the base pitch shaft 124 is translated along the shaft axis 108 via rotation of the pitch output gear 146. The base clamp/fire shaft 128 is interfaced with the clamp/fire output gear 148 via a screw thread so that the base clamp/fire shaft 128 is translated along the shaft axis 108 via rotation of the clamp/fire output gear 148. The roll input gear 152 is mounted and rotationally tied to the roll input shaft 150, which is rotationally tied to the roll input 112. The roll input gear 152 includes external gear teeth, which engage external gear teeth of the roll idler gear 154 to rotate the roll idler gear 154 in response to rotation of the roll input 112. The external teeth of the roll idler gear 154 engage external gear teeth of the roll output gear 144 to rotate the roll output gear 144 in response to rotation of the roll idler gear 154. The pitch input gear 154 is mounted and rotationally tied to the pitch input shaft 156, which is rotationally tied to the pitch input 114. The pitch input gear 158 includes external gear teeth, which engage external gear teeth of the pitch output gear 146 to rotate the pitch output gear 146 in response to rotation of the pitch input gear 158. The clamp/fire input gear 162 is mounted and rotationally tied to the clamp/fire input shaft 160, which is rotationally tied to the clamp/fire input 116. The clamp/fire input gear 162 includes external gear teeth, which engage external gear teeth of the claim/fire output gear 148 to rotate the clamp/fire output gear 148 in response to rotation of the clamp/fire input gear 162. 13A and 13B illustrate components of the drive assembly 104 of the surgical stapler base assembly 102 for generating translation of the base claim/fire shaft 128. In the illustrated embodiment, the clamp/fire input 116 and the clamp/fire input shaft 160 form an integral component. In 13A, the clamp/fire input 116, the clamp fire input shaft 160, the clamp/fire input gear 162, the clamp/fire output gear 148, and the base clamp/fire shaft 128 are shown in isolation to better illustrate how translation of the base clamp/fire shaft 128 along the shaft axis 108 is generated by rotation of the clamp/fire input 116. Specifically, rotation of the clamp/fire input 116 relative to the chassis 110 rotates the clamp/fire input shaft 160 relative to the chassis 110, which rotates the clamp/fire input gear 162 relative to the chassis 110, which rotates the clamp/fire output gear 148 relative to the chassis 110. In many embodiments, the base clamp/fire shaft 128 is constrained to rotate with the base roll shaft 120 so that rotation of the clamp/fire output gear 148 produces rotation of the clamp/fire output gear 148 relative to the base clamp/fire shaft 128 absent matching rotation of the base roll shaft 120 and the clamp/fire output gear 148. The clamp/fire output gear 148 and the base clamp/fire shaft 128 are interfaced via a clamp/fire screw thread interface 164. Rotation of the clamp/fire output gear 148 relative to the base clamp/fire shaft 128 produces translation of the base clamp/fire shaft 128 along the shaft axis 108 via the clamp/fire screw thread interface 164. 14A and 14B illustrate components of the drive assembly 104 of the surgical stapler base assembly 102 for generating translation of the base pitch shaft 124. In 13A, the pitch input 114, the pitch input shaft 156, the pitch input gear 158, the pitch output gear 146, and the base pitch shaft 124 are shown in isolation to better illustrate how translation of the base pitch shaft 124 along the shaft axis 108 is generated by rotation of the pitch input 114. Specifically, rotation of the pitch input 114 relative to the chassis 110 rotates the pitch input shaft 156 relative to the chassis 110, which rotates the pitch input gear 158 relative to the chassis 110, which rotates the pitch output gear 146 relative to the chassis 110. In many embodiments, the base pitch shaft 124 is constrained to rotate with the base roll shaft 120 so that rotation of the pitch output gear 146 produces rotation of the pitch output gear 146 relative to the base pitch shaft 124 absent matching rotation of the base roll shaft 120 and the pitch output gear 146. The pitch output gear 146 and the base pitch shaft 124 are interfaced via a pitch screw thread interface 166. Rotation of the pitch output gear 146 relative to the base pitch shaft 124 produces translation of the base pitch shaft 124 along the shaft axis 108 via the pitch screw thread interface 166. 15A, 15B, and 16 illustrate components of the drive assembly 104 of the surgical stapler base assembly 102 for generating rotation of the base roll shaft 120. In 15A, the roll input 112, the roll input shaft 150, the roll input gear 152, the roll idler gear 154, the roll output gear 144, and the base roll shaft 120 are shown in isolation to better illustrate how rotation of the base roll shaft 120 around the shaft axis 108 is generated by rotation of the roll input 112. Specifically, rotation of the roll input 112 relative to the chassis 110 rotates the roll input shaft 150 relative to the chassis 110, which rotates the roll input gear 152 relative to the chassis 110, which rotates the roll idler gear 154 relative to the chassis 110, which rotates the roll output gear 144 relative to the chassis 110, which rotates the base roll shaft 120 relative the chassis 110. 17 is a cross-sectional view through the output gears 144, 146, 148 illustrating roller bearings used to mount the output gears 144, 146, 148 to the chassis 110 and constrain rotation of the output gears 144, 146, 148 to rotation around the shaft axis 108. The roller bearings used to mount the output gears 144, 146, 148 include a shaft bearing 168, a roll output gear bearing 170, a pitch output gear bearing 172, and a clamp/fire output gear bearing 174. The shaft bearing 168 is mounted to the chassis 110 so that an outer race of the shaft bearing 168 is interfaced the chassis 110. The roll output gear 144 is mounted to the shaft bearing 168 so that an inner race of the shaft bearing 168 is interfaced with the roll output gear 144. The roll output gear bearing 170 is mounted inside a recess in the roll output gear 144 so that an outer race of the roll output gear bearing 170 is interfaced with the roll output gear 144. The pitch output gear 146 is mounted to the roll output gear bearing 170 so that an inner race of the roll output gear bearing 170 is interfaced with the pitch output gear 146. The pitch output gear bearing 172 is mounted inside a recess in the pitch output gear 146 so that an outer race of the pitch output gear bearing 172 is interfaced with the pitch output gear 146. The clamp/fire output gear 148 is mounted to the pitch output gear bearing 172 so that an inner race of the pitch output gear bearing 172 is interfaced with the clamp/fire output gear 148. The clamp/fire output gear bearing 174 is mounted inside a recess in the clamp/fire output gear 148 so that an outer race of the clamp/fire output gear bearing 174 is interfaced with the clamp/fire output gear 148. Although not shown in 17, an inner race of the clamp/fire gear bearing 174 is interfaced with an upper chassis, which is coupled with the chassis 110 and provides additional support for the output gears 144, 146, 148. Other variations are within the spirit of the present invention. Thus, while the invention is susceptible to various modifications and alternative constructions, certain illustrated embodiments thereof are shown in the drawings and have been described above in detail. It should be understood, however, that there is no intention to limit the invention to the specific form or forms disclosed, but on the contrary, the intention is to cover all modifications, alternative constructions, and equivalents falling within the spirit and scope of the invention, as defined in the appended claims. The term force is to be construed as encompassing both force and torque especially in the context of the following claims, unless otherwise indicated herein or clearly contradicted by context. The use of the terms a and an and the and similar referents in the context of describing the invention especially in the context of the following claims are to be construed to cover both the singular and the plural, unless otherwise indicated herein or clearly contradicted by context. The terms comprising, having, including, and containing are to be construed as open-ended terms i. e. , meaning including, but not limited to, unless otherwise noted. The term connected is to be construed as partly or wholly contained within, attached to, or joined together, even if there is something intervening. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range, unless otherwise indicated herein, and each separate value is incorporated into the specification as if it were individually recited herein. All methods described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. The use of any and all examples, or exemplary language , such as provided herein, is intended merely to better illuminate embodiments of the invention and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non-claimed element as essential to the practice of the invention. Preferred embodiments of this invention are described herein, including the best mode known to the inventors for carrying out the invention. Variations of those preferred embodiments may become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventors expect skilled artisans to employ such variations as appropriate, and the inventors intend for the invention to be practiced otherwise than as specifically described herein. Accordingly, this invention includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover, any combination of the above-described elements in all possible variations thereof is encompassed by the invention unless otherwise indicated herein or otherwise clearly contradicted by context. All references, including publications, patent applications, and patents, cited herein are hereby incorporated by reference to the same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety herein.",
                    "IMAGE": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAEBAMgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKxP7P1NNQ1KVZlkhuJFeBTOymMeWqMBwcchmGO5/GqraRrjeFp9La+Q3eQIbsSMrEb8/NgcHaMZHB9BVeTSfFsLtFZavCbaEKLc3B3SPiNgTIdnJ3FT+H4VbsbHxKmrpLfanBLZI7HykUKWUhgAfl9dp6+tdFWPqEXiD7cZdOuLM2+BiG4U9cHPIGcdPyqxGNVBtzKbc/u2WZUJALnbhlyDwPm49xWfbW3iZZI/tN7bOgZd2wAHGVzn5OTgOOMde3boKxLqLxINRd7O4sGtCfljnVtwHHcdeh/P2qe3TXPLja4ls/NG8usYbaflG0c88NnPtVWS38StaJtvbJLkuS+EJQLlcAZGegf863qKxvsmtfbA4vY/JF0XKkZ3QnHy9OCOec/wD1op7XX9lysd1G3mLIsR8zYYyXYq2dh6KUHQ9KqTaZ4q2XHk6xFulUiPeg/dHfuB6c8Arjjr7U+207xUlpcx3GrW0kjQSLC6pgrIWyrHjkAcdPwrS0W31S3ilGqXKTyHbtZWyOFAJxtGMnJxzWpRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRVabUbS3n8ma4RJMBtpPQE4GfTJBpbe/tLuN5ILiORI/vMrcDjP5Y71LDNHcQpNDIskUihkdTkMD0INPooooooooooooooooooooqu6ie5aN8mNFB29mJz19elVLnQbC7kmkljJaVURiDghUOQo9Bmn2GkRafHMiTzyiX73msD0AAwAABwKt28EdpaxW8fEcSBFz6AYFS0UUUUUUUUUUUUUUUUUUUVCn/H5N/up/Wpqr3zMlo7KxXGMleu3POPfGaxftmjm+BKRskQZCxXfuJCkHPOeM/r07z6bIsIjnW4ItHRx5cjZA2twQT7cHHFXLWTUZbwvMkCWTRBkAJMgY44PtV+iiiiiiiiiiiiiiiiiioU/4/Jv9xP61NXP3mtwhA17ZXFvDFORvlj+Rgqk5z79s1g3enStfSRy6hb/2Rd24CssqRToQRjbk4yAoBPvVSSHFxFc3DJMLQt5YhuDIDuyNz4JVc7u4GcDBzxXRG/aa9sjNqUqObjHkQRHb2+Vj6evpnt36eiiiiiiiiiimJNFI7okqM6feVWBK/X0p9FFFFFFQp/x+Tf7qf1qasvXrFNR077PIhdC4JGzeOhxkdxnFcpqWjvJMrA2nAwxnRw+fc45/IcUQ6Rd/YwkUSSfPlDCHKDkZ5IBU8dRnPTjrXbWaQ20K2kToWiGGVcZB9cdq5fUfEMg1CZbTWoo0Vioie0LbSDg8455B/Ormi6vLNqJtrnUhcyNkCJLYoFOM5J+gPfvWo9xd3F40Nq0cccf33ddxPbjn1BH4Utt9ttLOKK7mW6uncgOqhAep5+gFFtqErRFJ4la7ViHit23BeeMnoOMHmqOpXV/NGiWU6rMZQpSHB47gyMCAfwrWsJJZbCB5yDMUHmFem7vj8c1YoorOuJZ7nUVsVtH+yhQ81wxAUnPEYHVs9+wHHercVpBBLJLFEqPJjeQOuOlTUUUUUUVCn/H5N/up/WpqKzLjw9pV3PJNPZo8khy53EZPTPXr71cs7K30+1S2tY/LhTO1ck4zz3qneWNrCLm8jhP2qXA3hjndwFx6dulZn25biSGW6Ytbv8zSRq6RjqNhBAOc4znPQ8DFS6ZfQI2YpY0hLlTbmXeUwCSwPUDjp/Kq7ajdJa3ctj5KuPneWXkKOpAHrlmAzwT7ZNJBfXOqWtqXkuRKjKZtgRVJII65zg5OPb2qro1pPbahcl4p3smUsEfAMiZJUqF+/gE5DZPIreutQto7GOZY5VhR0YFYTjGR0wPem2+qwjT5mjWVXR5FCvCww244B446irVjDKs887X/ANpikxsUDhMde9XqKKKKKKKKKKgT/j8m/wB1P61PWdNrVrHOsEe6aRjtAjxjP1PFZs00d/eRrc35jSKR0ngVygUjGASCDjqdx46cDNWbe5Fr5KCV5IppTGFZyzKuSFcZ5x0H45+sV/G+l2cUEVzM6zOygyAyMGwWUjHOcj6etYja7qCWrpAxZFYsf3kayCMk/wAJB5/zgVn6qbeexDXUNwbgvsjFuwCKijI3cdcD35/KrGkaGrShJbeVLdjGPPC/I+DlSOOeWIGRwAK69dFhSHykuLgJuLHBXJJBHJ2+9RX1mLazMsupSIsXzK0hUYI6Y6fSsJ7fUZWkudQjH2CTBfyxtUgEbXAzuBOMnoBwfU1t6bHZNJdfYL1vLDB2eOYOCSOSSc88VBHqU1vfTpFFBdCSVUR1mVS3y55A79s1ofbNR/6BX/kwtH2zUf8AoFf+TC0fbNS/6BX/AJMLSNe6kFJGlZOP+fhat2ks80G+4g8l9xG3dnIB4Ptkc47VPRRRRRUCf8fk3+4n9araqkjQq6nKDKsjfdyehPqAfXjnPasbVbWKRI5FkkdWhc4KB2A2kEEY4GcdOmDT7Ay3emtDbwIJAE3KxU+WRjIPuVAHHfPSm6a8KXl1ex2FxcJ5jbJYo1OB3Gc/NjkDbkDn1qbVdRs5pLGUxzk28xk2vaScHYy/3eD83BrlrSzivdTfUtk+6EmIQOpQygqwZwBu44OOvQnAyKBAJbZo9OsZJTHI0byQXpJTsRtyDkDPGMHBxVy3ukkhe3u7l49o+eZyWG4nPORwPfn1rafSUuUtNVt5ZA0IyEhVBuHqCBz0/Ecd6pzP/wAJTp3myym08vcFJK7QduOAcgkg9f4eg5Bq7Hr0OpldJEYMtwjRlyflIC5YgZ3Yx0PTPGavWGjizt7kXV007zhVZ9oQBVGFAA9OeapNYw3kw2u9yIrshsbR8rLycgDv6Vav9Pj0+wnu7Oae3lgjaQfvmZGwM4KkkYPT1rYQlkViMEjJHpS0UUUUUUUVCn/H3N/ur/WpXRZEZHAKsMEHuK565nezlNsGEGAS8oiDGU/w5+o6++ajt4ZZ7BLS1AhmPySyRrgCNuTz2cD8QfrXSRxpDEkUahI0AVVUYAA6CstrS603TpmhvJZ3UM58457djgkY7dqqR25fzZLRbV4AqyAzZc8rngnnBzn8T71HHpUgCyMlujSyN5W2MYXcpIOPxORz0zmlXQo4jGtwlvcXQj8wuYwiSlSchlHHRhz2xntUGkXAvbOWGbTbdQoKbmznBz8v3PvDoSKS82WcdtAmk27mT5Xe3Qhj0BA+QYJyepHQ81qxadMrmaS3h80rsylw6hUHRQAOgqf7DIxGYbVcdC4Mp/XFWbe38jcTIXdsZJAA47ACqV7/AMTC/TTl5hiKzXR7EZyifiRk+w961KKKKKKKKKKhT/j8m/3U/rVXVJ9UhVP7Ns4bgkNu8yTZg9sfrWbONdn2yvp9uXB2lRMVyue+Dz9M1JDc+IEg2/2VbIwxtVZBtHXjr9KvabLqkxc6jbwwAcKqNuJ985q+xIUlRuIHA9a5UW+t5ZY9Khtlddj+Vc/LjngDPAyT0wea0tKi1D7WzXtnFCiKQj+cZGPTpzhfwrVmgiuE2yoGA5Geo+npVB5NP8OaasSDy4gT5cQOSxJycZ+v0Fc7a6/e32sG6ZZDZW/MiRdEB478secnH5dM9HPNeXsMn9mSwKBgJM4LKx74x1GP8jFaCBhGocgvgbiOmaivLlbOynuXGVijZyPXAziotMtGtLMCU7riQmWd/wC856/gOg9gKuUUUUUUUUUVAn/H5N/up/Wp6KjmnitojLNIsaDqzHArMm8S6XFKY/tKsy9SCAB+JIpsXiGO9YRWED3ExQOAGXaoPTcQTjp0qaDWVKg3cLW4KBw5O5ee2fX/AD14qRE1CW4tpzcJHDgmWDy+eRwM+oP8qmv55razkkt4DPMB8iDue2a4xdJ1PWblZb9JNsw5KuV+X6jovGMA5+bv96uvtNLtbSOMCKNnQfKxQfL/ALo7D/PJ5pf+PGbH/LtI34Rsf6H9D9eLlZ2s4ktobXqbmdI8eoB3N/46rVo0UUUUUUUUUVCn/H5N/uJ/Wpqq38d1Jb4tLhYJAwJZlDDHcVh67qckCQ2wEc9ytxFIqgH5kzwcDPORio7q7ubKZnnNrDM0gVgzsyrv2gFQQATwe/rWrp9xp9tFshdizNmSQocs3qSBj/CnLGFuQjKpTzZIiGHBVhvq1YuDY2w3At5Snrz0qzVNf9BlCHi2kb5T/wA82Pb6Ht6HjuKuVTv7ryoWijtmupWX/UpjkHjJJ4Aql/azadYBruyvAka4aRzGP/Z//r1Bb3d1dagL+bSb5Y0TbbRkRggHBZmBfhjwMdgPc1o/2jP/ANAq+/8AIf8A8XR/aM//AECr7/yH/wDF0keozvdRxHTbqNWOC77eOCc8EjHGOueRxWhRRRRRRRUCf8fk3+6n9anpskayxtG4yjDBHqK5WG507R9ZkiRR5SYjwmW2kkcn6VYuPLtNXubqe2N0AgiDYJClmJxzwOCo4/Kksgf7Mtrf7MyyozZKgN13cAjJ7jrVjVrWS8BiiWORvt0b7HbAYKgJBOD1AxUGn6XdG4W6hSKwSSPKmEhzgnIXBGB+HpWn5OrRcpd204/uywlCf+BA/wBKY+pqiNFqlnJbKwwX/wBZER/vDp/wICmR6inl+VFfWrRj7t0ZlOF+meW/Tv7VStLKzutclnsp7mSFov380c7hGkGApBBwxxnOOBgdK14tKs4pxOY2lmXlZJpGkK/TcTj8Ku0UUUUUUUUUUVAn/H5N/uJ/Wp6KztStsW5a2tyWeZGmEQCtIoPOemeKwJHtILlon0y6iRSWjCIFZeFwQS3Yj9frWnpOvpcstvd5ilK7onfaPNX1wCcHjkU+C7R7mMxgzSHfMEU/3jhST2G0Gp4NMuooI0OrXQ2qBhUiwPplM0/+z7r/AKC93/3xF/8AEUf2fdf9Be7/AO+Iv/iKibRC772vpS3qYICf/RdS/wBn3QGBq93/AN8Rf/EVLZWk1r5nnXclxuxjeOmB/Xr6egFW6KKKKKKKKKKKruTDcGXYzI6hSVBJUjPb05qdWDKGGcEZ5GKWq99HdS2rJZzLDOSMOy7gOeePpWULHxDvJbVYGU5+XyQMD0HH6086XqM1s0NzeW8oMe3iALlsnnv6j8qgi0jWrc4g1K2jQjlVtlHP1xz/AJHvW1aLcpBi7kR5dxOUGBjPA/Kp6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK//9k=",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/84/598/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9819965362548828,
                    "F_SPEC_PARAMS": [
                        "cost",
                        "treatment cost"
                    ],
                    "S_SPEC_PARAMS": [
                        "shape,",
                        "flow of the etching liquid in the cleaning tank may become uneven",
                        "removal of the oxide film may be insufficient"
                    ],
                    "A_PARAMS": [
                        "amount of etching liquid fluonitric acid solution",
                        "size of the sample plate"
                    ],
                    "F_SENTS": [
                        "Further, since the amount of etching liquid fluonitric acid solution used is increased, the cost is increased and the treatment cost of waste liquid is also increased."
                    ],
                    "S_SENTS": [
                        "However, the material, shape, and size of the sample plate are not described in the Patent Document 1.",
                        "When etching the sample plate together with the polycrystalline silicon fragments, the sample plate itself may become a baffle and the flow of the etching liquid in the cleaning tank may become uneven.",
                        "On the other hand, the removal of the oxide film may be insufficient if the etching amount is low.",
                        "Further in the Patent Document 1, if the thickness and weight of the sample plate before and after etching are measured in advance and the relation between etching time and etching amount is checked, the etching amount of the polycrystalline silicon fragments can be freely adjusted by controlling the etching time without using the sample plate."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "F_SIM_SCORE": 0.4781711995601654,
                    "S_TRIZ_PARAMS": [
                        "Shape",
                        "Harmful Factors Acting on Object"
                    ],
                    "S_SIM_SCORE": 0.6730555295944214,
                    "GLOBAL_SCORE": 1.6909432341655095
                },
                "sort": [
                    1.6909432
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11269328-20220308",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11269328-20220308",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-08-13",
                    "PUBLICATION_DATE": "2022-03-08",
                    "INVENTORS": [
                        "Nakyeong Kim",
                        "Sungmin Moon",
                        "Sanghak Lee",
                        "Jeongkyo Seo"
                    ],
                    "APPLICANTS": [
                        "LG ELECTRONICS INC.    ( Seoul , KR )"
                    ],
                    "INVENTION_TITLE": "Method for entering mobile robot into moving walkway and mobile robot thereof",
                    "DOMAIN": "G05D 10088",
                    "ABSTRACT": "The present invention is directed to a method of causing a mobile robot to enter a moving walkway, the method including setting a movement path including a moving walkway, recognizing, by the mobile robot, to enter the moving walkway included in the movement path, adjusting at least one of a speed of the mobile robot and a speed of a step belt of the moving walkway via communication between the mobile robot and the moving walkway, and moving the mobile robot onto the step belt of the moving walkway based on the adjusted speed.",
                    "CLAIMS": "1. A method for entering a mobile robot into a moving walkway comprising: setting a movement path including the moving walkway; recognizing that the mobile robot enters the moving walkway included in the movement path; adjusting at least one of a speed of the mobile robot and a speed of a step belt of the moving walkway via communication between the mobile robot and the moving walkway; and moving the mobile robot onto the step belt of the moving walkway based on the adjusted speed, wherein the adjusting includes: adjusting the at least one of the speed of the mobile robot and the speed of the step belt of the moving walkway based on whether or not a passenger is present on the moving walkway, adjusting the at least one of the speed of the mobile robot and the speed of the step belt of the moving walkway so that the speed of the mobile robot and the speed of the step belt of the moving walkway match each other, and adjusting the speed of the step belt of the moving walkway so as to match the speed of the mobile robot when no passenger is present on the moving walkway. 2. The method of claim 1, wherein the recognizing includes recognizing that the mobile robot enters the moving walkway via a sensor provided on the moving walkway. 3. The method of claim 1, wherein the adjusting further includes: receiving information indicating whether or not the passenger is present on the moving walkway from the moving walkway; and determining whether or not the passenger is present on the moving walkway based on the received information. 4. The method of claim 1, wherein the adjusting includes adjusting the speed of the mobile robot so as to match the speed of the step belt of the moving walkway when the passenger is present on the moving walkway. 5. The method of claim 1, wherein the adjusting includes adjusting the at least one of the speed of the mobile robot and the speed of the step belt of the moving walkway from a time when it is recognized that the mobile robot enters the moving walkway to a time when the mobile robot moves onto the step belt of the moving walkway. 6. The method of claim 1, wherein the moving further includes adjusting an orientation of the mobile robot so that a movement direction of the mobile robot moved onto the step belt of the moving walkway is parallel to a movement direction of the step belt of the moving walkway. 7. A mobile robot comprising: a communication unit configured to communicate with a moving walkway; and a processor configured to set a movement path including the moving walkway, to recognize that the mobile robot enters the moving walkway included in the movement path, to adjust at least one of a speed of the mobile robot and a speed of a step belt of the moving walkway via communication between the mobile robot and the moving walkway, and to move the mobile robot onto the step belt of the moving walkway based on the adjusted speed, wherein the processor is configured to: adjust the at least one of the speed of the mobile robot and the speed of the step belt of the moving walkway based on whether or not a passenger is present on the moving walkway, adjust the at least one of the speed of the mobile robot and the speed of the step belt of the moving walkway so that the speed of the mobile robot and the speed of the step belt of the moving walkway match each other, and adjust the speed of the step belt of the moving walkway so as to match the speed of the mobile robot when no passenger is present on the moving walkway. 8. The mobile robot of claim 7, wherein the processor is configured to recognize that the mobile robot enters the moving walkway via a sensor provided on the moving walkway. 9. The mobile robot of claim 7, wherein the processor is further configured to receive information indicating whether or not the passenger is present on the moving walkway from the moving walkway, and to determine whether or not the passenger is present on the moving walkway based on the received information. 10. The mobile robot of claim 7, wherein the processor is configured to adjust the speed of the mobile robot so as to match the speed of the step belt of the moving walkway when the passenger is present on the moving walkway. 11. The mobile robot of claim 7, wherein the processor is configured to adjust the at least one of the speed of the mobile robot and the speed of the step belt of the moving walkway from a time when it is recognized that the mobile robot enters the moving walkway to a time when the mobile robot moves onto the step belt of the moving walkway. 12. The mobile robot of claim 7, wherein the processor is further configured to adjust an orientation of the mobile robot so that a movement direction of the mobile robot moved onto the step belt of the moving walkway is parallel to a movement direction of the step belt of the moving walkway. 13. A non-transitory computer readable recording medium comprising a computer program for performing the method for entering a mobile robot into a moving walkway of claim 1.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to a method of causing a movable mobile robot to enter a moving walkway and a mobile robot that performs the method.",
                    "STATE_OF_THE_ART": "Various industrial robots for use in medical, aerospace, shipbuilding, and agricultural industries, for example, have been manufactured and operated. In recent years, in addition to a robot configured to perform only simple repetitive operations in place, a mobile robot capable of implementing a predetermined function while moving along with a user has been widely utilized to directly communicate with the user and provide user convenience. Accordingly, a mobile robot capable of moving while carrying out various operations has been widely utilized not only at home but also in places where a large number of people gather to individually perform various tasks or get guide services for example, an airport, a building lounge, and locations for events. In the case in which a mobile robot moves along a predetermined path in order to perform a predetermined operation in the places mentioned above, the mobile robot is capable of searching for an optimal path using acquired map information. Thus, a user can move along with the mobile robot as needed to perform the predetermined operation. For example, when the user wishes to be guided for a movement path to a specific location within a wide place, the mobile robot may guide the user the movement path. In this manner, the mobile robot may provide the user with a function of showing an optimal path in a predetermined place. However, since the mobile robot may have difficulty in using various transport devices for example, a moving walkway and an escalator available to the user according to the driving characteristics thereof, path guidance in consideration of positions of these transport devices may be difficult.",
                    "SUMMARY": [
                        "The foregoing summary is illustrative only and is not intended to be in any way limiting. In addition to the illustrative aspects, embodiments, and features described above, further aspects, embodiments, and features will become apparent by reference to the drawings and the following detailed description. The present disclosure is devised to provide a method for allowing a mobile robot to guide a user an optimal movement path in consideration of not only the ground which does not move but also a movement path in which various transport devices available to a user are provided and a mobile robot for the same. The present disclosure is devised to provide a method for allowing a mobile robot to stably enter a transport device in a path guidance process of guiding a user a movement path and a mobile robot for the same. The present disclosure is devised to provide a method for allowing a mobile robot to stably leave a transport device and a mobile robot for the same. The present disclosure is devised to provide an optimal method for allowing a mobile robot to enter a transport device without passenger inconvenience and to move rapidly based on whether or not a passenger has already been in the transport device. The present disclosure is not limited to the above-mentioned objectives, and other unmentioned objectives of the present disclosure may be easily understood by those skilled in the art from various embodiments of the present disclosure, which will be described later. In addition, it will be readily apparent that the objectives and advantages of the present disclosure may be realized by devices defined in the claims and combinations thereof. In order to address the above-described technical solutions, according to one embodiment, there is provided a method for a mobile robot to enter a moving walkway including setting a movement path including a moving walkway, recognizing that a mobile robot enters the moving walkway included in the movement path, adjusting at least one of a speed of the mobile robot and a speed of a step belt of the moving walkway via communication between the mobile robot and the moving walkway, and moving the mobile robot onto the step belt of the moving walkway based on the adjusted speed. In order to address the above-described technical solutions, according to another embodiment, there is provided a mobile robot including a communication unit configured to communicate with a moving walkway, and a processor configured to set a movement path including the moving walkway, to recognize that the mobile robot enters the moving walkway included in the movement path, to adjust at least one of a speed of the mobile robot and a speed of a step belt of the moving walkway via communication between the mobile robot and the moving walkway, and to move the mobile robot onto the step belt of the moving walkway based on the adjusted speed. In order to address the above-described technical solutions, according to a further embodiment, there is provided a non-transitory computer readable recording medium including a computer program for performing a method for a mobile robot to enter a moving walkway. According to an embodiment of the present disclosure, a mobile robot may provide a user with an optimal movement path actually available to the user among paths in which various transport devices available to the user are provided. According to an embodiment of the present disclosure, it is possible to allow a mobile robot to stably enter a transport device without losing the balance thereof and without creating anxiety of people around the mobile robot. According to an embodiment of the present disclosure, it is possible to allow a mobile robot to stably leave a transport device. According to the present disclosure, by determining whether or not a passenger has already been in a transport device before a mobile robot enters the transport device, it is possible to prevent the passenger from feeling uncomfortable due to a change in the speed of the transport device and to allow the mobile robot to move in the transport device at a high speed when no passenger is present in the transport device.",
                        "The above and other aspects, features, and advantages of certain embodiments will be more apparent from the following detailed description taken in conjunction with the accompanying drawings, in which: 1 illustrates the situation in which a mobile robot enters a moving walkway according to an embodiment. 2 illustrates a block diagram of a mobile robot for riding on a moving walkway according to an embodiment. 3 illustrates a flowchart of a method of causing a mobile robot to enter a moving walkway according to an embodiment. 4 is a flowchart illustrating a method of causing a mobile robot to enter a moving walkway through a process of the mobile robot recognizing entry thereof to the moving walkway using a sensor provided on the moving walkway according to an embodiment. 5 is a diagram of a mobile robot passing through a sensor provided on a moving walkway according to an embodiment. 6 is a flowchart illustrating a method of controlling, by a mobile robot, at least one of the speed of a step belt of a moving walkway and the speed of the mobile robot based on whether or not a passenger is present on the step belt of the moving walkway according to an embodiment. 7A to 7C illustrate examples of adjusting at least one of the speed of a mobile robot and the speed of a step belt of a moving walkway during a time interval from the time when the mobile robot recognizes entry thereof to the moving walkway to the time when the mobile robot moves to the step belt of the moving walkway according to an embodiment. 8 is a diagram illustrating an operation of a mobile robot when the mobile robot moves to a step belt of a moving walkway according to an embodiment. 9 illustrates an AI device according to an embodiment. 10 illustrates an AI server according to an embodiment. 11 illustrates an AI system according to an embodiment."
                    ],
                    "DESCRIPTION": "In the following detailed description, reference is made to the accompanying drawing, which form a part hereof. The illustrative embodiments described in the detailed description, drawing, and claims are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the spirit or scope of the subject matter presented here. Hereinafter, embodiments of the present disclosure will be described in detail with reference to the drawings so that those skilled in the art can easily carry out the present disclosure. The present disclosure may be embodied in many different forms and is not limited to the embodiments described herein. With respect to constituent elements used in the following description, suffixes module and unit are given or mingled with each other only in consideration of ease in the preparation of the specification, and do not have or serve as different meanings. In order to clearly describe the present disclosure, elements having no connection with the description are omitted, and the same or extremely similar elements are designated by the same reference numerals throughout the specification. In addition, some embodiments of the present disclosure will be described in detail with reference to exemplary drawings. When adding reference numerals to constituent elements of the respective drawings, it should be noted that the same or similar elements are denoted by the same reference numerals even though they are depicted in different drawings. In addition, in the following description of the present disclosure, a detailed description of known functions and configurations incorporated herein will be omitted when it may make the subject matter of the present disclosure rather unclear. In addition, it will be understood that the terms first, second, A, B, a, and b, for example, may be used herein to describe various elements according to the embodiments of the present disclosure. These terms are only used to distinguish one element from another element and, thus, are not intended to limit the essence, order, sequence, or number of elements. It will be understood that, when any element is referred to as being connected to coupled to, or joined to another element, it may be directly on, connected to or coupled to the other element or intervening elements may be present. It will be further understood that the terms comprises comprising includes and/or including when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, and/or components. In addition, for convenience of description, the present disclosure may be embodied by subdividing constituent elements, but these constituent elements may be embodied in a single device or module, or one constituent element may be divided into multiple devices or modules. Prior to describing various embodiments of the present disclosure, key terms will be described as follows. The term robot may refer to a machine that automatically operates or performs a given operation by abilities thereof. In particular, a robot that functions to recognize an environment and perform a motion based on self-determination may be referred to as an intelligent robot. Robots may be classified into industrial, medical, household, and military robots, for example, according to the purpose of use or the field of use thereof. According to an embodiment, a mobile robot may be defined as a robot that has the ability to move by itself and is capable of moving to perform a predetermined function. The term moving walkway refers to one of transport devices on which a user or a robot can ride and move. The user or the robot needs to move horizontally in order to ride on the moving walkway, and once ridden on the moving walkway, the user or the robot can be moved in a direction parallel to or obliquely to the ground. According to an embodiment, the moving walkway may be referred to as an autowalk, a moving sidewalk, a moving pavement, a people-mover, a travolator or a travelator, and may conceptually include a transport device such as an escalator. The term step belt refers to one constituent element of the moving walkway. The step belt is a predetermined moving structure for continuously moving a person or an object on the moving walkway, and may receive movement force from, for example, a step belt roller or a drive device connected thereto. According to an embodiment, the robot or a passenger can move onto the step belt of the moving walkway to continuously move along with the step belt. The term artificial intelligence AI refers to the field of studying artificial intelligence or a methodology capable of making the artificial intelligence, and the term machine learning refers to the field of studying methodologies that define and solve various problems handled in the field of artificial intelligence. The machine learning is also defined as an algorithm that enhances performance for a certain operation through a steady experience with respect to the operation. The term artificial neural network ANN may include an input layer and an output layer, and may selectively include one or more hidden layers. Each layer may include one or more neurons, and the artificial neural network may include a synapse that interconnects neurons. In the artificial neural network, each neuron may output the value of an activation function concerning signals input through the synapse, weights, and deflection thereof. The artificial intelligence may refer to a general model for use in the machine learning, which is composed of artificial neurons nodes forming a network by synaptic connection and has problem solving ability. The artificial neural network may be defined by a connection pattern between neurons of different layers, a learning process of updating model parameters, and an activation function of generating an output value. The model parameters refer to parameters determined by learning, and include weights for synaptic connection and deflection of neurons, for example. Then, hyper-parameters refer to parameters to be set before learning in a machine learning algorithm, and include a learning rate, the number of repetitions, the size of a mini-batch, and an initialization function, for example. It can be said that the purpose of learning of the artificial neural network is to determine a model parameter that minimizes a loss function. The loss function may be used as an index for determining an optimal model parameter in a learning process of the artificial neural network. The machine learning may be classified, according to a learning method, into supervised learning, unsupervised learning, and reinforcement learning. The supervised learning refers to a learning method for an artificial neural network in the state in which a label for learning data is given. The label may refer to a correct answer or a result value to be deduced by the artificial neural network when learning data is input to the artificial neural network. The unsupervised learning may refer to a learning method for the artificial neural network in the state in which no label for learning data is given. The reinforcement learning may refer to a learning method in which an agent defined in a certain environment learns to select a behavior or a behavior sequence that maximizes cumulative compensation in each state. The machine learning realized by a deep neural network DNN including multiple hidden layers among artificial neural networks is also called deep learning, and the deep learning is a part of the machine learning. In the following description, the machine learning is used as a meaning including the deep learning. The term autonomous driving or self-driving refers to a technology in which a vehicle drives autonomously, and the term autonomous vehicle refers to a vehicle that travels without a user's operation or with a user's minimum operation. For example, autonomous driving may include all of the technology of maintaining the lane in which a vehicle is driving, the technology of automatically adjusting a vehicle speed such as adaptive cruise control, the technology of causing a vehicle to automatically drive along a given route, and the technology of automatically setting a route, along which a vehicle drives, when a destination is set. The vehicle may include all of a vehicle having only an internal combustion engine, a hybrid vehicle having both an internal combustion engine and an electric motor, and an electric vehicle having only an electric motor, and may include not only an automobile but also a train and a motorcycle, for example. At this time, the autonomous vehicle may be seen as a robot having an autonomous driving function. The term extended reality XR is a generic term for virtual reality VR, augmented reality AR, and mixed reality MR. The VR technology provides only a CG image of a real-world object or background, for example, the AR technology provides a virtual CG image over an actual object image, and the MR technology is a computer graphic technology of providing an image obtained by mixing and combining virtual objects with the real world. The MR technology is similar to the AR technology in that it shows a real object and a virtual object together. However, the virtual object is used to complement the real object in the AR technology, whereas the virtual object and the real object are equally used in the MR technology. The XR technology may be applied to a head-mounted display HMD, a head-up display HUD, a mobile phone, a tablet PC, a laptop computer, a desktop computer, a TV, and a digital signage, for example, and a device to which the XR technology is applied may be referred to as an XR device. Hereinafter, various embodiments of the present disclosure will be described with reference to the drawings. 1 illustrates the situation in which a mobile robot 100 enters a moving walkway 150 according to an embodiment. According to an embodiment, mobile robot 100 is a self-moving device and is movable along a path set by a predetermined function. According to an embodiment, mobile robot 100 may include a drive unit such as a wheel and may be moved by the drive unit. Mobile robot 100 may set an optimal movement path thereof under the assumption that mobile robot 100 may utilize a transport device such as moving walkway 150 when moving along the path. Due to the fact that a step belt of moving walkway 150 continuously moves to horizontally move an object thereon, however, a passenger who attempts to ride on moving walkway 150 may experience the difference in relative speed with moving walkway 150. Such difference in relative speed may cause the passenger or mobile robot 100 to lose the balance thereof when riding on the step belt of moving walkway 150. Therefore, it may be necessary to appropriately control the speed of movement of mobile robot 100 to allow mobile robot 100 to stably ride on the step belt of moving walkway 150. 2 illustrates a block diagram of a mobile robot 200 for riding on a moving walkway 250 according to an embodiment. According to an embodiment, mobile robot 200 may include a communication unit 210 capable of performing communication with moving walkway 250, a processor 220 configured to control mobile robot 200 to realize various operations which may be performed by mobile robot 200 according to the following various embodiments, and a drive unit 230 capable of moving mobile robot 200. According to an embodiment, mobile robot 200 may receive predetermined information indicating the current state of moving walkway 250 for example, the current speed of the step belt of moving walkway 250, whether or not a passenger is on moving walkway 250, whether or not moving walkway 250 is currently operating, or the load currently applied to moving walkway 250 by communicating with moving walkway 250 through communication unit 210. According to an embodiment, communication unit 210 may be controlled by processor 220 to transmit predetermined information or instructions from mobile robot 200 to moving walkway 250, so that an operation of moving walkway 250 may be controlled in association with mobile robot 200. According to an embodiment, a communication technology available to communication unit 210 may be any of various communication technologies which can be used by those of ordinary skill, and may be, for example, a global system for mobile communication GSM, code division multiple Access CDMA, long term evolution LTE, 5G, wireless LAN WLAN, wireless-fidelity Wi-Fi, Bluetooth, radio frequency identification RFID, infrared data association IrDA, ZigBee, or near field communication NFC. According to an embodiment, processor 220 may control the direction in which mobile robot 200 moves and the speed of movement of mobile robot 200 by controlling drive unit 230. According to an embodiment, drive unit 230 may include components capable of imparting movement to mobile robot 200 such as a wheel and a motor within a range in which they may be easily adopted by those of ordinary skill. According to an embodiment, the speed of mobile robot 200 may be faster or slower than the speed of the step belt of moving walkway 250. For convenience, the following description of various embodiments will be made under the assumption that the speed of mobile robot 200 is faster than the speed of the step belt of moving walkway 250, but the relationship between the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 should not be interpreted as being limited to the following description. Hereinafter, various operations which may be performed by mobile robot 200 using communication unit 210 and processor 220 will be described according to various embodiments. 3 illustrates a flowchart of a method of causing mobile robot 200 to enter moving walkway 250 according to an embodiment. In step S310, mobile robot 200 may set a movement path including moving walkway 250 according to an embodiment. According to an embodiment, processor 220 of mobile robot 200 may set a predetermined movement path using a variety of algorithms for example, a Dijkstra algorithm available to those of ordinary skill, and may calculate the weight of each movement path in consideration of various devices, for example, provided in the movement path. According to an embodiment, mobile robot 200 may calculate the weight of a movement path in consideration of an available transport device for example, moving walkway 250 included in the path, and based on the calculation of the weight, may determine a movement path under the assumption that mobile robot 200 uses the available transport device. In this way, mobile robot 200 may provide a user with an optimal movement path in consideration of the use of a transport device conveniently available to the user. In step S320, mobile robot 200 may recognize that mobile robot 200 enters moving walkway 250 included in the movement path set in step S310 according to an embodiment. According to an embodiment, moving walkway 250 may include a continuously moving step belt, and a predetermined entry space for movement of mobile robot 200 may be provided in front of the step belt. As such, mobile robot 200 may recognize that mobile robot 200 enters moving walkway 250 before moving onto the step belt, and based on the recognized result, may additionally perform a predetermined operation required for entry thereof to moving walkway 250. According to an embodiment, entry of mobile robot 200 to moving walkway 250 refers to that mobile robot 200 reaches within a predetermined distance range from the step belt of moving walkway 250 in the process of moving along a path including moving walkway 250, or refers to that mobile robot 200 moves toward the step belt after transmitting predetermined information to moving walkway 250 by passing through a predetermined structure for example, a predetermined sensor provided on moving walkway 250. In step S330, according to an embodiment, mobile robot 200 may adjust at least one of the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 by communicating with moving walkway 250. According to an embodiment, processor 220 of mobile robot 200 may control communication unit 210 to communicate with moving walkway 250. Processor 220 may receive predetermined information from moving walkway 250 through communication unit 210, and may determine the degree to which at least one of the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 is adjusted based on the received information. In step S340, mobile robot 200 may move onto the step belt of moving walkway 250 based on the speed adjusted in step S330. According to an embodiment, processor 220 of mobile robot 200 may control at least one of the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 to allow mobile robot 200 to enter the step belt of moving walkway 250 in the state in which the speed of mobile robot 200 matches the speed of the step belt of moving walkway 250. In order to control the speed of mobile robot 200 according to an embodiment, processor 220 may control drive unit 230 to reduce or increase the speed of mobile robot 200 to a predetermined speed. In order to control the speed of the step belt of moving walkway 250 according to an embodiment, processor 220 may control communication unit 210 to transmit predetermined information or instructions required to reduce the speed of the step belt of moving walkway 250 to a specific speed. When it is determined to adjust both the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 according to an embodiment, mobile robot 200 may perform both the above-described speed adjustment process using drive unit 230 and the above-described communication process with moving walkway 250. 4 is a flowchart illustrating a method of causing mobile robot 200 to enter moving walkway 250 through a process of mobile robot 200 recognizing entry thereof to moving walkway 250 using a sensor provided on moving walkway 250 according to an embodiment. The features of steps S410, S430 and S440 may be the same as or similar to those of steps S310, S330 and S340 of 3, and thus, a detailed description thereof will be omitted. In step S420, according to an embodiment, mobile robot 200 may recognize that mobile robot 200 enters moving walkway 250 included in the movement path via a sensor provided on moving walkway 250. According to an embodiment, mobile robot 200 may recognize that mobile robot 200 enters moving walkway 250 using predetermined information transmitted through a predetermined sensor included in moving walkway 250. According to an embodiment, in order to recognize moving walkway 250 included in the movement path and determine whether or not mobile robot 200 enters moving walkway 250, mobile robot 200 may receive predetermined information from a sensor included in moving walkway 250 through communication unit 210. According to an embodiment, the predetermined information received from moving walkway 250 may include information indicating an entrance portion of moving walkway 250. The predetermined information received from moving walkway 250 may further include identification information indicating moving walkway 250. 5 is a diagram of a mobile robot 500 passing through a sensor 552 provided on a moving walkway 550 according to an embodiment. According to an embodiment, mobile robot 500 may receive predetermined information by passing through at least one sensor 552 provided on moving walkway 550. According to an embodiment, mobile robot 500 may pass through at least one sensor 552 prior to moving to a step belt 551. Mobile robot 500 may recognize that mobile robot 500 enters moving walkway 550 based on the predetermined information received from at least one sensor 552. According to an embodiment, in order to recognize that mobile robot 200 enters moving walkway 250, mobile robot 200 may use information on the current position of mobile robot 200. That is, communication unit 210 of mobile robot 200 may acquire information indicating the current position of mobile robot 200 such as GPS information, and processor 220 may recognize whether or not mobile robot 200 enters moving walkway 250 by comparing the current position of mobile robot 200 with the position of moving walkway 250 included in the movement path. According to an embodiment, mobile robot 200 may acquire in advance positional information of moving walkway 250 that mobile robot 200 tries to enter. According to an embodiment, when it is difficult to obtain GPS information, for example, since mobile robot 200 is moving in a room, mobile robot 200 may acquire predetermined information such as an NFC tag through a component such as an input unit not illustrated to determine the current position thereof. According to an embodiment, mobile robot 200 may determine whether or not a passenger is present on moving walkway 250, and may adjust at least one of the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 based on the determined result. According to an embodiment, the speed of mobile robot 200 may be adjusted so as to match the speed of the step belt of moving walkway 250 when a passenger is present on the step belt of moving walkway 250 that mobile robot 200 enters. When the speed of the step belt of moving walkway 250 is faster than the speed of mobile robot 200 according to an embodiment, processor 220 may control drive unit 230 to increase the speed of mobile robot 200 to the speed of the step belt of moving walkway 250. When the speed of the step belt of moving walkway 250 is slower than the speed of mobile robot 200 according to an embodiment, processor 220 may control drive unit 230 to reduce the speed of mobile robot 200 to the speed of the step belt of moving walkway 250. When a passenger is present on the step belt of moving walkway 250, increasing or reducing the speed of moving walkway 250 based on the entry of mobile robot 200 thereto may cause the passenger to lose the balance thereof or feel uncomfortable. Therefore, when a passenger is present on the step belt of moving walkway 250, the speed of mobile robot 200 may be adjusted, instead of adjusting the speed of the step belt of moving walkway 250. According to an embodiment, when no passenger is present on the step belt of moving walkway 250 that mobile robot 200 enters, the speed of the step belt of moving walkway 250 may be adjusted so as to match the speed of mobile robot 200. When the speed of the step belt of moving walkway 250 is faster than the speed of mobile robot 200 according to an embodiment, processor 220 may control communication unit 210 to transmit, to moving walkway 250, predetermined information or instructions indicating that it is necessary to reduce the speed of the step belt of moving walkway 250 to the speed of mobile robot 200. When the speed of the step belt of moving walkway 250 is slower than the speed of mobile robot 200 according to an embodiment, processor 220 may control communication unit 210 to transmit, to moving walkway 250, predetermined information or instructions indicating that it is necessary to increase the speed of the step belt of moving walkway 250 to the speed of mobile robot 200. According to an embodiment, mobile robot 200 may receive information indicating that a passenger is present on the step belt of moving walkway 250 through communication unit 210, and processor 220 may determine that a passenger is present on the step belt of moving walkway 250 based on the received information. According to an embodiment, the information indicating whether or not a passenger is present on the step belt of moving walkway 250 may include the magnitude of load applied to moving walkway 250, an image such as a virtual image or a thermal image acquired through, for example, an input unit provided on moving walkway 250, or information, obtained through at least one sensor provided on moving walkway 250, indicating whether or not a passenger has still remained on the step belt. According to an embodiment, mobile robot 200 may use an input unit not illustrated included therein in order to determine whether or not a passenger is present on the step belt of moving walkway 250. That is, mobile robot 200 may determine whether or not a passenger is present on the step belt of moving walkway 250 based on, for example, a virtual image or a thermal image acquired from the input unit not illustrated. 6 is a flowchart illustrating a method of controlling, by mobile robot 200, at least one of the speed of the step belt of moving walkway 250 and the speed of mobile robot 200 based on whether or not a passenger is present on the step belt of moving walkway 250 according to an embodiment. The features of steps S610, S620, and S660 may be the same as or similar to those of steps S410, S420, and S440 of 4, respectively, and thus, a detailed description thereof will be omitted. In step S630, mobile robot 200 may determine whether or not a passenger is present on the step belt of moving walkway 250 according to an embodiment. When a passenger is present on the step belt of moving walkway 250 according to an embodiment, mobile robot 200 may adjust the speed thereof so as to match the speed of the step belt of moving walkway 250 in step S640. When no passenger is present on the step belt of moving walkway 250 according to an embodiment, mobile robot 200 may adjust the speed of the step belt of moving walkway 250 so as to match the speed of mobile robot 200 in step S650. According to an embodiment, processor 220 may control communication unit 210 to transmit, to moving walkway 250, information indicating a target speed to which the speed of the step belt of moving walkway 250 is adjusted. The information indicating the target speed to be transmitted to moving walkway 250 according to an embodiment may indicate the current speed of mobile robot 200. According to an embodiment, mobile robot 200 may adjust the adjustment ratio between the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 based on whether or not a passenger is present on the step belt of moving walkway 250. For example, when no passenger is present on the step belt of moving walkway 250, in order to match the speed of the step belt of moving walkway 250 with the speed of mobile robot 200, the speed of the step belt of moving walkway 250 may be adjusted at a larger ratio than the speed of mobile robot 200. When a passenger is present on the step belt of moving walkway 250, in order to match the speed of mobile robot 200 with the speed of the step belt of moving walkway 250, the speed of mobile robot 200 may be adjusted at a larger ratio than the speed of the step belt of moving walkway 250. According to an embodiment, mobile robot 200 may adjust at least one of the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 during a time interval from the time when mobile robot 200 recognizes that mobile robot 200 enters moving walkway 250 to the time when mobile robot 200 moves onto the step belt of moving walkway 250. 7A to 7C illustrate examples of adjusting at least one of the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 during a time interval from the time when mobile robot 200 recognizes entry thereof to moving walkway 250 to the time when mobile robot 200 moves to the step belt of moving walkway 250 according to an embodiment. Referring to 7A to 7C, there may be a time interval 710 between the time t1 when mobile robot 200 recognizes that mobile robot 200 enters moving walkway 250 and the time t2 when mobile robot 200 moves onto the step belt of moving walkway 250, and at least one of the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 may be adjusted during time interval 710. The speed of mobile robot 200 and the speed of the step belt of moving walkway 250 become match each other according to the result adjusted during time interval 710. According to an embodiment, a time interval 720 during which the speed of mobile robot 200 and the speed of the step belt of moving walkway 250 are substantially adjusted by mobile robot 200 may be determined to be shorter than time interval 710 between time t1 and time t2. Referring to 7A, processor 220 of mobile robot 200 may control drive unit 230 to adjust the speed of mobile robot 200 during time interval 720. According to an embodiment, the speed of the step belt of moving walkway 250 may not be changed during time interval 720. 7A illustrates the result based on a control operation of mobile robot 200 when it is determined that a passenger is present on the step belt of moving walkway 250 according to an embodiment. Referring to 7B, processor 220 of mobile robot 200 may control communication unit 210 to transmit, to moving walkway 250, predetermined information or instructions for adjusting the speed of the step belt of moving walkway 250 during time interval 720. According to an embodiment, the speed of mobile robot 200 may not be changed during time interval 720. 7B illustrates the result based on a control operation of mobile robot 200 when it is determined that no passenger is present on the step belt of moving walkway 250 according to an embodiment. Referring to 7C, processor 220 of mobile robot 200 may control drive unit 230 to adjust the speed of mobile robot 200 during time interval 720, and may further control communication unit 210 to transmit, to moving walkway 250, predetermined information or instructions for adjusting the speed of the step belt of moving walkway 250. That is, not only the speed of mobile robot 200 but also the speed of the step belt of moving walkway 250 may be controlled during time interval 710 before mobile robot 200 moves onto the step belt of moving walkway 250. According to an embodiment, the time taken to adjust the speed of mobile robot 200 and the time taken to adjust the speed of the step belt of moving walkway 250 may be different from each other, and may be shorter than time interval 710. 8 is a diagram illustrating an operation of a mobile robot 800 when mobile robot 800 moves to a step belt 882 of a moving walkway 880 according to an embodiment. According to an embodiment, mobile robot 800 may move onto step belt 882, which continuously moves, after entering moving walkway 880. According to an embodiment, mobile robot 800 may be oriented at any of various angles at the time when mobile robot 800 moves onto step belt 882 according to a direction 810 in which mobile robot 800 enters moving walkway 880. According to an embodiment, direction 810 in which mobile robot 800 enters moving walkway 880 may be the direction in which mobile robot 800 moves, and may be related to the direction in which multiple wheels included in drive unit 230 of mobile robot 800 are arranged. For example, entry direction 810 may be perpendicular to the direction in which the wheels of drive unit 230 of mobile robot 800 are arranged. According to an embodiment, mobile robot 800 may enter step belt 882 in the process of moving in a direction not parallel to a movement direction 820 of step belt 882. According to an embodiment, it may be determined whether or not mobile robot 800 is disposed parallel to movement direction 820 of step belt 882. Various methods may be used to determine whether or not mobile robot 800 is aligned with movement direction 820 of step belt 882 according to an embodiment. For example, processor 220 may analyze an image obtained from an input unit not illustrated included in mobile robot 800 to analyze, for example, the difference between acquired movement direction 820 of step belt 882 and direction 810 in which mobile robot 800 is currently oriented or a change in the amount of impact detected at the time when the wheels of drive unit 230 are positioned on step belt 882, thereby determining whether or not mobile robot 800 is aligned with movement direction 820 of step belt 882. According to an embodiment, when it is determined that mobile robot 800 is not aligned with movement direction 820 of step belt 882, processor 220 may control drive unit 230 to change the orientation of mobile robot 800 so that mobile robot 800 which has entered step belt 882 in direction 810 moves in the same direction 860 as movement direction 820 of step belt 882. In this way, by correcting the orientation of mobile robot 800 so that mobile robot 800 moves in the same direction 860 as movement direction 820 of step belt 882, mobile robot 800 may stably escape moving walkway 880 while maintaining the balance thereof when passing over, for example, a raised spot on the edge of step belt 882. According to an embodiment, when it may be determined that direction 810 in which mobile robot 800 enters step belt 882 before moving onto step belt 882 is not parallel to movement direction 820 of step belt 882, processor 220 of mobile robot 800 may control drive unit 230 at an arbitrary time between the time when it is determined that direction 810 in which mobile robot 800 enters step belt 882 is not parallel to movement direction 820 of step belt 882 and time t1 when mobile robot 800 moves onto step belt 882 to change the orientation of mobile robot 800 so that mobile robot 800 moves in the same direction 860 as movement direction 820 of step belt 882. In this case, mobile robot 800 may stably enter step belt 882 while maintaining the balance thereof. 9 illustrates an AI device 900 according to an embodiment of the present disclosure. AI device 900 of 9 may correspond to mobile robot 200 of 2, and some of constituent elements of 9, which are not included in robot 200 of 2, may be selectively adopted within a range in which the embodiments of the present disclosure may be realized. AI device 900 may be realized into, for example, a stationary appliance or a movable appliance, such as a TV, a projector, a cellular phone, a smart phone, a desktop computer, a laptop computer, a digital broadcasting terminal, a personal digital assistant PDA, a portable multimedia player PMP, a navigation system, a tablet PC, a wearable device, a set-top box STB, a DMB receiver, a radio, a washing machine, a refrigerator, a digital signage, a robot, or a vehicle. Referring to 9, AI device 900 may include a communication unit 910, an input unit 920, a learning processor 930, a sensing unit 940, an output unit 950, a memory 970, and a processor 980, for example. Communication unit 910 may transmit and receive data to and from external devices, such as other AI devices 1100a to 1100e and an AI server 1000, using wired/wireless communication technologies. For example, communication unit 910 may transmit and receive sensor information, user input, learning models, and control signals, for example, to and from external devices. At this time, the communication technology used by communication unit 910 may be, for example, a global system for mobile communication GSM, code division multiple Access CDMA, long term evolution LTE, 5G, wireless LAN WLAN, wireless-fidelity Wi-Fi, Bluetooth, radio frequency identification RFID, infrared data association IrDA, ZigBee, or near field communication NFC. Input unit 920 may acquire various types of data. At this time, input unit 920 may include a camera for the input of an image signal, a microphone for receiving an audio signal, and a user input unit for receiving information input by a user, for example. Here, the camera or the microphone may be handled as a sensor, and a signal acquired from the camera or the microphone may be referred to as sensing data or sensor information. Input unit 920 may acquire, for example, input data to be used when acquiring an output using learning data for model learning and a learning model. Input unit 920 may acquire unprocessed input data, and in this case, processor 980 or learning processor 930 may extract an input feature as pre-processing for the input data. Learning processor 930 may cause a model configured with an artificial neural network to learn using the learning data. Here, the learned artificial neural network may be called a learning model. The learning model may be used to deduce a result value for newly input data other than the learning data, and the deduced value may be used as a determination base for performing any operation. At this time, learning processor 930 may perform AI processing along with a learning processor 1040 of AI server 1000. At this time, learning processor 930 may include a memory integrated or embodied in AI device 900. Alternatively, learning processor 930 may be realized using memory 970, an external memory directly coupled to AI device 900, or a memory held in an external device. Sensing unit 940 may acquire at least one of internal information of AI device 900, environmental information around AI device 900, and user information using various sensors. At this time, the sensors included in sensing unit 940 may be a proximity sensor, an illuminance sensor, an acceleration sensor, a magnetic sensor, a gyro sensor, an inertial sensor, an RGB sensor, an IR sensor, a fingerprint recognition sensor, an ultrasonic sensor, an optical sensor, a microphone, a lidar, a radar, and a temperature sensor, for example. Output unit 950 may generate, for example, a visual output, an auditory output, or a tactile output. At this time, output unit 950 may include, for example, a display that outputs visual information, a speaker that outputs auditory information, and a haptic module that outputs tactile information. Memory 970 may store data which assists various functions of AI device 900. For example, memory 970 may store input data acquired by input unit 920, learning data, learning models, and learning history, for example. Processor 980 may determine at least one executable operation of AI device 900 based on information determined or generated using a data analysis algorithm or a machine learning algorithm. Then, processor 980 may control constituent elements of AI device 900 to perform the determined operation. To this end, processor 980 may request, search, receive, or utilize data of learning processor 930 or memory 970, and may control the constituent elements of AI device 900 so as to execute a predictable operation or an operation that is deemed desirable among the at least one executable operation. At this time, when connection of an external device is required to perform the determined operation, processor 980 may generate a control signal for controlling the external device and may transmit the generated control signal to the external device. Processor 980 may acquire intention information with respect to user input and may determine a user request based on the acquired intention information. At this time, processor 980 may acquire intention information corresponding to the user input using at least one of a speech to text STT engine for converting voice input into a character string and a natural language processing NLP engine for acquiring natural language intention information. At this time, at least a part of the STT engine and/or the NLP engine may be configured with an artificial neural network learned according to a machine learning algorithm. Then, the STT engine and/or the NLP engine may have learned by learning processor 930, may have learned by learning processor 1040 of AI server 1000, or may have learned by distributed processing of these processors. Processor 980 may collect history information including, for example, the content of an operation of AI device 900 or feedback of the user with respect to an operation, and may store the collected information in memory 970 or learning processor 930, or may transmit the collected information to an external device such as AI server 1000. The collected history information may be used to update a learning model. Processor 980 may control at least some of the constituent elements of AI device 900 in order to drive an application program stored in memory 970. Moreover, processor 980 may combine and operate two or more of the constituent elements of AI device 900 for the driving of the application program. 10 illustrates AI server 1000 according to an embodiment of the present disclosure. Referring to 10, AI server 1000 may refer to a device that causes an artificial neural network to learn using a machine learning algorithm or uses the learned artificial neural network. Here, AI server 1000 may be constituted of multiple servers to perform distributed processing, and may be defined as a 5G network. At this time, AI server 1000 may be included as a constituent element of AI device 900 so as to perform at least a part of AI processing together with the AI device. AI server 1000 may include a communication unit 1010, a memory 1030, learning processor 1040, and a processor 1060, for example. Communication unit 1010 may transmit and receive data to and from an external device such as AI device 900. Memory 1030 may include a model storage unit 1031. Model storage unit 1031 may store a model or an artificial neural network 1031a which is learning or has learned via learning processor 1040. Learning processor 1040 may cause artificial neural network 1031a to learn learning data. A learning model may be used in the state of being mounted in AI server 1000 of the artificial neural network, or may be used in the state of being mounted in an external device such as AI device 900. The learning model may be realized in hardware, software, or a combination of hardware and software. In the case in which a part or the entirety of the learning model is realized in software, one or more instructions constituting the learning model may be stored in memory 1030. Processor 1060 may deduce a result value for newly input data using the learning model, and may generate a response or a control instruction based on the deduced result value. 11 illustrates an AI system 1100 according to an embodiment of the present disclosure. Referring to 11, in AI system 1100, at least one of AI server 1000, a robot 1100a, an autonomous vehicle 1100b, an XR device 1100c, a smart phone 1100d, and a home appliance 1100e is connected to a cloud network 1110. Here, robot 1100a, autonomous vehicle 1100b, XR device 1100c, smart phone 1100d, and home appliance 1100e, to which AI technologies are applied, may be referred to as AI devices 1100a to 1100e. Cloud network 1110 may constitute a part of a cloud computing infra-structure, or may refer to a network present in the cloud computing infra-structure. Here, cloud network 1110 may be configured using a 3G network, a 4G or long term evolution LTE network, or a 5G network, for example. That is, respective devices 1100a to 1100e and 1000 constituting AI system 1100 may be connected to each other via cloud network 1110. In particular, respective devices 1100a to 1100e and 1000 may communicate with each other via a base station, or may perform direct communication without the base station. AI server 1000 may include a server which performs AI processing and a server which performs an operation with respect to big data. AI server 1000 may be connected to at least one of robot 1100a, autonomous vehicle 1100b, XR device 1100c, smart phone 1100d, and home appliance 1100e, which are AI devices constituting AI system 1100, via cloud network 1110, and may assist at least a part of AI processing of connected AI devices 1100a to 1100e. At this time, instead of AI devices 1100a to 1100e, AI server 1000 may cause an artificial neural network to learn according to a machine learning algorithm, and may directly store a learning model or may transmit the learning model to AI devices 1100a to 1100e. At this time, AI server 1000 may receive input data from AI devices 1100a to 1100e, may deduce a result value for the received input data using the learning model, and may generate a response or a control instruction based on the deduced result value to transmit the response or the control instruction to AI devices 1100a to 1100e. Alternatively, AI devices 1100a to 1100e may directly deduce a result value with respect to input data using the learning model, and may generate a response or a control instruction based on the deduced result value. Hereinafter, various embodiments of AI devices 1100a to 1100e, to which the above-described technology is applied, will be described. Here, AI devices 1100a to 1100e illustrated in 11 may be specific embodiments of AI device 900 illustrated in 9. Robot 1100a may be realized into a guide robot, a transportation robot, a cleaning robot, a wearable robot, an entertainment robot, a pet robot, or an unmanned flying robot, for example, through the application of AI technologies. Robot 1100a may include a robot control module for controlling an operation, and the robot control module may refer to a software module or a chip realized in hardware. Robot 1100a may acquire information on the state of robot 1100a using sensor information acquired from various types of sensors, may detect recognize the surrounding environment and an object, may generate map data, may determine a movement route and a driving plan, may determine a response with respect to user intersection, or may determine an operation. Here, robot 1100a may use sensor information acquired from at least one sensor among a lidar, a radar, and a camera in order to determine a movement route and a driving plan. Robot 1100a may perform the above-described operations using a learning model configured with at least one artificial neural network. For example, robot 1100a may recognize the surrounding environment and the object using the learning model, and may determine an operation using the recognized surrounding environment information or object information. Here, the learning model may be directly learned in robot 1100a, or may be learned in an external device such as AI server 1000. At this time, robot 1100a may directly generate a result using the learning model to perform an operation, but may transmit sensor information to an external device such as AI server 1000 and receive a result generated by the external device to perform an operation. Robot 1100a may determine a movement route and a driving plan using at least one of map data, object information detected from sensor information, and object information acquired from an external device, and a drive unit may be controlled to drive robot 1100a according to the determined movement route and driving plan. The map data may include object identification information for various objects arranged in a space along which robot 1100a moves. For example, the map data may include object identification information for stationary objects, such as the wall and the door, and movable objects such as a flowerpot and a desk. Then, the object identification information may include names, types, distances, and locations, for example. In addition, robot 1100a may perform an operation or may drive by controlling the drive unit based on user control or interaction. At this time, robot 1100a may acquire interactional intention information depending on a user operation or voice expression, and may determine a response based on the acquired intention information to perform an operation. Autonomous vehicle 1100b may be realized into a mobile robot, a vehicle, or an unmanned air vehicle, for example, through the application of AI technologies. Autonomous vehicle 1100b may include an autonomous driving control module for controlling an autonomous driving function, and the autonomous driving control module may mean a software module or a chip realized in hardware. The autonomous driving control module may be a constituent element included in autonomous vehicle 1100b, but may be a separate hardware element outside autonomous vehicle 1100b so as to be connected thereto. Autonomous vehicle 1100b may acquire information on the state of autonomous vehicle 1100b using sensor information acquired from various types of sensors, may detect recognize the surrounding environment and an object, may generate map data, may determine a movement route and a driving plan, or may determine an operation. Here, autonomous vehicle 1100b may use sensor information acquired from at least one sensor among a lidar, a radar, and a camera in the same manner as robot 1100a in order to determine a movement route and a driving plan. In particular, autonomous vehicle 1100b may recognize the environment or an object with respect to an area outside the field of vision or an area located at a predetermined distance or more by receiving sensor information from external devices, or may directly receive recognized information from external devices. Autonomous vehicle 1100b may perform the above-described operations using a learning model configured with at least one artificial neural network. For example, autonomous vehicle 1100b may recognize the surrounding environment and the object using the learning model, and may determine a driving line using the recognized surrounding environment information or object information. Here, the learning model may be directly learned in autonomous vehicle 1100b, or may be learned in an external device such as AI server 1000. At this time, autonomous vehicle 1100b may generate a result using the learning model to perform an operation, but may transmit sensor information to an external device such as AI server 1000 and receive a result generated by the external device to perform an operation. Autonomous vehicle 1100b may determine a movement route and a driving plan using at least one of map data, object information detected from sensor information, and object information acquired from an external device, and a drive unit may be controlled to drive autonomous vehicle 1100b according to the determined movement route and driving plan. The map data may include object identification information for various objects arranged in a space , a road along which autonomous vehicle 1100b drives. For example, the map data may include object identification information for stationary objects, such as streetlights, rocks, and buildings, and movable objects such as vehicles and pedestrians. Then, the object identification information may include names, types, distances, and locations, for example. In addition, autonomous vehicle 1100b may perform an operation or may drive by controlling the drive unit based on user control or interaction. At this time, autonomous vehicle 1100b may acquire interactional intention information depending on a user operation or voice expression, and may determine a response based on the acquired intention information to perform an operation. XR device 1100c may be realized into a head-mount display HMD, a head-up display HUD provided in a vehicle, a television, a cellular phone, a smart phone, a computer, a wearable device, a home appliance, a digital signage, a vehicle, a stationary robot, or a mobile robot, for example, through the application of AI technologies. XR device 1100c may obtain information on the surrounding space or a real object by analyzing three-dimensional point cloud data or image data acquired from various sensors or an external device to generate positional data and attribute data for three-dimensional points, and may output an XR object by rendering the XR object to be output. For example, XR device 1100c may output an XR object including additional information about a recognized object so as to correspond to the recognized object. XR device 1100c may perform the above-described operations using a learning model configured with at least one artificial neural network. For example, XR device 1100c may recognize a real object from three-dimensional point cloud data or image data using a learning model, and may provide information corresponding to the recognized real object. Here, the learning model may be directly learned in XR device 1100c, or may be learned in an external device such as AI server 1000. At this time, XR device 1100c may directly generate a result using the learning model to perform an operation, but may transmit sensor information to an external device such as AI server 1000 and receive the generated result to perform an operation. Robot 1100a may be realized into a guide robot, a transportation robot, a cleaning robot, a wearable robot, an entertainment robot, a pet robot, or an unmanned flying robot, for example, through the application of AI technologies and autonomous driving technologies. Robot 1100a to which the AI technologies and the autonomous driving technologies are applied may refer to, for example, a robot having an autonomous driving function, or may refer to robot 1100a which interacts with autonomous vehicle 1100b. Robot 1100a having an autonomous driving function may collectively refer to devices that move by themselves along a given moving line without user control, or move by determining a moving line by themselves. Robot 1100a and autonomous vehicle 1100b, which have an autonomous driving function, may use a common sensing method in order to determine at least one of a movement route or a driving plan. For example, robot 1100a and autonomous vehicle 1100b, which have an autonomous driving function, may determine at least one of the movement route or the driving plan using information sensed by a lidar, a radar, and a camera. Robot 1100a, which interacts with autonomous vehicle 1100b, may be provided separately from autonomous vehicle 1100b so as to be connected to the autonomous driving function of autonomous vehicle 1100b inside or outside autonomous vehicle 1100b, or may perform an operation associated with a user who has got on autonomous vehicle 1100b. At this time, robot 1100a, which interacts with autonomous vehicle 1100b, may acquire sensor information instead of autonomous vehicle 1100b to provide the information to autonomous vehicle 1100b, or may acquire sensor information and generate surrounding environment information or object information to provide the information to autonomous vehicle 1100b, thereby controlling or assisting the autonomous driving function of autonomous vehicle 1100b. Alternatively, robot 1100a, which interacts with autonomous vehicle 1100b, may monitor the user who has got on autonomous vehicle 1100b or may control the functions of autonomous vehicle 1100b via interaction with the user. For example, when it is determined that a driver is in a drowsy state, robot 1100a may activate the autonomous driving function of autonomous vehicle 1100b or may assist the control of a drive unit of autonomous vehicle 1100b. Here, the functions of autonomous vehicle 1100b controlled by robot 1100a may include not only the autonomous driving function, but also a function provided in a navigation system or an audio system provided in autonomous vehicle 1100b. Alternatively, robot 1100a, which interacts with autonomous vehicle 1100b, may provide information to autonomous vehicle 1100b or assist the function thereof at the outside of autonomous vehicle 1100b. For example, robot 1100a may serve as a smart traffic light that provides traffic information including, for example, traffic signal information to autonomous vehicle 1100b, or may serve as an automatic electric charger of an electric vehicle that may interact with autonomous vehicle 1100b and may be automatically connected to a charge port of the vehicle. Robot 1100a may be realized into a guide robot, a transportation robot, a cleaning robot, a wearable robot, an entertainment robot, a pet robot, an unmanned flying robot, or a drone, for example, through the application of AI technologies and XR technologies. Robot 1100a, to which the XR technologies are applied, may refer to a robot which is a control or interaction target in an XR image. In this case, robot 1100a may be provided separately from XR deice 1100c and may operate in cooperation with XR device 1100c. When robot 1100a, which is a control or interaction target in an XR image, acquires sensor information from sensors including a camera, robot 1100a or XR device 1100c may generate an XR image based on the sensor information, and XR device 1100c may output the generated XR image. Then, such robot 1100a may operate based on a control signal input through XR device 1100c or via intersection with the user. For example, the user may check the XR image corresponding to the viewpoint of robot 1100a, which is remotely linked, via an external device such as XR device 1100c, and may adjust an autonomous driving route of robot 1100a or control an operation or driving thereof via interaction with the robot, or may check information on an object around thereof. Autonomous vehicle 1100b may be realized into a mobile robot, a vehicle, or an unmanned air vehicle, for example, through the application of the AI technologies and the XR technologies. Autonomous vehicle 1100b, to which the XR technologies are applied, may refer to an autonomous vehicle having an XR image providing device, or may refer to an autonomous vehicle as a control or interaction target in an XR image, for example. Particularly, autonomous vehicle 1100b as a control or interaction target in an XR image may be provided separately from XR device 1100c and may operate in cooperation with XR device 1100c. Autonomous vehicle 1100b having the XR image providing device may acquire sensor information from sensors including a camera, and may output an XR image generated based on the acquired sensor information. For example, autonomous vehicle 1100b may include an HUD to output an XR image, thereby providing an occupant with an XR object corresponding to a real object or an object in the screen. At this time, when the XR object is output to the HUD, at least a portion of the XR object may be output so as to overlap with a real object to which the passenger's gaze is directed. On the other hand, when the XR object is output to a display provided in autonomous vehicle 1100b, at least a portion of the XR object may be output so as to overlap with an object in the screen. For example, autonomous vehicle 1100b may output XR objects corresponding to objects such as a lane, another vehicle, a traffic light, a traffic sign, a two-wheeled vehicle, a pedestrian, and a building. When autonomous vehicle 1100b as a control or interaction target in an XR image acquires sensor information from sensors including a camera, autonomous vehicle 1100b or XR device 1100c may generate an XR image based on the sensor information, and XR device 1100c may output the generated XR image. Then, autonomous vehicle 1100b may operate based on a control signal input through an external device such as XR device 1100c or via interaction with the user. The above-described method for the entry of a mobile robot to a moving walkway according to the present disclosure may be provided as a program to be executed in a computer and may be recorded on a computer readable recording medium. The method for the entry of a mobile robot to a moving walkway according to the present disclosure may be executed via software. When executed via software, the constituent elements of the present disclosure are code segments that execute required operations. The program or the code segments may be stored in a processor readable medium. The computer readable recording medium includes all kinds of recording devices in which data is stored in a computer readable manner. Examples of the computer readable recording device include a ROM, a RAM, a CD-ROM, a DVD-ROM, a DVD-RAM, a magnetic tape, a floppy disc, a hard disc, and an optical data storage device. In addition, the computer readable recording medium may be distributed in a computer device connected thereto via a network so that a computer readable code may be stored and executed in a distribution manner. From the foregoing, it will be appreciated that various embodiments of the present disclosure have been described herein for purposes of illustration, and that various modifications may be made without departing from the scope and spirit of the present disclosure. Accordingly, the various embodiments disclosed herein are not intended to be limiting, with the true scope and spirit being indicated by the following claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/28/693/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9787126779556274,
                    "F_SPEC_PARAMS": [
                        "moving while carrying out various operations"
                    ],
                    "S_SPEC_PARAMS": [
                        "difficulty in using various transport devices"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Accordingly, a mobile robot capable of moving while carrying out various operations has been widely utilized not only at home but also in places where a large number of people gather to individually perform various tasks or get guide services for example, an airport, a building lounge, and locations for events."
                    ],
                    "S_SENTS": [
                        "However, since the mobile robot may have difficulty in using various transport devices for example, a moving walkway and an escalator available to the user according to the driving characteristics thereof, path guidance in consideration of positions of these transport devices may be difficult."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Speed"
                    ],
                    "F_SIM_SCORE": 0.46970829367637634,
                    "S_TRIZ_PARAMS": [
                        "Convenience of Use"
                    ],
                    "S_SIM_SCORE": 0.5429450273513794,
                    "GLOBAL_SCORE": 1.6850393384695053
                },
                "sort": [
                    1.6850393
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11221628-20220111",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11221628-20220111",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-08-14",
                    "PUBLICATION_DATE": "2022-01-11",
                    "INVENTORS": [
                        "Jonas Holgersson",
                        "Mattias Kamfors",
                        "Stefan Bergstrm"
                    ],
                    "APPLICANTS": [
                        "HUSQVARNA AB    ( Huskvarna , SE )"
                    ],
                    "INVENTION_TITLE": "Work area marking for a robotic working tool",
                    "DOMAIN": "G05D 10265",
                    "ABSTRACT": "A robotic work tool 100 comprising a controller 110 and at least one magnetic sensor 170 arranged to sense a magnetic boundary signal emitted by a boundary wire, and a first magnetic guide signal emitted by a first guide wire 261, wherein the controller 110 is configured to: detect an at least partial crossing of the first guide wire 261 from a first work zone to a second work zone, determine an operating status and if the operating status indicates that a crossing is allowed, allow the robotic work tool to cross the first guide wire 261 to the second work zone, and if not, control the operation of the robotic work tool so that the first guide wire 261 is not crossed.",
                    "CLAIMS": "1. A robotic work tool comprising: a controller; and at least one magnetic sensor arranged to sense a magnetic boundary signal emitted by a boundary wire, and a first magnetic guide signal emitted by a first guide wire, wherein the boundary wire is arranged to demarcate a work area, and the first guide wire is arranged to at least partially demarcate at least a first work zone and a second work zone, wherein the first work zone and the second work zone are both comprised in the work area, wherein the controller is configured to: detect an at least partial crossing of the first guide wire from the first work zone to the second work zone; determine an operating status, the operating status indicating whether a crossing of the first guide wire to enter the second work zone is allowed, and if the operating status indicates that the crossing is allowed, allow the robotic work tool to cross the first guide wire to the second work zone, and if not, control operation of the robotic work tool so that the first guide wire is not crossed; wherein the at least one magnetic sensor is further arranged to sense a second magnetic guide signal emitted by a second guide wire and the controller is further configured to control the operation of the robotic work tool to: follow the second guide wire to search for the first guide wire, follow the second guide wire during a crossing of the first guide wire, stop following the second guide wire as the first guide wire has been crossed, or only allow a crossing of the first guide wire when the robotic work tool is following the second guide wire. 2. The robotic work tool according to claim 1, wherein the controller is configured to control the operation of the robotic work tool so that the first guide wire is not crossed by following the first guide wire at least partially around the second work zone, whereby the controller controls the robotic work tool to stop following the first guide wire. 3. The robotic work tool according to claim 2, wherein the controller is configured to control the operation of the robotic work tool so that the first guide wire is not crossed by following the first guide wire at least partially around the second work zone until the second guide wire is detected, whereby the controller controls the robotic work tool to again follow the second guide wire away from the second work zone. 4. The robotic work tool according to claim 2, wherein the controller is configured to control the operation of the robotic work tool so that the first guide wire is not crossed by following the first guide wire at least partially around the second work zone until the second guide wire is detected, whereby the controller controls the robotic work tool to again follow the second guide wire into the second work zone. 5. The robotic work tool according to claim 1, wherein the controller is further configured to control the operation of the robotic work tool to follow the boundary wire to search for the first guide wire. 6. The robotic work tool according to claim 1, wherein the controller is further configured to control the operation of the robotic work tool to navigate according to navigation system data, to search for the first guide wire. 7. The robotic work tool according to claim 1, wherein the operating status is based on a categorization of the second work zone, wherein the operating status is based on a time spent in the first work zone, wherein the operating status is based on an operating time, or wherein the operating status is based on a time of day. 8. The robotic work tool according to claim 1, wherein the first magnetic guide signal corresponds to a current signal being propagated through the first guide wire, the first magnetic guide signal being generated by the current signal as the current signal propagates through the boundary wire. 9. The robotic work tool according to claim 1, wherein the controller is configured to detect the first guide wire by detecting a crossing of the first guide wire, or wherein the controller is configured to detect the first guide wire by detecting that the corresponding received first magnetic guide signal exceeds a threshold value. 10. The robotic work tool according to claim 1, wherein the robotic work tool comprises at least two magnetic sensors and wherein the controller is configured to follow the second guide wire by controlling the operation of the robotic work tool so that a first of the at least two magnetic sensors detects a magnetic signal associated with the second guide wire being followed at a first polarity and a second of the at least two magnetic sensors detects the magnetic signal associated with the second guide wire being followed at a second polarity. 11. The robotic work tool according to claim 1, wherein the controller is configured to follow the first guide wire by controlling the operation of the robotic work tool so that a first of the at least one magnetic sensors detects a magnetic signal associated with the first guide wire being followed at a maintained first amplitude. 12. The robotic work tool according to claim 1, wherein the controller is configured to detect an at least partial crossing of the second guide wire from the first work zone to the second work zone, by detecting a change in polarity that a first magnetic sensor of the at least one magnetic sensor detects a magnetic signal associated with the second guide wire while being followed. 13. A robotic work tool system comprising: a robotic work tool according to claim 1, the boundary wire; and the first guide wire, wherein the first guide wire is arranged to be laid in so that the first guide wire partially follows the boundary wire, extending at least partially into a work area bounded by the boundary wire when not following the boundary wire so that the first work zone is formed on one side of the first guide wire and the second work zone is formed on another side of the first guide wire. 14. A robotic work tool comprising: a controller; and at least one magnetic sensor arranged to sense a magnetic boundary signal emitted by a boundary wire, and a first magnetic guide signal emitted by a first guide wire, wherein the boundary wire is arranged to demarcate a work area and the first guide wire is arranged to at least partially demarcate at least a first work zone and a second work zone, wherein the first work zone and the second work zone are both comprised in the work area, wherein the controller is configured to: detect an at least partial crossing of the first guide wire from the first work zone to the second work zone; determine an operating status, the operating status indicating whether a crossing of the first guide wire to enter the second work zone is allowed, and if the operating status indicates that the crossing is allowed, allow the robotic work tool to cross the first guide wire to the second work zone, and if not, control operation of the robotic work tool so that the first guide wire is not crossed; wherein the controller is configured to control the operation of the robotic work tool so that the first guide wire is not crossed by returning the robotic work tool to the first work zone. 15. A method for use in a robotic work tool comprising at least one magnetic sensor arranged to sense a magnetic signal emitted by a boundary wire and a first magnetic signal emitted by a first guide wire and sense a second magnetic guide signal emitted by a second guide wire, wherein the boundary wire is arranged to demarcate a work area, and the guide wire is arranged to at least partially demarcate at least a first and a second work zone, wherein the first work zone and the second work zone are both comprised in the work area, wherein the method comprises: detecting an at least partial crossing of the first guide wire from the first work zone to the second work zone; determining an operating status, the operating status indicating whether a crossing of the first guide wire to enter the second work zone is allowed, and if the operating status indicates that the crossing is allowed, allowing the robotic work tool to cross the first guide wire to the second work zone, and if not, controlling operation of the robotic work tool so that the first guide wire is not crossed; and controlling operation of the robotic work tool to: follow the second guide wire to search for the first guide wire, follow the second guide wire during a crossing of the first guide wire, stop following the second guide wire as the first guide wire has been crossed, or only allow a crossing of the first guide wire when the robotic work tool is following the second guide wire. 16. A computer readable medium for carrying computer instructions that when loaded into a controller of the robotic working tool, causes the robotic working tool to operate according to the method according to claim 15.",
                    "FIELD_OF_INVENTION": "This application relates to robotic working tools and in particular to a robotic working tool, a robotic working tool system, a computer readable medium and a method for improved work area marking by a robotic working tool, such as a lawnmower.",
                    "STATE_OF_THE_ART": "Automated or robotic power tools such as robotic lawnmowers are becoming increasingly more popular. In a typical deployment, as is depicted in 1 showing a traditional robotic working tool system, a robotic working tool 100 is set to operate in a work area 205 following its internal navigation routine. Traditionally the perimeter of this work area 205 is marked by a boundary wire 250 through which an electrical signal 245 is transmitted. As the signal travels through the boundary wire 250 it will generate a magnetic field around the boundary wire 250, which the robotic working tool 100 may detect using a magnetic field sensor, such as a coil with a magnetic core. In such an arrangement the robotic working tool 100 may be kept within the work area 205 by being programmed to turn as the magnetic field caused by the signal 245 is detected, either to exceed a threshold indicating a closeness to the boundary wire or to change polarity indicating a cable crossing or a combination of the two. Such a system works satisfactorily when only one work area is to be marked, but has one weakness in that if the boundary wire is laid out so that it encompasses a partial area or zone 205 which is only accessible by a narrow corridor 205. To enable the robotic work tool 100 to find its way into the work zone 205, prior art systems teach to use a guide wire 260 which the robotic work tool 100 can follow to enter or exit through the channel 205. To enable the robotic work tool to detect the guide cable and differentiate it from the boundary wire, a guide control signal 246 may be transmitted through the guide cable 260, where the guide control cable 246 is different from the control signal 245. As noted above, such systems function satisfactorily when only one work area is to be defined, but if a work area is to be divided into different zonesas the inventors have realizedthe system suffers and requires that several boundary wires are used, one for each zone. One example system is that of US 2005/0230166 A1. Furthermore, several guide cables may be necessary to enable the robotic work tool to enter and/or exit the zone if it has a narrow opening. The solution that is provided for by the prior art is to use supplemental navigation means, such as Global Positioning Systems or rely on deduced reckoning and navigate according to a map. However, such systems are costly and require that a detailed map is provided. Furthermore, they do not function in all areas as they require a good satellite reception, which may not be available in areas with a great deal of foliage. There is thus a need for a robotic working tool system that is able to provide for a plurality of working zones without requiring a multitude of wires or supplemental navigation aids.",
                    "SUMMARY": [
                        "It is therefore an object of the teachings of this application to overcome or at least reduce those problems and problems discussed above and below by providing a robotic work tool comprising a controller and at least one magnetic sensor arranged to sense a magnetic boundary signal emitted by a boundary wire, and a first magnetic guide signal emitted by a first guide wire, wherein the boundary wire is arranged to demarcate a work area and the guide wire is arranged to at least partially demarcate at least a first and a second work zone, wherein the first work zone and the second work zone are both comprised in the work area, wherein the controller is configured to: detect an at least partial crossing of the first guide wire from a first work zone to a second work zone, determine an operating status, the operating status indicating whether the first guide wire may be crossed to enter the second work zone, and if the operating status indicates that a crossing is allowed, allow the robotic work tool to cross the first guide wire to the second work zone, and if not, control the operation of the robotic work tool so that the first guide wire is not crossed. This enables the forming of different work zones simply by using a guide wire that is laid out in a clever manner. This is a very simple solution to a long standing problem, that in some embodiments require only a minimum of modification to the contemporary robotic working tools. The inventors have thus provided a highly simple and elegant solution to a long-standing problem as per the teachings herein by providing a robotic working tool system comprising a boundary wire and a first guide wire, where the first guide wire is arranged to be laid in so that it partially follows the boundary wire, extending at least partially into a work area bounded by the boundary wire when not following the boundary wire so that a first work zone is formed on one side of the first guide wire and a second work zone is formed on another side of the first guide wire. It is also an object of the teachings of this application to overcome the problems by providing a method for use in a robotic work tool comprising at least one magnetic sensor arranged to sense a magnetic signal emitted by a boundary wire and a first magnetic signal emitted by first guide wire, wherein the boundary wire is arranged to demarcate a work area and the guide wire is arranged to at least partially demarcate at least a first and a second work zone, wherein the first work zone and the second work zone are both comprised in the work area, wherein the method comprises: detecting an at least partial crossing of the first guide wire from a first work zone to a second work zone, determining an operating status, the operating status indicating whether the first guide wire may be crossed to enter the second work zone, and if the operating status indicates that a crossing is allowed, allowing the robotic work tool to cross the first guide wire to the second work zone, and if not, controlling the operation of the robotic work tool so that the first guide wire is not crossed. It is also an object of the teachings of this application to overcome the problems by providing a computer readable medium for carrying computer instructions that when loaded into a controller of a robotic working tool or robotic working tool system, causes the robotic working tool or robotic working tool system to operate according to a method as above and herein. Other features and advantages of the disclosed embodiments will appear from the following detailed disclosure, from the attached dependent claims as well as from the drawings. Generally, all terms used in the claims are to be interpreted according to their ordinary meaning in the technical field, unless explicitly defined otherwise herein. All references to a/an/the [element, device, component, means, step, etc] are to be interpreted openly as referring to at least one instance of the element, device, component, means, step, etc. , unless explicitly stated otherwise. The steps of any method disclosed herein do not have to be performed in the exact order disclosed, unless explicitly stated.",
                        "The invention will be described in further detail under reference to the accompanying drawings in which: 1 shows an example of a robotic lawnmower system according to the prior art; 2A shows an example of a robotic lawnmower according to an embodiment of the teachings herein; 2B shows a schematic view of the components of an example of a robotic lawnmower according to an embodiment of the teachings herein; 3A and 3B each shows a schematic overview of a robotic lawnmower system according to an embodiment of the teachings herein; 4 shows a schematic view of a computer-readable medium carrying computer instructions according to an example embodiment of the teachings herein; and 5 shows a flowchart for a method for a robotic working tool according to an example embodiment of the teachings herein."
                    ],
                    "DESCRIPTION": "The disclosed embodiments will now be described more fully hereinafter with reference to the accompanying drawings, in which certain embodiments of the invention are shown. This invention may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided by way of example so that this disclosure will be thorough and complete, and will fully convey the scope of the invention to those skilled in the art. Like numbers refer to like elements throughout. It should be noted that all indications of rotational speeds, time durations, work loads, battery levels, operational levels etc. are given as examples and may be varied in many different ways as would be apparent to a skilled person. The variations may be for individual entities as well as for groups of entities and may be absolute or relative. Returning to 1 showing a schematic view of a robotic working tool system 200. The schematic view is not to scale. Although 1 is aimed to show an example of prior art systems, many of the components of such a system are common to a system of the teachings according herein and the differences will be detailed with reference to 2A, 2B and 3. As stated in the background section, the robotic working tool system 200 comprises a charging station 210 and a boundary wire or cable 250 arranged to enclose a work area 205, in which the robotic lawnmower 100 is supposed to operate. The robotic working tool 100 is exemplified by a robotic lawnmower, but the teachings herein may also be applied to other robotic working tools adapted to operate within a work area. The charging station 210 has a charger 220, in this embodiment coupled to two charging plates. The charging plates are arranged to co-operate with corresponding charging plates of the robotic lawnmower 100 for charging a battery referenced 180 in 2B of the robotic lawnmower 100. The charging station 210 also has, or may be coupled to, a signal generator 240 for providing a control signal 245 to be transmitted through the boundary wire 250. The signal generator 240 thus comprises a controller for generating the control signal. In one embodiment the control signal 245 comprises an alternating current, such as a continuously or regularly repeated current signal. The control signal may be a CDMA signal CDMACode Division Multiple Access. The control signal may also or alternatively be a pulsed control signal, the control signal thus comprising one or more current pulses being transmitted periodically. The control signal 245 may also or alternatively be a continuous sinusoidal wave. As is known in the art, the current signal will generate a magnetic field around the boundary wire 250 which sensors referenced 170 in 2B of the robotic lawnmower 100 will detect. For the context of this application detecting a signal will include, but not be limited to, detecting the magnetic field generated by the signal as it travels through a wire by receiving the voltages generated by the sensor as it is exposed to the magnetic field and processing these voltages in order to identify them as having been generated by the signal. For example a pulsed signal will give rise to a series of voltage flanks that may be identified as corresponding to the pulsed signal, by the flanks relative time distances. As the robotic lawnmower 100 or more accurately, the sensor referenced 170 in 2B crosses the boundary wire 250 the direction of the magnetic field will change, the polarity will change. The robotic lawnmower 100 will thus be able to determine that the boundary wire has been crossed, and take appropriate action by controlling the driving of the rear wheels 130 to cause the robotic lawnmower 100 to turn a certain angular amount and return into the work area 205, or alternatively, reverse into the work area and then turn. The robotic work tool 100 may also be arranged to determine that a crossing is eminent by determining that the received magnetic field is received at an amplitude exceeding a threshold value. For its operation within the work area 205, in the embodiment of 1, the robotic lawnmower 100 may alternatively or additionally use a satellite navigation device, possibly supported by a deduced reckoning navigation sensor to navigate the work area 205. The robotic working tool system 200 also comprises at least one further cable or wire, namely a guide wire 260 for enabling the robotic working tool 100 to find the charging station more quickly than having to randomly find it or simply follow the boundary wire 250. To enable the robotic working tool 100 to differentiate the additional wires 260 from the boundary wire, a different signal 246 may be transmitted through the additional wires 260. 2A shows a perspective view of a robotic working tool 100, here exemplified by a robotic lawnmower 100, having a body comprising a cover 132 and a chassis 140 and a plurality of wheels 130 only one shown. As can be seen, the robotic lawnmower 100 may comprise charging skids for contacting contact plates when docking into a charging station not shown in 2A, but referenced 210 in 1 and 3 for receiving a charging current through, and possibly also for transferring information by means of electrical communication between the charging station and the robotic lawnmower 100. 2B shows a schematic overview of the robotic working tool 100, also exemplified here by a robotic lawnmower 100, having a chassis 140 and a plurality of wheels 130. It should be noted that even though the description given herein will be focused on robotic lawnmowers, the teachings herein may also be applied to robotic cleaners such as robotic vacuum cleaners and/or robotic floor cleaners, robotic ball collectors, robotic mine sweepers, robotic farming equipment, or other robotic working tools to be employed in a work area defined by a boundary wire. In the exemplary embodiment of 2B the robotic lawnmower 100 has 4 wheels 130, two front wheels 130 and the rear wheels 130. At least some of the wheels 130 are drivably connected to at least one electric motor 150. It should be noted that even if the description herein is focused on electric motors, combustion engines may alternatively be used possibly in combination with an electric motor. In the example of 2B, each of the rear wheels 130 is connected to a respective electric motor 150. This allows for driving the rear wheels 130 independently of one another which, for example, enables steep turning. The robotic lawnmower 100 also comprises a controller 110. The controller 110 may be implemented using instructions that enable hardware functionality, for example, by using executable computer program instructions in a general-purpose or special-purpose processor that may be stored on a computer readable storage medium disk, memory etc 120 to be executed by such a processor. The controller 110 is configured to read instructions from the memory 120 and execute these instructions to control the operation of the robotic lawnmower 100 including, but not being limited to, the propulsion of the robotic lawnmower. The controller 110 may be implemented using any suitable, publically available processor or Programmable Logic Circuit PLC. The memory 120 may be implemented using any commonly known technology for computer-readable memories such as ROM, RAM, SRAM, DRAM, FLASH, DDR, SDRAM or some other memory technology. The robotic lawnmower 100 further has at least one sensor 170; in the example of 2B there are four sensors divided into a first sensor pair 170 and a second sensor pair 170, respectively arranged at each wheel 130, 130 to detect a magnetic field not shown and for detecting a boundary wire and/or for receiving and possibly also sending information from a signal generator 240. The sensors 170 may thus be arranged as front sensors 170 and rear sensors 170. In some embodiments, the sensors 170 may be connected to the controller 110, and the controller 110 may be configured to process and evaluate any signals received from the sensor pairs 170, 170. The sensor signals may be caused by the magnetic field being generated by a control signal being transmitted through a boundary wire. This enables the controller 110 to determine whether the robotic lawnmower 100 is close to or crossing a boundary wire 250, or inside or outside an area enclosed by the boundary wire 250. This also enables the robotic lawnmower 100 to receive, and possibly send, information from/to the control signal 245. The robotic lawnmower 100 also comprises a grass cutting device 160, such as a rotating blade 160 driven by a cutter motor 165. The grass cutting device being an example of a work tool 160 for a robotic working tool 100. The cutter motor 165 is connected to the controller 110 which enables the controller 110 to control the operation of the cutter motor 165. The controller may also be configured to determine the load exerted on the rotating blade, by for example measure the power delivered to the cutter motor 165 or by measuring the axle torque exerted by the rotating blade. The robotic lawnmower 100 also has at least one battery 180 for providing power to the motors 150 and the cutter motor 165. 3A shows a schematic view of a robotic working tool system 200 in one embodiment according to the teachings herein. It should be noted that the system according to the teachings include many of the components of the prior art system and for clarity's sake, some components will not be shown in 3A, such as the charger 220. The schematic view is not to scale. As in the prior art, the robotic working tool system 200 comprises a charging station 210 and a boundary wire 250 arranged to enclose a work area 205, in which the robotic lawnmower 100 is supposed to operate. However, the inventors have realised by inventive and insightful reasoning, that the work area 205 may be divided into a clever manner into several zones by utilizing two guide wires, a first guide wire 261 and a second guide wire 262. Each guide wire 261, 262 is provided with a guide control signal of its own 246 and 247 respectively. To enable the robotic work tool 100 to differentiate between the control signals, they all differ from one another, as is also illustrated in 3A. The first guide wire 261 is utilized as a second boundary wire to delimit at least one zone. In one embodiment the first guide wire 261 is laid out in a manner where it follows borders of the work area 205, such as walls of houses for example or the boundary wire 250. By laying the first guide wire 261 in a loop and making sure that the exit follows the entry to the charging station, i. e. the first guide wire is laid adjacent itself when entering and exiting the charging station, the signal in the first guide wire will cancel itself and the first guide wire will not be detected at such transitions. Alternatively, the first guide wire may exit and enter the charging station along the boundary wire 250, whereby it will also not affect the operation of the robotic work tool. By laying the first guide wire so that it alternates between following the boundary wire 250 and obstacles such as the house, the first guide wire effectively delimits zones of the work area 205. In the example embodiment of 3A, three additional zones are delimited 205, 205 and 205 respectively, providing a total of three operating or work zones, or four if one counts the remainder of the work area 205 as one zone. The work area 205 thus comprises one or more work zone, wherein each work zone is bounded by the boundary wire and at least one guide cable, thereby utilizing the guide cables to also act as boundary wires, at least on a partial section of the guide cable. To enable the robotic work tool 100 to find its way into and out of a work zone 205, 205, 205, the second guide wire 262 may be used. In such an embodiment, the second guide wire 262 is laid so that it at least crosses the first guide wire, preferably at a positions where it is easy or at least possible for the robotic work tool 100 to enter/exit the zone. In the example of 3A, the second guide wire is laid so that it enters and exits all three work zones 205, 205 and 205. The second guide wire 262 may also be used by the robotic work tool 100 to find the first guide wire 261. It should be noted that the use of the second guide wire 262 is optional, and the definition of multiple work zones using only one wire is made possible by the clever laying of the first guide wire 261. As the first guide wire 261 is laid along a perimeter or boundary of the work area 205, apart from when it transitions from an obstacle boundary to the boundary wire or vice versa, the robotic work tool 100 will not be able to escape the work are 205 by simply crossing the first guide wire 261 as the boundary wire 250 or an obstacle boundary such as a house will prevent the robotic work tool 100 from escaping the work area 205. Viewing the example of 3A, the first guide wire 261, acting as a second boundary wire, is laid along one wall of the house. At a corner of the house, the first guide wire 261 is laid so that it transitions or crosses the work area 205 from the house to the boundary wire 250. The first guide wire 261 is laid so that it follows the boundary wire 250 for a distance, after which the first guide wire 261 again transitions the work area 205 back to the house. A first work zone 205 has thereby been defined. Similarly a second work zone 205 and a third work zone 205 are also defined. As the robotic work tool 100 encounters or detects that it is about to cross the first guide wire 261, it is configured to determine a current operating status. The operating status indicates the current objective of the robotic work tool 100 with regards to the first guide wire, namely whether the guide wire is used as a guide wire or to demarcate a work zone, and also to indicate whether a switch from one work zone to another should be made. The operating status may indicate that the first guide wire may be crossed, to enable entry into or out of a work zone. In such case, the robotic work tool 100 will cross the first guide wire 261 and enter or exit the corresponding work zone. The operating status may indicate that the first guide wire may be crossed only when following the second guide wire, enabling for a controlled entry/exit of a work zone. If the operating status indicates that the first guide wire may be crossed, and in an instance where the robotic work tool is following the second guide wire, the robotic work tool may, in one embodiment, be configured to stop following the second guide wire as the first guide wire has been crossed. The operating status may indicate that the first guide wire may not be crossed. In such case, the robotic work tool 100 will not cross the first guide wire 261 and remain within the current work zone. The operating status may indicate that a work zone should be bypassed. In such case, the robotic work tool 100 will follow the first guide wire 261, possibly until it again reaches the second guide wire 262 whereby the robotic work tool will remove itself from the first guide wire 261, i. e. stop following the first guide wire 261. A work zone may thus be bypassed. In one such embodiment, the robotic work tool removes itself from the first guide wire 261 by following the second guide wire 262, at least for a while, possibly approximately 0. 3, 0. 5, 0. 7 or 1 meter or 1, 2, 3, 4, or 5 seconds away from the first guide wire and the work zone. The robotic work tool may also or alternatively be configured to follow the first guide wire until it again reaches the second guide wire 262 whereby the robotic work tool will remove itself from the first guide wire 261, i. e. stop following the first guide wire 261, but to enter the work zone along the second guide wire. A work zone may thus be entered in a controlled manner. In one embodiment the operating status is based on a categorization of a work zone. The corresponding work zone may be identified through a counter counting the order of encountered work zones when following the second guide wire. The corresponding work zone may alternatively or additionally be identified through use of supplemental navigation means. The categorization may indicate whether a zone should be entered or not or whether it should be simply by passed. This allows for defining no go zones, keep out zones and zones that should simply be bypassed. In one embodiment the operating status is based on a time spent in the current work zone. This enables the robotic work tool to remain within a work zone for at least a minimum time to ensure that the zone is operated on sufficiently. In one embodiment the operating status is based on an operating time. This enables a timed control of the operation of the robotic work tool, in that a zone is only entered after sufficient time has been spent outside it. In one embodiment the operating status is based on a time of day. This enables a scheduling of the robotic work tool's work. It should be noted that the first guide wire 261 may alternatively or additionally be laid so that it crosses from one side portion of the boundary wire 250 to another portion of the boundary cable. An example is shown in 3B, where a plurality of work zones 205 have been defined using a single guide wire 262. Returning to the use of a second guide wire for finding the work zones, if the first and second guide wires are laid so that they cross each other and a crossing of the first guide wire is only allowed along the second guide wire a more precise control of the robotic work tool's crossing from one zone to another may be achieved. In one embodiment, the robotic work tool may be configured to follow the boundary wire to find a work zone. In such an embodiment, the controller 110 of the robotic work tool 100 is configured to control the operation of the robotic work tool to follow the boundary wire 250 to search for the first guide wire 261. In one embodiment, the robotic work tool may be configured to receive a position through a navigation system, such as a global positioning system, and navigate according to such position data to search for the first guide wire 261. In such an embodiment, the controller 110 of the robotic work tool 100 is configured to control the operation of the robotic work tool to navigate according to navigation system data, such as GPS data, to search for the first guide wire 261. In such an embodiment, a GPS sensor is comprised in the robotic work tool. Possibly as part of a navigation sensor 190 which may also or alternatively comprise deduced reckoning means, such as odometers and/or a compass. In such an embodiment, the gist of the invention is still utilized, namely to enable different work zones to be formed simply by using an extra guide wire. It should be noted that as the robotic working tool 100 already includes all or at least most hardware components necessary to detect and operate according to multiple control signals, a simple software update may be sufficient to adapt an existing robotic working tool 100 to operate according to the teachings herein. Such software update may be provided by loading a set of computer instructions into the controller of the robotic working tool 100. Such computer instructions may be carried by a computer readable medium as shown in 4, which shows a schematic view of a computer-readable product 10 according to one embodiment of the teachings herein. The computer-readable product is configured to carry or store a computer program or computer program instructions 11 along with application related data. The computer-readable product 10 may be a data disc as in 4 or a Universal Serial Bus, a memory card or other commonly known computer readable products, these being examples of transitory mediums. The computer-readable product 10 may be inserted or plugged in or otherwise connected to a computer-readable product reader 12 configured to read the information, such as the program instructions 11 stored on the computer-readable product 12 and possibly execute the instructions or to connect to a device configured to execute the instructions such as a robotic working tool 100, as the one disclosed in 2A and 2B. The robotic working tool 100 may thus connect wirelessly or through a wired connection to a computer-readable product reader 12 this being an example of a non-transitory medium to receive the computer instructions 11. The robotic working tool 100 may in one embodiment comprise the computer-readable product reader 12 to receive the computer instructions 11. When loaded into and executed by a controller the computer instructions may cause the robotic working tool 100 to operate according to a method as shown in 5 which shows a flowchart for a general method for a robotic working tool according to hereinThe robotic working tool 100 is thus configured to perform a method comprising detecting 510 a first guide wire 261, and then determining 520 an operating status and if the operating status indicates that a crossing is allowed, allowing the robotic work tool to cross 530 the first guide wire 261 to the second work zone, and if not, controlling 540 the operation of the robotic work tool so that the first guide wire 261 is not crossed. The invention has mainly been described above with reference to a few embodiments. However, as is readily appreciated by a person skilled in the art, other embodiments than the ones disclosed above are equally possible within the scope of the invention, as defined by the appended patent claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/28/216/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9960545301437378,
                    "F_SPEC_PARAMS": [
                        "works satisfactorily"
                    ],
                    "S_SPEC_PARAMS": [
                        "function in all areas",
                        "good satellite reception,",
                        "costly",
                        "require that a detailed map is provided"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Such a system works satisfactorily when only one work area is to be marked, but has one weakness in that if the boundary wire is laid out so that it encompasses a partial area or zone 205 which is only accessible by a narrow corridor 205."
                    ],
                    "S_SENTS": [
                        "Furthermore, they do not function in all areas as they require a good satellite reception, which may not be available in areas with a great deal of foliage.",
                        "However, such systems are costly and require that a detailed map is provided.",
                        "Furthermore, several guide cables may be necessary to enable the robotic work tool to enter and/or exit the zone if it has a narrow opening.",
                        "The solution that is provided for by the prior art is to use supplemental navigation means, such as Global Positioning Systems or rely on deduced reckoning and navigate according to a map."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "F_SIM_SCORE": 0.4886288642883301,
                    "S_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.48652276396751404,
                    "GLOBAL_SCORE": 1.6836303442716598
                },
                "sort": [
                    1.6836303
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10905513-20210202",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10905513-20210202",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2017-12-19",
                    "PUBLICATION_DATE": "2021-02-02",
                    "INVENTORS": [
                        "Charles J. Scheib",
                        "Matthew Colin Vargas",
                        "Andrea Bajo",
                        "Jaime Hernandez",
                        "Koray Sahin"
                    ],
                    "APPLICANTS": [
                        "Verb Surgical Inc.    ( Mountain View , US )"
                    ],
                    "INVENTION_TITLE": "Sensors for detecting sterile adapter and tool attachment for use in a robotic surgical system",
                    "DOMAIN": "A61B 4610",
                    "ABSTRACT": "Generally, a system for use in a robotic surgical system may be used to determine an attachment state between a tool driver, sterile adapter, and surgical tool of the system. The system may include sensors used to generate attachment data corresponding to the attachment state. The attachment state may be used to control operation of the tool driver and surgical tool. In some variations, one or more of the attachment states may be visually output to an operator using one or more of the tool driver, sterile adapter, and surgical tool. In some variations, the tool driver and surgical tool may include electronic communication devices configured to be in close proximity when the surgical tool is attached to the sterile adapter and tool driver.",
                    "CLAIMS": "1. A tool driver for use in a robotic surgical system, the tool driver comprising: a housing configured to couple to a sterile adapter, the housing comprising a first side defining a distal end of the housing and having a sterile adapter engagement feature mateable with a corresponding tool driver engagement feature on the sterile adapter, and a sterile adapter sensor configured to generate a sensor signal when the tool driver engagement feature is mated with its corresponding sterile adapter engagement feature; and at least one rotatable output drive supported by a second side of the housing and configured to communicate torque to an input drive of a surgical tool through the sterile adapter, and wherein the second side is perpendicular to the first side such that the at least one rotatable output drive is arranged perpendicular to the first side having the tool driver engagement feature and the sterile adapter sensor. 2. The tool driver of claim 1, wherein the sterile adapter engagement feature comprises one or more of a recess and a projection. 3. The tool driver of claim 1, wherein the sterile adapter sensor is configured to generate the sensor signal when the tool driver engagement feature contacts the sterile adapter sensor. 4. A robotic surgical system comprising: a tool driver comprising: a first housing configured to attach to a surgical tool via a sterile adapter, the first housing comprising at least one projection extending from a surface of the first housing and configured to bias away from the surface, the projection comprising a surgical tool sensor configured to generate a sensor signal comprising at least one attachment state between the tool driver and the surgical tool, and the surgical tool sensor comprises a proximity sensor comprising a magnet coupled to a first end of the projection facing the surgical tool and a magnetic field transducer coupled to a second end of the projection; and at least one rotatable output drive supported by the first housing and configured to communicate torque to an input drive of the surgical tool through the sterile adapter. 5. The system of claim 4, wherein the projection is disposed between a pair of the rotatable output drives. 6. The system of claim 4, wherein the first housing comprises a plurality of the projections arranged in a bilaterally symmetrical arrangement. 7. The system of claim 4, wherein the projection comprises a compliant material. 8. The system of claim 4, wherein the projection comprises at least one of a coil spring and a leaf spring. 9. The system of claim 4, further comprising a second surgical tool sensor disposed at a distal end of the first housing. 10. The system of claim 9, wherein the second surgical tool sensor comprises a proximity sensor comprising a magnetic field transducer. 11. The system of claim 4, further comprising: a surgical tool comprising: a second housing configured to attach to the sterile adapter, the second housing comprising a sterile adapter engagement feature comprising a magnetic projection; at least one input drive supported by the second housing and configured to receive the torque communicated from an output drive of the tool driver through the sterile adapter; and an end effector extending from the second housing and operatively coupled to the input drive. 12. The system of claim 11, wherein the magnetic projection comprises a first tapered surface and a second tapered surface opposite the first tapered surface. 13. The system of claim 12, wherein a distal end of the surgical tool comprises the sterile adapter engagement feature. 14. The system of claim 4, wherein the surgical tool sensor generates a sensor signal corresponding to a plurality of attachment states between the tool driver and the surgical tool, and the plurality of attachment states comprise partial attachment, full attachment, partial detachment, full detachment, and improper attachment. 15. The system of claim 4, wherein the first housing comprises a plurality of the projections arranged in a bilaterally symmetrical arrangement.",
                    "FIELD_OF_INVENTION": "This invention relates generally to robotic surgical systems, including but not limited to sterile adapters for creating a sterile barrier around portions of a robotic surgical system.",
                    "STATE_OF_THE_ART": "Minimally-invasive surgery MIS, such as laparoscopic surgery, involves techniques intended to reduce tissue damage during a surgical procedure. For instance, laparoscopic procedures typically involve creating a number of small incisions in the patient , in the abdomen, and introducing one or more tools and at least one camera through the incisions into the patient. The surgical procedures are then performed using the introduced tools, with the visualization aid provided by the camera. Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery. MIS may be performed with non-robotic or robotic systems. Conventional robotic systems, which may include robotic arms for manipulating tools based on commands from an operator, may provide many benefits of MIS while reducing demands on the surgeon. Control of such robotic systems may require control inputs from a user , surgeon or other operator via one or more user interface devices that translate manipulations or commands from the user into control of the robotic system. For example, in response to user commands, a tool driver having one or more motors may actuate one or more degrees of freedom of a surgical tool when the surgical tool is positioned at the surgical site in the patient. Similar to traditional surgical procedures, it is important to maintain a sterile environment in the surgical field during robotic MI However, various components , motors, encoders, sensors, of the tool driver and other aspects of the robotic surgical system generally cannot practically be sterilized using conventional sterilization methods such as heat. One solution to maintain sterility is to provide a sterile barrier between the tool driver and other system components that may appear in the surgical field such as robotic arms, and the surgical tool, thereby providing a non-sterile side for the tool driver and a sterile side for the surgical tool. However, the sterile barrier generally should not interfere with how the tool driver actuates the surgical tool. Furthermore, as a tool driver may need to actuate different surgical tools throughout a surgical procedure, the sterile barrier may facilitate simple and efficient exchange or swapping of surgical tools on a tool driver, without compromising the sterile barrier. Proper engagement and attachment of a surgical tool to a tool driver may aid in forming a sterile barrier. Thus, it may be desirable to provide additional systems, devices, and method related to sterile adapters for use in robotic surgery.",
                    "SUMMARY": [
                        "Described herein are systems, devices, and methods for determining one or more attachment states between a tool driver, sterile adapter, and surgical tool for control of a robotic surgical system to aid in proper engagement of the sterile adapter to the system and formation of a sterile barrier. These systems and methods may also be used to communicate a state of the sterile barrier and operation of the robotic surgical system to help an operator and/or other users efficiently understand the attachment and engagement states of the system. Systems and methods as described herein may be used to guide an operator in performing sterile adapter and surgical tool engagement, instead of, for example, depending on the operator to manually confirm proper attachment among a sterile adapter, surgical tool, and tool driver. Generally, the systems and methods described herein for use in a robotic surgical system may use a tool driver configured to couple to a sterile adapter and a surgical tool. The tool driver may include at least one sterile adapter sensor and surgical tool sensor configured to generate at least one sensor signal corresponding to an attachment state , presence, engagement, attachment, disengagement, detachment, absence, between one or more of the tool driver, sterile adapter, and surgical tool. A controller may be coupled to the tool driver, and the controller may include a processor and a memory. In some variations, the controller may be configured to receive at least one sensor signal and generate attachment data using the sensor signal. The attachment data may include at least one attachment state between the tool driver, the sterile adapter, and a surgical tool. The tool driver may be controlled using the attachment data. One or more steps in forming the sterile barrier may be automatically performed by the system based on the attachment data generated from the sensor signals. These features may, for example, improve a surgical tool switching process and reduce operator error in forming a sterile barrier by ensuring that a proper attachment sequence is followed for engaging the sterile adapter and surgical tool to the tool driver. In some variations, the surgical tool may be actuated by the tool driver when complete attachment among the tool driver, the sterile adapter, and the surgical tool has been determined, or the surgical tool may be inhibited from actuation by the tool driver when one or more system components is not sensed and/or is improperly attached. In some variations, a robotic surgical system may include a tool driver comprising a first housing configured to attach to a surgical tool via a sterile adapter. The first housing may comprise at least one projection extending from a surface of the first housing and be configured to bias away from the surface. The projection may comprise at least one first surgical tool sensor configured to generate a sensor signal comprising at least one attachment state between the tool driver and the surgical tool. At least one rotatable output drive may be supported by the first housing and be configured to communicate torque to an input drive of the surgical tool through the sterile adapter. In some variations, the first surgical tool sensor may comprise a proximity sensor comprising a magnet coupled to a first end of the projection and a magnetic field transducer coupled to a second end of the projection. The projection may be disposed between a pair of the rotatable output drives. The first housing may comprise a plurality of the projections arranged in a bilaterally symmetrical arrangement. The projection may comprise a compliant material. The projection may comprise at least one of a coil spring and a leaf spring. A second surgical tool sensor may be disposed at a distal end of the first housing. In some of these variations, the second surgical tool sensor may comprise a proximity sensor comprising a magnetic field transducer. In some variations, a surgical tool may comprise a second housing configured to attach to the sterile adapter. The second housing may comprise a sterile adapter engagement feature comprising a magnetic projection. At least one input drive may be supported by the second housing and configured to receive the torque communicated from an output drive of the tool driver through the sterile adapter. An end effector may extend from the second housing and be operatively coupled to the input drive. In some of these variations, the magnetic projection may comprise a first tapered surface and a second tapered surface opposite the first tapered surface. In some of these variations, a distal end of the surgical tool may comprise the sterile adapter engagement feature. In some variations, a tool driver for use in a robotic surgical system may comprise a housing configured to couple to a sterile adapter. The housing may comprise a sterile adapter engagement feature mateable with a corresponding tool driver engagement feature on the sterile adapter, and a sterile adapter sensor configured to generate a sensor signal when the tool driver engagement feature is mated with its corresponding sterile adapter engagement feature. At least one rotatable output drive may be supported by the housing and configured to communicate torque to an input drive of a surgical tool through the sterile adapter. In some variations, a distal end of the housing may comprise the sterile adapter engagement feature and the sterile adapter sensor. The sterile adapter engagement feature may comprise one or more of a recess and a projection. The sterile adapter sensor may be configured to generate the sensor signal when the tool driver engagement feature contacts the sterile adapter sensor. In some variations, one or more of a tool driver, sterile adapter, and surgical tool may comprise respective housings each including an optical waveguide configured to visually communicate an attachment state among the tool driver, sterile adapter, and surgical tool to an operator. Furthermore, a tool driver may include an illumination source coupled to an optical waveguide configured to propagate light to a sterile adapter and surgical tool. Attachment of the tool driver, sterile adapter, and surgical tool to each other may mechanically couple their corresponding optical waveguides together such that light generated by the tool driver may be output by the optical waveguide of the surgical tool via propagation through the tool driver and sterile adapter. These features may provide an operator an intuitive indication of the attachment state of the surgical system to aid in efficient tool switching and sterile barrier formation. In some variations, a tool driver may include one or more surgical tool sensors configured to generate at least one sensor signal corresponding to an attachment state between the tool driver and a surgical tool. For example, a surgical tool sensor may be disposed in one or more biasing pegs or other projections in the tool driver, where the one or more biasing pegs may be configured to contact a sterile adapter and urge at least a portion of the peg away from the tool driver and toward a surgical tool. The surgical tool sensor may be disposed in a predetermined portion of the biasing peg and be configured to generate the sensor signal. The sensor signal may be transmitted to a controller for processing and analysis , to determine the attachment state between the tool driver and the surgical tool. Additionally or alternatively, a surgical tool may include at least one sterile adapter engagement feature comprising a magnetic projection configured for aiding attachment of the surgical tool to the sterile adapter. The magnetic projection may be sensed by another surgical tool sensor to generate the sensor signal. In some variations, a tool driver may include at least one sterile adapter sensor configured to generate a sensor signal when one or more engagement features on each of the sterile adapter and the tool driver mate. In some variations, a robotic surgical system may include a tool driver comprising a first housing configured to attach to a surgical tool via a sterile adapter. The first housing may comprise at least one projection extending from a surface of the first housing and be configured to bias away from the surface. The projection may comprise at least one first surgical tool sensor configured to generate a sensor signal comprising at least one attachment state between the tool driver and the surgical tool. At least one rotatable output drive may be supported by the first housing and be configured to communicate torque to an input drive of the surgical tool through the sterile adapter. In some variations, the first surgical tool sensor may comprise a proximity sensor comprising a magnet coupled to a first end of the projection and a magnetic field transducer coupled to a second end of the projection. The projection may be disposed between a pair of the rotatable output drives. The first housing may comprise a plurality of the projections arranged in a bilaterally symmetrical arrangement. The projection may comprise a compliant material. The projection may comprise at least one of a coil spring and a leaf spring. A second surgical tool sensor may be disposed at a distal end of the first housing. In some of these variations, the second surgical tool sensor may comprise a proximity sensor comprising a magnetic field transducer. In some variations, a surgical tool may comprise a second housing configured to attach to the sterile adapter. The second housing may comprise a sterile adapter engagement feature comprising a magnetic projection. At least one input drive may be supported by the second housing and be configured to receive the torque communicated from an output drive of the tool driver through the sterile adapter. An end effector may extend from the second housing and be operatively coupled to the input drive. In some variations, the magnetic projection may comprise a first tapered surface and a second tapered surface opposite the first tapered surface. In some of these variations, a distal end of the surgical tool comprises the sterile adapter engagement feature. In some variations, a tool driver for use in a robotic surgical system may comprise a housing configured to couple to a sterile adapter. The housing may comprise a sterile adapter engagement feature mateable with a corresponding tool driver engagement feature on the sterile adapter, and a sterile adapter sensor configured to generate a sensor signal when the tool driver engagement feature is mated with its corresponding sterile adapter engagement feature. At least one rotatable output drive may be supported by the housing and be configured to communicate torque to an input drive of a surgical tool through the sterile adapter. In some of these variations, a distal end of the housing may comprise the sterile adapter engagement feature and the sterile adapter sensor. The sterile adapter engagement feature may comprise one or more of a recess and a projection. The sterile adapter sensor may be configured to generate the sensor signal when the tool driver engagement feature contacts the sterile adapter sensor. In some variations, the respective housings of the tool driver and surgical tool may define portions to support respective electronic communication devices that may lie in close proximity to each other when the surgical tool is attached to the sterile adapter and tool driver. Close proximity between the electronic communication devices may reduce signal interference, improve power efficiency, and enable wireless power transfer between electronic devices disposed within the tool driver and surgical tool. For example, an electronic communication device of the tool driver may be in a same plane as a rotatable output drive disk of the tool driver. A corresponding electronic communication device of the surgical tool may be disposed in a projection of a surgical tool housing on a side facing the tool driver. When the tool driver, sterile adapter, and surgical tool are attached to each other, a distance between the electronic communication devices may be reduced, if not minimized, in order to improve one or more of signal-to-noise ratio SNR and power transfer efficiency. In some variations, a robotic surgical system may include a tool driver comprising a first housing configured to attach to a surgical tool via a sterile adapter. At least one output drive may be coupled to a corresponding rotatable output drive disk each supported by the first housing. The output drive may be configured to communicate torque to an input drive of the surgical tool through the sterile adapter. A first electronic communication device may be configured to wirelessly communicate with the surgical tool and disposed substantially in a plane of the output drive disk. In some variations, the surgical tool may comprise a second housing configured to couple to the sterile adapter. The second housing may comprise a projection and a second electronic communication device configured to wirelessly communicate with the tool driver. The second electronic communication device may be disposed in the projection. An end effector may extend from the second housing and be operatively coupled to the input drive. The input drive may be supported by the second housing and configured to receive torque communicated from the output drive of the tool driver through the sterile adapter. In some variations, the sterile adapter may comprise a frame configured to be interposed between the tool driver and the surgical tool. A plate assembly may be coupled to the frame. The frame may comprise a communication portion configured to support the projection of the surgical tool substantially in a plane of the plate assembly when the surgical tool is attached to the sterile adapter and the plate assembly is biased toward the tool driver. At least one rotatable coupler may be supported by the plate assembly and configured to communicate torque from the output drive of the tool driver to the input drive of the surgical tool. A proximal end of the first housing may be configured to support the first electronic communication device. In some of these variations, a proximal end of the surgical tool may comprise the projection. In some of these variations, a proximal end of the frame may comprise the communication portion.",
                        "1 is an illustrative schematic of a portion of a robotic surgical system depicting a tool driver, sterile adapter, sterile barrier, and surgical tool. 2 is an illustrative state diagram for a robotic surgical control system. 3A-3C are perspective views of a variation of a robotic surgical system depicting one or more of a tool driver, sterile adapter, and surgical tool. 3A is a perspective view of a tool driver, 3B is a perspective view of a sterile adapter coupled to the tool driver, and 3C is a perspective view of a surgical tool coupled to the sterile adapter and tool driver. 4A-4E are illustrative views of a variation of a tool driver in different configurations. 4A is a plan view of the tool driver. 4B-4C are cross-sectional side views of the tool driver depicted in 4A. 4D-4E are detailed cross-sectional side views of respective 4B-4C. 5A-5C are illustrative views of a variation of a tool driver and surgical tool. 5A is a plan view of the tool driver. 5B is a cross-sectional side view of the tool driver depicted in 5A. 5C is a detailed cross-sectional side view and a portion of a surgical tool and the tool driver depicted in 5B. 6A-6B are illustrative views of some variations of a sterile adapter and surgical tool. 6A is a cross-sectional side view of one variation of a sterile adapter and surgical tool. 6B is a detailed cross-sectional side view of another variation of a sterile adapter and surgical tool. 7A-7C are illustrative views of a variation of a tool driver and sterile adapter. 7A is a plan view of the tool driver coupled to a sterile adapter, 7B is a cross-sectional side view of the tool driver and sterile adapter depicted in 7A, and 7C is a detailed cross-sectional side view of the tool driver and sterile adapter depicted in 7B. 8 is a detailed cross-sectional side view of a variation of a tool driver and surgical tool including respective electronic communication devices. 9A-9B are block diagram schematics of a variation of a robotic surgical system."
                    ],
                    "DESCRIPTION": "Described here are systems, devices, and methods for controlling a robotic surgical system using a tool driver, sterile adapter, and surgical tool. As shown generally in the schematic of 1, a robotic surgical system 10 may comprise a tool driver 100 configured to actuate a surgical tool 120. One or more drive outputs 102 of the tool driver 100 may, for example, actuate one or more drive inputs not shown on a proximal portion 122 of the surgical tool 120, thereby causing movement , grasping, cutting of an end effector not shown located at a distal end of a tool shaft 124. Additionally, a sterile barrier 150 may be placed between the tool driver 100 and the surgical tool 120, forming a barrier between an interior, non-sterile side including the tool driver 100 and an exterior, sterile side including the surgical tool 120 which may, for example, be located at a sterile surgical site. The sterile barrier 150 may, for example, include a sterile drape 130 configured to cover at least the tool driver 100, and a sterile adapter 110 coupled to the sterile drape 130 and located between the tool driver 100 and the surgical tool 120. The sterile adapter 110 may be configured to communicate or otherwise transmit an actuation force , rotary torque, linear movement from at least one drive output 102 of the tool driver 100 to at least one drive input of the surgical tool 120. Examples of tool drivers 100, sterile adapters 110, and surgical tools 120 are described in more detail herein. Generally, the systems and methods described herein may include attaching a sterile adapter and a surgical tool to a tool driver. One or more sensors disposed in one or more of the tool driver, sterile adapter, and surgical tool may be configured to generate sensor signals used to monitor and advance the attachment process. For example, attachment of the sterile adapter to the tool driver may be sensed and visually communicated to an operator through color-coded light output from optical waveguides , light pipe of one or more of the tool driver and the sterile adapter. Furthermore, upon determination of sterile adapter attachment, the tool driver may be actuated without operator input to fully engage , attach the sterile adapter to the tool driver in order to prepare the surgical system for surgical tool attachment. On the other hand, when the sensor detects improper attachment to the tool driver, the operator may be notified of the error through color-coded light output from an optical waveguide of the tool driver. In such situations, the tool driver may be inhibited from operating , to prevent damage to the system and/or operator. Disengagement and/or detachment of the surgical system may also be sensed and subsequently communicated to an operator , visually communicated using an optical waveguide. Using sensors to determine an attachment state between the tool driver, sterile adapter, and surgical tool and communicating the attachment state to an operator may, for example, aid in efficiently performing tool switching and proper sterile barrier formation. In some variations, an attachment state between a tool driver, sterile adapter, and surgical tool may be output by the robotic surgical system to an operator using one or more output modalities. In some variations, one or more of the tool driver, sterile adapter, and surgical tool may include a visual output device for communicating the attachment state to an operator quickly and intuitively. For example, upon attachment of the sterile adapter and/or surgical tool to the tool driver, respective optical waveguides of the attached components may indicate the attachment state. Additionally or alternatively, the surgical system may include other visual output devices such as a user console , surgeon bridge and/or display device. The operator may additionally or alternatively receive audio and haptic output from the respective audio and haptic devices. Attachment state data may be generated using one or more sensor signals. In some variations, a tool driver may include at least one surgical tool sensor configured to generate a sensor signal corresponding to an attachment state between the tool driver and a surgical tool. The sensor signal may be used to generate attachment data. The attachment data may be used to control the tool driver and/or output an attachment state to an operator. In some variations, one or more biased projections , biasing pegs may extend from a surface of a tool driver housing. The projection may be biased to contact and urge a portion of a sterile adapter away from a tool driver housing and towards a surgical tool. A first surgical tool sensor , proximity sensor may be disposed within the projection and configured to determine an amount of movement of the projection relative to the tool driver housing. An attachment state between the tool driver and surgical tool may be derived from the surgical tool sensor data. In some variations, a tool driver may include a second surgical tool sensor configured to generate another sensor signal corresponding to attachment between the surgical tool and the sterile adapter to the tool driver. In some variations, the second surgical tool sensor may be a proximity sensor , magnetic field transducer configured to detect a magnetic projection of a surgical tool. As other examples, the second surgical tool sensor may include a switch , that detect physical contact between the surgical tool and/or sterile adapter to the tool, an optical sensor, an inductive sensor, and/or other suitable sensor. An attachment state between the tool driver and surgical tool may be derived from the sensor signal generated from the second surgical tool sensor. In some variations, the magnetic projection may be configured to mate with a corresponding recess in the sterile adapter. The surfaces of the projection may be configured to guide attachment of the surgical tool to the sterile adapter. Thus, the projection may aid in proper alignment and attachment , seating of the surgical tool to the sterile adapter. In some variations, a tool driver may include at least one sterile adapter sensor configured to generate a sensor signal when the sterile adapter is fully attached to the tool driver. For example, the sterile adapter sensor may generate the sensor signal when corresponding engagement features on the tool driver and the sterile adapter make contact and mate. In other examples, the sterile adapter sensor may additionally or alternatively include another suitable sensor , proximity sensor for detecting when the sterile adapter is fully attached to the tool driver. The sensor signal may be used to generate attachment data for controlling the tool driver. The sterile adapter sensor may be disposed on a portion of the housing of the tool driver , distal end such that improper contact and/or partial attachment of the sterile adapter to the tool driver generates a corresponding sensor signal. In some variations, the tool driver and surgical tool may include electronic communication devices disposed in a tool driver and surgical tool to communicate tool and system data with each other. The communication devices may be configured to be in close proximity , within a few millimeters when the surgical tool is attached to the sterile adapter and tool driver. Close proximity between the electronic communication devices permits reduced signal interference and may enable wireless power transfer between the tool driver and surgical tool. A first electronic communication device of the tool driver may be disposed close to a side of a tool driver housing facing the surgical tool to minimize the distance between the first electronic communication device and the surgical tool. Likewise, a second electronic communication device of the surgical tool may be disposed close to a side of the surgical tool housing facing the tool driver to minimize the distance between the second electronic communication device and the tool driver. The sterile adapter may include a communication portion configured to support the second electronic communication device such that the distance between the surgical tool and tool driver is minimized when the sterile adapter is interposed between them. I. MethodsDescribed herein are methods for controlling a robotic surgical system using the systems and devices described herein. Generally, the methods described here include using one or more sensors to generate an attachment state between a surgical tool, sterile adapter, and tool driver. In response to the attachment state, a controller may control operation of the tool driver and output , notify the attachment state to an operator. For example, the methods described here may include determining partial attachment of a sterile adapter to a tool driver, and subsequently actuating output drives of the tool driver to fully attach the sterile adapter to the tool driver. The system may notify the attachment state to the operator , partial attachment, full attachment, partial detachment, full detachment, improper attachment using one or more of audio, visual and haptic output. Any of the devices and systems as described herein may be used to perform the methods discussed herein. For example, a system may include a tool driver configured to attach to a surgical tool via a sterile adapter. The tool driver may comprise at least one sterile adapter sensor and/or at least one surgical tool sensor. These sensors may be configured to generate at least one sensor signal used to generate an attachment state of the system , the sterile adapter sensor may be configured to generate a sensor signal corresponding to an attachment state between the tool driver and the sterile adapter, and/or the surgical tool sensor may be configured to generate a sensor signal corresponding to an attachment state between the surgical tool/sterile adapter and the tool driver. A controller comprising a processor and a memory may be coupled to the tool driver and used to control actuation of the tool driver. Generally, the methods described herein may include receiving the sensor signal generated by one or more sensors of the tool driver and generating attachment data using the sensor signal. For example, a controller may receive and process the sensor signal to generate attachment data. The attachment data may comprise at least one attachment state between the tool driver, the sterile adapter, and the surgical tool. Tool driver operation and/or operator notification may be based on the attachment data. The methods described herein may, for example, aid proper engagement of a surgical tool and sterile adapter to the tool driver for formation of a sterile barrier. This may have one or more benefits, such as efficient tool switching and sterile barrier formation, as well as increased safety as the system need not rely on operator confirmation of an attachment state. 2 is a state diagram that describes an illustrative method of controlling a system as described herein. Of course, the exemplary variation described in 2 is provided for the sake of illustrative description and is non-limiting. In some variations, the attachment state of the system may be communicated to the operator using one or more output methods , described in detail with respect to 3A-3C. For example, a status of the system may be communicated to the operator using a set of light patterns emitted from respective optical waveguides , light pipe of one or more of the tool driver, sterile adapter, and surgical tool. The light patterns described herein may, for example, comprise one or more of flashing light, occulting light, isophase light, etc. , and/or light of any suitable light/dark pattern. For example, flashing light may correspond to rhythmic light in which a total duration of the light in each period is shorter than the total duration of darkness and in which the flashes of light are of equal duration. Occulting light may correspond to rhythmic light in which the duration of light in each period is longer than the total duration of darkness. Isophase light may correspond to light which has dark and light periods of equal length. Light pulse patterns may include one or more colors , different color output per pulse, light intensities, and frequencies. Variations of optical waveguides of the system are described in further detail herein. In some variations, one or more of visual, audible, and haptic output may be provided to an operator to communicate an attachment state of the system. As another example, the attachment state of the system may additionally or alternatively be visually communicated to the operator using a display device. Variations of display devices of the system are described in further detail herein and may comprise, for example, one or more of an LED display, touch screen display, user console, virtual reality headset, and other suitable displays. As another example, the attachment state of the system may additionally or alternatively be audibly communicated using an audio device. Variations of audio devices of the system are described in further detail herein and may comprise, for example, one or more of a speaker, user console, virtual reality headset, and other suitable audio devices. As yet another example, the attachment state of the system may additionally or alternatively be haptically communicated using a haptic device. Variations of haptic devices of the system are described in further detail herein and may comprise, for example, a vibrational motor in at least one of the tool driver, distal portion of the arm, input device , hand-held controller, and other suitable haptic devices. Sterile Adapter Ready StateIn some variations, the control process 200 may include a sterile adapter ready state 202 in which both a sterile adapter and surgical tool are fully detached from a tool driver. One or more surgical tool sensors , described in detail with respect to 4D-4E and 6B may output a sensor signal corresponding to a detachment state between the tool driver and a surgical tool. A sterile adapter attachment sensor , described in detail with respect to 7C may output a sensor signal corresponding to a detachment state between the tool driver and a sterile adapter. Consequently, the tool driver may be inhibited from driving an output drive in the sterile adapter ready state 202 based on these sensor signals. Referring back to 2, a tool driver may output a first light pattern 220 corresponding to the sterile adapter ready state 202. In some variations, the tool driver may output a slow pulse of colored light , blue light at a predetermined light intensity using a first optical waveguide of the tool driver , see 3A. For example, the first light pattern may comprise pulses of light having a duration of about half a second or more. Additionally or alternatively, a display device may output text or an image indicating that the system is ready for attachment of a sterile adapter to the tool driver. For example, a user console , surgeon bridge coupled to the system may display a message to the operator such as Please attach the sterile adapter to the tool driver and/or Ready for sterile adapter attachment. In some variations, the sterile adapter ready state 202 may correspond to a first audio pattern and first haptic pattern that inhibits audio and haptic output. In other variations, an audio device may output a sound effect , bing, ping, beep, and/or verbal message at a predetermined time interval to remind an operator to attach a sterile adapter to the tool driver. Sterile Adapter Engagement StateAn operator may at least partially attach a sterile adapter to the tool driver. In response, a controller may determine that the system transitions from a sterile adapter ready state 202 to a sterile adapter engagement state 204 corresponding to partial attachment of the sterile adapter to the tool driver. In some variations, partial attachment of the sterile adapter to the tool driver may mean at least a portion of the sterile adapter may be attached to the tool driver, but at least another portion of the sterile adapter may be detached or disengaged from the tool driver. For example, in a variation in which a tool driver includes at least one rotatable output drive , rotary axis drive, a sterile adapter may include a frame and at least one rotatable coupler configured to transmit torque from the rotatable output drive to a surgical tool. Partial attachment of the sterile adapter to the tool driver may mean the frame may be attached to the tool driver, but at least one rotatable coupler of the sterile adapter may not be operatively coupled with corresponding output drives of the tool driver and thereby fail to transmit torque. For example, 7A-7B illustrate attachment of an exemplary sterile adapter to a tool driver. In some variations, an operator may attach a proximal end of the sterile adapter to the tool driver before attachment of a distal end of the sterile adapter. The system may then determine the transition from the ready state 202 to the engagement state 204 when a sensor signal corresponding to partial attachment of the sterile adapter to the tool driver 222 is generated by a sterile adaptor sensor. One or more surgical tool sensors , see 4D, 4E, and 5C may generate a sensor signal corresponding to detachment between the surgical tool and the tool driver. This combination of sensor signals may correspond to partial attachment where the sterile adapter is partially attached to the tool driver, but not ready for surgical tool loading 206. In response to the partial attachment of the sterile adapter to the tool driver 222, a controller of the system may actuate one or more rotatable output drives of the tool driver to physically engage the rotatable output drives of the tool driver to corresponding rotatable couplers of the sterile adapter 224. For example, the sterile adapter may comprise a frame and a plate assembly coupled to the frame. The plate assembly may be configured to have a range of motion perpendicular to a plane of the frame. The plate assembly may comprise at least one rotatable coupler supported by the plate assembly. The rotatable coupler may be configured to communicate the torque output from the output drive of the tool driver. In systems including other variations of tool drivers and sterile adapters, the controller of the system may actuate at least a portion of the tool driver in any suitable manner so as to engage an output drive to a corresponding portion of a sterile adapter. For example, in a system including a tool driver having at least one linear output drive and a sterile adapter having at least one linearly-movable coupler or other interface configured to engage the linear output drive, the controller may actuate the linear output drive , distally and/or proximally in an axial direction so as to engage the linear output drive with the linearly-movable coupler. In some variations, the tool driver may output a second light pattern in response to determining the partial attachment state of the sterile adapter. In some variations, the tool driver may output a fast pulse of colored light , blue light at a predetermined light intensity using an optical waveguide of the tool driver. For example, the second light pattern may comprise pulses of light having a duration of less than about half a second , about second. In some variations, as further described herein, attachment of the sterile adapter to the tool driver may mechanically couple the optical waveguides , light pipes of the sterile adapter and tool driver together such that light emitted by an illumination source of the tool driver may be propagated through and distributed by the optical waveguide of the sterile adapter. The light output by the sterile adapter confirms to the operator that the sterile adapter is attached to the tool driver. This may allow an operator to quickly identify the sterile adapter engagement state 204 by looking at multiple visual indicators , color, pulse frequency, light distribution, output by the tool driver and the sterile adapter. Additionally or alternatively, a display device may output text or an image indicating that the sterile adapter is partially attached to the tool driver in the sterile adapter engagement state 204. For example, a user console coupled to the system may display a message to the operator such as Sterile adapter detected, Sterile adapter attached to tool driver, and/or Engaging sterile adapter disks. In some variations, an audio device may output a second audio pattern corresponding to the sterile adapter engagement state. For example, the audio device may output a set of fast, short bings, or other suitable sound effects at a predetermined frequency and volume for a predetermined length of time. A second haptic pattern may correspond to the sterile adapter engagement state and inhibit haptic output. In other variations, the audio device may output the message displayed by the user console and/or verbally output the operational steps of the tool driver in response to attachment of the sterile adapter , Engaging sterile adapter disks. Conversely, the operator may detach the sterile adapter from the tool driver by, for example, rotationally lifting the sterile adapter off from a distal end of the tool driver. The system may transition from the sterile adapter engagement state 204 to the sterile adapter ready state 202 when a sensor signal from a sterile adaptor sensor is generated that corresponds to detachment between the sterile adapter and the tool driver 226. Likewise, one or more surgical tool sensors may generate a sensor signal corresponding to detachment between the surgical tool and the tool driver. The tool driver may be inhibited from actuating an output drive in the sterile adapter detachment state 226. In some variations, one or more of visual, audio, and haptic output may also be inhibited when detachment of the sterile adapter is sensed 226. Surgical Tool Loading Readiness StateFrom the sterile adapter engagement state 204, a controller may determine a transition of the system into a surgical tool loading readiness state 206. For example, the controller may rotate the rotatable output drives of the tool driver for a predetermined number of revolutions in one or more directions , clockwise, counter-clockwise to fully attach the sterile adapter to the tool driver. The tool driver and sterile adapter may be in a tool loading readiness state 206 when the rotatable couplers of the sterile adapter are fully physically engaged with their corresponding rotatable output drives of the tool driver. Upon full attachment, torque generated by the output drives may be communicated to the couplers of the sterile adapter. In some variations, the rotatable output drives may rotate until a change in torque is detected in the output drives using one or more torque sensors and rotary encoders. A change in torque may correspond to added resistance from the rotatable couplers that indicate that the rotatable couplers have physically engaged with their corresponding output drives of the tool driver. In some variations, one or more of the tool driver and sterile adapter may output a third light pattern from the system to notify an operator of the tool loading readiness state 206. In some variations, one or more of the tool driver and sterile adapter may output solid colored light , blue light at a predetermined light intensity using an optical waveguide of the tool driver and/or sterile adapter. Additionally or alternatively, a display device may output text or an image indicating that the system is ready for attachment of a surgical tool to the sterile adapter and tool driver. For example, a user console coupled to the system may display a message to the operator such as Please attach a surgical tool to the tool driver and/or Ready for tool loading. In some variations, an audio device may output a third audio pattern corresponding to the tool loading readiness state 206. For example, the audio device may output one or more extended bings having a duration longer than a short bing at a predetermined frequency and volume for a predetermined length of time. A third haptic pattern may correspond to the surgical tool loading readiness state and inhibit haptic output. In other variations, the audio device may output the message displayed by the user console and/or verbally describe the actuation of the tool driver in response to the tool loading readiness state 206. In some circumstances, after attachment of the sterile adapter to the tool driver, an operator may detach the sterile adapter from the tool driver. The system may transition from the tool loading readiness state 206 to the sterile adapter ready state 202 when a sensor signal generated by a sterile adaptor sensor corresponds to detachment of the sterile adapter from the tool driver 226. Likewise, one or more surgical tool sensors may generate a sensor signal corresponding to detachment of a surgical tool from the tool driver. Surgical Tool Engagement StateOnce the sterile adapter is fully attached to the tool driver, an operator may partially attach a surgical tool to the sterile adapter. In response, a controller may determine that the system transitions from the surgical tool loading readiness state 206 to the surgical tool engagement state 208 corresponding to partial attachment of the surgical tool to the sterile adapter 228. In some variations, partial attachment of the surgical tool to the sterile adapter may mean at least a portion of the surgical tool may be attached to the sterile adapter, while another portion of the surgical tool may be detached or disengaged from the sterile adapter. For example, the housing of the surgical tool may be attached to a frame of the sterile adapter to the tool driver, but input drives of the surgical tool may not be operatively coupled with corresponding rotatable couplers of the sterile adapter. The system may then transition from the readiness state 206 to the tool engagement state 208 of a sensor signal corresponding to partial attachment between the surgical tool and the sterile adapter 228 is generated by one or more surgical tool sensors. One or more projections , biasing pegs of the tool driver see , 4B-4E and accompanying description herein may generate a sensor signal corresponding to the projections being pushed downward toward a surface of the tool driver momentarily as the operator pushes the surgical tool longitudinally over a surface of the sterile adapter. The projections may then bias away from the surface of the tool driver as the surgical tool becomes seated within the plate assembly of the sterile adapter. For example, a surface of the sterile adapter may comprise mating features , projections that create an uneven surface and are configured to mate with corresponding mating features , recesses on the sterile adapter. When an operator slides the surgical tool over the sterile adapter, the projections may slide over the surface of the plate assembly unevenly so as to momentarily push the plate assembly downward. When the projections mate into corresponding recesses, the plate assembly may bias back upward. In some variations, a mating feature of a surgical tool may be used to determine an attachment state of the surgical tool. For example, a second surgical tool sensor , proximity sensor disposed on a distal end of the tool driver and facing the sterile adapter may generate a sensor signal corresponding to the presence of the surgical tool when a magnetic projection of the surgical tool is slid into a corresponding recess of the sterile adapter see , 6A-6B and accompanying description herein. For example, the output of the second surgical tool sensor may indicate that the surgical tool is attached to and fully seated in the sterile adapter so as to be ready for input drive coupling. In some variations, a tool driver may further comprise a sterile adapter sensor configured to generate a sensor signal corresponding to an attachment state between the tool driver and the sterile adapter. For example, as further described herein with reference to 7A-7C, in some variations, a distal end of the sterile adapter may be mechanically latched to the tool driver to engage with the sterile adapter sensor. The combination of these surgical tool and sterile adapter sensor signals may be used by a controller to transition the system from the tool loading readiness state 206 to the tool engagement state 208. In some variations, the tool driver and surgical tool may each comprise respective electronic communication devices such as wireless communication devices configured to communicate with each other and/or transfer power wirelessly when they are in close proximity , within a few centimeters. Attachment of the surgical tool to the sterile adapter and the tool driver may correspond to the electronic communication devices becoming within a predetermined range of each other. An exemplary arrangement with such respective electronic communication devices is described below with reference to 8. In response to the tool engagement state 208, a controller of the system may actuate one or more rotatable output drives of the tool driver to physically engage the rotatable output drives of the tool driver with corresponding rotatable input drives of the surgical tool. In some variations, the tool driver may output a fourth light pattern in response to the partial attachment state of the surgical tool. In some variations, one or more of the tool driver, sterile adapter, and surgical tool may output a fast pulse of colored light , green light at a predetermined light intensity using one or more optical waveguides of the tool driver, sterile adapter, and surgical tool. For example, the fourth light pattern may comprise pulses of light having a duration of less than about half a second , about second. In some variations, attaching the surgical tool to the sterile adapter may mechanically couple the waveguides of the surgical tool, sterile adapter, and the tool driver together such that light generated by the tool driver may be propagated to and distributed by the optical waveguides of the sterile adapter and/or surgical tool. This may allow an operator to quickly confirm the tool engagement state 208 between the tool driver and the surgical tool by simply looking at which system component is emitting light. Additionally or alternatively, a display device may output text or an image indicating that the surgical tool is partially attached to the tool driver in the tool engagement state 208. For example, a user console coupled to the system may display a message to the operator such as Surgical tool detected, Surgical tool attached to tool driver, and/or Engaging surgical tool input drives. In some variations, an audio device may output a fourth audio pattern corresponding to the tool engagement state 208. For example, the audio device may output a set of fast, short bings at a predetermined frequency , different from the frequency of the second audio pattern and predetermined volume for a predetermined length of time. A haptic device may output a fourth haptic pattern corresponding to the tool engagement state 208. For example, a haptic device of one or more of the tool driver and surgical tool may output a small vibration. In other variations, the audio device may output the message displayed by the user console and/or verbally describe the actuation of the tool driver in response to surgical tool attachment , actuation of one or more output drives. In some variations, the electronic communication device , wireless communication device of the surgical tool in the tool engagement state may transmit tool functionality and other data , security data, utilization data, diagnostic data, manufacturing data, to the electronic communication device of the tool driver. In turn, the electronic communication device of the surgical tool may receive authentication data and/or other data calibration data, usage data, log data, surgical system data, patient data, procedure data, regulatory data, that may be stored in a memory of the surgical tool. Surgical Tool Readiness StateA controller may control a system in the tool engagement state 208 to transition 230 into a surgical tool readiness state 210 that corresponds to a full attachment state between the surgical tool, sterile adapter, and tool driver. For example, a tool driver may rotate the rotatable output drives for a predetermined number of revolutions in one or more directions , clockwise, counter-clockwise to fully seat the surgical tool in the sterile adapter. The surgical tool in the tool readiness state 210 may then be actuated via the tool driver to operate the surgical tool under operator guidance. The tool driver and surgical tool may be in a tool readiness state 210 when the rotatable input drives of the surgical tool are fully physically engaged , fully attached with their corresponding rotatable output drives of the tool driver. When the surgical tool is fully attached , fully seated, torque generated by the output drives may be communicated to the input drives of the tool driver. In some variations, the rotatable output drives may rotate until a predetermined change in torque is detected in the output drives using one or more torque sensors and rotary encoders. A reduction in torque may indicate that the input drives of the surgical tool encountered resistance and have physically engaged with their corresponding output drives of the tool driver. In some variations, one or more of the tool driver, sterile adapter, and surgical tool may output a fifth light pattern to indicate the tool readiness state 210. In some variations, one or more of the tool driver, sterile adapter, and surgical tool may output solid colored light , green light at a predetermined light intensity using an optical waveguide of one or more of the tool driver, sterile adapter, and surgical tool. Additionally or alternatively, a display device may display a message to the operator such as Tool ready for use and/or Tool fully attached. In some variations, an audio device may output one or more extended bings longer than a short bing at a predetermined frequency different from the third audio pattern and volume for a predetermined length of time. A fifth haptic pattern may inhibit haptic output. In other variations, the audio device may output the message displayed by the user console and/or verbally describe the actuation of the tool driver in response to the tool readiness state 210. In some variations, an electronic communication device , wireless communication device of the surgical tool may transmit tool functionality and other data , security data, usage data, log data, diagnostic data, manufacturing data, to the electronic communication device of the tool driver. In turn, the electronic communication device of the surgical tool may receive authentication data and/or other data calibration data, usage data, surgical system data, patient data, procedure data, regulatory data, that may be stored in a memory of the surgical tool. In some variations, the tool driver may be configured to wirelessly transfer power from the tool driver to the surgical tool in the surgical tool readiness state using a near-field wireless power transfer system. Surgical Tool Release StateThe methods described here may also control a tool driver in response to operator release of a surgical tool and/or sterile adapter from a tool driver. Generally, the operator may detach one or more of the surgical tool and sterile adapter from the tool driver through partial detachment , by actuating a surgical tool release mechanism or full detachment , physical removal by lifting and/or pulling of the surgical tool and/or sterile adapter from the tool driver. Partial detachment may be analogous to partial attachment. For example, the operator may detach the sterile adapter having the surgical tool partially or fully attached thereon from the tool driver 232 to transition from either the tool engagement state 208 or tool readiness state 210 to the surgical tool release state 212. The sterile adapter in the surgical tool release state 212 may be partially detached so as to lie on the tool driver but not output torque communicated by the tool driver. The operator is then able to wholly remove the surgical tool and sterile adapter from the tool driver to transition from the surgical tool release state 212 to the surgical tool removed state 214. As shown in 2, the surgical tool that is partially attached , in the tool engagement state 208 may transition to the surgical tool release state 212 either through partial detachment of the sterile adapter from the tool driver 232 or partial detachment of the surgical tool from the sterile adapter 234. For example, when an operator detaches the surgical tool from the surgical system , for tool switching, the operator may actuate a tool release mechanism not shown on the surgical tool to bias a portion of the surgical tool away from the sterile adapter. That is, an actuation mechanism , lever, button of the surgical tool may release and/or partially separate the surgical tool from of the sterile adapter. This corresponds to transition from either the tool engagement state 208 or tool readiness state 210 to a surgical tool release state 212. In some variations, transition from either the tool engagement state 208 or the tool readiness state 210 to the tool release state 212 may correspond to generation of a sensor signal from one or more surgical tool sensors corresponding to detachment between the surgical tool and the sterile adapter 234. For example, one or more projections , biasing pegs of the tool driver may generate a sensor signal corresponding to at least one projection being biased toward a surface of the tool driver , the sterile adapter partially exerting a downward force against the tool driver when being detached. In some variations, a second surgical tool sensor , proximity sensor, magnetic field transducer, Hall effect sensor disposed on a distal end of the tool driver may generate a sensor signal corresponding to the presence of the surgical tool. This signal may indicate that the surgical tool is at least in partial contact with the sterile adapter. A sterile adapter sensor may generate a sensor signal corresponding to attachment between the tool driver and the sterile adapter. The combination of these surgical tool and sterile adapter sensor signals may be used by the controller to transition the system to the surgical tool release state 212. In some variations, the electronic communication devices of the tool driver and surgical tool may communicate and/or transfer power with each other as they are still within close proximity to each other. In some variations, the sterile adapter may be partially detached from the tool driver by lifting up and separating , unlocking a distal end of the sterile adapter from the tool driver. For example, transition from the tool engagement state 208 or the tool readiness state 210 to the tool release state 212 may correspond to generation of a sensor signal from a sterile adaptor sensor corresponding to detachment between the sterile adapter and the tool driver 232. In this state, the surgical tool may still be fully or partially attached to the sterile adapter. Likewise, one or more surgical tool sensors may output a sensor signal corresponding to detachment between the surgical tool and the tool driver. In response, the tool driver may be inhibited from driving an output drive. This combination of sensor signals may comprise partial detachment. For example, the sterile adapter may lie on the tool driver but be functionally decoupled from the tool driver. In response to the tool release state 212, a controller may inhibit actuation of the rotatable output drives of the tool driver. In some variations, the tool driver may output a sixth light pattern in response to the partial detachment state. In some variations, one or more of the tool driver and sterile adapter may output a fast pulse of colored light , red light at a predetermined light intensity using one or more optical waveguides of the tool driver and sterile adapter. For example, the sixth light pattern may comprise pulses of light having a duration of less than about half a second , about second. In some variations, detachment of the surgical tool from the sterile adapter may mechanically decouple the third optical waveguide of the surgical tool from the second optical waveguide of the sterile adapter such that light generated by the tool driver may be distributed to and output only by the first optical waveguide of the tool driver and/or second optical waveguide of the sterile adapter. This may allow an operator to quickly confirm the tool release state 212 based on which system component is emitting light. Additionally or alternatively, a display device may output text or an image indicating that the surgical tool is partially detached from the sterile adapter or tool driver in the tool release state 212. For example, a user console coupled to the system may display a message to the operator such as Surgical tool disengaged, please remove the surgical tool, Sterile adapter disengaged, please remove the sterile adapter, and/or Do you wish to remove the surgical tool? In some variations, an audio device may output a sixth audio pattern corresponding to the tool release state 212. For example, the audio device may output a set of fast, short bings at a predetermined frequency different from the frequency of the second and fourth audio pattern and volume for a predetermined length of time. A haptic device may output a sixth haptic pattern corresponding to the tool engagement state 208. For example, a haptic device of the tool driver may output a small vibration. In other variations, the audio device may output the message displayed by the user console and/or verbally describe tool driver response to operator detachment. Surgical Tool Removed StateAn operator may fully detach the surgical tool and/or sterile adapter from the tool driver. In response, a controller may determine that the system transitions from the surgical tool release state 212 to the surgical tool removed state 214 corresponding to complete detachment of the surgical tool from the tool driver 236. Transition from the tool release state 212 to the tool removed state 214 may be based on generation of a sensor signal from one or more surgical tool sensors corresponding to detachment of the surgical tool from the tool driver 236. For example, one or more biased projections of the tool driver may generate a sensor signal corresponding to the surgical tool being withdrawn from contact with the projections of the tool driver. Furthermore, a second surgical tool sensor disposed on a distal end of the tool driver may output a sensor signal corresponding to the absence of the surgical tool when a magnetic projection of the surgical tool is not detected. The combination of these surgical tool sensor signals may be used by the controller to transition the system to the tool removed state 214. In some variations, electronic communication devices of the tool driver and surgical tool may move out of range of each other in the removed state 214. In response to the surgical tool removed state 214, the controller may inhibit actuation of the rotatable output drives of the tool driver. In some variations, the tool driver may output a seventh light pattern in response to tool removal. In some variations, one or more of the tool driver and sterile adapter may output solid colored light , blue light at a predetermined light intensity using respective optical waveguides of the tool driver and/or sterile adapter. Additionally or alternatively, a display device may output text or an image indicating that the surgical tool is removed from the tool driver in the tool removed state 214. For example, a user console coupled to the system may display a message to the operator such as Surgical tool removed and/or Please attach a surgical tool. In some variations, audio and haptic output may be inhibited in the tool removed state 214. In other variations, the audio device may output the message displayed by the user console and/or verbally describe the tool driver response to operator detachment. Once a surgical tool is removed, the operator may either attach another surgical tool and/or sterile adapter to the tool driver. For example, the controller may transition the system from the tool removed state 214 to the sterile adapter ready state 202 when a sensor signal generated by a sterile adaptor sensor corresponds to detachment between the sterile adapter and the tool driver 238. The tool driver may be inhibited from driving an output drive in the sterile adapter detachment state 238. In some variations, one or more of visual, audio, and haptic output may also be inhibited when the sterile adapter is detached 238. In some variations, the controller may transition from a tool removed state 214 to a tool loading readiness state 206. Transition from the tool removed state 214 to the tool loading readiness state 206 may be triggered by a sensor signal generated by a sterile adaptor sensor corresponding to full attachment between the sterile adapter and the tool driver 240. In response to sterile adapter attachment 240, a controller may inhibit actuation of the rotatable output drives of the tool driver. In some variations, the tool driver may output a third light pattern in response to the sterile adapter attachment state 240. For example, the tool driver may output solid colored light , blue light at a predetermined light intensity using an optical waveguide of the tool driver and/or sterile adapter. Additionally or alternatively, a display device may output text or an image indicating that the system is ready for attachment of a surgical tool to the sterile adapter and tool driver. For example, a user console coupled to the system may display a message to the operator such as Please attach a surgical tool to the tool driver and/or Ready for tool loading. In some variations, an audio device may output the third audio pattern corresponding to the tool loading readiness state 206 as described herein. Haptic output may be inhibited as in the third haptic pattern. In some variations, control of the tool driver may be based on attachment state and time. For example, if partial attachment of one or more of a sterile adapter and surgical tool to a tool driver does not transition to full attachment within a predetermined period of time and/or a predetermined number of attempts, then a controller may inhibit tool driver output and notify the operator of the attachment error. In response, the operator may repeat the attachment process. II. DevicesA robotic surgical system may include one or more of the components necessary to perform robotic surgery using the devices as described herein. Generally, the devices described herein for use in a robotic surgical system may include one or more of a tool driver, a sterile adapter, and a surgical tool. The tool driver may include at least one rotatable output drive configured to communicate torque to the surgical tool through the sterile adapter. The sterile adapter may include a frame configured to be interposed between the tool driver and the surgical tool. A plate assembly may be coupled to the frame and at least one rotatable coupler may be supported by the plate assembly and configured to communicate the torque from the output drive of the tool driver to the surgical tool. The surgical tool may include at least one input drive configured to receive the torque communicated from the tool driver. The surgical tool may further include an end effector operatively coupled to the input drive. In some variations, the tool driver may include an illumination source configured to emit light and an optical waveguide configured to propagate the emitted light to a sterile adapter. The sterile adapter and surgical tool may include respective optical waveguides configured to receive, propagate, and distribute the received light. In some variations, the tool driver may include at least one sterile adapter sensor and surgical tool sensor configured to generate a sensor signal used in turn to generate attachment data. The tool driver and surgical may each further comprise an electronic device configured for wireless communication and/or wireless power transfer. The electronic devices may be disposed in respective housings such that the electronic devices are in close proximity to each other when the tool driver, sterile adapter, and surgical tool are attached to each other. Optical WaveguideThe tool driver, sterile adapter, and surgical tool as described herein may include one or more output devices configured to communicate information to an operator such as an attachment state, system or device state, and other information , patient data, procedure data, . Information may be communicated visually by one or more of the tool driver, sterile adapter, and surgical tool and provide an intuitive indication of the attachment state of the surgical system to aid in efficient tool switching and sterile barrier formation. For example, one or more of the tool driver, sterile adapter, and surgical tool may include an optical waveguide , light pipe, light distribution guide, for allowing the operator to visualize attachment state information generated by the system. One or more optical waveguides may receive light from a light source , illumination source of a tool driver using a predetermined combination of light output parameters , wavelength, frequency, intensity, pattern, duration to confirm a formation state of the sterile barrier and/or robotic surgical system. In some variations, an optical waveguide may be configured to receive and propagate light from an illumination source upon mechanical attachment to another optical waveguide of the robotic surgical system such that they are in optical communication. For example, a tool driver optical waveguide may be configured to output light to an input of a sterile adapter optical waveguide upon attachment to the tool driver. That is, light emitted and propagated by the tool driver may be received by the sterile adapter only after the sterile adapter is properly attached to the tool driver. This allows an operator to easily confirm an attachment state based on light output from the set of optical waveguides. The optical waveguides may be formed integral with the housings of the system to simplify manufacturing and allowing for a compact design and minimal power usage. Additionally or alternatively, the system may include a user console having additional visual output devices , display device. In some variations, the operator may receive audio and haptic feedback, as described herein, corresponding to the attachment state of the system. A. Tool Driver 3A is a perspective view of a tool driver 310 comprising a first optical waveguide 320 for communicating to a user , an operator an attachment state of the system using outputted light. As shown in the variation depicted in 3A, the tool driver 310 may comprise a first housing 312, a set of surgical tool sensors 314, and a set of rotatable output drives 316. The set of output drives 316 may be supported by the first housing 312 and may be configured to communicate torque to a surgical tool through a sterile adapter shown in 3B and 3C. The tool driver 310 may be coupled to, for example, a distal end of a robotic arm not shown. The first housing 312 may comprise a first optical waveguide 320. An optical waveguide may refer to a physical structure that guides electromagnetic waves such as visible light spectrum waves to passively propagate and distribute received electromagnetic waves. Non-limiting examples of optical waveguides include optical fiber, rectangular waveguides, light tubes, light pipes, combinations thereof, or the like. For example, light pipes may comprise hollow structures with a reflective lining or transparent solids configured to propagate light through total internal reflection. The optical waveguides described herein may be made of any suitable material or combination of materials. For example, in some variations, the optical waveguide may be made from optical-grade polycarbonate. In some variations, the housings and frames as described herein may be co-injected molded to form the optical waveguides. In other variations, the optical waveguides may be formed separately and coupled to a respective housing or frame. As shown in 3A, the first optical waveguide 320 may be disposed along an exterior surface of the first housing 312. For example, the first optical waveguide 320 may be flush with an exterior surface of the first housing 312. In another example, the first optical waveguide 320 may be at least partially recessed or at least partially projected from the exterior surface of the first housing 312. In some variations, the first optical waveguide 320 may comprise a plurality of portions , disposed on opposite sides of the tool driver 310. For example, as shown in 3A, the first optical waveguide 320 may include a strip located at least in part on a proximal end of the first housing 312. The strip may include a first end extending at least partially onto a first side , left side of the first housing 312 and a second end extending at least partially onto a second side , right side of the first housing 312. As another example, the first optical waveguide 320 may cover a substantial portion of the exterior of the first housing 312 , a proximal portion, side portions. In some variations, the optical waveguides described herein may comprise one or more portions configured to emit light. For example, at least one of the portions may comprise one or more shapes including, for example, a circle, triangle, rectangle, diamond, polygon, symbol , plus/minus sign, arrow, lock, , combinations thereof, or the like. For example, the first optical waveguide 320 may comprise three circles on each of the first and second sides of the first housing 312. The circles may be coupled to corresponding illumination sources, as described in detail herein, and configured to output light corresponding to a respective status of a tool driver, sterile adapter, and surgical tool. For example, a first circle may pulse blue light while a second and third circle may emit a solid red light when the tool driver is operational and in a sterile adapter ready state 202. These light patterns may correspond to the tool driver in a ready state and the sterile adapter and surgical tool unattached to the tool driver. In some variations, the first optical waveguide 320 may be located on the first housing 312 so as to be easily viewed simultaneously from multiple vantage points. In some variations, the optical waveguides described herein may comprise a surface texture including, for example, a multi-faceted surface configured to increase visibility from predetermined vantage points. For example, the first optical waveguide 320 may comprise a convex shape. The housing 312 may further comprise one or more illumination sources not shown coupled to the first optical waveguide 320. For example, the illumination source may be disposed at one or more of a left side, right side, proximal end, and distal end of the housing. The illumination source may be coupled to a power source through a robotic arm and configured to emit light using a predetermined combination of light output parameters , wavelength, frequency, intensity, pattern, duration, . For example, the illumination source may be controlled by a controller to emit a plurality of light patterns having different colors corresponding to different attachment states, as described herein. In some variations, a characteristic of the light , color, pattern, may correspond to an attachment state between at least two of the tool driver, the sterile adapter, and the surgical tool, as described for example with respect to 2. Non-limiting examples of an illumination source include incandescent, electric discharge , excimer lamp, fluorescent lamp, electrical gas-discharge lamp, plasma lamp, , electroluminescence , light-emitting diodes, organic light-emitting diodes, laser, , induction lighting, and fiber optics. In 3A, the illumination source may be disposed within the first housing 312 but in some variations may be disposed external to the first housing 312. In some variations, the illumination source may be disposed within a proximal end of the first housing 312. The first optical waveguide 320 may be configured to receive the light emitted by the illumination source , as a bezel or other suitable structure located over the illumination source. In some variations, the first optical waveguide 320 may be configured to emit a predetermined percentage of light received from the illumination source and propagate the remaining percentage of light to a second optical waveguide 340 of a sterile adapter 330 shown in 3B. For example, the first optical waveguide 320 may be configured to emit between about 10% and about 100% of light received from the illumination source. In some variations, the first optical waveguide 320 may be configured to emit about 33% of received light, and the second optical waveguide 340 may be configured to receive about 66% of the light emitted from the illumination source. In some other variations, the first optical waveguide 320 may be configured to emit about 50% of received light, and the second optical waveguide 340 may be configured to receive about 50% of the light emitted from the illumination source. In some variations, the first optical waveguide 320 may comprise one or more outputs configured to physically mate with corresponding inputs of the second optical waveguide 340 so as enable light transmission between the first and second optical waveguides 320, 340. For example, mating between an output of the first optical waveguide 320 and a corresponding input of the second optical waveguide 340 may be facilitated with complementary and corresponding features , latch, interlocking tabs, tab-and-slot alignment features, mateable ridge and groove interfaces, and/or other suitable mating features, . In some variations, one or more outputs of the first optical waveguide 320 may physically mate with corresponding inputs of the second optical waveguide 340 only when the tool driver and the sterile adapter are properly , fully and operationally engaged or attached to one another. For example, if the sterile adapter is only partially engaged or attached to the tool driver, the first and second optical waveguides 320, 340 may be misaligned, thereby reducing and/or preventing light propagation from the first optical waveguide 320 to the second optical waveguide 340, thereby providing a visual indicator to the operator that the sterile adapter 330 is not properly attached to the tool driver 310. When the sterile adapter 330 is attached to the tool driver 310, light emitted from an illumination source of the tool driver 310 may be propagated through the first and second optical waveguides 320, 340, as described in further detail below. In some variations, the first optical waveguide 320 may emit substantially all the light received from the illumination source. B. Sterile Adapter 3B is a perspective view of a sterile adapter 330 coupled to the tool driver 310. In some variations, the sterile adapter 330 may comprise a second optical waveguide 340 for communicating an attachment state of the system , sterile adapter using outputted light to a user such as an operator. As shown in 3B, the sterile adapter 330 may comprise a frame 332, a plate assembly 334 coupled to the frame 332, and at least one rotatable coupler 336 supported by the plate assembly 334. The plate assembly 334 may be configured to move up and down relative to the frame 332 within a predetermined range of motion, and the rotatable couplers 336 may be similarly configured to rotate and move up and down relative to the plate assembly 334. The sterile adapter 330 may be placed over the surgical tool sensors 314 and output drives 316. For example, when the sterile adapter 330 is attached to the tool driver 310 and surgical tool 350 as shown in 3C, the frame 332 may be configured to be interposed between the tool driver 310 and the surgical tool 350. The rotatable couplers 336 may be configured to communicate torque generated by the output drive 316 of the tool driver 310 to the surgical tool 350. The frame 332 may comprise a second optical waveguide 340. As shown in 3B, the second optical waveguide 340 may be disposed along an exterior surface of the frame 332 , along a lengthwise side portion of the sterile adapter 330, around at least a portion of the perimeter of the frame 332, . For example, the second optical waveguide 340 may include a strip extending at least partially onto a first side , left side of the frame 332 and at least partially onto a second side , right side of the frame 332. The strip may further extend at least partially onto a distal side of the frame 332 so as to couple the first and second sides of the second optical waveguide 340. As depicted, the second optical waveguide 340 may be flush with the frame 332. In some variations, the second optical waveguide 340 may be located on the frame 332 so as to be easily viewed simultaneously from multiple vantage points. The second optical waveguide 340 may be configured to receive light emitted from an output of the first optical waveguide 320. For example, the second optical waveguide 340 may be configured to receive light emitted from the tool driver 310 upon attachment of the sterile adapter 330 to the tool driver 310, such that the second optical waveguide 340 distributes light , becomes illuminated via light emitted from the tool driver 310 and propagated by the first optical waveguide 320 to the second optical waveguide 340. In some variations, the second optical waveguide 340 may be configured to emit a predetermined percentage of light received from the first optical waveguide 320 and propagate the remaining percentage of light to a third optical waveguide 360 of a surgical tool 350 shown in 3C. In some variations, the second optical waveguide 340 may be configured to emit about 33% of the light emitted from the illumination source, and the third optical waveguide 360 may be configured to emit about 33% of the light emitted from the illumination source. In some other variations, the second optical waveguide 340 may be configured to emit about 50% of the light emitted from the illumination source. In some variations, the second optical waveguide 340 may comprise one or more outputs configured to physically mate with corresponding inputs of a third optical waveguide 360 , in a manner similar to mating between the first and second optical waveguides, as described herein. For example, mating between an output of the second optical waveguide 340 and a corresponding input of the third optical waveguide 360 may be facilitated with complementary and corresponding features , interlocking tabs, tab-and-slot alignment features, mateable ridge and groove interfaces, and/or other suitable mating features, . In some variations, one or more outputs of the second optical waveguide 340 may physically mate with corresponding inputs of the third optical waveguide 360 only when the sterile adapter and surgical tool are properly , fully and operationally engaged or attached to one another. For example, if the surgical tool 350 is only partially engaged or attached to the sterile adapter 330, the second and third optical waveguides 340, 360 may be misaligned, thereby preventing light propagation from the second optical waveguide 340 to the third optical waveguide 360, thereby providing a visual indicator that the surgical tool 350 is not properly attached to the sterile adapter 330. When the surgical tool 350 is attached to the sterile adapter 330 and the sterile adapter 330 is attached to the tool driver 310, light emitted from an illumination source of the tool driver 310 may be propagated through the first, second, and third optical waveguides 320, 340, 360. In some variations, the second optical waveguide 340 may emit substantially all the light received from the first optical waveguide 320. In some variations, the sterile adapter 330 may comprise a second illumination source not shown such that the second optical waveguide 340 may be configured to receive the light emitted by the second illumination source. In some of these variations, the second optical waveguide 340 may receive light only from the second illumination source and not the first illumination source of the tool driver 310. In some variations, the second illumination source may be battery powered. In some variations, the second optical waveguide 340 may be configured to emit a predetermined percentage of light received from the second illumination source and propagate the remaining percentage of light to the third optical waveguide 360. For example, the second optical waveguide 340 may be configured to emit between about 10% and about 100% of light received from the second illumination source. In some other variations, the second optical waveguide 340 may be configured to emit about 50% of received light, and the third optical waveguide 360 may be configured to receive about 50% of the light emitted from the second illumination source. In some variations, the second optical waveguide 340 and second illumination source may be configured to propagate light to the first and third optical waveguides 320, 360. In some of these variations, the second optical waveguide 320 may be configured to emit about 33% of received light, and the first and third optical waveguides 320, 360 may be configured to each emit about 33% of the light emitted from the second illumination source. In some variations, the sterile adapter 330 may comprise a switch configured to activate the second illumination source upon physical attachment of the second optical waveguide 340 to at least a portion of the tool driver , a distal end of the tool driver, first optical waveguide, . The switch may be, for example, a conductive contact switch, mechanical contact switch , slide switch, and the like. Accordingly, the second optical waveguide 340 may emit light received from the second illumination source only when the tool driver 310 and the sterile adapter 330 are properly , fully and operationally attached engaged or attached to each other and the switch is activated. C. Surgical Tool 3C is a perspective view of a surgical tool 350 coupled to the sterile adapter 330 and tool driver 310. In some variations, the surgical tool 350 may comprise a third optical waveguide 360 for communicating an attachment state to an operator. As shown in 3C, the surgical tool 350 may comprise a second housing 352 configured to couple to the sterile adapter 330. The surgical tool 350 may comprise at least one input drive not shown supported by the second housing 352 and configured to receive the torque communicated from the output drive of the tool driver 310. The surgical tool 350 may further comprise an end effector not shown that may extend from the second housing 352 and be operatively coupled to at least one input drive. The second housing 352 may comprise a third optical waveguide 360 configured to receive light from the second optical waveguide 340. As shown in 3C, the third optical waveguide 360 may be disposed along an exterior surface of the second housing 352 , extending widthwise and perpendicular to the second optical waveguide 340 of the sterile adapter 330. For example, the third optical waveguide 360 may include a strip extending at least partially onto a first side , left side of the second housing 352 and at least partially onto a second side , right side of the second housing 352. The strip may further extend at least partially onto a third side , top side of the second housing 352 so as to couple the first and second sides of the third optical waveguide 360. The third optical waveguide 360 may be configured to receive light emitted from an output of the second optical waveguide 340 of the sterile adapter 330. For example, the third optical waveguide 360 may be configured to receive light emitted from the tool driver 310 upon attachment of the surgical tool 350 to the sterile adapter 330 when the sterile adapter 330 is attached to the tool driver 310, such that the third optical waveguide 360 distributes light , becomes illuminated via light emitted from the tool driver 310 and propagated by the first and second optical waveguides 320, 340 to the third optical waveguide 360. In other words, when the surgical tool 350 is attached to the sterile adapter 330 and the sterile adapter 330 is attached to the tool driver 310, light emitted from an illumination source of the tool driver 310 may be propagated through the first, second, and third optical waveguides 320, 340, 360. In some variations, the third optical waveguide 360 may emit substantially all the light received from the second optical waveguide 340. In some variations, the surgical tool 350 may comprise a third illumination source not shown such that the third optical waveguide 360 may be configured to receive the light emitted by the third illumination source. In some of these variations, the third optical waveguide 360 may receive light only from the third illumination source and not the first and/or second illumination sources. In some variations, the third illumination source may be battery powered or wirelessly powered using an electronic device as described in detail with respect to 8. In some variations, the third optical waveguide 360 may emit substantially all the light emitted from the third illumination source. In some variations, the third optical waveguide 360 may be configured to emit a predetermined percentage of light received from the third illumination source and propagate the remaining percentage of light to the first and/or second optical waveguides 320, 340. For example, the third optical waveguide 360 may be configured to emit between about 10% and about 100% of light received from the third illumination source. In some other variations, the third optical waveguide 360 may be configured to emit about 50% of emitted light, and the second optical waveguide 340 may be configured to emit about 50% of the light emitted from the third illumination source. In some variations, the third optical waveguide 360 and third illumination source may be configured to propagate light to the second and/or first optical waveguides 340, 320. In some of these variations, the third optical waveguide 360 may be configured to emit about 33% of emitted light, and the first and second optical waveguides 320, 340 may be configured to each emit about 33% of the light emitted from the third illumination source. In some variations, the surgical tool 350 may comprise a switch configured to activate the third illumination source upon physical attachment of the third optical waveguide 360 to at least a portion of the sterile adapter , a distal end of the sterile adapter, second optical waveguide, . The switch may be, for example, a conductive contact switch, mechanical contact switch , slide switch, and the like. Accordingly, the third optical waveguide 360 may emit light received from the third illumination source only when the surgical tool 350 and the sterile adapter 330 are properly , fully and operationally attached engaged or attached to each other and the switch is activated. In some variations, the optical waveguides of the tool driver 310, sterile adapter 330, and surgical tool 350 may be disposed along different portions of the system to aid identification of the attached device component. For example, the first optical waveguide 320 may be disposed at an end of the tool driver 310, the second optical waveguide 340 may be disposed along a length of the sterile adapter 330, and the third optical waveguide 360 may be disposed perpendicular to the second optical waveguide 340 and across a top of the surgical tool 350. In other variations, three optical waveguides may disposed respectively at a distal end, intermediate portion, and proximal end. Additionally or alternatively, one or more optical waveguides may be disposed along one or more of a robotic arm, display, surgical platform, or the like. For example, an optical waveguide disposed on one or more portions of a robotic arm may be configured to communicate an attachment state of the robotic arm to a tool driver. As another example, a surgical platform may comprise one or more optical waveguides disposed along a perimeter of a top surface of the platform and may be configured to communicate one or more of an operational state, attachment state, procedure state, or the like, of the robotic surgical system. Any of the optical waveguides as described herein may communicate a state of any of the components of the system. Surgical Tool SensorsThe tool driver as described herein may include one or more surgical tool sensors configured to generate a sensor signal corresponding to an attachment state , partial attachment, full attachment, partial detachment, full detachment, improper attachment between the tool driver and surgical tool. The surgical tool sensors described herein may be used to directly or indirectly determine a proximity of a surgical tool to the tool driver. The attachment state may be used to control the tool driver and/or notified to an operator. The surgical tool sensor may be, for example, a proximity sensor that may be used to determine a position of the surgical tool relative to the tool driver. For example, the surgical tool sensor may be disposed in one or more projections , cylindrical pegs configured to bias away from a surface of the tool driver housing and toward a surgical tool. When a sterile adapter is coupled to the tool driver for attachment, the projections may be configured to contact the sterile adapter to urge a plate assembly upward and away from the tool driver. When a surgical tool is attached to the sterile adapter, the surgical tool will urge the plate assembly toward the tool driver and reduce projection height. By placing a surgical tool sensor within one or more of the projections, a sensor signal may be generated that corresponds to a change in height of the projection and an attachment state between the tool driver and surgical tool. 4A-4E are illustrative views of an exemplary variation of a tool driver 400 configured to attach to a surgical tool via a sterile adapter. 4A is a plan view of the tool driver 400. The tool driver 400 may comprise a first housing 402 configured to attach to a surgical tool via a sterile adapter. The tool driver 400 may comprise one or more rotatable output drives 410 supported by the first housing 402 where the output drives 410 may be configured to communicate torque to an input drive of the surgical tool through a sterile adapter. One or more of the output drives 410 may comprise one or more of a torque sensor 440 and rotary output encoder 460 configured to generate one or more sensor signals used to determine a change in torque of the output drive 410. In some variations, a sterile adapter sensor may comprise at least one of the torque sensor 440 and rotary output encoder 460 4B and 4C. A change in torque may correspond to a change in engagement , attachment, detachment between a rotatable coupler of a sterile adapter and the rotatable output drive 410 of the tool driver 400. 4A depicts six output drives 410 arranged on a surface of the first housing 402 in a bilaterally symmetric arrangement. Similarly, four projections 420 may be arranged in a bilaterally symmetric arrangement and may also be disposed between pairs of the output drives 410. This arrangement may aid detection of lateral and/or longitudinal misalignment of one or more of the sterile adapter and tool driver to the tool driver 400. Sensor signals from each of the projections 420 may be used together to generate attachment data. Although 4A depicts a tool driver with six rotary output drives 410 and four projections 420, it should be understood that in other variations, the tool driver may include fewer or more output drives 410 and/or projections 420. Additionally or alternatively, the tool driver 400 may include at least one linear output drive , a drive providing an axially-moving output, such as described in patent application Ser. 15/803,659, filed on Nov. 3, 2017 and entitled TOOL DRIVER WITH LINEAR DRIVES FOR USE IN ROBOTIC SURGERY, which is incorporated herein in its entirety by reference. Although some variations of the tool driver 400 may comprise a single projection 420 having a surgical tool sensor 424, a plurality of spaced apart first surgical tool sensors 424 may allow the system to determine an orientation of a plate assembly relative to the sterile adapter frame. That is, knowledge of an attachment state between the sterile adapter and tool driver 400 may be improved by using a plurality of first surgical tool sensors. For example, a surgical tool may be improperly attached to a sterile adapter and tool driver 400 when fewer than four projections 420 are pressed down to a predetermined height. If only three projections are urged downward with the fourth projection in a higher position, then the sensor signal output by the first surgical tool sensors 424 may correspond to an improper attachment state where the surgical tool is askew relative to the sterile adapter. In this position, one or more of the rotatable couplers 410 may be unable to communicate torque to the input drive of the surgical tool. 4B-4C are cross-sectional side views of the tool driver 400 along the H-H lines depicted in 4A. The tool driver 400 may comprise at least one rotatable output drive 410 supported by the first housing 402 and at least one projection 420 extending from a surface of the first housing 402 and configured to bias away from the surface, as shown in 4B. In some variations, the projection 420 may comprise a first surgical tool sensor 424 configured to generate a sensor signal comprising at least one attachment state between the tool driver 400 and the surgical tool. In some variations, the first surgical tool sensor 424 may be a proximity sensor configured to determine a proximity of first end of the projection 420 relative to the second end of the projection 420. As shown in 4D and 4E, the proximity sensor may comprise a magnet 422 coupled to a first end of the projection 420 disposed on an exterior side of the first housing 402 and a magnetic field transducer 425 coupled to a second end of the projection 420 disposed on an interior side of the first housing 402. In some variations, the magnetic field transducer may be an analog sensor. In some variations, the projection 420 may comprise a complaint material 426 configured to bias the first end of the projection 420 away from the first housing 402. For example, the compliant material 426 may be a coil spring 414 4D and 4E coupled between the first end of the projection 420 and a surface of the first housing 402. In other variations, the projection 420 may comprise a leaf spring. 4A, 4B, and 4D illustrate the projections 420 in a first configuration where the projection 420 is fully biased away from a surface of the housing 402. For example, in the first configuration, the projection 420 is not in contact with either of the sterile adapter or the surgical tool. The first configuration of the projection 420 corresponds to, for example, a detachment state between the tool driver, sterile adapter, and surgical tool. 4A, 4C, and 4E illustrate the projections 420 in a second configuration where the projection 420 is fully retracted toward the surface of the first housing 402 due to a compressive force such as from attachment of a surgical tool and sterile adapter not shown. The second configuration of the projection 420 corresponds to, for example, an attachment state of the sterile adapter and/or surgical tool. In some variations, an attachment state may correspond to a position of the projection 420 over time. For example, when the first surgical tool sensor 420 is in a first or second configuration for a predetermined amount of time, the sensor signal may correspond to either a detached state or attached state. A sterile adapter may be in a partial attachment state when the projection 420 transitions quickly from the first configuration to the second configuration and back to the first configuration. For example, an operator may attach a sterile adapter to a tool driver by rotating the sterile adaptor over the projections 420 to urge the projection toward the second configuration. Once the frame of the sterile adapter is latched into the tool driver, then the projections 420 may be biased toward the first configuration. In some variations, a proximity sensor of a projection may comprise a Hall effect sensor. The magnet may be made of any suitable material or combination of materials. For example, in some variations, the magnet may be a permanent magnet, ferromagnetic magnetic, and paramagnetic magnet, and may be made from aluminum, platinum, iron, nickel, cobalt, copper, titanium, alloys or combinations thereof, or the like. In some variations, a tool driver may comprise at least one second surgical tool sensor configured to directly sense a location of a surgical tool used for generating a sensor signal corresponding to an attachment state between the tool driver and surgical tool. 5A is a plan view of a tool driver 510 comprising a first housing 512, a set of output drives 514, and a set of projections 516 disposed on a surface of the first housing 512, a sterile adapter sensor 518 disposed on a distal end of the first housing 512, and a proximal end 522 of the first housing 512. In some variations, an electronic device not shown may be depicted within the proximal end 522. 5B is a cross-sectional side view of the tool driver 500 along the M-M line depicted in 5A. 5B depicts a second surgical tool sensor 520 disposed within a distal end of the first housing 512. In some variations, an attachment state may correspond to a position of the projection 420 over time. For example, when the first surgical tool sensor 424 is in a first or second configuration for a predetermined amount of time, the sensor signal may correspond to either a detached state or attached state. A sterile adapter may be in a partial attachment state when the projection 420 transitions quickly from the first configuration to the second configuration and back to the first configuration. For example, an operator may attach a sterile adapter to a tool driver by rotating the sterile adaptor over the projections 420 to urge the projection toward the second configuration. Once the frame of the sterile adapter is latched into the tool driver, then the projections 420 may be biased toward the first configuration. 5C is a detailed cross-sectional side view of the tool driver 510 depicted in 5B and shows a distal end of the surgical tool 530 within sensor range of the second surgical tool sensor 520. In 5C, a second surgical tool sensor 520 may be disposed within a distal end of the first housing 512 of the tool driver 510. In some variations, the second surgical tool sensor 520 may comprise a proximity sensor. For example, the proximity sensor may be a magnetic field transducer such as a Hall effect sensor. Opposite the second surgical tool sensor 520, a distal end of the surgical tool 530 may comprise a second housing 532 configured to attach to a sterile adapter not shown for the sake of clarity. The second housing 532 of the surgical tool 530 may comprise a magnetic projection 542 configured to mate with a corresponding recess in a sterile adapter. The magnetic projection 542 may comprise a magnet as described herein. The magnetic projection 542 may comprise a first tapered surface 544 and a second tapered surface 546 opposite the first tapered surface 544. A sterile adapter engagement feature 540 may comprise the distal end of the surgical tool 530 including the magnetic projection 542. The magnetic projection 542 may extend from a surface of the surgical tool 530 and be configured to slide over portions of the sterile adapter and be placed within a recess of the sterile adapter, as described in more detail with respect to 6A and 6B. As discussed herein, the surgical tool 530 may comprise at least one input drive supported by a housing of the surgical tool 530 and may be configured to receive torque communicated from an output drive 514 of the tool driver 510 through the sterile adapter. For example, the tool drive may further include a torque sensor 560 and rotary output encoder 570. An end effector not shown may extend from the surgical tool housing and be operatively coupled to the input drive. 6A-6B are cross-sectional side views of a sterile adapter 610 and a surgical tool 620 comprising corresponding engagement features for mating the surgical tool 620 to the sterile adapter 610 in a desired orientation. For example, one or more engagement features may be configured to prevent an operator from attaching a distal end of the surgical tool to a proximal end of the sterile adapter. In some variations, a sterile adapter engagement feature 640 of the surgical tool 620 may comprise a magnetic projection 642 see 6B comprising a magnet as described herein and configured to be directly sensed by a second surgical tool sensor 690 of the tool driver. For example, the second surgical tool sensor 690 may comprise at least one of an inductive sensor, optical sensor, magnetic sensor, conductive contact switch, and/or mechanical contact switch. As depicted in 6A, a surgical tool 620 may comprise a sterile adapter engagement feature 640 that may protrude from a surface of the surgical tool 620 and be configured to engage with a surface of a sterile adapter 610. The sterile adapter engagement feature 640 and surgical tool engagement feature 650 may be disposed at respective distal ends of the surgical tool 620 and sterile adapter 610. The sterile adapter 610 may comprise a plate assembly 680 coupled to a frame 670, and a surgical tool engagement feature 650 disposed in the plate assembly 680 and configured to mate with the sterile adapter engagement feature 640. In some variations, a proximal end of the surgical tool 620 may comprise a projection configured to support an electronic communication device of the surgical tool 620, as discussed in more detail with respect to 8. 6B is a detailed cross-sectional view of a distal end of the surgical tool 620 and sterile adapter 610. As discussed herein, the sterile adapter engagement feature 640 may project from a surface of the sterile adapter 620. In some of these variations, the sterile adapter engagement feature 640 may comprise a magnetic projection 642 comprising a first tapered surface 644 and a second tapered surface 646 opposite the first tapered surface 644. The tapered surfaces 644, 646 may be angled to allow the surgical tool 620 to slide over one or more portions of the sterile adapter 610 until the magnetic projection 642 mates with , slides into the surgical tool engagement feature 650. For example, the sterile adapter engagement feature 640 may comprise a trapezoidal shape. The surgical tool engagement feature 650 may comprise a recess configured to hold the sterile adapter engagement feature 640 and limit movement of the surgical tool 620 relative to the sterile adapter 610. The recess may be distal to an output drive disc 682. A second surgical tool sensor 690 of a tool driver, as described herein, may overlap , disposed below the surgical tool engagement feature 650. In some variations, the surgical tool 620 and sterile adapter 610 may comprise a plurality of spaced-apart engagement features. In some variations, the surgical tool engagement feature 640 may comprise a recess while the sterile adapter 650 may comprise a projection. Sterile Adapter SensorIn some variations, a tool driver may include at least one sterile adapter sensor configured to generate a sensor signal when the sterile adapter is fully attached to the tool driver. For example, the sterile adapter sensor may generate the sensor signal when the sterile adapter is physically latched onto a distal portion of the tool driver. The sensor signal may correspond to an attachment state , full attachment, full detachment between the tool driver and sterile adapter. The tool driver and sterile adapter may each be configured so as to allow only one-way engagement between the sterile adapter and tool driver. That is, the distal end of the sterile adapter will not engage , latch into with the tool driver unless the proximal ends of the sterile adapter and tool driver are mated to each other. The attachment state may be used to control the tool driver and/or notify an operator of the attachment state of the system. The sterile adapter sensor may be, for example, a switch sensor configured to generate a sensor signal upon physical contact with the sterile adapter. In some variations, as shown in 4A, the first housing 402 may comprise a sterile adapter sensor 430 disposed at a distal end of the first housing 402. The sterile adapter sensor 430 may be configured to generate a sensor signal corresponding to an attachment state between the tool driver 400 and a sterile adaptor not shown for clarity. Variations of a sterile adapter sensor are described in more detail herein with respect to 7A-7C. In some variations, a proximal end 470 of the first housing 402 may be configured to support an electronic communication device of the tool driver 400, as described in more detail herein with respect to 8. 7A is a plan view of a tool driver 710 coupled to a sterile adapter 720. The sterile adapter 720 may comprise a plate assembly 724 coupled to a frame 722. The frame 722 may be configured to be interposed between the tool driver 710 and surgical tool not shown for clarity. A set of rotatable couplers 726 may be supported by the plate assembly 724 and configured to communicate torque from an output drive 714 of the tool driver 710 to the surgical tool. 7B-7C are cross-sectional side views of the sterile adapter 720 and tool driver 710 along the K-K line in 7A. The tool driver 710 may comprise a housing 712 configured to couple to the sterile adapter 720. The housing 712 may comprise a sterile adapter engagement feature 740 mateable with a corresponding tool driver engagement feature 750 on the sterile adapter 720. A sterile adapter sensor 730 may be coupled to the sterile adapter engagement feature 740 and configured to generate a sensor signal when the tool driver engagement feature 750 is mated with its corresponding sterile adapter engagement feature 740. That is, the sterile adapter sensor 730 is disposed at a location that does not generate a sensor signal until the sterile adapter is securely attached to the tool driver. In some variations, the sterile adapter engagement feature 740 and sterile adapter sensor 730 may each be disposed on a distal end of the tool driver 710 on a side of a tool driver housing perpendicular to the rotatable output drive 714. As shown in 7C, the sterile adapter engagement feature 740 may comprise a projection, and the tool driver engagement feature 750 may comprise a recess. The projection may be configured , tapered to allow the sterile adapter 720 to slide over the projection. The projection may then mate with the recess of the tool driver engagement feature 750. For example, the projection of the sterile adapter engagement feature 740 may comprise a tapered surface on one side and a flat surface opposite the tapered surface. The recess of the tool driver engagement feature 750 may be configured to limit movement of the sterile adapter 720 relative to the tool driver 710. In some variations, a distal portion of the frame 722 projecting perpendicularly from a plane of the plate assembly 724 and comprising the tool driver engagement feature 750 may comprise a compliant material that may aid an operator in mating the sterile adapter to the tool driver using respective engagement features. In some variations, the tool driver 710 and sterile adapter 720 may comprise a plurality of spaced-apart engagement features. In some variations, the sterile adapter engagement feature 740 may comprise a recess while the tool driver engagement feature 740 may comprise a projection. Placing the sterile adapter engagement feature 740 and tool driver engagement feature 750 further away from the rotatable plate assembly 724 may help ensure that mating of the engagement features 740, 750 corresponds to proper attachment of the sterile adapter 720 to the tool driver 710. Accordingly, it is preferable for the engagement features 740, 750 to be the final portions of the sterile adapter 720 and tool driver 710 that couple to each other when the sterile adapter 720 is attached to the tool driver 720. In some variations, the sterile adapter sensor 730 may comprise a proximity sensor configured to detect attachment between the sterile adapter engagement feature 740 and tool driver engagement feature 750. For example, the proximity sensor may comprise at least one of a conductive contact switch, mechanical contact switch , slide switch, Hall Effect sensor, forces sensor, optical sensor, combinations thereof, or the like. In 7C, the sterile adapter sensor 730 may comprise a switch 732 including a torsion spring configured to bias the switch 732 to an initial, reset position. In some variations, the switch 732 may be disposed adjacent to the sterile adapter engagement feature 740 such that the switch 732 is depressed when the engagement features 740, 750 mate. Electronic DeviceIn some variations, at least one of the tool driver and surgical tool may include one or more electronic communication devices configured to transmit data to each other. Generally, a tool driver and surgical tool may include respective communication devices in close proximity to each other when the surgical tool is attached to the sterile adapter and tool driver. Communication performance may depend at least in part on placement of the communication devices within the tool driver and surgical tool. For example, minimizing the distance between the electronic communication devices may improve one or more of signal-to-noise ratio SNR and power efficiency. In some variations, the electronic devices may comprise a wireless power transfer system. A. Tool DriverIn some variations, a tool driver may comprise an electronic communication device configured to communicate wirelessly with a corresponding electronic communication device of a surgical tool. This may allow the system to perform a number of functions with the surgical tool. For example, the tool driver may communicate with the surgical tool to identify and authenticate the surgical tool, determine compatibility, download tool usage information , log data, configure settings of the surgical tool, communicate calibration data, or the like. 8 is a cross-sectional side view of a variation of a tool driver 810 coupled to a surgical tool 850 through a sterile adapter 840. As shown in 8, the tool driver 810 may comprise a first housing 812 configured to attach to a surgical tool 850 via a sterile adapter 840. The tool driver 810 may further comprise at least one output drive 822 coupled to a corresponding rotatable output drive disk 820 each supported by the first housing 812. The output drive 822 may be configured to communicate torque to an input drive not shown of the surgical tool 850 through the sterile adapter 840. The first housing 812 may comprise a first electronic communication device 830 disposed substantially in a plane of the output drive disk 820. The first electronic communication device 830 may be configured to wirelessly communicate with the surgical tool 850. In some variations, a proximal end of the first housing 812 may be configured to support the first communication device 814 in a first communication portion 814. For example, the proximal end of the first housing 812 may comprise a projection , first communication portion 814. The output drive disk 820 and first communication portion 814 may have substantially the same height. In some variations, an electronic communication device may comprise a wireless communication board comprising radiofrequency RF circuitry , RF transceiver including one or more of a receiver, transmitter, and/or optical , infrared receiver and transmitter. RF circuitry may receive and transmit RF signals , electromagnetic signals from the surgical tool and other devices. The RF circuitry converts between electrical signals and electromagnetic signals and communicates with other communications devices using the electromagnetic signals. The RF circuitry may include one or more of an antenna system, an RF transceiver, one or more amplifiers, a tuner, one or more oscillators, a digital signal processor, a CODEC chipset, a subscriber identity module SIM card, memory, and the like. Short-range wireless communication using the electronic communication devices may use one or more communications standards, protocols and technologies including but not limited to Bluetooth, near-field communication NFC, and radio-frequency identification RFID. In some variations, the electronic communication device may be powered by a battery. B. Sterile AdapterIn some variations, a sterile adapter may be configured to be interposed between a tool driver and surgical tool and formed in such a manner as to minimize a distance between their respective electronic communication devices. As shown in 8, the sterile adapter 840 may comprise a frame 842 configured to be interposed between the tool driver 810 and surgical tool 850. The frame 842 may be coupled to a plate assembly not shown in 8. The frame 842 of the sterile adapter 840 may comprise a communication portion 846 configured to support a projection 854 of the surgical tool 850. The projection 854 may be substantially in a plane of the plate assembly when the surgical tool 850 is attached to the sterile adapter 840 and the plate assembly is biased toward the tool driver 810. Accordingly, when a second electronic communication device 832 is disposed in the projection 854, the second electronic communication device 832 may be substantially in a plane of the plate assembly. At least one rotatable coupler 844 may be supported by the plate assembly and configured to communicate torque from the output drive 822 of the tool driver 810 to the input drive of the surgical tool 850. As shown in 8, a proximal end of the frame 842 may comprise the communication portion 846. In some variations, the communication portion 846 may attach to a proximal end of the first communication portion 814. The communication portion 846 may further support the projection 854 of the surgical tool housing 852. In some variations, the communication portion 846 of the frame 842 may be thinner than other portions of the frame 842 in order to reduce a distance between the communication devices 830, 832. In some variations, the frame 842 of the sterile adapter 840 may be formed without the communication portion 846 to allow the proximal ends of the tool driver 810 and surgical tool 850 to be brought closer together. C. Surgical ToolIn some variations, a surgical tool may comprise an electronic communication device configured to communicate wirelessly with a corresponding electronic communication device of a tool driver. This may allow the surgical tool to perform a number of functions. For example, the surgical tool may communicate with the tool driver to identify and authenticate the tool driver and/or surgical system, determine compatibility, communicate calibration data, or the like. As shown in 8, the surgical tool 850 may comprise a second housing 852 configured to couple to the sterile adapter 840. The second housing 852 may comprise a projection. In some variations, an end effector not shown may extend from the second housing 852 and be operatively coupled to an input drive of the surgical tool 850. The input drive may be supported by the second housing 852 and be configured to receive torque communicated from the output drive 822 of the tool driver 810 through the sterile adapter 840. The second housing 852 may comprise a second communication device 832 configured to wirelessly communicate with the tool driver 810. The second communication device 832 may be disposed in the projection 854. The second communication device 832 may be of the same or different configuration as the first communication device 830 described herein. In some variations, a proximal end of the surgical tool 850 may comprise the projection 854. For example, the projection 854 of the surgical tool 850 may be configured to support the second communication device 832. The output drive disk 830 and proximal end of the first housing 812 may have substantially the same height. One or more of the projection 854 and second communication device 832 may be disposed in substantially the same plane as the rotatable coupler 844. By providing the second electronic communication device 832 within the projection 854 that extends away from the surgical tool 850, the internal configuration of the surgical tool need not be modified to accommodate the electronic communication device. In some variations, the projection 854 having the second communication device 832 may be removably attached from the surgical tool 850. For example, a surface of the projection 854 facing the surgical tool 850 may comprise one or more fasteners , hooks configured to couple to one or more of an underside of the surgical tool 850 and the frame 842 of the sterile adapter 840. In some variations, when the surgical tool 850 and sterile adapter 840 are attached to the tool driver 810, a distance between the first and second communication devices 830, 832 may be less than about 10 mm. In some variations, the distance between the first and second communication devices 830, 832 may be less than about 6 mm. In some variations, the distance between the first and second communication devices 830, 832 may be between about 3 mm and about 6 mm. Generally, the plate assembly of the sterile adapter 840 may move relative to the frame 842 by more than about 5 mm. These short distances between the electronic communication devices may enable wireless power transfer to one or more devices of the surgical tool, such as the communication device and/or sensor. In some variations, the electronic devices may transfer power using one or more of inductive coupling and capacitive coupling. III. SystemsGenerally, the robotic surgical systems described herein may include a robotic arm and corresponding control system coupled to a tool driver, sterile adapter, and surgical tool. In some variations, a tool driver may comprise one or more sensors configured to generate sensor signals. Those signals may be received by a controller and used to generate attachment data corresponding to an attachment state between the tool driver, sterile adapter, and surgical tool. The control system may accordingly control one or more of the robotic arm and tool driver using the attachment data. As described in more detail herein, the controller may be coupled to one or more networks using a network interface. The controller may include a processor and memory coupled to a communication interface comprising a user interface. The controller may automatically perform one or more steps of a sterile barrier formation process, and thus improve a surgical tool switching process and reduce operator error by following a proper attachment sequence for engaging the sterile adapter and surgical tool to the tool driver. 9A-9B are block diagrams of a variation of a robotic surgical system 900. The system 900 may comprise a control system 920 configured to control one or more of a robotic arm 910, tool driver 912, sterile adapter 914, and surgical tool 916. The robotic arms 910 may be located at a surgical platform , table, bed, having attached at a distal end one or more of a tool driver 912, sterile adapter 914, and surgical tool 916 , end effector. The robotic arm 910 may include a plurality of links that are actuated so as to position and orient the tool driver 912. The robotic arms 916 may be mounted on a table, in a cart, ceiling, sidewall, or other suitable support surface. In some variations, the system 900 may include one or more sensors configured to generate sensor signals corresponding to an attachment state between two or more of the tool driver 912, sterile adapter 914, and surgical tool 916. For example, the sensors may be disposed in the tool driver and configured to sense one or more of a presence, engagement, and/or attachment of the tool driver to the sterile adapter and surgical tool. The tool driver 912, sterile adapter 914, and surgical tool 916 may be coupled to the control system 920 through one or more wired or wireless communication channels. Any wired connections may be optionally built into the floor and/or walls or ceiling. The control system 920 may be coupled to one or more networks 970, databases 940, and/or servers 950. The network 970 may comprise one or more databases 940 and servers 950. In some variations, a remote operator not shown may be coupled one or more networks 970, databases 940, and servers 950 through a user console 960 , surgeon bridge. In some variations, one or more of the tool driver 912 and surgical tool 916 may be coupled directly to any of the network 970, database 940, server 950, or each other. Processing, data generation, and analysis may be performed at any one of the devices of the system 900 or distributed throughout a plurality of devices. A user such as a surgeon or other operator may use the user console 960 to remotely manipulate the robotic arms 910 and/or surgical tools 916 , tele-operation. The user console 960 may be located in the same procedure room as the robotic surgical system 900, in an adjacent or nearby room, or tele-operated from a remote location in a differently building, city, country, etc. In some variations, a plurality of user consoles 960 may be provided, for example to control additional surgical tools, and/or to take control of one or more surgical tools at a primary user console. The will permit, for example, a surgeon to take over or illustrate a technique during a surgical procedure with medical students and physicians-in-training, or to assist during complex surgeries requiring multiple surgeons acting simultaneously or in a coordinated manner. Control SystemThe tool drivers, sterile adapters, and surgical tools as described herein may couple to one or more control systems , computer systems and/or networks. 9B is a block diagram of the control system 920. The control system 920 may comprise a controller 922 comprising a processor 924 and a memory 926. In some variations, the control system 920 may further comprise one or more of a communication interface 930. The controller 922 may be coupled to the communication interface 930 to permit an operator to remotely control the control system 920, robotic arm 910, tool driver 912, surgical tool 916, sensors, and any other component of the system 900. The communication interface 930 may comprise a network interface 932 configured to connect the control system 920 to another system , Internet, remote server, database over a wired and/or wireless network. The communication interface 930 may further comprise a user interface 934 configured to permit an operator to directly control the control system 920. A. ControllerA control system 920, as depicted in 9B, may comprise a controller 922 in communication with the robotic surgical system 900 , robotic arm 910, tool driver 912, surgical tool 916. The controller 920 may comprise one or more processors 924 and one or more machine-readable memories 926 in communication with the one or more processors 924. The processor 924 may incorporate data received from memory 926 and operator input to control the system 900. The memory 926 may further store instructions to cause the processor 924 to execute modules, processes and/or functions associated with the system 900. The controller 922 may be connected to one or more of the robotic arm 910, tool driver 912, and surgical tool 916 by wired and/or wireless communication channels. The controller 922 may be configured to control one or more components of the system 900, such as robotic arm 910, tool driver 912, surgical tool 916, communication interface 930, and the like. The controller 922 may be implemented consistent with numerous general purpose or special purpose computing systems or configurations. Various exemplary computing systems, environments, and/or configurations that may be suitable for use with the systems and devices disclosed herein may include, but are not limited to software or other components within or embodied on a surgeon bridge, servers or server computing devices such as routing/connectivity components, multiprocessor systems, microprocessor-based systems, distributed computing networks, personal computing devices, network appliances, portable , hand-held or laptop devices. Examples of portable computing devices include smartphones, personal digital assistants PDAs, cell phones, tablet PCs, wearable computers taking the form of smartwatches and the like, and portable or wearable augmented reality devices that interface with the operator's environment through sensors and may use head-mounted displays for visualization, eye gaze tracking, and user input. i. ProcessorThe processor 924 may be any suitable processing device configured to run and/or execute a set of instructions or code and may include one or more data processors, image processors, graphics processing units, physics processing units, digital signal processors, and/or central processing units. The processor 924 may be, for example, a general purpose processor, Field Programmable Gate Array FPGA, an Application Specific Integrated Circuit ASIC, or the like. The processor 924 may be configured to run and/or execute application processes and/or other modules, processes and/or functions associated with the system and/or a network associated therewith. The underlying device technologies may be provided in a variety of component types including metal-oxide semiconductor field-effect transistor MOSFET technologies like complementary metal-oxide semiconductor CMOS, bipolar technologies like emitter-coupled logic ECL, polymer technologies , silicon-conjugated polymer and metal-conjugated polymer-metal structures, mixed analog and digital, combinations thereof, or the like. ii. MemoryIn some variations, the memory 926 may include a database not shown and may be, for example, a random access memory RAM, a memory buffer, a hard drive, an erasable programmable read-only memory EPROM, an electrically erasable read-only memory EEPROM, a read-only memory ROM, Flash memory, combinations thereof, or the like. As used herein, database refers to a data storage resource. The memory 926 may store instructions to cause the processor 924 to execute modules, processes, and/or functions associated with the control system 920, such as sterile barrier formation, notification, robotic arm control, tool driver control, surgical tool control, sensor control, sensor signal processing, communication, authentication, or user settings, or the like. In some variations, storage may be network-based and accessible for one or more authorized users. Network-based storage may be referred to as remote data storage or cloud data storage. Sensor signal and attachment data stored in cloud data storage , database may be accessible to respective users via a network, such as the Internet. In some variations, database 940 may be a cloud-based FPGA. Some variations described herein relate to a computer storage product with a non-transitory computer-readable medium also may be referred to as a non-transitory processor-readable medium having instructions or computer code thereon for performing various computer-implemented operations. The computer-readable medium or processor-readable medium is non-transitory in the sense that it does not include transitory propagating signals per se , a propagating electromagnetic wave carrying information on a transmission medium such as space or a cable. The media and computer code also may be referred to as code or algorithm may be those designed and constructed for a specific purpose or purposes. Examples of non-transitory computer-readable media include, but are not limited to, magnetic storage media such as hard disks, floppy disks, and magnetic tape; optical storage media such as Compact Disc/Digital Video Discs CD/DVDs; Compact Disc-Read Only Memories CD-ROMs; holographic devices; magneto-optical storage media such as optical disks; solid state storage devices such as a solid state drive SSD and a solid state hybrid drive SSHD; carrier wave signal processing modules; and hardware devices that are specially configured to store and execute program code, such as Application-Specific Integrated Circuits ASICs, Programmable Logic Devices PLDs, Read-Only Memory ROM, and Random-Access Memory RAM devices. Other variations described herein relate to a computer program product, which may include, for example, the instructions and/or computer code disclosed herein. The systems, devices, and methods described herein may be performed by software executed on hardware, hardware, or a combination thereof. Hardware modules may include, for example, a general-purpose processor or microprocessor or microcontroller, a field programmable gate array FPGA, an application specific integrated circuit ASIC, or the like. Software modules executed on hardware may be expressed in a variety of software languages , computer code, including C, C++, Java, Python, Ruby, Visual Basic, and/or other object-oriented, procedural, or other programming language and development tools. Examples of computer code include, but are not limited to, micro-code or micro-instructions, machine instructions, such as produced by a compiler, code used to produce a web service, and files containing higher-level instructions that are executed by a computer using an interpreter. Additional examples of computer code include, but are not limited to, control signals, encrypted code, and compressed code. B. Communication InterfaceThe communication interface 930 may permit an operator to interact with and/or control the system 900 directly and/or remotely. For example, a user interface 934 of the system 900 may include an input device for an operator to input commands and an output device for an operator and/or other users , technicians to receive output , view patient data on a display device related to operation of the system 900. In some variations, a network interface 932 may permit the control system 920 to communicate with one or more of a network 970 , Internet, remote server 950, and database 940 as described in more detail herein. i. User InterfaceUser interface 934 may serve as a communication interface between a user , operator and the control system 920. In some variations, the user interface 934 may comprise an input device and output device , touch screen and display and be configured to receive input data and output data from one or more sensors, input device, output device, network 970, database 940, and server 950. For example, sensor signals generated by a sterile adapter sensor and surgical tool sensor may be processed by processor 924 and memory 926, and output visually by one or more output devices , optical waveguides. Sensor signals and/or attachment data may be received by user interface 934 and output visually, audibly, and/or through haptic feedback through one or more output devices. As another example, operator control of an input device , joystick, keyboard, touch screen may be received by user interface 934 and then processed by processor 924 and memory 926 for user interface 934 to output a control signal to one or more of the robotic arm 910, tool driver 912, and surgical tool 916. In some variations, the user interface 934 may function as both an input and output device , a handheld controller configured to generate a control signal while also providing haptic feedback to an operator. In some variations, the devices, systems, and methods comprise one or more elements described in patent application Ser. 15/712,052, filed on Sep. 21, 2017, and titled USER CONSOLE SYSTEM FOR ROBOTIC SURGERY, which is hereby incorporated by reference in its entirety. 1. Output DeviceAn output device of a user interface 934 may output sensor data corresponding to a patient and/or system 900, and may comprise one or more of an optical waveguide, display device, audio device, and haptic device. The display device may be configured to display a graphical user interface GUI. The user console 960 may include an integrated display and/or video output that may be connected to output to one or more generic displays, including remote displays accessible via the internet or network. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. A display device may permit an operator to view procedure data, attachment data, system data, tool data, patient data, and/or other data processed by the controller 922. In some variations, an output device may comprise a display device including at least one of a light emitting diode LED, liquid crystal display LCD, electroluminescent display ELD, plasma display panel PDP, thin film transistor TFT, organic light emitting diodes OLED, electronic paper/e-ink display, laser display, and/or holographic display. An audio device may audibly output patient data, tool data, attachment data, sensor data, system data, alarms and/or warnings. For example, the audio device may output an audible warning when improper attachment occurs between the tool driver, sterile adapter, and surgical tool. In some variations, an audio device may comprise at least one of a speaker, piezoelectric audio device, magnetostrictive speaker, and/or digital speaker. In some variations, an operator may communicate with other users using the audio device and a communication channel. A haptic device may be incorporated into one or more of the input and output devices to provide additional sensory output , force feedback to the operator. For example, a haptic device may generate a tactile response , vibration to confirm operator input to an input device , joystick, keyboard, touch surface. In some variations, the haptic device may include a vibrational motor configured to provide haptic tactile feedback to a user. Haptic feedback may in some variations confirm attachment and detachment of the sterile adapter or surgical tool to the tool driver. Additionally or alternatively, haptic feedback may notify that operation of the tool driver is inhibited from driving an output drive due to improper attachment and/or detachment in order to prevent potential harm to the operator and/or system. In some variations, the devices, systems, and methods comprise one or more elements described in Patent Application Ser. 62/432,538, filed on Dec. 9, 2016, and titled USER INTERFACE DEVICES FOR USE IN ROBOTIC SURGERY, which is hereby incorporated by reference in its entirety. 2. Input DeviceSome variations of an input device may comprise at least one switch configured to generate a control signal. In some variations, the input device may comprise a wired and/or wireless transmitter configured to transmit a control signal to a wired and/or wireless receiver of a controller 922. For example, an input device may comprise a touch surface for an operator to provide input , finger contact to the touch surface corresponding to a control signal. An input device comprising a touch surface may be configured to detect contact and movement on the touch surface using any of a plurality of touch sensitivity technologies including capacitive, resistive, infrared, optical imaging, dispersive signal, acoustic pulse recognition, and surface acoustic wave technologies. In variations of an input device comprising at least one switch, a switch may comprise, for example, at least one of a button , hard key, soft key, touch surface, keyboard, analog stick , joystick, directional pad, pointing device , mouse, trackball, jog dial, step switch, rocker switch, pointer device , stylus, motion sensor, image sensor, and microphone. A motion sensor may receive operator movement data from an optical sensor and classify an operator gesture as a control signal. A microphone may receive audio and recognize an operator voice as a control signal. ii. Network InterfaceAs depicted in 9A, a control system 920 described herein may communicate with one or more networks 970 and computer systems 950 through a network interface 932. In some variations, the control system 920 may be in communication with other devices via one or more wired and/or wireless networks. The network interface 932 may facilitate communication with other devices over one or more external ports , Universal Serial Bus USB, multi-pin connector configured to couple directly to other devices or indirectly over a network , the Internet, wireless LAN. In some variations, the network interface 932 may comprise a radiofrequency receiver, transmitter, and/or optical , infrared receiver and transmitter configured to communicate with one or more devices and/or networks. The network interface 932 may communicate by wires and/or wirelessly with one or more of the sensors, user interface 934, network 970, database 940, and server 950. In some variations, the network interface 930 may comprise radiofrequency RF circuitry , RF transceiver including one or more of a receiver, transmitter, and/or optical , infrared receiver and transmitter configured to communicate with one or more devices and/or networks. RF circuitry may receive and transmit RF signals , electromagnetic signals. The RF circuitry converts electrical signals to/from electromagnetic signals and communicates with communications networks and other communications devices via the electromagnetic signals. The RF circuitry may include one or more of an antenna system, an RF transceiver, one or more amplifiers, a tuner, one or more oscillators, a digital signal processor, a CODEC chipset, a subscriber identity module SIM card, memory, and the like. A wireless network may refer to any type of digital network that is not connected by cables of any kind. Examples of wireless communication in a wireless network include, but are not limited to cellular, radio, satellite, and microwave communication. The wireless communication may use any of a plurality of communications standards, protocols and technologies, including but not limited to Global System for Mobile Communications GSM, Enhanced Data GSM Environment EDGE, high-speed downlink packet access HSDPA, wideband code division multiple access W-CDMA, code division multiple access CDMA, time division multiple access TDMA, Bluetooth, near-field communication NFC, radio-frequency identification RFID, Wireless Fidelity Wi-Fi , IEEE 802. 11a, IEEE 802. 11b, IEEE 802. 11g, IEEE 802. 11n, Voice over Internet Protocol VoIP, Wi-MAX, a protocol for email , Internet Message Access Protocol IMAP, Post Office Protocol POP, instant messaging , eXtensible Messaging and Presence Protocol XMPP, Session Initiation Protocol for Instant Messaging, Presence Leveraging Extensions SIMPLE, Instant Messaging and Presence Service IMPS, Short Message Service SMS, or any other suitable communication protocol. Some wireless network deployments combine networks from multiple cellular networks or use a mix of cellular, Wi-Fi, and satellite communication. In some variations, a wireless network may connect to a wired network in order to interface with the Internet, other carrier voice and data networks, business networks, and personal networks. A wired network is typically carried over copper twisted pair, coaxial cable, and/or fiber optic cables. There are many different types of wired networks including wide area networks WAN, metropolitan area networks MAN, local area networks LAN, Internet area networks IAN, campus area networks CAN, global area networks GAN, like the Internet, and virtual private networks VPN. As used herein, network refers to any combination of wireless, wired, public, and private data networks that are typically interconnected through the Internet, to provide a unified networking and information access system. The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the invention. However, it will be apparent to one skilled in the art that specific details are not required in order to practice the invention. Thus, the foregoing descriptions of specific variations of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed; obviously, many modifications and variations are possible in view of the above teachings. The variations were chosen and described in order to best explain the principles of the invention and its practical applications, and they thereby enable others skilled in the art to best utilize the invention and various implementations with various modifications as are suited to the particular use contemplated. It is intended that the following claims and their equivalents define the scope of the invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/13/055/109/0.pdf",
                    "CONTRADICTION_SCORE": 0.988863468170166,
                    "F_SPEC_PARAMS": [
                        "reduced patient scarring, less patient pain, shorter patient recovery periods,",
                        "lower medical treatment costs"
                    ],
                    "S_SPEC_PARAMS": [
                        "maintain a sterile environment",
                        "cannot practically be sterilized",
                        "heat"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery."
                    ],
                    "S_SENTS": [
                        "Similar to traditional surgical procedures, it is important to maintain a sterile environment in the surgical field during robotic MI However, various components , motors, encoders, sensors, of the tool driver and other aspects of the robotic surgical system generally cannot practically be sterilized using conventional sterilization methods such as heat."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Side Effects"
                    ],
                    "F_SIM_SCORE": 0.4541338384151459,
                    "S_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object",
                        "Temperature"
                    ],
                    "S_SIM_SCORE": 0.6671514511108398,
                    "GLOBAL_SCORE": 1.6828394462664922
                },
                "sort": [
                    1.6828394
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10926422-20210223",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10926422-20210223",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-01-03",
                    "PUBLICATION_DATE": "2021-02-23",
                    "INVENTORS": [
                        "Chi-Huan Shao",
                        "Chi-Shun Chang",
                        "Ching-Yu Lin"
                    ],
                    "APPLICANTS": [
                        "DELTA ELECTRONICS, INC.    ( Taoyuan , TW )"
                    ],
                    "INVENTION_TITLE": "Heat dissipating system of robot",
                    "DOMAIN": "B25J 190054",
                    "ABSTRACT": "A heat dissipating system of robot is provided. The heat dissipating system includes a gas supply device and a robot. The gas supply device is configured to provide a high-pressure gas. The robot is in communication with the gas supply device and includes a housing, an inlet and at least one valve. The housing defines an inner space. The inlet is disposed on the housing and is in communication with the gas supply device and the inner space. The at least one valve is disposed on the housing and is in communication with the inner space. The high-pressure gas outputted by the gas supply device is guided into the inner space through the inlet, and the high-pressure gas accommodated in the inner space is released through the at least one valve when the at least one valve is open.",
                    "CLAIMS": "1. A heat dissipating system, comprising: a gas supply device configured to provide a high-pressure gas; and a robot in communication with the gas supply device, comprising: a housing, wherein the housing defines an inner space; an inlet disposed on the housing, wherein the inlet is in communication with the gas supply device and the inner space; and at least one valve disposed on the housing, wherein the at least one valve is in communication with the inner space, wherein the high-pressure gas outputted by the gas supply device is guided into the inner space through the inlet, and the high-pressure gas accommodated in the inner space is released through the at least one valve when the at least one valve is open, wherein the valve is a one-way valve, the valve is configured to allow the high-pressure gas accommodated in the inner space to be released to a space outside the robot through the valve, and the valve is configured to prevent an external air outside the robot from flowing into the inner space through the valve. 2. The heat dissipating system according to claim 1, wherein the valve comprises a ball, an elastic element and a stopping block connected with each other, the stopping block comprises a hollow portion, the housing comprises an accommodation space, the accommodation space is in communication with the inner space and the space outside the robot via a first opening and a second opening, the valve is disposed in the accommodation space, the ball is aligned with the first opening, a diameter of the ball is larger than a diameter of the first opening, the stopping block is aligned with the second opening, a diameter of the stopping block is equal to a diameter of the second opening, and the accommodation space is in communication with the space outside the robot via the hollow portion. 3. The heat dissipating system according to claim 2, wherein when a pressure of the high-pressure gas in the inner space is higher than an external pressure, the ball is driven to move toward the stopping block by the pressure of the high-pressure gas, the elastic element is compressed by the ball, the valve is opened, the accommodation space is in communication with the inner space via the first opening, and the high-pressure gas accommodated in the inner space is released to the space outside the robot through the accommodation space and the hollow portion, wherein when the pressure of the high-pressure gas in the inner space is smaller than the external pressure, the ball is driven to move toward the first opening by a resilience of the elastic element, the first opening is fully covered by the ball, the valve is closed, the accommodation space is isolated from the inner space, and the external pressure comprises an atmospheric pressure, an elasticity of the elastic element and/or a weight of the ball. 4. The heat dissipating system according to claim 1, wherein the valve is an electronic valve, the robot further comprises a control unit electrically connected with the valve, and the control unit is configured to control the on/off of the valve. 5. The heat dissipating system according to claim 4, wherein the robot further comprises at least one temperature sensor electrically connected with the control unit, the temperature sensor is configured to detect a temperature of the inner space and generate a feedback signal accordingly, and the control unit receives the feedback signal and control the on/off of the valve according to the feedback signal. 6. The heat dissipating system according to claim 5, wherein the at least one temperature sensor is electrically connected with the gas supply device, and the gas supply device receives the feedback signal and controls a flow of the outputted high-pressure gas according to the feedback signal. 7. The heat dissipating system according to claim 6, wherein the robot further comprises plural temperature sensors corresponding to plural valves at different positions, and the control unit controls the on/off of the valve according to the corresponding temperature sensor. 8. The heat dissipating system according to claim 1, wherein the valve is disposed neighboring to plural elements having high heat dissipation demand in the inner space. 9. The heat dissipating system according to claim 1, wherein the robot further comprises at least one gas channel disposed in the inner space of the robot, and the gas channel is corresponding to and in communication with the valve. 10. The heat dissipating system according to claim 9, wherein the gas channel has a ditch disposed neighboring to an exterior of an element having high heat dissipation demand. 11. The heat dissipating system according to claim 1, wherein a pressure of the high-pressure gas is larger than an atmospheric pressure. 12. The heat dissipating system according to claim 1, wherein the housing of the robot has sealing characteristic, and when the valve is closed, the inner space of the robot is isolated from a space outside the robot.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to a heat dissipating system, and more particularly to a heat dissipating system of a robot.",
                    "STATE_OF_THE_ART": "Nowadays, robots have been extensively applied. Robots usually have rotating joints for moving and operating, and the joints are driven by motors. Since the inner space of the robot is relatively sealed, it is hard to naturally dissipate the heat generated by the rotation of joints and the operation of motors. Therefore, the heat is accumulated, and the over-accumulated heat may affect the operation of the robot. Consequently, it is important to dissipate the heat in the inner space of the robot for keeping the robot operating normally. For dissipating the heat of the inner space of the robot, the gas outputted by the external gas source is guided into the inner space of the robot. The prior heat dissipation technique disposes the gas tube with holes in the inner space of the robot. The gas outputted by the external gas source is guided into the gas tube and is released into the inner space of the robot through the holes of the gas tube. Therefore, the elements may probably be over-heated, motor, are cooled. However, disposing the gas tube increases the cost, and the design of the wiring inside the robot is more complicated. Therefore, there is a need of providing a heat dissipating system of a robot in order to overcome the above drawbacks.",
                    "SUMMARY": [
                        "An object of the present disclosure provides a heat dissipating system of a robot. The gas supply device in communication with the robot outputs the high-pressure gas to the inner space of the robot. When the valve of the robot is open, the high-pressure gas in the inner space is released through the valve. Consequently, the inner space of the robot is cooled. In addition, the high-pressure gas for heat dissipation is supplied by the gas supply device, and thus the gas supply device can adjust the flow of the outputted high-pressure gas according to the actual requirements. Moreover, the high-pressure gas accommodated in the inner space of the robot is released only when the valve is open. Since the pressure of the high-pressure gas is larger than the atmospheric pressure of the external air, the external air is prevented from flowing into the inner space of the robot through the valve when the valve is open. Therefore, the robot has a high Ingress Protection IP. In accordance with an aspect of the present disclosure, there is provided a heat dissipating system of robot. The heat dissipating system includes a gas supply device and a robot. The gas supply device is configured to provide a high-pressure gas. The robot is in communication with the gas supply device and includes a housing, an inlet and at least one valve. The housing defines an inner space. The inlet is disposed on the housing and is in communication with the gas supply device and the inner space. The at least one valve is disposed on the housing and is in communication with the inner space. The high-pressure gas outputted by the gas supply device is guided into the inner space through the inlet, and the high-pressure gas accommodated in the inner space is released through the at least one valve when the at least one valve is open. The above contents of the present disclosure will become more readily apparent to those ordinarily skilled in the art after reviewing the following detailed description and accompanying drawings, in which:",
                        "1 is a schematic perspective view illustrating a heat dissipating system of a robot according to an embodiment of the present disclosure; 2 is a schematic perspective view illustrating the robot of 1; 3 is a partial schematic perspective view illustrating the robot of 1; 4 is a cross-section view of the partial structure of the robot of 3; 5A is a schematic perspective view illustrating the valve of 3; 5B is an enlarged graph of the dashed line block of 4; and 6 is a schematic view showing the partial interior structure of the robot of 1."
                    ],
                    "DESCRIPTION": "The present disclosure will now be described more specifically with reference to the following embodiments. It is to be noted that the following descriptions of preferred embodiments of this disclosure are presented herein for purpose of illustration and description only. It is not intended to be exhaustive or to be limited to the precise form disclosed. 1 is a schematic perspective view illustrating a heat dissipating system of a robot according to an embodiment of the present disclosure, 2 is a schematic perspective view illustrating the robot of 1, 3 is a partial schematic perspective view illustrating the robot of 1, and 4 is a cross-section view of the partial structure of the robot of 3. As shown in 1, 2, 3 and 4, a heat dissipating system 1 of robot includes a gas supply device 10 and a robot 20, and the gas supply device 10 and the robot 20 are in communication with each other. The gas supply device 10 is configured to provide the high-pressure gas to the robot 20. The pressure of the high-pressure gas is higher than the atmospheric pressure. The gas supply device 10 is able to adjust the flow of the outputted high-pressure gas. The robot 20 includes an inlet 21, a housing 22 and at least one valve 23. The inlet 21 and the valve 23 are disposed on the housing 22, and the housing 22 defines an inner space 24. The inner space 24 is in communication with the inlet 21 and the valve 23 respectively, and the inlet 21 is in communication with the gas supply device 10. The high-pressure gas outputted by the gas supply device 10 flows into the inner space 24 of the robot 20 through the inlet 21, and the inner space 24 is filled with the high-pressure gas. When the valve 23 is closed, the inner space 24 of the robot 20 is isolated from the space outside the robot 20 due to the sealing characteristic of the housing 22 of the robot 20. In addition, the housing 22 has a hollow tubulous space for wiring, and thus the high-pressure gas is allowed to flow into anywhere, even the end axle of the robot 20, in the housing 22 without leaking. Therefore, there is no need to dispose additional gas tube. When the valve 23 is open, the high-pressure gas in the inner space 24 is released to the space outside the robot 20 through the valve 23. Since the pressure of the high-pressure gas is larger than the atmospheric pressure of the external air, the external air is prevented from flowing into the inner space 24 of the robot 20 through the valve 23. Consequently, the inner space 24 of the robot is cooled, and the robot 20 has a high IP. The robot 20 is for example but not limited to a service robot, a collaborative robot or an industrial robot. In addition, the communication way between the inlet 21 of the robot 20 and the gas supply device 10 can be adjusted according to actual requirements and is not limited to the tube shown in 1. The communication way only has to protect the high-pressure gas from leaking during the high-pressure gas being outputted to the inlet 21 by the gas supply device 10. Moreover, the inlet 21 is allowed to be disposed on anywhere on the housing 22 if the inlet 21 will not retard the operation of the robot 20. The inlet 21 is not limited to be disposed on the position shown in 2. In an embodiment, the valve 23 is disposed neighboring to the elements having higher heat dissipation demand such as motors and axles. When the valve 23 is open, the high-pressure gas in the inner space 24 of the robot 20 is released through the valve 23. Therefore, the airflow near the valve 23 is larger, which benefits cooling the elements near the valve 23, and thus the heat dissipation efficiency is enhanced. In an embodiment, the valve 23 is a one-way valve configured for limiting the flow direction of the gas. Accordingly, the high-pressure gas accommodated in the inner space 24 is released to the space outside the robot 20 through the valve 23, and the external air is prevented from flowing into the inner space 24 through the valve 23. In this embodiment, the valve 23 is opened by the pressure difference between the high-pressure gas in the inner space 24 and the external air. Thus, the high-pressure gas is automatically released through the valve 23 without controlling the valve 23. In this embodiment, the valve 23 is a valve being able to realize the one-way air flowing. In an embodiment, as shown in 5A and 5B, the valve 23 as a one-way valve includes a ball 231, an elastic element 232 and a stopping block 233 connected with each other. The stopping block 233 has a hollow portion 234. Preferably but not exclusively, the elastic element 232 is a spring. The housing 22 has an accommodation space 221. The accommodation space 221 is in communication with the inner space 24 and the space outside the robot 20 via the first opening 222 and the second opening 223 respectively. The valve 23 is disposed within the accommodation space 221. The ball 231 is aligned with the first opening 222, and the diameter of the ball 231 is larger than the diameter of the first opening 222. The accommodation space 221 is in communication with or isolated from the inner space 24 according to the relative position between the ball 231 and the first opening 222. The stopping block 233 is aligned with the second opening 223, and the diameter of the stopping block 233 is equal to the diameter of the second opening 223. The accommodation space 221 is in communication with the space outside the robot 20 via the hollow portion 234. Concretely, when the pressure of the high-pressure gas in the inner space 24 is larger than the external pressure, the pressure difference between the pressure of the high-pressure gas and the external pressure drives the ball 231 to move toward the stopping block 233. The elastic element 232 is compressed by the ball 231. Consequently, the valve 23 is opened. The accommodation space 221 is in communication with the inner space 24 via the first opening 222, and the high-pressure gas in the inner space 24 is released to the space outside the robot 20 through the accommodation space 221 and the hollow portion 234. The external pressure is for example but not limited to include the atmospheric pressure, the elasticity of the elastic element 232 and/or the weight of the ball 231. Whereas, when the pressure of the high-pressure gas in the inner space 24 is smaller than the external pressure, or when the gas supply device 10 stops providing the high-pressure gas, the resilience of the elastic element 232 pushes the ball 231 to move toward the first opening 222. The first opening 222 is fully covered by the ball 231. Consequently, the valve 23 is closed. The accommodation space 221 is isolated from the inner space 24, and the external air outside the robot 20 is prevented from flowing into the inner space 24 through the hollow portion 234 and the accommodation space 221. In an embodiment, the valve 23 is an electronic valve. The robot 20 further includes a control unit not shown. The control unit is electrically connected with the valve 23 and is configured to control the on/off of the valve 23. Moreover, the above-mentioned one-way valve can be an electronic valve as well. By the control unit controlling the on/off of the valve 23 and the gas supply device 10 adjusting the flow of the high-pressure gas, the heat dissipating system 1 can adjust the intensity of cooling the inner space 24 of the robot 20 according to the actual heat dissipation demands. Therefore, unnecessary loss is avoided. In an embodiment, the robot 20 further includes at least one temperature sensor not shown. The temperature sensor is disposed in the inner space 24 of the robot 20 and is electrically connected with the control unit and the gas supply device 10. The temperature sensor is configured to detect the temperature of the inner space 24 of the robot 20 and generate a feedback signal accordingly. The control unit receives the feedback signal outputted by the temperature sensor, and the control unit controls the on/off of the valve 23 according to the temperature of the inner space 24 reflected by the feedback signal. In an embodiment, plural temperature sensors are disposed in the different areas of the inner space 24, and the plural temperature sensors are corresponding to the valves 23 at different positions. When the control unit receives the feedback signal from a temperature sensor, the control unit can control the on/off of the valve 23 corresponding to that temperature sensor according to the area temperature reflected by the feedback signal. In an embodiment, control unit monitors the input voltage or the input current of the robot 20 via the electric control system. When the robot 20 is in a high energy consumption state, the heat dissipating system 1 requires better heat dissipating ability. Accordingly, the control unit controls the valve 23 to open and controls the gas supply device 10 to increase the flow of the high-pressure gas. When the robot 20 is in a low energy consumption state, the heat dissipating system 1 requires less heat dissipating ability. Accordingly, the control unit controls the valve 23 to close and/or controls the gas supply device 10 to decrease the flow of the high-pressure gas or stop providing the high-pressure gas. In an embodiment, the robot 20 includes at least one gas channel 25. The gas channel 25 is disposed in the inner space 24 and is in communication with the inner space 24 and the valve 23. Moreover, the gas channel 25 is disposed neighboring to the elements having high heat dissipation demand such as motors and axles. For example, as shown in 6, where the structure inside the housing 22 is depicted with dashed lines. The gas channel 25 is a ditch disposed neighboring to the exterior of the circular wall of the motor stator 26, and the position of the ditch is corresponding to the position of the valve 23. By disposing the gas channel 25, the high-pressure gas is guided therein. Therefore, the flow of the high-pressure gas passing through the elements neighboring to the gas channel 25 is increased, and the heat dissipation to specific elements in the inner space 24 is enhanced. From the above descriptions, the present disclosure provides a heat dissipating system of a robot. The gas supply device in communication with the robot outputs the high-pressure gas to the inner space of the robot. When the valve of the robot is open, the high-pressure gas in the inner space is released through the valve. Consequently, the inner space of the robot is cooled. In addition, the high-pressure gas for heat dissipation is supplied by the gas supply device, and thus the gas supply device can adjust the flow of the outputted high-pressure gas according to the actual requirements. Moreover, the high-pressure gas accommodated in the inner space of the robot is released only when the valve is open. Since the pressure of the high-pressure gas is larger than the atmospheric pressure of the external air, the external air is prevented from flowing into the inner space of the robot through the valve when the valve is open. Therefore, the robot has a high Ingress Protection IP. Furthermore, when the valve is open, the high-pressure gas in the inner space of the robot is released through the valve, and the flow of the gas near the valve is larger. Accordingly, for enhancing the efficiency of heat dissipation, the valve is disposed near the elements having high heat dissipating requirement, and even the gas channel is disposed accordingly. While the disclosure has been described in terms of what is presently considered to be the most practical and preferred embodiments, it is to be understood that the disclosure needs not be limited to the disclosed embodiment. On the contrary, it is intended to cover various modifications and similar arrangements included within the spirit and scope of the appended claims which are to be accorded with the broadest interpretation so as to encompass all such modifications and similar structures.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/22/264/109/0.pdf",
                    "CONTRADICTION_SCORE": 0.9885804057121277,
                    "F_SPEC_PARAMS": [
                        "heat is accumulated,",
                        "heat",
                        "operation of the robot"
                    ],
                    "S_SPEC_PARAMS": [
                        "cost,",
                        "design of the wiring inside the robot is more complicated"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Therefore, the heat is accumulated, and the over-accumulated heat may affect the operation of the robot."
                    ],
                    "S_SENTS": [
                        "However, disposing the gas tube increases the cost, and the design of the wiring inside the robot is more complicated."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Temperature",
                        "Force Torque"
                    ],
                    "F_SIM_SCORE": 0.7046796679496765,
                    "S_TRIZ_PARAMS": [
                        "Convenience of Use",
                        "Level of Automation"
                    ],
                    "S_SIM_SCORE": 0.47737260162830353,
                    "GLOBAL_SCORE": 1.6796065405011178
                },
                "sort": [
                    1.6796066
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11324559-20220510",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11324559-20220510",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2021-09-24",
                    "PUBLICATION_DATE": "2022-05-10",
                    "INVENTORS": [
                        "Vladimir Nekhendzy",
                        "Dimitri Sokolov"
                    ],
                    "APPLICANTS": [
                        "SPIRO ROBOTICS, INC.    ( Palo Alto , US )"
                    ],
                    "INVENTION_TITLE": "Robotic-assisted navigation and control for airway management procedures, assemblies and systems",
                    "DOMAIN": "A61B 3420",
                    "ABSTRACT": "Airway management methods, devices, assemblies and systems. Methods, devices, assemblies and systems may include robotic movement and control of an intubation tube introducer or guide, and may include utilizing image data from one or more image sensors. The methods, devices, assemblies and systems may optionally be used in endotracheal intubation procedures.",
                    "CLAIMS": "1. An intubation system, comprising: an integrated and handheld dual-video tracheal intubation assembly, the assembly dimensioned and configured to be held by a single hand of a user, the assembly including; a housing; a first elongate imaging member, the first elongate imaging member including an elongate body and a first video camera disposed at a distal region of the elongate body; a disposable second elongate imaging member that is sized and configured to be releasably coupled to the housing to create operable communication therebetween, the disposable second elongate imaging member including a flexible elongate endotracheal tube introducer sized to be disposed within an endotracheal tube and to allow the endotracheal tube to be moved axially over the introducer, and a second video camera disposed at a distal region of the introducer; and a cover that is sized and configured to be releasably coupled to the housing, the cover including an elongate channel defining an elongate lumen, the elongate channel sized and dimensions such that at least a portion of the elongate body of the first elongate imaging member is disposed within the elongate lumen, and an endotracheal tube channel sized and dimensioned to releasably secure the endotracheal tube thereto and restrict movement of the endotracheal tube relative to the endotracheal tube channel in at least one direction relative to the cover, the endotracheal tube including a lumen sized to receive the introducer therein; and an actuator disposed within the housing and configured such that when the disposable second elongate imaging member is releasably coupled to the housing, the actuator is in operable communication with the introducer such that the actuator is configured to be activated to facilitate the controlled movement of the introducer and the second video camera relative to the first video camera, wherein the assembly is configured such that when the endotracheal tube is interfaced with the endotracheal tube channel and when the disposable second elongate imaging member is releasably coupled to the housing, the assembly disposes the first video camera at an initial distance from the second video camera prior to actuation of the actuator. 2. The system of claim 1, further comprising a processor configured to receive as input one or more of information indicative of a signal from the first video camera of one or more upper airway anatomical landmarks or information indicative of a signal from the second video camera when the cover is at least partially disposed in an upper airway of a patient, and cause communication to the actuator to control robotic movement of the introducer and the second video camera relative to the first video camera. 3. The system of claim 1, wherein the first video camera has a first angle of view, and the second video camera has a second angle of view, wherein the first angle of view is greater than the second angle of view and provides visualization of a larger region of the patient's upper airway anatomy with the first video camera than with the second video camera as the introducer and the second video camera are moved relative to the first video camera. 4. The system of claim 1, wherein the intubation system is further configured such that when the endotracheal tube is releasably coupled to the endotracheal tube channel and when the disposable second elongate imaging member is releasably coupled to the housing and extending through the endotracheal tube lumen, the first video camera is maintained at an initial axial distance not greater than 3 cm relative to the second video camera prior to actuation of the actuator. 5. The system of claim 1, wherein the assembly is sized and configured such that when the second imaging member is coupled to the housing, when the cover is releasably coupled to the housing, and when the endotracheal tube is releasably coupled to the endotracheal tube channel, the assembly, including the first video camera and the second video camera, is movable as an integrated unit with the single hand of the user. 6. The system of claim 1, wherein the actuator is configured to facilitate one or more of controlled axial navigation or controlled deflection of the introducer and the second video camera relative to the first video camera. 7. The system of claim 1, wherein the first elongate imaging member has a length, the first video camera has an angle of view, and the endotracheal tube channel has a configuration such that the first video camera is adapted to visualize axial controlled movement of the introducer relative to the first video camera when a portion of the introducer is moved within the endotracheal tube lumen. 8. The system of claim 1, wherein the first video camera is maintained substantially axially aligned with the second video camera prior to actuation of the actuator. 9. The system of claim 1, wherein the first video camera is maintained axially aligned with the second video camera prior to actuation of the actuator. 10. The system of claim 1, wherein the second video camera is maintained at an initial axial distance not greater than 3 cm away from the first video camera prior to actuation of the actuator. 11. The system of claim 10, wherein the second video camera is maintained at a horizontal distance not greater than 4 cm from the first video camera prior to actuation of the actuator. 12. The system of claim 1, wherein the actuator is configured to facilitate controlled distal movement of the introducer at least 2 cm relative to an initial position of the introducer. 13. The system of claim 1, wherein the assembly further comprises a distal movement limiter that is configured to limit distal movement of the introducer relative to the initial position and relative to the first video camera. 14. The system of claim 13, wherein the distal movement limiter is configured to prevent the introducer from contacting a tracheal carina when the first video camera is maintained in an upper airway of a patient. 15. The system of claim 13, wherein the assembly is configured such that the introducer is prevented from being distally moved more than 40 cm from the initial position. 16. The system of claim 13, wherein the assembly is configured such that the introducer is prevented from being distally moved more than 30 cm from the initial position. 17. The system of claim 1, wherein the actuator is configured to facilitate controlled distal movement of the introducer at least 5 cm relative to an initial position of the introducer. 18. The system of claim 1, wherein the disposable second elongate imaging member comprises a disposable housing, and wherein a proximal end of the introducer is disposed within the disposable housing and secured to the disposable housing, and wherein a section of the introducer distal to the proximal end is movable relative to the disposable housing. 19. The system of claim 18, wherein the disposable housing has a window through which the introducer extends outside of the disposable housing. 20. The system of claim 1, further comprising a manually activated controller to facilitate manual robotic movement of the introducer relative to the first video camera. 21. The system of claim 20, wherein the intubation system is further adapted to allow for manual robotic control of the introducer when the second video camera is disposed in a lower airway and when the first video camera is disposed in an upper airway. 22. The system of claim 1, wherein the intubation system is further adapted to allow for manual robotic control of the introducer when the introducer is disposed in an upper airway and when the first video camera is disposed in the upper airway. 23. The system of claim 1, wherein the disposable second elongate imaging member comprises one or more introducer deflection actuators. 24. The system of claim 23, wherein the one or more introducer deflection actuators are adapted to rotate. 25. The system of claim 23, further comprising a plurality of pullwires that extend through the introducer and when tensioned cause deflection of the introducer, wherein at least one pullwire is secured to each of the one or more introducer deflection actuators such that movement of the deflection actuator tensions the at least one pullwire secured thereto. 26. The system of claim 25, wherein the disposable second elongate imaging member further comprises a plurality of introducer axial movement actuators. 27. The system of claim 26, wherein the introducer extends between first and second introducer axial movement actuators such that movement of the first and second introducer axial movement actuators causes axial movement of the introducer. 28. The system of claim 27, wherein a section of the introducer is outside of the disposable housing proximal to the first and second introducer axial movement actuators. 29. The system of claim 23, wherein each of the one or more introducer deflection actuators are in operable communication with a rotatable actuator disposed in the housing. 30. The system of claim 29, wherein the rotatable actuator is in operable communication with a motor disposed in the housing. 31. The system of claim 1, wherein the disposable second elongate imaging member comprises a plurality of introducer axial movement actuators. 32. The system of claim 31, wherein the plurality of introducer axial movement actuators have a geared interface. 33. The system of claim 31, wherein the introducer extends between first and second introducer axial movement actuators. 34. The system of claim 1, wherein the disposable second elongate imaging member comprises an opening facing the housing when coupled thereto, and wherein the housing comprises a sensor positioned to sense axial movement of the introducer through the opening that faces the housing.",
                    "STATE_OF_THE_ART": "Airway management includes a variety of procedures aimed at handling, overseeing, caring for, applying a procedure to, manipulating, controlling, establishing and securing patient's upper and/or lower airway, which may occur in a variety of medical settings and locations, and in different patient populations. Airway management procedures may include navigating a medical device within a patient, and optionally visualizing the navigation. Some airway management procedures may include establishing an artificial airway within a patient's body. For example only, tracheal intubation TI is an airway management procedure that involves placement of an intubation tube in this example, an endotracheal tube, ETT into a patient's trachea to ventilate patient's lungs and assure adequate oxygenation and gas exchange, and to help protect the patient's airway from aspiration of substances such as gastric contents, blood, secretions and/or surgical debris. TI may be performed, for example, in the operating room OR when the patient is being anesthetized for elective or emergent surgery, or in a broad range of urgent and/or emergent airway situations outside of OR, such as in an intensive care unit ICU, in an emergency department, in out-of-OR procedures and code situations, and in out-of-hospital settings paramedics, EMS, and other forms of patient transfer, . TI is the most important technique for management of high risk or difficult cases, and is a common rescue technique when other forms of airway management fail. Failure to perform TI on the first attempt or at all typically results in significant harm to patients such as hypoxia-related adverse events or complications , cardiac arrest, brain damage, death or airway trauma, or may require more invasive treatments such as emergent front of neck access eFONA to establish an airway. Additionally, even in non-emergency or non-urgent airway management procedures, failure to establish, secure or control an airway or safely navigate to a desired airway location on a first TI attempt can complicate the procedure and cause harm to the patient. The difficult or failed TI usually is caused by the operator experiencing difficulty with visualization of the patient's airway anatomy, with navigation of the ETT to the larynx glottic opening and the vocal cords, and with manual placement of the ETT through and past the vocal cords and into the patient's trachea. Each of these critical TI steps Visualization, Navigation, Placement, either individually, or in combination, can provide for TI difficulty/failure. The most commonly used conventional techniques for TI include use of direct laryngoscopy, video laryngoscopy, and flexible intubation scopes, but their performance remains suboptimal. For example, direct laryngoscopy involves the use of a metal blade to retract patient's tongue to directly visualize the patient's airway and to manually navigate and pass the ETT into the patient's trachea. Direct laryngoscopy is limited by disadvantages, such as the need to align airway axes for better visualization of the patient's larynx and vocal cords, a narrow field of view which is easily obscured by blood or secretions, and challenges in controlling the patient's tongue, making both visualization, navigation and passing the ETT to and through the vocal cords difficult. Direct laryngoscopy also does not permit for visual confirmation of proper ETT placement in the trachea, which may lead to unrecognized ETT misplacement esophageal intubation and resulting life-threatening hypoxia. Video laryngoscopy utilizes a fixed video camera, which is placed inside the patient's upper airway above the vocal cords to provide for enlarged visualization of the anatomical structures. However, it does not address all difficulties in successfully performing TI, as the ETT navigation and placement difficulties persist. The navigation of the ETT to and through the glottic opening is impaired due to indirect visualization of the patient's upper airway anatomy on the video monitor and ETT manipulation at an acute angle in relation to the vocal cords. A metal stylet frequently needs to be placed inside the ETT to facilitate ETT navigation and placement, which leads to the stylet-induced airway trauma in at least 1. 1% of cases. Although visualization of ETT passing through the vocal cords is greatly enhanced with video laryngoscopy, the ETT misplacement esophageal intubation and resulting life-threatening hypoxia may still occur. Additionally, video laryngoscopy does not allow for troubleshooting of the ETT navigation and placement below the vocal cords lower airway, where the ETT advancement occurs blindly. Furthermore, video laryngoscopy does not allow for instant confirmation of the ETT placement in the trachea. Additionally, still, video laryngoscopy does not allow the user to properly position the ETT inside the patient's trachea, to avoid both, too deep and too high ETT placement. Flexible intubation scopes FIS, which utilize steerable video endoscopy cameras, may be helpful for patients with a severely compromised airway, but are still disadvantaged by a narrow field of view and the need for complex manual dexterity requiring advanced training and expertise. The visualization of the airway anatomy is greatly diminished, providing for a close-up view only, with a loss of orientation landmarks. Moreover, such visualization is easily obscured or lost due to the presence of even a minimal amount of blood and/or secretions. With FIS use, the operator must be highly proficient with maneuvering the device around soft airway tissue obstruction, especially in unconscious patient, when TI must be performed expeditiously. Another significant FIS limitation includes the inability to observe and troubleshoot ETT advancement into the patient's trachea from above the vocal cords. Airway management medical procedures , TI utilizing existing techniques and devices yield suboptimal success rates and outcomes, such as first-pass success rates. For example, first-pass TI failure rate in difficult airway situations can range between 8%-54% for conventional techniques depending on the device used, TI location, patient population and expertise of the provider. Additionally, significantly higher first-pass TI failure rates are also observed for pediatric patients with difficult airways and some other patients' categories, obese patients, patients with head and neck cancer, patients with cervical spine C-spine problems, etc. Failure to achieve TI on the first attempt leads to increased incidence of major complications, such as major airway trauma, airway swelling, lack of oxygen hypoxia, cardiac arrest and brain damage. There is a need for airway management methods, systems and devices that can more reliably and consistently provide better outcomes, such as being able to provide higher first-pass success rates and/or more reliably navigate medical devices. For example, there is a need for new and improved devices and methods for assisting intubation and improving TI success rates. Additionally, it may be beneficial to provide multi-functional airway management platforms that can be used in a variety of airway management procedures, clinical situations, locations and settings, and in a variety of patient populations.",
                    "SUMMARY": [
                        "The disclosure herein relates to airway management methods, devices and systems. While tracheal intubation is provided as an example of airway management herein, it is understood that the disclosure is not so limited, and that the concepts herein may be applied or applicable to other airway management procedures and settings. For example, concepts herein may be used in bronchoscopy procedures or in ENT procedures such as endoscopies flexible nasolaryngoscopy, esophagoscopy, , and endoscopy-assisted surgical airway procedures vocal cord injections, laryngologic surgery, . One aspect of the disclosure herein is related to systems, devices and methods for robotically assisting tracheal intubation of a patient. In some instances, the robotic assistance includes one or more of automatic and/or manual robotic controls and/or movements of an introducer, which may include a visualization guide. A device may include an integrated handheld assembly that is adapted and/or configured to allow for the robotic control movement of the introducer, such as a visualization guide. An introducer may also act as a visualization delivery guide for an intubation tube, such as an ETT. In some embodiments, the introducer may include or be an endoscope. In some variations, an integrated device for robotically assisting intubation of a patient may include a handheld housing which may, for example, include a display or other monitor screen, a laryngoscope coupled to the housing and including a first image sensor, an actuating member movable within the housing, an endoscope extending from the actuating member wherein the endoscope member includes a second image sensor and is configured to removably couple to an intubation tube, and at least one actuator in the housing configured to automatically guide the endoscope via the actuating member, based at least in part on one or more images from at least one of the first image sensor and/or the second image sensor. In some variations, a method for performing a robotically-assisted intubation procedure on a patient includes acquiring one or more images with at least one of a laryngoscope coupled to a handheld housing and an endoscope coupled to the handheld housing. The endoscope may extend from an actuating member movable within the handheld housing and the endoscope may be removably coupled to an intubation tube. The method may further include automatically guiding , advancing, retreating, and/or rotating, based on the one or more acquired images, the endoscope and the intubation tube via the actuating member. In some variations, the method may further include decoupling the intubation tube from the endoscope , manually or automatically advancing the intubation tube off of the endoscope. In some variations, an integrated robotic device may include a handheld housing which may, for example, include a display or other monitor screen, a laryngoscope coupled to the housing and including a first image sensor, an actuating member movable within the housing and coupleable to an endoscope including a second image sensor, and at least one actuator. The actuators may be configured to automatically move the actuating member based at least in part on one or more images from at least one of the first image sensor and the second image sensor. The endoscope may, for example, be configured to removably couple to an intubation tube. In some variations, an integrated robotic device, may include a handheld housing which may, for example, include a display or other monitor screen, an actuating member movable within the housing and coupleable to an endoscope comprising an image sensor; and at least one actuator in the housing configured to automatically move the actuating member, based at least in part on one or more images from the image sensor. An exemplary benefit of some devices herein is that they are configured to be operated by a single user, handheld and are portable. One aspect of the disclosure is a method for performing a robotically-assisted airway management procedure , intubation procedure on a patient, comprising: acquiring one or more images with at least one of a first imaging member coupled to a handheld housing and an introducer , a flexible or rigid endoscope coupled to the handheld housing, wherein the introducer extends from an actuating member movable within the handheld housing and wherein the introducer is removably coupled to an intubation tube; and automatically guiding the introducer via the actuating member, based on the one or more acquired images. One aspect of the disclosure is a robotic-assisted handheld airway management device, comprising: a handheld housing sized and configured to be held by a single hand of a user, a first imaging member coupler , including laryngoscope coupler, and a second imaging member coupler having a least one surface that is sized and configured to be releasably secured to a second imaging member to allow for single-handed movement of the first and second imaging member with a single hand of an operator when the second imaging coupler is releasably secured to the second imaging member. One aspect of the disclosure is a robotic-assisted handheld airway management device , intubation device, comprising: a handheld housing sized and configured to be held by a single hand of a user, a first imaging member or first imaging member coupler, and an introducer coupler having at least one surface that is configured to be indirectly or directly releasably secured to an introducer , a flexible endoscope, wherein coupling the introducer to the introducer coupler facilitates controlled robotic-assisted movement of the introducer relative to the handheld housing. One aspect of the disclosure is a method of assembling a handheld airway management , for intubation system capable of providing one or more images, and adapted for robotic-assisted control of an introducer during an airway management , intubation procedure, comprising: providing a handheld housing that is configured to be held by a single hand of a user, the housing including an direct or indirect introducer coupler and either first imaging member or a first imaging member coupler; coupling a blade to the handheld housing such that the first imaging member is disposed in a channel lumen of the blade; releasably securing an endotracheal tube to an endotracheal tube coupler of the blade; positioning an introducer within the tracheal tube; and creating operable communication between the introducer and the housing. One aspect of the disclosure is a blade sized and configured to be releasably secured to a handheld airway management , intubation housing, the blade comprising: a first channel lumen having a curved configuration and a tracheal tube channel, the tracheal tube channel disposed on a side of the blade such that when a tracheal tube is releasably coupled to the tracheal tube channel, at least a portion of a tracheal tube lumen substantially follows a curved configuration of the first channel lumen. One aspect of the disclosure is a handheld robotic-assisted handheld airway management , intubation assembly, comprising: a handheld housing including an introducer coupler; a laryngoscope or a laryngoscope coupler; a blade; and a tracheal tube, the handheld housing, laryngoscope, blade and tracheal tube together dimensioned and configured to interact such that when an introducer , a flexible endoscope is directly or indirectly releasably secured to the handheld housing and disposed in the tracheal tube, a first optical sensor on the laryngoscope and a second optical sensor on a distal end of the introducer are maintained axially within 2 cm of each other, and optionally distally aligned or optionally substantially distally aligned. One aspect of the disclosure is an integrated handheld device for robotic-assisted airway management , intubation of a patient, comprising: a handheld housing sized and configured to be held by a hand of a user; a first image sensor; an actuating member optionally comprising a motor; the housing having a coupler configured to releasably couple the introducer directly or indirectly to the housing. One aspect of the disclosure is a handheld robotic-assisted handheld airway management assembly. The assembly is configured such that when an imaging member , a second imaging member is releasably secured to a housing, the assembly is adapted such that the assembly can distally move an introducer of the imaging member at least 10 cm, and optionally from 10 cm to 60 cm. Any of the second imaging members herein optionally do not include an image sensor. One aspect of the disclosure is any of the second imaging members herein without an image sensor, wherein the second imaging members may include a flexible introducer. The second imaging member without an image sensor may be configured to be coupled to any of the housings herein to create operable communication between the housing and the introducer. An aspect of the disclosure is a method of facilitating airway management , intubation of a patient, that includes receiving input an input about a patient's condition related to one or more of an intubation procedure, a condition of the patient related to a nasal cavity, and/or a condition of the patient related to an oral cavity, and/or a condition related to upper and/or lower airway structures; accessing historical image data related to the one or more of the intubation procedure, the condition of the patient related to a nasal cavity, or the condition of the patient related to an oral cavity, and/or a condition related to upper and/or lower airway structures; and utilizing the accessed historical image data to one or more of recognize at least a portion of the patient's anatomy or control the delivery an imaging device through the patient's nasal cavity or oral cavity and/or upper and/or lower airway structures. One aspect of this disclosure is an integrated handheld assembly. The assembly includes a detachable introducer assembly and a housing, wherein the introducer assembly includes an introducer housing. An end of the introducer may be secured to one region of the introducer housing, and one region of the introducer may be movable through and relative to the introducer housing , an example of which is shown in 19F. One aspect of the disclosure is a handheld airway management , intubation system, comprising: a handheld housing , 1410, 1710; an introducer assembly , 1499, 1740, wherein the handheld housing and the introducer assembly are each sized and configured so that the introducer assembly may be releasably secured to the handheld housing to thereby create operable communication between the handheld housing and the introducer assembly. Any of the devices, systems, assemblies, or handhelds or introducers herein, wherein the introducer includes a working channel, which may optionally extend to a distal end of the introducer. Any of the devices, systems, assemblies or methods herein wherein an image processor is disposed in an external device , external computer with a graphics processing unit, smartphone, that is in communication wired or wireless with any of the housings herein, and optionally wherein information related to acquired image data are communicated from the housing to the external device for processing. One aspect of the disclosure is an intubation system, comprising: an integrated and handheld dual-video tracheal intubation assembly assembly, the assembly dimensioned and configured to be held by a single hand of a user, the assembly including: an elongate housing , 1710 comprising an elongate endotracheal tube channel 1713; a first elongate imaging member , 1730 including a first image sensor; a second elongate imaging member , 1740 including a flexible elongate endotracheal tube introducer introducer sized to be disposed within an endotracheal tube and to allow the endotracheal tube to be moved axially over the elongate tracheal tube introducer, and a second image sensor disposed at a distal region of the introducer; and a cover that is sized and configured to be releasably coupled to the housing, the cover including an elongate channel defining an elongate lumen, the elongate channel sized and dimensions such that at least a portion of the first imaging member is disposed within the elongate lumen, and an endotracheal tube channel, wherein when the cover is releasably coupled to the housing, the housing endotracheal tube channel , 1713 and the cover endotracheal tube channel are positioned and configured so as to form a continuous elongate endotracheal tube channel. One aspect of the disclosure is a disposable cartridge for use with a robotically controlled medical system. The cartridge may include a flexible elongate introducer , 1770; a cartridge housing , 1744, wherein a first end of the introducer is secured to the cartridge housing, the cartridge housing including, a plurality of introducer deflection actuators , 1741, 1743, a plurality of pullwires, and at least one pullwire secured to each one of the plurality of introducer deflection actuators, and a plurality of introducer axial movement actuators, wherein the introducer extends between first and second introducer axial movement actuators and is axially movable relative to the plurality of introducer axial movement actuators in response to movement of the plurality of introducer axial movement actuators. One aspect of the disclosure is related to systems that are adapted to cause first and second images to be displayed on a display viewable by an operator, wherein the first and second images may be obtained using any of the first and second image sensors herein. Any of the displays herein may be part of any of the assemblies herein, or they may be separate components not considered part of an assembled assembly, but still viewable during a procedure by an operator. One aspect of the disclosure is a dual-video integrated intubation assembly including a housing and a second imaging member, the second imaging member sized and configured to be releasably secured to the housing, wherein the housing and the second imaging members have flat or substantially flat faces or portions that are adapted with one or communications elements that are adapted to communicate with each other when the flat or substantially flat faces or portions are interfaced , examples of which are shown in exemplary 14A, 14B, and 17A-19K, wherein flat or substantially flat interfacing faces or portions can be more easily seen in the side view of 17A, 17F, 18A, 19C-19E, 19J and 19K. The communication elements in the two faces or portions may be arranged in or on the faces or portions so as to communicate with a corresponding communication element in the other face or portion when the faces or portions are interfaced with each other. One aspect of the disclosure is a computer executable method that is adapted to receive input indicative of image data from at least one of a first image sensor or a second image sensor, and initiate or cause the robotically controlled movement of an introducer in response thereto, to thereby move the introducer to or towards at least one anatomical landmark identified optionally automatically by the computer executable method.",
                        "1 depicts a schematic of an example variation of a device for assisting navigation and/or intubation in a patient. 2A depicts an example variation of a device for assisting navigation and/or intubation in a patient. 2B depicts a front view of the example variation of a device shown in 2A, including a cover. 2C depicts a rear view of the example variation of the device shown in 2B. 3A depicts a cross-sectional view of an example variation of a manual actuating system for guiding an actuating member in a device for assisting navigation and/or intubation in a patient. 3B depicts a perspective view of an example variation of an automatic actuation system for guiding an actuating member in a device for assisting navigation and/or intubation in a patient. 3C and 3D depict example variations of actuation units for guiding an actuating member in a device for assisting navigation and/or intubation in a patient. 4A and 4B depict example variations of a cover in a device for assisting navigation and/or intubation in a patient. 5 and 6 illustrate example variations of an automatic actuation system for articulating a distal end of a scope member in a device for assisting navigation and/or intubation in a patient. 7 is a flowchart of an example variation of a method for tracheal intubation of a patient. 8A illustrates an exemplary sequence illustrating automatic image/landmark recognition. 8B illustrates an exemplary sequence showing manual image/landmark recognition. 9A illustrates an exemplary sequence of image recognition followed by automatic robotic control of the introducer. 9B illustrates an exemplary sequence of image recognition followed by manual robotic control of the introducer. 10 illustrates exemplary steps that include manual robotic control at a time subsequent to automatic robotic control. 11 illustrates a portion of an exemplary integrated and handheld intubation assembly. 12 illustrates a portion of an exemplary integrated and handheld intubation assembly. 13 illustrates a portion of an exemplary integrated and handheld intubation assembly. 14A illustrates an exemplary integrated and handheld dual-video tracheal intubation assembly. 14B illustrates an exemplary housing and first imaging member. 14C illustrates an exemplary second imaging member. 14D illustrates a portion of an exemplary second imaging member. 15 illustrate an exemplary integrated and handheld dual-video tracheal intubation assembly disposed in an upper airway. 16A illustrates an exemplary view on an exemplary display of image data from a first image sensor when maintained in an upper airway. 16B illustrates an exemplary view displayed on an exemplary display of image data from first and second image sensors. 16C illustrates an exemplary view displayed on an exemplary display of image data from first and second image sensors, while the first image sensor is maintained in an upper airway. 16D illustrates an exemplary view displayed on an exemplary display of image data from first and second image sensors, while the first image sensor is maintained in an upper airway. 17A and 17B illustrate side and top views, respectively, of an exemplary integrated and handheld dual-video tracheal intubation assembly first imaging member not shown, shown unassembled. 17C illustrates a bottom view of an exemplary integrated and handheld dual-video tracheal intubation assembly, shown assembled. 17D illustrates a side view of an exemplary integrated and handheld dual-video tracheal intubation assembly, shown assembled. 17E illustrates a top view of an exemplary integrated and handheld dual-video tracheal intubation assembly, shown assembled. 17F illustrates a side view of an exemplary integrated and handheld dual-video tracheal intubation assembly, shown unassembled. 17G illustrates a top view of an exemplary integrated and handheld dual-video tracheal intubation assembly, shown unassembled. 18A shows a side view of an exemplary housing. 18B shows a side view of an exemplary section view of housing from 18A. 18C shows a top view of the exemplary housing from 18A. 18D shows a front, end view of the exemplary housing from 18A. 18E and 18F show perspective top views of the exemplary housing from 18A. 18G, 18H, 18I and 18J illustrate an exemplary housing and optional internal components. 19A illustrates a bottom view of an exemplary second imaging member, including an optional image sensor at a distal end region. 19B illustrates a top view of an exemplary second imaging member. 19C illustrates a side view of an exemplary second imaging member. 19D illustrates a back, end view of a second imaging member coupled to an intubation tube. 19E illustrates a front, end view of a second imaging member coupled to an intubation tube. 19F illustrates a top view of an exemplary second imaging member with a top face removed to show internal components. 19G shows a bottom perspective view of an exemplary housing of a second imaging member. 19H shows a bottom view of an exemplary housing of a second imaging member. 19I shows a top perspective view of an exemplary housing of a second imaging member. 19J shows a front, end view of an exemplary housing of a second imaging member. 19K shows a back, end view of an exemplary housing of a second imaging member. 20 schematically shows an exemplary integrated and handheld dual-video assembly. 21 illustrates an exemplary method of using any of the integrated handheld assemblies herein."
                    ],
                    "DESCRIPTION": "Examples of various aspects and variations of the inventions are described herein and illustrated in the accompanying drawings. The following description is not intended to limit the inventions to these embodiments, but rather to enable a person skilled in the art to make and use these inventions. The disclosure herein is related to airway management methods, devices and systems. While tracheal intubation is provided as an example of airway management herein, it is understood that the disclosure is not so limited, and that the concepts herein may be applied or applicable to other airway management procedures, locations and settings. For example, concepts herein may be used in bronchoscopy procedures or in endoscopic ENT procedures such as flexible nasolaryngoscopy, esophagoscopy, vocal cord injections, certain laryngologic surgical procedures, and other endoscopy procedures involving upper gastrointestinal GI tract , gastroscopy, esophagoscopy etc. Airway management procedures herein may include any of the following exemplary and non-limiting procedures in both adult and pediatric patients: 1 endoscopic evaluation of the airway in a patient to determine or establish: a the presentation of and the relationship between different parts of the upper airway anatomy, b the size and location of the lesions and/or location and the extent of the pathological processs, c the feasibility of the supraglottic airway devices SGAs, , laryngeal mask airway, placement and the likelihood of successful SGA ventilation, d whether awake and/or TI is feasible, e an optimal TI navigation pathway to the larynx, f the optimal TI device use and the optimal TI strategy; 2 facilitating the exchange of the ETT; 3 Evaluation of the positioning and/or patency of ETT and/or confirming the correct placement of ETT inside the trachea; 4 facilitating placement and confirming proper positioning of the double lumen tubes; 5 facilitating an extubation trial; 6 performing the bronchoscopy; 7 performing a nasal and/or an oral TI; 8 performing the ENT procedures, such as endoscopies, esophagoscopies, biopsies, injections, certain laryngologic surgical procedures; and 9 performing other airway therapeutic, interventional and/or diagnostic procedures. As used herein, airway management procedures may be used in any of these non-limiting locations and settings: OR, ICU, ED, out-of-OR locations endoscopy suits, imaging scanners, different ambulatory and hospital settings, procedure rooms, , and in the field EMS and battlefield airway management, both on site and during the patient transport. Airway management as used herein may include or be used in visualization procedures, diagnostic procedures, interventional procedures, surgical procedures, and/or therapeutic and diagnostic procedures. Airway management concepts described herein may find utility in non-medical applications. Devices and systems adapted for assisting navigation and/or intubation. Some non-limiting aspects of the disclosure herein are directed to portable, hand-held, and integrated dual-video enhanced visualization and navigation systems that are adapted for guiding an ETT into a patient's trachea during a TI procedure. Such systems are adapted and configured to provide and improve all 3 critical steps required for successful TI: Visualization, Navigation/Movement and Placement. An integrated dual-video system that is configured, adapted and sized to be held in a single hand of a user provides the benefit that the user is able to single-handedly hold and optionally control the dual-video integrated and enhanced visualization and navigation system, allowing a single operator to reliably facilitate the navigation and movement of an ETT introducer guide into a trachea of the patient. The integrated dual-video systems herein provide enhanced visualization of the patient's airway anatomy, which facilitates enhanced navigation of the ETT in and around the patient's anatomical structures as well as improved ETT placement insertion both through the glottic opening and into the patient's trachea during a TI procedure, all of which help reduce TI trauma, increase the likelihood of first pass TI success the success of TI on the first TI attempt. The disclosure herein may refer to navigation when describing movement of an introducer. It is understood that in some instances navigation may also refer to automatic determination of how or where the introducer is to be robotically moved , using image recognition. Navigation may be performed manually in embodiments in which an operator determines how or where to move the introducer, such as, for example, based on viewing an image on a display. Described herein are variations of devices, systems and methods for robotically assisting navigation and movement of an introducer within an airway or other passageway in a patient, such as during an intubation procedure , orotracheal intubation, nasotracheal intubation, . In some examples herein, the robotically assisted movement herein may include using artificial intelligence AI to enable the robotically assisted navigation. As shown in the schematic of 1, in some variations, an integrated robotic device or system 100 may include a housing 110, a laryngoscope or baton 120 , a video laryngoscope coupled to the housing and including a first image sensor, and an actuating member 140 disposed at least partially in the housing. Housing 110 may, for example, be an integrated, handheld device so as to be portable and/or configured to be easily operated by a single person. The actuating member may be movable within the housing, and an introducer such as an endoscope 150 , video endoscope may extend from the actuating member 140. The introducer may include a second image sensor and may be configured to be slidably coupled to an intubation tube such as an ETT, wherein the coupling may comprise the ETT being slidably disposed about the introducer. When the introducer is robotically moved to a passageway of a patient , trachea, the introducer may function as a steering guide for intubation tube advancement during the intubation procedure. Intubation tube advancement may be performed, for example, either manually or automatically by passing the intubation tube over the introducer. Once intubation tube placement such as in a trachea is confirmed via real-time images from the one or more image sensors such as the introducer image sensor when the introducer is in the trachea, the intubation tube may be decoupled from the introducer which may include sliding the introducer proximally out of and relative to the ETT, and the introducer and the rest of the device 100 may be withdrawn from the patient's anatomy, leaving the intubation tube , ETT in place. In any of the examples and embodiments herein, the robotically assisted navigation/movement can include robotically assisted navigation of an introducer, of which endoscopes which may be referred to as scopes herein described herein are examples. The introducers herein are generally described as including one or more image sensors, but in alternative systems they may not include an image sensor, or the image sensor may not be continuously used. In examples in which the introducer does not include an image sensor or the image sensor is not in continuous use, the introducer may be robotically navigated with AI-assistance using images obtained from an integrated image sensor, such as an image sensor associated with a first imaging member, such as a video laryngoscope, for example, exemplary details of which are described herein. When utilizing a non-optical introducer, placement of the intubation tube below the vocal cords lower airway is not visualized with the non-optical introducer, and thus intubation tube placement below the cords cannot be immediately confirmed. A non-optical introducer, may, however, depending on the application and clinical situation, provide simplicity and cost advantages compared to optical introducers such as the optical introducers described herein that include one or more image sensors. Although the devices, systems, assemblies and methods are primarily described herein with reference to an intubation procedure, it should be understood that the devices and methods may be used to assist other medical airway management procedures involving navigation through one or more passageway, such as endoscopy procedures , bronchoscopy, . For example, navigation of the introducer may be robotically-assisted using devices and methods such as those described herein, except that the introducer may be automatically guided using AI/automated techniques without being coupled to an intubation tube. For example, the device may robotically assist navigation of the introducer during any suitable medical endoscopic procedure to provide for a faster and/or less traumatic endoscopic procedure compared to conventional manual techniques. As an illustrative example, the devices, systems, assemblies and methods described herein may, instead of automatically guiding an intubation tube, automatically guide an introducer endoscope 150 based one or more images obtained with the introducer and or first imaging member , laryngoscope. In some variations, the introducer endoscope 150 may include one or more channels for irrigation, drug delivery, tissue biopsy and/or deployment of a surgical instrument, for example. Even further, in some variations, devices, system, assemblies and methods herein may be used in other applications , non-medical applications in which automated navigation/movement may be helpful, such as navigation within a passageway that is challenging to otherwise access. Exemplary system 100 in 1 may optionally include at least one display 118 configured to display one or more images from a first image sensor in a first imaging member , laryngoscope and/or a second image sensor of a second imaging member which may comprise an introducer, and may be configured to display images from both the first and second image sensors simultaneously, such as split-screen or picture-in-picture. The images may be displayed continuously or substantially continuously or sequentially during the procedure. System 100 may further include at least one actuator 116 in the housing that is configured to automatically guide/move the introducer , an endoscope via the actuating member 140, based at least in part on one or more images from the first image sensor and/or the second image sensor. In some variations, the system may further include a cover or blade coupled to the housing, where the cover may include a first channel sized and configured to receive a first elongate imaging member such as a video baton and a second channel configured to stably receive at least a portion of an intubation tube in which an introducer may be disposed. The cover may further include a tongue retracting member such as an angled or curved member configured to retract a tongue of a patient during an intubation procedure, examples of which are shown in figures herein. In some variations, one or more actuators may be under operational control by one or more processors 112 configured to analyze images or image data from one or both of first and second image sensors in the system using suitable AI , machine learning methods. Various electronics 114 , power sources, electronic communication lines, in the system or housing 110 and/or display 118 may power the one or more processors, image sensors, light guides, etc. in the system. AI-based image recognition in real-time or near real-time of the patient's upper airway anatomy above the vocal cords and/or lower airway anatomy below the vocal cords may, for example, trigger robotic-assisted tracheal intubation with the system. As described in further detail below, the visualization and navigation/movement to or towards the vocal cords may, for example, be based on identifying one or more key anatomic recognition points provided by the image sensors of the first and/or second imaging members. The system may perform robotic-assisted navigation that is activated either automatically in response to recognizing airway anatomy, and/or in response to a manual activation , through user operation of a user interface element. For example, in a fully automated mode of exemplary device or assembly 100, the actuator may automatically maneuver the endoscope using AI and/or other robotic-assisted navigation, and electromechanical control of the actuating member. Additionally or alternatively, the device or system may operate in an automated mode with an exemplary manual assist. In such an automated-manual assist mode, for example, the actuating member and/or introducer may be controlled manually, such as with the use of a user interface device , joystick or through the device display 118. Articulation of a distal tip of the introducer when the device is in the automated-manual assist mode may, for example, occur automatically under automatic robotic control of one or more actuators in the system. When used for TI, the systems herein may be user-friendly, portable, handheld, video-triggered and AI-enabled robotic assisted automated intubation systems with enhanced functionality that improve all three critical steps required for successful TI: visualization, navigation and placement, and increase first-pass intubation success rate, decrease overall intubation time, reduce intubation-related airway trauma, and/or improve patient safety. For example, the systems herein may, in an ergonomic package operable by a single user, combine multiple imaging , video modules that allow the device to perform intubation using AI or robotic-assisted introducer navigation. The robotic-assisted intubation interface may, for example, maneuver at least the guiding introducer through the glottis and into the patient's trachea, where it may serve as a guide for ETT advancement and placement. The ETT may, for example, be preloaded onto the introducer and advanced over the introducer after the introducer is advanced into the trachea and after proper placement in the trachea is confirmed. Among other advantages such as those described herein, the systems herein may be configured to provide continuous visual feedback and optional closed-loop robotic assistance for real-time troubleshooting and/or intelligent intervention during tracheal intubation, both from above and below the vocal cords, thereby improving TI success rate, improving the intubation speed and reducing TI trauma. Additionally, combined use of the first and second imaging members may allow for reliable and faster triggering of AI and the associated robotic interface, due at least in part to an enlarged and clear view of the upper airway anatomical landmarks provided by the first imaging member such as laryngoscope 120. Another advantage of acquiring initial imaging through the first imaging member refers to the fact that visualization of the patient's anatomy is much less affected by blood and secretions compared to the image provided by the second imaging member an image sensor at a distal region of an introducer. Furthermore, combined use of a first and second imaging members, the angular orientation of their respective video cameras and the placement and maintenance of their video cameras in close axial proximity to each other and to the glottic opening, provides for the shortest, quickest navigation pathway for the introducer and ETT to the vocal cords and trachea. Furthermore, the actuation of the introducer within the patient's airway is greatly facilitated by the device cover or blade, which is configured to perform tongue retraction, thereby creating greater pharyngeal space for introducer actuation and maneuvering. One of the optionally significant benefits of some of the systems herein is that the system may be configured to allow a user to observe the intubation procedure in its entirety, both from above upper airway and below the vocal cords lower airway, to permit immediate visual confirmation of intubation tube placement inside the trachea during intubation using an introducer imaging sensor, and to assure that the intubation tube is positioned optimally inside the patient's airway, exemplary embodiments of which are described below. Furthermore, the integrated dual-video capability of systems herein may significantly reduce intubation-associated airway trauma and soft tissue injury that is often associated with conventional devices, even in the situations when visualization of the upper airway anatomical landmarks is limited in the absence of full glottis exposure. For example, after identifying the anatomical structures reliably associated with the glottic opening epiglottis, arytenoid cartilages, during initial video image capture with a first image sensor, the system can automatically maneuver the introducer through the glottic opening into the patient's trachea even when the glottic opening cannot be visualized. The device may also reduce the risk of esophageal intubation and/or disturbance, defragmentation, bleeding, and/or airway soiling during intubation, such as when tumors or other space-occupying lesions are present inside the patient's airway. In some variations, the systems may be useful in situations in which increased distance from the patient's airway is desirable to decrease the likelihood of airborne and/or contact transmission of infection from the patient to the operator of the device. For example, because of the minimal airway manipulation required and the automated nature of the intubation performed, the operator of the integrated system may be able to hold the system from a suitable distance , arm's length and avoid the risk of directly looking inside the patient's airway to reduce the likelihood of transmission of infection , from patients with contagious bacterial and/or viral disease, such as COVID-19 and others. The systems herein may be used in, for example, for routine elective and/or anticipated and/or unanticipated difficult tracheal intubation in any suitable setting, such as OR, ICU, emergency department, out-of-OR locations , clinics, code situations, , pre-hospital conditions field and battlefield airway management, and/or other elective and/or urgent and/or emergent situations. Additionally, the systems may be used for TI for a wide range of patients and across a variety of diagnostic and/or therapeutic procedures, such as where airway support and/or protection of the patient's airway and/or pulmonary hygiene is desired or indicated, such as the patients undergoing interventional endoscopy procedures bronchoscopy, GI endoscopy, , transesophageal echocardiogram, CT and MRI imaging procedures, any medical procedures that may require sedation and/or airway support and/or airway protection, etc. The systems may be useful for TI in specific patient populations where increased TI difficulty may be anticipated, such as those who are obese, have obstructive sleep apnea, patients with head and neck cancer and other pathology, elderly patients, patients at high risk for dental damage, patients in whom neck movements are not desirable, trauma patients, patients for whom it is important to minimize adverse cardiovascular responses to intubation , hypertension, tachycardia, arrhythmias, , critically ill patients and others. The device may, in some variations, by useful for TI among adults and pediatric patients. 2A-2C are schematic illustrations of a merely exemplary portable, hand-held integrated dual-video robotic assembly or system 200, which may be adapted for visualization and navigation and placement during of an intubation procedure. As shown in 2A, system 200 may include handheld housing 210, electronics system 214 , including one or more processors, one or more power sources, etc. , examples of which are described herein, and one or more actuators 216. The device may further include a first elongate imaging member such as a laryngoscope with a baton 220 or other imaging member with at least one image sensor 222 disposed at a distal region of the first imaging member as shown, and an actuating member 240 movable within housing 210. Additionally, system 200 may include an introducer such as endoscope 250 with image sensor 252 disposed at a distal region of the introducer as shown. Introducer 250 may be configured to be coupled to an intubation tube , an ETT for use during a TI procedure. In this context, coupled to includes an ETT being axially movable about or over the introducer. As shown in 2B, 4A, and 4B, system 200 may further include cover 260 including channels sized and configured for receiving the first imaging member , laryngoscope baton 220 therein and at least part of the ETT and introducer , endoscope 250. As shown in 4A and 4B, cover 260 may further include a distal region that includes member 268 that is configured to manipulate tissue , patient tongue during an intubation procedure. Any of the covers herein may include a member similar to member 268 configured to manipulate tissue , patient tongue during an intubation procedure. While additional exemplary aspects of system 200 for enhanced visualization and navigation during intubation are described in further detail below with reference to 2A-2C, 3A-3D, A-4B, 5, and 6, it should be understood that the described aspects may be applied to other variations of systems and devices having other sizes, shapes, etc. Handheld HousingsExemplary housing 210 may be sized and configured to enclose various software and hardware components for performing robotic-assisted TI, such as electronic components , processors, memory, power sources, motors, and/or an actuators for guiding the introducer, such as during an intubation procedure. The housings herein may further be sized and configured to integrate the first imaging member , a video laryngoscope and the second imaging member , endoscope into a single, user-friendly portable and handheld system that may be operable and controlled by a single hand or a single user. Both the handheld housings herein , housing 210 and the integrated dual-video assemblies herein which may include the handheld housing may advantageously be sized and configured to be held by a single hand of a single operator, providing the benefits described herein. The housings herein , housing 210 may be configured as a handheld housing that may be held ergonomically in the hand of a user. The handheld housing may be contoured , with finger grips, or otherwise configured for a particular hand , left hand, right hand, or may be suitable for either hand. For example, as shown in the front view depicted in 2A, the handheld housing may be configured for use with a left hand. In some variations, the housing may include one or more other ergonomic features, such as cushioning to improve user comfort , foam or rubber padding, silicone gel, , frictional features , rubberized grip, textural features such as ribbing, . In some examples, the handheld housing may be between about 10 cm and 75 cm in length, such as between about 15 and about 45 cm in length, but may be any suitable size that is compatible with device ergonomics that is able to be held by a single hand of an operator. In some embodiments the housing is from 2 cm to 15 cm wide, such as from 4 cm to 10 cm wide. In some embodiments the housing is from 5 cm to 20 cm measured top to bottom, such as from 5 cm to 15 cm. The housings herein , housing 210 may be made of any suitable rigid or semi-rigid material. For example, the housing may include plastic that is formed through a suitable injection molding process. In some variations, the housing may include one or more separate components , shells that are coupled together through one or more suitable fasteners , epoxy or other adhesive, mechanical fasteners and/or mating features , threaded or snap-fit or complementary features on different housing components. The housing may enclose, in an interior volume, various electronic components, actuators, and/or other aspects of the device, examples of which are described herein. Electronics SystemsThe systems or assemblies herein may include an electronics system , electronic system 214 that may include at least one processor and/or at least one memory device. At least a portion of electronics system 214 may, for example, be arranged in the housing and/or a display coupled to the housing described in further detail below. A memory device may store instructions , in the form of software, computer executable methods for one or more processor to analyze images from the first imaging sensor and/or the second imaging sensor, and/or perform AI-based analysis of such images with smart image recognition technology and/or machine learning to automatically navigate the introducer into the trachea during intubation. The processors may also be configured to perform automated control of the actuator in the device for guiding at least a portion of the actuating member and/or introducer for intubation assistance. The automated navigation may perform the intubation procedure in a user-friendly manner, with little user training or experience to enable successful intubation. Additional details of such AI or machine learning algorithms are described further below. Electronics system, conceptually designated as exemplary electronics system 214, may further include other components for supporting the device, such as at least one power supply. In some variations, the power supply may include at least one battery as a self-contained power supply , to help facilitate device portability. Additionally or alternatively, the housings and/or display may include a power supply connector or port 211 that may enable a wired power connection, such as to an external AC or DC power supply. Additionally or alternatively, the housings may include a wired connection , cable and/or wireless communication module for communicating with a free-standing video monitor. Furthermore, in some variations, the housings may include an emergency stop control , button which halts automated robotic assistance of the device such as by intention of the operator , suddenly ceasing current to actuators, . In some variations, the housings may further include a power button that controls powering the system on and off, and/or ports for downloading the images and updating software programs. Actuation Units, Actuators, Actuating MembersThe housings herein , housing 210 may include one or more actuators 216 configured to automatically guide movements of actuating member 240 for AI or robotic-assisted intubation while the introducer is coupled , releasably coupled to an intubation tube. In the example of 2A, advancement of actuating member 240 may also extend the effective operating length or working length of endoscope 250. As described in further detail below, actuators 216 may be integrated in the housing and exert its actuation on the actuating member 240 through a coupled interface, such that the generated movements are transmitted along the longitudinal axis of the actuating member and along the longitudinal axis of the endoscope 250 extending from the actuating member. One or more actuators 216 may additionally be included to operate and/or articulate the distal end of the endoscope 250, as further described below. The device may include any suitable actuator and mechanical or electromechanical assemblies for controlling the actuating member 240 and/or the introducer. For example, exemplary actuators 216 and associated controllers may include suitable drive electronics, one or more electrical motors, hydraulics, and/or pneumatics, as well as suitable mechanical assemblies, connections, joints, and/or controllers. For example, control assemblies, connections, and joints for controlling the actuating member and/or endoscope may include longitudinal elements with bidirectional push-pull cables, wire pulley assemblies, chain drives, hinges, slide-crank mechanisms, piezoelectric elements, pneumatic elements and assemblies, magnetic elements, adjustable couplings, sleeves, belts, gears, pushers, plungers, movable racks, compressions springs, translational rotary-to-linear and/or linear-to-rotary motion modules, gear drives, or other suitable motors and/or joints, etc. Other suitable fasteners and bearing surfaces may furthermore be included in actuators assemblies 216. Exemplary Actuators 216 may be activated automatically and/or manually. For example, in some examples, actuators 216 may be activated through one or more processors executing image recognition software instructions, in response to the processors recognizing one or more anatomic landmarks through such image recognition techniques. This automatic activation may, for example, be part of a fully automated mode of the device. Additionally, or alternatively, actuators 216 may be selectively engaged and/or disengaged in response to a user selection of one or more user interface elements. For example, actuators 216 may be activated by selection of an AI operational button 280 or the like as shown in 2C and deactivated by selection of a STOP button 284 or the like as shown in 2C. It should be understood that the system may additionally or alternatively include other user interface elements to activate/deactivate a fully automated mode of the device, such as a toggle switch, touch-sensitive pad, touch screen, user interface icon on display screen, etc. In some variations, manual actuation of actuating member 240 , with a joystick or other user interface element, as described below may instantly override the fully automated mode. In these variations, the fully automated mode may then be allowed to resume after the user presses AI operation button 280, for example. Additionally, or alternatively, the fully automated mode may resume after a predetermined period of time , period of inactivity or non-movement of the actuating member 240, predetermined duration of time, while in the manual assist mode. In yet other variations, a selected manual assist mode may override the fully automated mode as long as a clutch , button, switch, joystick, or other suitable selectable mechanism is engaged, while release of the clutch button may allow the fully automated mode to resume. DisplaysAs shown in exemplary 2A-2C, any of the systems or assemblies herein , system 200 may optionally further include a display, such as display 218, such as a monitor screen, touch screen, or the like. The display may be configured to display image data and/or a user interface. For example, the display may be configured to display single-channel images from only the first image sensor , a laryngoscope image sensor 222 or only the second image sensor , introducer image sensor 252. As another example, the displays herein may be configured to display multi-channel images from both the first and second image sensors , 222 and 252, such as in a split screen and/or picture-in-picture arrangement of images from the, for example only, laryngoscope and/or endoscope image sensors. In some variations, the display may be configured with default or preprogrammed display sequences. As one example of a default display sequence, the display may be configured to initially display a video feed or signal from a video laryngoscope to provide visual feedback of initial entry into the patient's airway , upper airway and/or of identified anatomic structures, then automatically transition to display a multi-channel set of images , split screen, picture-in-picture from an endoscope image sensor , 252 and/or an laryngoscope image sensor , 222 upon activation of AI-based actuation either automatically or through manual activation, as described above and/or upon pressing a selectable user interface element on the device and/or display. In some variations, other toggling of video feeds from the various image sensors may be accomplished through certain preprogrammed operation of user interface elements , pressing a picture-in-picture button or icon twice or several times in succession may result in a full image of the patient's trachea and intubation tube placement or other anatomy as provided from the endoscope image sensor 252. In some examples, the displays herein may be coupled to the housing 210 or other housing herein , on a proximal portion of the housing. The display may include any suitable display elements , LCD. In some variations, the display may be coupled to the housing via a rotatable or pivoting coupling, such that it may swivel around a longitudinal axis and/or tilt around a vertical and/or lateral axes and be viewable from multiple angles. Alternatively, the systems herein may include a multi-faced , dual-sided display to permit viewing of displayed content from multiple angles simultaneously. In any of the embodiments herein a display coupled to a handheld housing may be portable sized, such as between about 8 cm and 15 cm high and between about 10 cm and about 18 cm wide, but the display may be any suitable size and/or shape. Additionally, or alternatively, any of the systems herein may include and be communicatively coupled to a remote display that is not part of an integrated assembly. For example, the systems herein may include one or more ports in any of the housings herein for a wired communication to a display device. As another example, the systems may include a wireless communication module and antenna to communicate content for display to other screens , via cellular mobile network, WiFi, . First Elongate Imaging Member , a LaryngoscopeAny of the integrated dual-video systems or assemblies herein may include a first imaging member , a video laryngoscope, which may include an elongate flexible body and a first image sensor , a video camera disposed at a distal region of the elongate body. Any of the first imaging members herein may also be referred to as first elongate imaging members, indicating in general that they have an elongate configuration. For example, as shown in exemplary 2A-2C, system 200 may include a first imaging member , video laryngoscope. The first imaging member may include a baton 220 or other elongate member extending distally from housing 210, as shown. Baton 220 may include a high-resolution image sensors at a distal region for providing video images , video camera, one or more light guides for providing illumination to the image sensor field of view, and electronic signal wires for transmitting video data or images for processing and/or display on the display 218 or other suitable display. The image sensors 222 may be located at a distal end of the elongate member, and may be adapted to provide a relatively wide angle of view for enlarged and clear visualization of the patient's anatomy in both axial and horizontal planes during an intubation procedure. The relatively wide angle of view and the angular position of the first imaging sensor relative to the second image sensor can help reliably visualize and identify critical anatomical landmarks in an upper airway, thus enhancing the navigation of the introducer as it is moved distally relative to the first image sensor. The combined use of the first and second imaging members, the optionally large angle of view and the placement and maintenance of the video cameras of the first and second imaging members in close axial proximity to each other and to the glottic opening, provide for the shortest, quickest navigation pathway for the introducer and ETT to the vocal cords and trachea, as described in more detail below. In some merely optional but not limiting examples, a first image sensors of a first imaging member may have a wider angle of view than a second image sensor of an introducer. Accordingly, a first imaging member may provide for an enlarged and clearer image of patient's anatomy compared to conventional devices, which may facilitate a faster, more reliable AI image recognitions and/or initiation of robotic-assisted control and movement of the introducer using the system. Any of the image sensors herein may optionally include a charge-coupled device CCD, CMOS sensor, and/or other suitable sensors, and may be combined with any suitable optical elements such as an objective lens, etc. In some embodiments, first and second video cameras of the first and second imaging members may have the same angle of view, while the first image sensor can still provide visualization of a larger anatomical area relative to the second image sensor by being maintained proximal or behind the second image sensor as the introducer is advanced distally toward a glottic opening, for example. In some embodiments still, the first video camera may have the smaller angle of view than the second video camera, but the first image sensor can still provide visualization of a larger anatomical area relative to the second image sensor by being maintained proximal to or behind the second image sensor as the introducer is advanced distally toward a glottic opening, for example. Exemplary methods of use that include utilizing first and second simultaneously presented video signals from the first and second video cameras are included in more detail below. In any of the systems herein, a first imaging member may comprise a laryngoscope, which may include a baton or other elongated member that is between about 10 cm and about 15 cm long, but may be any size appropriate for an adult and/or pediatric patient population. As is described below, the systems herein may optionally include universal handheld housings that are adapted to be able to be used interchangeably with first imaging members of different sizes for different patient populations , adult and pediatric, which provides more functionality to the universal handheld housing. As shown in exemplary 2A, exemplary laryngoscope baton 220 may generally taper in diameter as it extends away from the housing 210, and/or may gently curve or flare to accommodate imaging components, and/or provide for better angling during an intubation procedure, etc. However, first imaging member 220 may have any suitable shape and size for use with patient anatomy. In some examples, the first imaging member , which may include a laryngoscope baton, may be flexible and may have an at-rest curved configuration, and in some examples it may have a straight or substantially straight configuration that is adapted to be bent within the cover. First imaging member 220 may be either removably or releasably coupled to, or integrally or permanently coupled to housing 210 , intended for reuse. For example, first imaging member 220 may remain sterile between uses by use of a disposable cover 260 as further described below, or may be sterilized between uses with a suitable disinfectant, etc. Alternatively, in some variations, first imaging member 220 may be modular, or detachable from housing 210 , snap-fit connections, , and may be disposable and/or swapped out and replaced by different first imaging member 220 between different uses of system 200. For example, first imaging member 220 may be swapped and replaced by a different first imaging member 220 to avoid the need to sterilize the baton between uses. As another example, first imaging member 220 may be removed to facilitate separate sterilization between uses. As another example, different first imaging members 220 may have different lengths, diameters, and/or shapes for different types of patients , adult patients, pediatric patients, such that first imaging member 220 may be replaced by a different first imaging member 220 of a different desired size depending on the patient and/or situation. Furthermore, as described in further detail below, a cover such as cover 260 may be sized appropriately , with a channel defining a lumen of a suitable diameter and/or length for the dimensions of first imaging member 220. Actuating Member and Introducer EndoscopeIn some examples, the integrated assembly or system may optionally include one or more actuating members as well as an introducer. For example, in exemplary 2A-2C, device 200 may include an actuating member 240 and an introducer , endoscope 250 extending from the actuating member 240. In this non-limiting example, actuating member 240 and endoscope 250 may, for example, be joined or couplable together end-to-end to form a flexible member. Generally, a combination of actuating member 240 and endoscope 250 or other introducer may include a flexible insertion tube or rigid video stylet or the like to engage with an intubation tube, at least one image sensor 252 arranged at a distal end of the introducer 250, and an articulable distal tip of the introducer 250 that may be controllable by one or more tensioning elements such as one or more pullwires secured to a distal region of the introducer, or other suitable control mechanisms. For example, the introducer , introducer 250 may include a high resolution image sensors or video chip camera module at its distal end, such as is shown in exemplary 2A, such as a CCD or CMOS image sensors, an objective lens, and/or other suitable optical configurations. In an exemplary embodiment, the image sensor may, for example, be between about 2 mm and about 4 mm in size and provide a wide angle of view , at least 90 degrees, at least 180 degrees, at least 270 degrees, or 360 degrees, . One or more light guides , carrying LED or other illumination may pass through the shafts of the actuating member and introducer to provide illumination during an airway management procedure. Signal wires along the shafts of the introducer may carry images or image data from the image sensor to an image processor, which may optionally be disposed in the housing and/or a display, or which may be disposed in an external device. Furthermore, the distal end of the introducer 250 may include a bendable, articulating structure , with jointed segments that may be controlled by angulation pullwires or tensioning cables or other suitable mechanisms. Such angulation wires may, for example, control up-down and right-left steering movements of the articulating tip of the introducer 250. At least a proximal end of optional actuating member 240 may be coupled to and/or located within housing 210 and controlled by one or more actuators. Actuating member 240 may be longitudinally advanced , at least partially out of housing 210 and retracted proximally back along a longitudinal axis and/or rotated by one or more actuators 216, and therefore control advancement, retreat and/or rotation of introducer 250. In some variations, actuating member 240 may extend the working length of introducer 250, in that the advancement of actuating member 240 may enable the distal end of introducer 250 to be located farther distally than introducer 250 is capable of doing so alone. Furthermore, actuating member 240 may be retractable inside the housing 210, such as, for example, with selectable button 290 as shown in 2C. In some variations, actuating member 240 and introducer 250 may be integrally connected, such as in a permanent fashion. In other words, actuating member 240 and the introducer may optionally be structurally and functionally integrated. Thus, in some variations, an entire flexible member including both actuating member 240 and introducer 250 may remain coupled to housing 210, and the entire flexible member and potentially the entire device 200 may be fully sterilized between uses. In other variations, actuating member 240 may be detachable from housing 210, such that actuating member 240 and introducer 250 may be sterilized separately from housing 210. For example, actuating member 240 may be releasably coupled to housing 210 through connector fittings, fasteners, mechanical interfit , threads, interference fitting, nesting, or in any suitable manner. In some variations, a disposable protective sheath or other cover may be removably placed over at least introducer 250, which may eliminate the need for full sterilization of the actuating member, introducer, and/or entire device between uses. In other variations, actuating member 240 with its connections to user interface device 270 , including guide 217 and/or a control member 274 such as a joystick as shown in 2C and 3, for example may be integrated in a single module, which removably attaches to housing 210 and to actuating controls 216 , snap-fit, plug-and-play connections, . In another variation, the actuation controls, in full or in part, can also be included in such module. Such an integrated module, with either permanent or detachable connection to introducer 250, can easily be removed from housing 210 and replaced with a new instance of a similar module and/or swapped out for a new and different module that contains, for example, a different actuating member 240 with a different outside diameter, length, etc. This can further allow the housing to function as a universal handheld that is adapted for use with introducers of different sizes, which may allow a single handheld housing to be used to treat different patient populations , adults and pediatrics. Alternatively, in some variations, any actuating member and any introducer , an endoscope herein may be detachable from one another. An actuating member and an introducer may be coupled together through connector fittings, fasteners, electromechanical interfit , threads, interference fitting, bands, nesting, or in any suitable manner. Once coupled, an actuating member and an introducer may function as a continuous single member. For example, introducer 250 may be removably attached from actuating member 240 or housing 210 such that introducer 250 may be disposable. In this example, introducer 250 need not be sterilized, as it may optionally be discarded after use. External surfaces of actuating members and introducer may optionally be covered in a coating , a polymeric material that provides for an atraumatic, biocompatible, and watertight smooth surface. In some embodiments, an actuating member and/or an introducer may include an outer diameter that is between about 2. 5 mm and about 7. 0 mm. The proximal part of an actuating member , between about 0. 5 cm and about 6. 0 cm long or even the whole length of an actuating member may, in some variations, have a larger outer diameter as needed to accommodate various connections to an actuators in a housing and/or the user interface device as further described below. The total combined length of the actuating member and introducer may be, for example, between about 30 cm and about 70 cm. In some variations, an actuating member may be between about 10 cm and about 40 cm long, and the introducer may be between about 20 cm and about 30 cm long. However, diameters, and lengths of an actuating member and/or an introducer may be varied for different applications , adult vs. pediatric patients, . For example, a variation of the device that may be suitable for assisting nasotracheal intubation may include an introducer that is longer, such as between about 20 cm and about 60 cm, which may contribute to an overall combined length between an actuating member and an introducer that is between about 30 cm and about 100 cm long. Other exemplary structural aspects of actuating members and introducers are described in further detail below. Actuating MembersAny of the integrated systems or assemblies herein may optionally include one or more actuating members, and the disclosure that follows may apply to any actuating members herein. At least a proximal end of exemplary actuating member 240 may be located in housing 210 and/or display 218. Actuating member 240 may be driven in multiple degrees of freedom to result in corresponding motions of the introducer , endoscope 250. For example, actuating member 240 may be actuated via an electromechanical coupling in linear advancement forward-back along a longitudinal axis, and axial rotation rotation around a longitudinal axis. Other actuation at or near actuating member 240, such as via guidewires described above, may cause the distal end of the introducer to additionally articulate in up-down and/or right-left tip movements. Collectively, actuating member 240 and the introducer may be constructed such that actuation and movements at actuating member 240 may result in transmission of all desired degrees of freedom to the introducer. Similar to that described above, any suitable actuators 216 and accompanying control systems for driving actuating member 240 may be included in the device, such as drive electronics, one or more electrical motors, hydraulics, pneumatics, and/or various mechanical parts assemblies, connectors, joints, controllers, as suitable. Control assemblies, connections, and joints may be configured for smooth and precise transmission of the desired actuated mechanical motions along the actuating member to the introducer and the distal articulating tip of the introducer. As shown in exemplary 2A, in some variations actuating member 240 may travel along a designated path within housing 210. For example, as shown in 2C, actuating member 240 may be arranged along guide 217 in housing 210 or on a surface of the housing 210. Guide 217 may, for example, include a guiding channel , at least between about 0. 5 cm to about 1. 0 cm wider than the actuating member 240, or any suitable rail, track, or other guiding structure. In other words, actuating member 240 may travel within the guiding channel as it is driven by the one or more actuators 216 described above in a fully automated mode of the device. Although guide 217 is shown in 2C as curved, it should be understood that in other variations guide 217 may be straight or another suitable shape that fits within housing 210. 3B-3D illustrate exemplary variations of actuators that are configured to move the actuating members herein along a guide. 3B is a schematic illustration of axial actuator 216a configured to move actuating member 240 in an axial or longitudinal direction , in advancement and/or retreat, to thereby axial move the introducer in a similar fashion. For example, as shown in 3C, axial actuator 216a may include one or more matched pairs of opposing driving wheels which engage actuating member 240 via grooves, frictional elements, and/or the like. The driving wheels in each matched pair of driving wheels wheel A and wheel B may be located on opposite sides of actuating member 240 or the introducer such that their synchronous rotation in opposite directions urges actuating member 240 of the introducer forward and backward in an axial or longitudinal direction, such as along guide 217. Other suitable mechanisms, such as one or more of slider crank systems, belt or pulley systems, plunger actuators, corkscrew mechanisms, etc. may additionally or alternatively provide axial actuation of actuating member 240. Furthermore, as shown in 3B and 3D, one or more actuator connections to rotational actuator 216r may also be coupled to actuating member 240. The rotational actuator 216r may include one or more pullwire or other tensioning element attachments coupled to actuating member 240 configured to control a side-to-side, or left and right, movement, similar to that described below with respect to 3B. However, any other suitable mechanism such as rollers, etc. may be used to automatically actuate the rotational movement of actuating member 240 and hence the guiding introducer. Additionally or alternatively, actuating member 240 may travel within guide 217 as actuating member 240 is driven manually such as via the user interface device 270, a touch screen, voice commands, etc. , such as in a manual assist mode. For example, as shown in 3A, user interface device 270 , including a control member 274 such as a joystick may be coupled to actuating member 240. User interface device 270 may be engaged within control member guide 272 as shown in 2C and 3A, which may generally follow or parallel the trajectory of guide 217 for the actuating member 240. User interface device 270, including a control member 274, such as a joystick may be coupled to actuating member 240 as shown in 3A via control wires 276 attached at connection points 278 and 279 , with fasteners, welding, or in any other suitable manner. For example, the top connections 278 between control member 274 and actuating member 240 may control a manual forward movement F as a user manually manipulates , pushes and/or pulls the control member 274 forward and backward within the guide 272. As another example, the side connections 279 between the control member 274 and actuating member 240 may control a side-to-side, or left and right movement L and R as a user manually manipulates , pushes side to side the control member 274 within the guide 272. Accordingly, user interface device 270 may allow for a manual assist mode , manual advancement and/or rotation of the actuating member 240 and hence the introducer , endoscope 250, but with automated actuation of the endoscope's distal articulated tip. These connections 278 and 279 may remain inactive in a fully automated mode. In some variations, the manual assist mode may be confirmed with a user interface element 282 , button shown in 2C, whereby selection of the user interface element 282 may cause the device to enter the mode in which automated movements are limited to articulation of the distal tip of the introducer , endoscope 250, while linear and/or rotational endoscope movements are controlled manually as described above , with user interface device 270. The user interface element 282 may, for example, help the operator activate the manual assist mode if such a mode is desired. In some variations, any of the actuating members herein may be self-expanding. For example, as shown in 2A, actuating member 240 may include a self-expanding construction, such as including interlocking rings and/or spiral elements that transition from a compressed state to an extended state. Similar to that described above, actuating member 240 may be oriented along a guide 217 and actuated with an actuators 216 and/or manually with a suitable user interface device. Although actuating member 240 is shown in 2A as compressed within a straight guide channel, it should be understood that other shapes of guide channels may be possible. Furthermore, at least a portion of one or more of any actuating member, any actuator, a user interface device, and/or a guide may be in a module display 218 coupled to the housing 210. For example, as shown in 2A, in some variations, the proximal end of actuating member 240 may terminate in the display, where one or more actuators 216 similar to that described above may automatically move axially and/or rotationally actuating member within a guide 217 that is in the display and articulate the tip of the endoscope, as described above. Additionally or alternatively, in some variations, the proximal end of actuating member 240 may terminate in the display or module, where a user interface device similar to that described above may be used to manually move axially and/or rotationally actuating member 240 within a guide that is in the display. Furthermore, additionally or alternatively, in some variations, the proximal end of an actuating member 240 may terminate in the display, where the actuating member may self-expand in a manner similar to actuating member 240 as described above. For example, the actuating member 240 with a proximal portion in the display may be straight and axially aligned with the endoscope 250, such that self-expansion of the actuating member 240 results in easier extension of the endoscope working length. Introducers , Flexible or Rigid EndoscopeAny of the dual-video integrated systems or assemblies herein may include an introducer that is sized such that it can be used to guide the delivery of an intubation tube such as an ETT into a trachea, for example. A ETT may be advanced over any of the introducers herein. Any endoscope described or shown herein may be considered to be a mere example of an ETT introducer. It is understood that the description of any endoscope or scope herein may be considered to inherently be a description of a more general introducer, as that term is used herein. In an example herein, introducer 250 may be an endoscope, such that endoscope 250 may be removably coupled , telescopically engaged to an intubation tube for an intubation procedure. Endoscope 250 may be configured to enter and navigate inside the patient's airway, and serves as an introducer for ETT advancement during an intubation procedure. As described above, the introducer may have multiple degrees of freedom controlled by driving an actuating member, including longitudinal forward-back movement, axial right-left rotation, and up-down, and right-left articulating motions at its distal end. For example, as shown in 5, an arrangement 600 including the distal end of introducer 250 may include one or more sets of antagonistic cables 62 and 63 extending from a navigation wheel 61 towards the distal end region 250d of introducer 250. Each set of antagonist cables may, for example, correspond to a degree of freedom for the articulating tip of introducer 250. As shown in the exemplary 6, for example, an arrangement 650 may include at least a first set 64a of antagonistic cables operated by a navigation wheel 61a to control right-left articulation, a second set 64b of antagonistic cables operated by a navigation wheel 61b to control up-down articulation, and/or a third set 64c of antagonistic cables operated by a navigation wheel 61c to provide for tensioning of actuating member 240. An arrangement for articulating scope tip movement may alternatively include only one or two of these sets , only a set 64b of antagonistic cables operated by a navigation wheel 61b to control up-down articulation. The cables may, for example, be coupled to the actuation portion 240 via mechanical fasteners, welding, etc. The navigation wheels 61a-61c may include wheels, sprockets, rotation knobs, and/or the like. In some variations, the length of the distal tip bending section may be between about 4-5 cm, and the articulation steering of the tip at the bending section may be between about 120 degrees and 180 degrees in all directions up-down, right-left. Furthermore, the axial rotation of the introducer may be between about 90 degrees and about 180 degrees in both right and left directions. In some variations, an introducer , an endoscope includes a flexible member with optical, electrical, and mechanical functionality as described above, including transmission of light, video capture, mechanical actuation, and distal tip articulations. Alternatively, in some variations, the introducer may include a video stylet. The stylet may, for example, have the same optical, electrical, and mechanical functionalities similar to a flexible scope member, but may be more rigid to due to material and/or structure , a more rigid construction including metal. In some variations, at least a portion of the stylet may be malleable. Use of a rigid video stylet instead of a flexible member may be useful, for example, in some applications in which greater torsional rigidity is desirable to permit better transmission of linear and rotational movements between an actuating member and the introducer, easier maneuverability around the patient's airway , obstructing glottic lesions, smoother advancement during insertion and navigation in patient anatomy, and easier advancement of the intubation tube. As is stated herein, the endoscopes herein are examples of more generalized introducers for the intubation tubes, and the introducer may , an endoscope or may not include an image sensor. As such, any description herein of a device or system that includes an endoscope or scope is understood to include an introducer, which optionally may not include an imaging sensor. Transitional RegionAny of the integrated assemblies or systems herein may optionally include a transitional region or segment between an actuating member and an introducer. The transitional region may have intermediate stiffness and/or other gradual changes in structural characteristics, to help ensure smooth and uninterrupted transmission of actuation from an actuating member to the introducer. In some variations, a transitional region may additionally or alternatively include a series of successively increasing flexible segments and/or a coil spring to transition from a stiffer actuating member to a more flexible introducer. Alternatively and/or additionally, in variations such as where an actuating member and introducer are removably coupled, a relatively stiff transition region , between about 1-2. 5 times the length of the outer diameter of an actuating member and/or an introducer may be helpful between the actuating member and the introducer to restore continuity of electromechanical functionality and/or other functions. The transitional region may include coupling that provides the same structural and task functionality including uninterrupted transmission of light, video capture/analysis, and mechanical actuation, as described elsewhere for variations in which an actuating member and introducer are integrally coupled. Optical/electrical functionality may be maintained between the actuating member and the endoscope with one or more suitable mating connectors , connectors associated with respective routing PCBs, etc. Control of the distal tip may also be maintained through mechanical solutions such as coaxial cable connectors, push-button latches, pin and socket arrangements, wire lugs, plates, pins, screws, articulating joints, etc. Other Structural FeaturesAny of the integrated systems or assembles herein may include one or more structural features that are adapted to help prevent buckling and/or loop formation during its forward-back advancement along the linear axis and/or rotation, which helps improve smooth transmission of movements from the actuating member to the introducer. In some variations, shaft stiffness and torqueability may be increased by incorporating flat, spiral interlocking metal bands with gaps therebetween , bands under an outer polymer cover to maintain flexibility. These spiral bands may be covered by fine strands of stainless steel wire or other suitable material, braided into a tubular mesh and covered with an extruded polymer layer to create a smooth outer surface. Some exemplary solutions for improving advancement pushability and rotation torqueability are described in further detail below. For example, an actuating member and/or the introducer may be guided through a somewhat continuous physical guide, such as a guide channel , similar to an overtube environment. This guide channel may help constrain the actuating member and/or the introducer and keep the combined length taut during maneuvering, thereby reducing kinks and other issues. For example, as described above, an actuating member 240 may be constrained in a guide 217 in the housing 210 and/or display 218, and the endoscope 250 may be lodged within an intubation tube, which in itself provides a rigid guiding channel for the endoscope 250. Additionally, as described below, the endoscope 250 may be constrained in an intubation tube channel 266 in the cover 260. Furthermore, as described in further detail below, the overtube environment may provide an active channel in which manipulation , manual manipulation of the intubation tube may be easily performed, even while maintaining automatic, robotic-assisted guidance of the endoscope. Furthermore, the surrounding intubation tube and/or the cover may help constrain the introducer into a straight path generally aligned with the patient's airway during intubation, which further reduces buckling and/or loop formation within the actuating member and/or introducer. Other structural features may help reduce friction. For example, a guide such as guide 217 for the actuating member, an actuating member itself, and/or the introducer itself, may be lubricated , with long-lasting commercial lubricants and/or having an outer surface of low-friction materials to provide for decreased friction during actuation of the actuating member. Additionally or alternatively, an actuating member may have increased shaft stiffness that may help prevent buckling and loop formation during its linear advancement and rotation inside the curved trajectory of the guide. For example, actuating member 240 may be stiffer than introducer 250, which may be softer and more flexible to better facilitate maneuvering. In some variations, shaft stiffness along the length of the combined actuating member 240 and introducer 250 may be varied by varying the outer layer material composition of actuating member and/or introducer 250. For example, the outer base layer may include a polymer with two types of resin, and the polymer may be extruded over a wire mesh forming an outer structure. The stiffness may be varied by varying the composition of the combined resin, and the resulting polymer layer may further provide an atraumatic, biocompatible, and watertight surface. Shaft stiffness may additionally or alternatively be varied with an adjustable shaft stiffening coil wires, or other suitable mechanical elements. In some variations, shaft stiffness may additionally or alternatively be increased at actuating member 240, making at least part of actuating member 240 have a larger outer diameter. For example, the proximal end of actuating member 240 may have a flared diameter, which may also advantageously allow for increased operating surface with actuators 216. CoversAny of the systems and assemblies herein may further include a cover , cover 260 which allows for and is adapted to provide an integrated dual-imaging enhanced visualization and navigation system that is adapted to be held and controlled by a single hand of a single operator. Covers herein may be configured to be advanced over a tongue of the patient both above the epiglottis of the patient , in or near the vallecular and under the epiglottis of the patient, providing for versatile placement of the cover. Additionally or alternatively, a cover may be preferentially configured for placement either above or below the epiglottis. In some variations, such as that shown in 4A and 4B, cover 260 or any other cover, such as the cover shown in 17A-17G may also include a displacement member 268 configured to facilitate retraction of a patient's tongue, thereby improving the pharyngeal space to improve scope movement inside the airway. The displacement member may be, for example, angled, linear, curved, or otherwise shaped to be placed inside a patient's mouth in an atraumatic manner. In an example variation, the displacement member may be between about 3 cm and about 4 cm wide , for adult patients, and suitably smaller for pediatric patients. As shown in 4A, the cover may comprise two sections, including a first channel , 264 and a second channel , 266. First channel may include a lumen that is sized and configured to receive a first imaging member therein , a laryngoscope baton, and second channel may be sized and configured to releasably secure an intubation tube , ETT thereto and restrict movement of the tracheal tube relative to the cover in at least one direction. The channels may optionally be form fit to the received first imaging member and tracheal tube. First channel 264 may be adapted to be releasably coupled to, such as snap onto, housing 210 at connection 262, thereby removably attaching to the housing 210. In some variations, the first channel may be between 15 cm and 20 cm long, and angulated forward between about 4 cm and about 6 cm from the tip in a manner and direction that will maximize the view of different parts of the patient's upper airway anatomy above the vocal cords and glottic structures to enhancing visualization and movement of the introducer. However, these dimensions may be different for covers that are adapted for use with pediatric patients. Second channel 266 is adapted to be releasably secured to the intubation tube , ETT, which may be disposed about the introducer , a preloaded ETT. The intubation tube channel may function to provide a mechanism for secure positioning of the intubation tube close to the displacement member 268. The intubation tube channel 266 may optionally be configured to be coupled to , snap onto the housing 210 at connection 263. The intubation tube channel may further include one or more latches, clips, or other fasteners 267 to help retain the intubation tube within the intubation tube channel. In an example variation, the intubation tube channel may be between about 15 cm and about 30 cm long for adult patients and about half this length for pediatric patients. Various sizes , diameters of intubation tube may be accommodated within the intubation tube channel 266. Furthermore, the second channel intubation tube channel need not define an internal lumen, but rather may be partially open on its side , with a longitudinal slot to allow intubation removal from the cover when intubation has been completed. For example, the second channel may comprise a cross sectional configuration that is semi-circular, or with a groove or depression formed therein also see 1713 in 17A that is sized and configured to be releasably secured to an ETT. In some examples, as shown in 4B, intubation tube channel 266 and first imaging member channel 264 may optionally be detachable from one another, such as along detachable joining region 269 with one or more connectors, perforations, etc. This may be useful, for example, as one of the options to enable a laryngoscope to provide an effective video laryngoscopy only back-up intubation option, in the event that AI navigation fails or become problematic. For example, intubation tube channel 266 may be detached along joining region 269 and removed, which allows a first imaging member and remaining portion of cover 260 to be used in a conventional manual video laryngoscopic intubation manner, where the intubation tube is manipulated by an operator outside of cover 260 it should be understood that detaching the intubation tube channel 266 may not be required for all the patients if a preloaded intubation tube can be pushed through the intubation tube channel 266 of cover 260 into the patient's trachea. In another back-up intubation option, intubation tube channel 266 of the cover may be used for a combined video laryngoscopy-flexible endoscopy technique, where one operator performs video laryngoscopy, and a second operator manually performs flexible video endoscopy-assisted intubation. In yet another variation of a back-up intubation technique, detachable introducer 250 or any other introducer herein may be manually used as an intubation tube introducer to facilitate intubation tube placement into the trachea. At the distal end of cover 260, the two channels 264 and 266 may terminate adjacent to and substantially axially aligned with each other, such that the image sensors of the first imaging member and the introducer are very near to one another. In this manner, the location of the distal end of the intubation tube optionally coaxial and surrounding introducer 250 may be better localized to the field of view, thereby improving the ability to place introducer 250 and the intubation tube. Additionally, the dual channel arrangement can allow for a shortest distance of introducer robotically controlled movement to the glottis and into the trachea compared to the other approaches where an introducer is separately advanced over a longer total distance, thereby resulting in quicker and more successful intubation on the first attempt. Dual channel covers or blades may result in the first imaging member and the introducer assuming or having similar curvatures along the length of the cover, which is shown generally in, for example, 17C and 17D. The integrated dual-image sensor systems and assemblies herein are generally adapted such that when the introducer or second imaging member optionally a housing thereof is releasably coupled to the housing, the first and second image sensors are disposed or maintained at an initial distance relative to one another, which may optionally but not necessarily provide predictable starting locations for the image sensors relative to one another. As used herein, being disposed or maintained at an initial distance from each may refer to any frame of reference and may include any spacing therebetween in space. For example, the image sensors may be maintained at a horizontal and axial distance relative to each other. In some examples, the sensors may be aligned in one frame of reference but still maintained at a distance from one another with a different reference point or axis. For example, in some exemplary embodiments, first and second image sensors may be substantially axially aligned with each other in the proximal-distal direction, and spaced at some distance laterally or horizontally from each other. In these examples, the sensors are considered to be disposed and maintained at an initial distance from one another even though they are axially aligned. In some exemplary embodiments, the two image sensors may be initially substantially axially aligned with each other when assembled even if there is some minimal axial offset. The images sensors may be quite close to each other, and in some embodiments, the two image sensors may be maintained relative to each other such that the axial distance between the two sensors is not greater than 3 cm when the second imaging member which includes an introducer is releasably coupled to the housing. The actual distance between the sensors may be different than an axial spacing therebetween if there is an axial spacing due to a natural horizontal lateral offset of the two sensors when assembled together. When integrated into an assembly, a single hand of the operator can hold the integrated system with the two image sensors disposed or maintained at an initial distance relative to each other, such as not greater than an axial distance of 3 cm. In some examples, the second image sensor may initially be disposed within a proximal region of an ETT, or even within the second imaging member housing and not yet advanced into the ETT. In these examples, the assembly is still considered to be adapted such that the assembly disposes the first video camera at an initial distance from the second video camera prior to actuation of the actuator. In some examples, the two images sensor may be maintained at a distance relative to each other when the sensors are assembled into the assembly, and at some time during use the sensors may become axially aligned while the two sensors are disposed in an upper airway. This may occur if, after assembly, the second image sensor is initially proximal to the first image sensor at some maintained initial distance, and wherein the second image sensor is moved distally some distance relative to the first image sensor until it becomes axially aligned with the first image sensor. The two sensors in this example may be axially aligned only for a moment if the second image sensor continues to be moves distally, for example. Exemplary Methods for Providing Enhanced Visualization and Navigation of an Intubation Tube Introducer in Airway Management Procedures Optionally During an Intubation ProcedureThe disclosure herein includes aspects related to methods of enhanced visualization, navigation and placement of intubation tube introducers. 7 illustrates as a flow chart a merely exemplary method of positioning an ETT in a trachea during an intubation procedure. As shown in exemplary 7, method 700 for assisting performance of a robotically-assisted intubation procedure may include acquiring one or more images 710 with a first imaging member , laryngoscope or a second imaging member , an endoscope with an integrated hand-held system or assembly, initializing automatic guidance of an introducer , endoscope with an actuating member 720, automatically guiding the introducer , endoscope 730 via the actuating member based on one or more images, advancing an intubation tube over the introducer , endoscope 732, visually confirming intubation tube placement 740, such as with an introducer image sensor, decoupling the introducer and intubation tube 750, withdrawing the handheld system or assembly including introducer 760 and first imaging member 710 portions out of patient's mouth. In some variations, the method 700 may be performed with one or more variations of the devices described herein. The acquiring one or more images step 710 may comprise one or more images acquired using one or more image sensors in the integrated system or assembly, such as with a laryngoscope , video laryngoscope and/or an endoscope that are part of the integrated system or assembly. The images may be interpreted manually and subsequently automatic guidance of the introducer may be initiated with, for example, a user selection of an automated mode , activating an AI button such as button 280 shown in 2C or by selection of the automated-manual assist mode as described herein. Additionally or alternatively, images may be interpreted by one or more processors applying one or more suitable computer vision and/or machine learning algorithms to the one or more images to identify a suitable anatomical target and automatically initiate actuated guidance of the endoscope. In some variations, initialization of the automatic guidance of the introducer , endoscope 720 may be based on one or more images acquired from a laryngoscope, while in some variations such initialization may be based on one or more images acquired from both a laryngoscope and an introducer, and in some examples such initialization may be based on one or more images acquired from the introducer alone. As shown in the exemplary 7, the method may include automatically guiding the introducer , endoscope 730 via an actuating member in the handheld housing based on one or more images from a laryngoscope and/or an endoscope. Guiding the introducer may include, for example, automatically guiding the introducer via the actuating member in a longitudinal forward and backward motion, and/or axial left-right rotations. Furthermore, guiding the introducer may include articulating the distal end of the introducer with at least one of the several degrees of freedom, including up-down and left-right articulations. While in some variations the introducer may be guided automatically using a robotic system utilizing suitable AI image processing techniques, additionally or alternatively the introducer may be robotically guided manually automated-manual assist mode using a suitable user interface device , joystick, wherein the actuating mechanism disposed in the handle can robotically control the movement of the introducer in response to the operator's manual interaction with the system , a joystick. Various AI or machine learning methods may be performed to automatically guide the introducer. For example, suitable image recognition or processing algorithms may identify anatomical landmarks in images from the device such as that described herein. AI-assisted intubation targeting of the vocal cords may utilize specialized targeting software, which analyzes multiple image streams, marks and displays the target target crosshairs, analyzes an expected intubation tube navigation trajectory, identifies the distinct characteristics of the tracheal opening visible between the vocal cords, and/or displays this information in real time on display screen. Different targeting methods have been described in the literature. One or more modules that perform particular functions, including but not limited to real time computerized image processing, recognition and mapping, visual guidance, guidance information, and interfacing with robotic intubation interface, can be used to achieve fully automated or automated-manual assist robotic intubation. The special purpose logic circuitry, such as FPGA field programmable gate array and/or an ASIC application-specific integrated circuit and/or other applications can be used in the methods and devices such as those described herein. One or more various computer vision and machine learning algorithms may be used in this invention, including SLAM simultaneous localization and mapping, Spatial Transformer Module STM-inspired techniques, deep neural networks DNN and convoluted neural networks CNN learning, and others. If performed during an intubation procedure, the method 700 may further include advancing an intubation tube that is coupled to disposed about the introducer , endoscope 732. The intubation tube may, for example, be telescopically engaged with , surround the introducer. While the introducer is manually or automatically guided, the intubation tube may be advanced over the introducer toward a target position. In some variations, the intubation tube may be advanced manually. In some variations, the intubation tube may be advanced automatically using one more suitable actuating systems, such as those described herein. Furthermore, in some variations, the intubation tube may be advanced manually during certain parts of its travel, and advanced automatically during other parts of its travel , as desired by a user. Furthermore, in some variations the method may include manually advancing the intubation tube while maintaining automated guidance of the introducer. For example, a user may be holding the integrated system with one hand, and manually manipulate the intubation tube over the introducer with the other hand , push the intubation tube forward on the introducer and/or rotate the intubation tube and/or introducer together as a unit toward a location in a targeted image, such as an image from an introducer image sensor that is disposed in a trachea. Accordingly, in some variations, the manual advancement of the intubation tube may, for example, help reduce the travel distance of the intubation tube toward a target position and further improve the speed of intubation. Throughout an intubation procedure, as the introducer and intubation tube are automatically and/or manually advanced into a target position, images from the first imaging member and/or the second imaging member may be displayed, optionally continuously for some epoch or period of time, in real-time or near real-time to the user. The images, such an image from an introducer image sensor while in the trachea, may be used to immediately confirm proper intubation tube placement 740, an optionally optimal intubation tube positioning, and/or allow immediate identification of intubation tube misplacement, and/or allow a user to troubleshoot the intubation procedure both from above and below the vocal cords, including problems with intubation tube advancement, which may prompt suitable interventions. For example, the first imaging sensor is proximally spaced from the introducer image sensor and may provide a view and identification of a larger anatomical region, which can provide an additional view of the tracheal tube movement that provides more information about placement than simply using the introducer image sensor alone. Once the intubation tube is visually confirmed to be properly placed in the trachea, which is generally confirmed with the introducer imaging sensor that is also placed within the trachea and below the cords, the introducer may then be removed from within the intubation tube 750 automatically through robotic actuation and/or manually, and the introducer may be withdrawn 760 while leaving the intubation tube in place. Example. A merely exemplary, non-limiting, method of performing TI using an integrate dual-video system or assembly such as those described herein is described below. It is understood that not all steps need be performed, and the order may be modified if suitable. 1. A disposable dual-channel cover , 260 is coupled to a handheld housing , housing 210, and a laryngoscopy baton , 220 is placed inside a laryngoscopy channel of the cover. The system is powered on. 2. A disposable introducer , endoscope is coupled to the handheld housing directly or indirectly, such as to an actuating member of the system. The introducer is automatically checked by the device for full functionality upon connection with the handheld housing and such as to the actuating member. 3. The introducer is optionally lubricated and placed inside a selected ETT. The ETT is placed inside the intubation tube channel which need not be an internal lumen of the cover. 4. The user performs manual video laryngoscopy using the video laryngoscope the display screen is in default single picture mode showing the laryngoscope image feed, and identifies anatomic structures on the display of the system. Image recognition and/or AI interface is activated and initiates automated robotic actuation of the actuating member and introducer through the actuating interface. The motions of the actuating member are fully transmitted to the introducer. The actuation may also be activated manually by selecting an AI operational mode button on the housing of the device or by using the automated-manual assist mode. 5. Upon actuation, a split or picture-in-picture video screen feature is displayed, allowing the user to observe TI in its entirety, optionally continuously and in real time. This display mode may be activated automatically or manually. 6. In a fully automated mode, the device automatically maneuvers the introducer into the patient's trachea using AI or robotic-assisted navigation, including forward-back movement, axial rotation, and/or steering tip articulation. In some examples this is performed using video data from the first image sensor alone, and in some examples this may be performed using video data from the first and second image sensors. For example, initial movement of the introducer may be based automatically based primarily or solely based on video data from the first imaging sensor which may be a video laryngoscope image sensor 7. In an automated-manual assist mode and in manual assist mode, manual operation of the device with a user interface device may instantly override a fully automated mode, for example. In a manual mode, the forward-back and/or axial rotation of the actuating member can be controlled manually, and the articulation of the distal end of the endoscope may be automated. Fully automated mode can be allowed to resume after the user presses the AI operational mode button, for example. In an automated-manual assist mode, the operator manually exerts the control of the automatic actuators in the system. 8. During navigation of the introducer, upon recognition of an airway anatomy, a visual indicator , a square or a circle may optionally appear on the screen around the displayed airway. The introducer articulation can move the tip in the direction of the geometrical center point of the detected glottis opening and through the vocal cords. 9. Introducer actuation around the anatomic structures during the TI sequence may be continuously visualized by the user via the, for example, video laryngoscopy image feed or signal that is displayed on the display. Visually presenting video data from the first image sensor , video laryngoscope image sensor with the larger angle of view on the display provides for operator/user to better troubleshoot navigation and to enable manual intervention or assistance, if needed. For example, while viewing a video signal from the first image sensor, the user can move the entire integrated system as a unit with a single handle, which may provide for slightly adjustment in positioning of the integrated system. 10. The introducer may be automatically navigated around the patient's anatomy, through the tracheal opening, and advanced inside the patient's trachea. 11. The ETT can be manually advanced distally off the introducer or automatically advanced into the patient's trachea, allowing for visual confirmation of proper ETT placement using image data from the introducer image source. ETT advancement can be observed continuously, both from above first image sensor data and below the vocal cords send image sensor on the display screen, and tracheal ETT placement is visually confirmed. Additionally, visual confirmation of tracheal ETT placement can be observed more closely by obtaining a full picture of the patient tracheal anatomy, by optionally pressing a picture-in-picture button twice on the display screen optionally accomplished with other mechanisms such as using a remote control, audio commands, or a touch screen/display. Additionally, the ETT may be optimally positioned in the trachea not too deep, not too high above the tracheal carina. 12. The ETT can be released from the intubation tube channel 266 of the cover. The system is removed from the patient's mouth, while leaving the ETT in place in the trachea. 13. A cuff of the ETT can be inflated, and manual or mechanical ventilation through the ETT is commenced using a ventilating bag or automatic ventilator. ETT tracheal placement may optionally be further confirmed by any suitable means , breath sounds, EtCO2. 14. One of the advantages of the integrated dual-video systems herein is that if AI/robotic-assistance TI fails for any reason, the operator has a back-up option to complete TI using the system as only a video laryngoscope, for example, with a variety of conventional TI options, as described above. 15. In embodiments in which an actuating member and an introducer are separable, if a problem occurs while advancing the ETT through the vocal cords , ETT repeatedly catches on the glottic structures, the introducer can be disconnected from the actuating member and used as a hand-held introducer endoscope to further facilitate directing the ETT through the glottic opening. 15 illustrates generally an integrated, dual-video handheld intubation assembly including any described herein after it has been positioned into an upper airway of a patient. 16A-16D illustrate merely exemplary images that may be captured and/or shown on a display during any of the intubation procedures described, which may include using any of the handheld systems or assemblies herein. The disclosure that follows describes 16A-16D in the context of video data or video images displayed on a display during an intubation system and exemplary benefits of systems herein that are adapted to display the image data from first and second image sensors. 16A illustrates a view provided by a first image source disposed at a distal region of a first imaging member, such as a video camera. The epiglottis and larynx are labeled in the 16A, which may optionally, but not necessarily be, a panoramic view as indicated. As can be seen, it is difficult-to-impossible to view the vocal cords in this image. Any of the methods herein may optionally include receiving as input data that is indicative of the anatomical view shown in 16A, and causing an output that initiates an automatic robotic controlled movement of the introducer. In alternative embodiments described in the context of exemplary modes herein, the system does not need to automatically control the movement, but rather a user may manually cause the robotic movement of the introducer via the operable communication between the disposable introducer and the housing. 16B illustrates image , video data from a first image sensor on the left and image data from a second image sensor on the right. The image data from the first image sensor on the left provides visualization of the introducer as shown after the introducer has been moved distally relative to its initial position in which the second image sensor video camera is initially maintained at a distance from the first image sensor , video camera and relative to the first image source. As can be seen, as the introducer is distally moved optionally also rotated and/or deflected, the second image sensor at the distal end of the introducer is also advanced distally relative to the first image sensor. The image data captured by the first image sensor, as shown, provides a view of the introducer as it is being robotically advanced automatically and/or manually and also provides a view of a larger anatomical region than the view provided by the introducer image sensor. In the event that the second image data is compromised , due to blood or secretions in the vicinity of the second image sensor, for example, the image from the first image sensor can advantageously help determine where the introducer is, and also may help to further facilitate continued movement of the introducer automatic or manual robotic movement. 16B illustrates the introducer after it has been robotically moved to some extent towards the glottic opening and under the epiglottis, as shown. 16C illustrates the image data from the first and second image sensors after the introducer has been advanced through the glottic opening, wherein the second image sensor provides visualization of the trachea as shown in the view to the right in 16C. The first image sensor image data on the left in the figure continues to show the properly positioned introducer, which is again a benefit of the integrated nature of the dual-video intubation assemblies herein. 16D again shows image data from the two image sources, and shows an endotracheal tube after it has been advanced over the introducer towards the glottic opening, through the glottic opening, and into the trachea, as shown. The window or opening shown in the upper left section of the right image of 16D is a standard side opening in the ETT referred to as the Murphy eye. The generally linear feature extending from the top right corner is a radiopaque line of the ETT that illustrates where the main, distal, opening of the ETT is. Some of the disclosure set forth above describes identifying or recognizing anatomical landmarks or locations in an image to help guide the introducer, towards vocal cords, through the vocal cords and into the trachea. For example, landmarks include but are not limited to the epiglottis, vocal cords, arytenoid cartilages, pyriform sinuses, tongue, a geometrical center point of the glottic opening, the trachea wall tracheal rings, regions of an image that are darker than adjacent regions, etc. In some methods and devices herein, identifying or recognizing anatomical landmarks or location which may generally be described herein as image recognition may be performed or accomplished automatically. For example without limitation, and as is set forth above, the image or landmark recognition may be performed using AI or image recognition software configured and adapted to automatically recognize certain landmarks based on one or more images or image data received while using the system. 8A illustrates an exemplary sequence illustrating automatic image/landmark recognition, which may then facilitate the robotic control and navigation of the introducer , flexible and/or rigid endoscope, additional details of which are described herein. In some methods and devices herein, identifying or recognizing anatomical landmarks may be performed or accomplished manually by medical personnel or other operators. For example, a physician may provide input , touch, audio, to the system to identify or recognize one or more aspects of an image displayed on a screen or display. For example, a physician may touch a touchscreen at the location of recognized vocal cords in the displayed image, or a location to which the operator wants the introducer to be moved. 8B illustrates an exemplary sequence showing manual image/landmark recognition, which may then facilitate the robotic control and navigation of the endoscope, additional details of which are described herein. It is understood that aspects of 8A and 8B may be combined. For example, without limitation, automatic recognition may take place , as part of a device mode, and manual confirmation of the automatically recognized aspect of the image , manually touching a confirmation icon on the display may be required before endoscope navigation is initiated. Any of the systems herein may include one or more processors that has stored therein an executable method , software, firmware, algorithms, that is adapted to receive instructions or input that is directly or indirectly based on a user interaction with a display optionally touch and/or audio interaction and/or haptic feedback while the display presents at least one image still or video. The executable methods may facilitate robotic control of introducer movement via an actuating member in the housing and/or first imaging member or other member that is in operable communication with the introducer. Some of the disclosure set forth above describes robotic control of the movement of the introducer through the vocal cords and into the trachea. For example, some disclosure herein is related to automatic robotic control of an introducer. 9A illustrates an exemplary sequence of image recognition such as either of those shown in 8A and 8B followed by automatic control of the introducer into the trachea or other lumen depending on the medical procedure. For example, automatic movement of an introducer may occur with any of the actuators and/or actuation members herein following one or more image recognition and/or processing steps. In some instances, movement of the introducer may be at least partially manually controlled, such as but not limited to with user movement of an actuator such as a joystick, wheel, slider, or other similar actuator which may be in operable communication with an internal housing actuator to control one or more types of movement of the introducer. Additional details on types of optional movement control , distal-proximal, rotation, and/or distal-tip deflection are described elsewhere herein. It is understood that the exemplary robotic control steps in 9A and 9B may be combined with the image processing and/or image recognition from 8A and/or 8B. Methods, devices and/or systems herein may not include automatic image processing and/or recognition. Alternatively, methods, devices and systems herein may be adapted and configured with a mode or used in a manner that does not include the use of automatic image processing and/or recognition. Both instances may generally be referred to herein as a manual mode. 8B includes exemplary steps that may be included in a manual mode other steps, such as any of those herein may obviously be included in the overall introducer navigation process. The introducer control in 9A and/or 9B or as described elsewhere herein may be included in a manual mode. In some instances, a manual mode may be initiated in response to a user action or triggering event, such as by pushing a button to initiate a manual mode, or by touching a touch screen to identify part of an image. In some instances, any of the devices herein may default to a manual mode or may not be adapted with AI or other automatic image processing and/or recognition. As is described herein, methods, systems and devices herein may be used or include an automated mode but it is understood that they may have other modes or be used in other ways as well. An automated mode may include automatic image processing or recognition , such as shown in 8A and automatic robotic scope control and/or navigation , such as shown in 9A, which can optionally be performed in a continuous, closed loop manner. In some instances, the device and methods operate in an automated mode in response to a user selection or initiation of the mode, such as by for example only pushing an AI button on the handheld , 2C. In some instances, an automated mode may be default and may function in automated mode without needed user initiation. In some instances, an automated mode may be initiated or occur after any number of modes or steps have already occurred, such as after a manual control mode , 9B that the user wishes to discontinue and return to an automated mode. Any of the devices, systems and methods herein may be used or have a mode in which an automated mode continues while and as long as a medical professional continues to actuate an actuator such as a button or switch on the handheld. In these modes, the system can be adapted such that when the user stops actuating the actuator , releases a button or switch, the automatic scope control and/or navigation ceases. For example only, any of the AI buttons on the handheld may serve as this type of actuator that once released, the automated mode may cease , a dead man's switch. This mode may be used with any other mode or method herein. Any of the devices and systems herein may be adapted such that a user may override or stop an automated scope control/navigation by interacting with the handheld including any display associated therewith. 10 which is similar to 9B illustrates exemplary steps that include manual robotic control at a time subsequent to automatic robotic control other steps may of course be included. For example only, an operator may use a handheld housing actuator , a joystick, wheel, slider, to override optionally immediately causing an automated mode to stop or be paused an automated navigation. The user control may provide for one or more of forward-back distal-proximal and rotation. Some systems may be adapted such that an automated mode may be activated again after user interaction with the handheld , pressing an AI button. For example, in 10, after the manual control, the method may again return to automatic control, and may loop back and forth as desired by the operator or device. Any of the systems, devices and methods herein may be adapted such that an operator may indicate a navigation pathway or location along a pathway on a display, such as by touching a particular location on the image that is presented on the display, or even using voice commands for which the system may be adapted to receive, process, and initiate an event. The system may be adapted to then automatically navigate to or towards that location. This is similar to 9A, wherein the landmark recognition in this example comprises indicating a location or intermediate pathway location and not necessarily a particular anatomical landmark. As shown in 9A, any of the devices and systems herein may be adapted to automatically control the introducer navigation , such as having a mode that is adapted to provide this functionality. With automatic image processing and/or recognition, the system may be adapted to receive image data and determine a pathway for the introducer , scope. With manual image recognition, the system may be adapted to process the manual identification and determine a pathway for the scope. The onboard AI may then communicate instructions to facilitate the control of the on-board actuator to thereby facilitate smart, atraumatic, AI-enabled, robotic-assisted navigation of the scope into the patient's airway. The sequence may repeat in a closed loop manner. As is set forth above, once the scope is placed inside the patient's trachea, the operator can manually advance the tracheal tube over the scope using the scope as an atraumatic intubation guide, similar to a guidewire in other medical procedures. Tracheal tube advancement may be continuously visualized by the operator on a display screen such as the handheld or stationary monitor, optionally both from above and below the vocal cords, with either a single image optionally able to toggle between more than one or at least two images being displayed at the same time. When the term laryngoscope is used herein, it is understood that the term may refer to a traditional laryngoscope, but the term may also refer to any type of device that is adapted to provide at least video imagery to an operator, and preferably but not required panoramic video. In general, these are referred to herein as first imaging members or first elongate imaging members. First imaging members may be manufactured and packaged coupled to the handheld housing, or they may be releasably coupled to the housing just prior to the procedure by an operator or assistant. A laryngoscope herein may generally be referred to as any of a video guide VG, and optional a panoramic video guide PVG, and a first imaging member, and may include an image sensor , video camera at its distal end, and optical and electrical wiring and communication functionality. The first imaging member may be functionally similar to the video baton of existing video laryngoscopes and may include any features or functionality thereof. It is also understood that any of the video monitors , displays herein may be integrated into any of the handheld housings herein, or they may also be a separate, free-standing video monitor including being part of a detachable component that can be releasably secured to the handheld. As is set forth herein, any of the video monitors may include a touch screen, which may be adapted to responds to taps, swipes, and any other type of manual commands. Any of the video monitors herein may also be adapted to be responsive to audio input , voice commands or haptic commands. The terms display, screen, and monitor may be used interchangeably herein. In any of the systems herein, the introducer may be an endoscope rigid and/or flexible, which may also simply be referred to herein as a scope. The terminology is understood to not necessarily be limiting in functionality. In some instances, the introducer is adapted to be robotically controlled automatically and/or manually, and may include a flexible optionally at least partially flexible elongate tubular member or shaft and one or more of an optional distal camera, electrical wiring, optional optical transmission elements, or one or more elongate elements , pull wires used in articulating the distal tip. As is set forth herein, the introducer may be adapted to be releasably secured to any of the housings indirectly or directly, optionally via a coupler wherein the coupling creates operable robotic communication between the introducer and the handheld housing to facilitate robotic control of the introducer, which is described in more detail elsewhere herein. Any of the covers herein which may also be referred to herein as blades may include a first elongate channel adapted to receive a first imaging member therein, and the covers may also be adapted to be releasably secured to a tracheal tube to restrict movement of the tracheal tube relative to the cover in at least one direction. The covers may include a separate tracheal tube lumen, or they may be configured with a tracheal tube stabilizer to which a tracheal tube may be releasably secured, such as one or more clips or a partial channel. The covers are generally configured such that the position of the tracheal tube, when it is initially releasably secured to the cover, is generally maintained relative to the first imaging member channel and lumen, additional details of which are set forth above. As is set forth herein, the housings may be packaged with an introducer , an endoscope or other elongate tracheal tube guiding device already attached or coupled to the housing. In alternative examples, an introducer is not packaged securely coupled to the housing, and is releasably secured or coupled to the housing prior to the medical procedure by an operator or assistant. In either scenario, an introducer is in operable communication with the housing to facilitate robotic control of the introducer and optionally transmit optical data from the introducer to the housing, which may be displayed on a display as set forth herein. The introducer may optionally be releasably secured to a coupler on the housing, wherein the coupler may be any suitable component or components including a separate component such as an adaptor that couples to both the handheld and the introducer sized and configured to interact or interface with the guiding introducer and become releasably secured thereto. Releasably secured in this context refers to being able to secure the introducer to the housing so the two are not easily separated during normal use of device during an intubation procedure, and may include a locking mechanism that may be additionally actuated to lock the introducer to the handheld. For example, a locking mechanism may include an additional step or movement of the introducer to lock the introducer in place relative to the handheld housing. 2A illustrates an example of an introducer 250 that is releasably secured to a coupler of the housing, which may be considered to include an end of actuating member 240. In the example of 2A, actuating member 240 and other similar internal movable actuating members may be considered to be a robotic extension of the introducer after the introducer is releasably secured to the coupler of the housing. That is, the actuating member and introducer may be moved together and may be considered part of the same movable unit and data can be communicated from the introducer to the housing. 2C illustrates an additional example of housing that includes an internal movable robotic extension or actuating member. As shown in the example of 2C, the internal movable robotic extension or actuating member is at least partially disposed or arranged along and/or within a guide 217 within the housing. As described above, the guide is sized and shaped to allow the actuating member to move relative to the guide yet guide the movement of the actuating member within the housing. 11 illustrates an additional example of a robotic handheld device or assembly adapted to facilitate delivery of a tracheal tube to a trachea or other device to a different lumen depending on the procedure. Any other feature that can be suitably incorporated into or suitably modify the example in 11 may be included even if the text is silent on its inclusion. The device or assembly 1100 includes, among other components, a handheld housing 1110, cover or blade 1160 coupled thereto, and display 1118. In this example, handheld housing 1110 includes a universal docking station 1130, which may be disposed within the housing. An introducer delivered into a patient's trachea or other lumen may need to be within an outer diameter limit depending on the anatomy and/or use. For example, with pediatric patients the scope may need to be smaller than a introducer that can be used with adult patients. Depending on the use of the devices herein, it may be desirable to be able to releasably secure a plurality of introducers, flexible or rigid, to a common , universal handheld housing, wherein these scopes may have different outer diameters OD and sizes and/or possible different relative locations of the optical transmission lines within the introducer. One consideration is ensuring that the introducer, regardless of its OD and size, can be releasably secured to the handheld housing and will be in operation communication with the housing , robotically movable and optionally facilitating transmission of optical , image data. It may thus be beneficial for the handheld device to include a universal docking station or other universal coupler such as station 1130 in 11, which is adapted to be able to releasably secure a variety of flexible and rigid introducers with different OD and of different sizes to the handheld housing and ensure operable communication therewith. A universal docking station or universal coupler may be adapted such that it may be able to receive differently-sized introducers , having different circular outer cross-sectional dimensions, different introducer lengths, different locations of optional optical transmission elements, . In some examples, the universal docking station may include a plurality of adaptors that are each sized and configured to interface correctly with the flexible/rigid introducer, regardless of its OD and size. In the example in 11, robotically controllable introducer 1150 is shown having been already coupled to housing 1110 and to docking station 1130, which may be within the housing. Once coupled, the introducer 1150 can be moved in any of the manners described herein with actuators within the housing , distal-proximal, rotational, tip deflection in at least four different directions. 11 also shows an exemplary scope or introducer guide 1117 that is associated with an outer or close to outer region of the housing and may extend from an outer housing surface, compared to internal guide 217 in 2C, for example. 11 illustrates an introducer 1150 that is stably received within guide 1117, which in this example is disposed at the top region of the housing. This location may allow the introducer to be secured and movable relative to the top of the housing, yet still have the central introducer region extend through the tracheal tube ETT. Guide 1117 may have a tubular configuration through which the introducer is passed before being coupled to the coupler and universal docking station 11130. Alternative guide configurations may be used as well. For example, guide 1117 may not form a complete lumen, such as if the guide has a general C cross-sectional shape, as long as the introducer is movable relative to the guide and the guide prevents the introducer from being disassociated from the guide. As is set forth above, in some embodiments when the introducer is directly or indirectly releasably coupled to the handheld housing , intubation procedures, the introducer may be robotically moved, including distal advancement toward a trachea. In some embodiments the assembly is optionally adapted to be able to robotically advance the introducer at least 10-12 cm either with automatic and/or manual robotic navigation past the tip of the ETT so that it may be advanced into secure position in the trachea and sufficiently deep into the patient's trachea. In some embodiments the handheld assembly is configured to be able to robotically move the introducer from 5 cm to 40 cm, or 5 cm to 60 cm, distally once the introducer is operatively coupled directly or indirectly to the housing. For example only, the handheld housing may be adapted to move a robotic extension and thus move the introducer distally from 5 cm to 40 cm, and optionally not more than 40 cm. Alternatively, with reference to 11, the assembly may optionally be able to move the introducer from 5 cm to 40 cm distally, or 5 cm to 60 cm distally if the assembly does not include an actuating member or robotic extension, for example. In this disclosure, the phrases actuating member, robotic extension, and introducer or endoscope extension may be used interchangeably. The total length of the introducer may depend on the procedure. For example, in bronchoscopy procedures, the introducer may optionally have a length that is 50 cm-60 cm. Additionally, not all procedures that utilize the handheld systems and assemblies herein require the use of a cover and/or first imaging member , bronchoscopy, and when used in these procedures a first imaging member and cover may optionally be removed/detached from the handheld to remove unnecessary components and simplify the procedure. Any of the devices herein may include an introducer distal movement limiter that is adapted to prevent the introducer or robotic extension from being moved distally beyond a certain distance relative to a starting axial position. This may occur due to the construction of the assembly, and certain feedback mechanisms to handheld from AI and software, which may inherently create a distal movement limiter when an introducer simply cannot be moved any further distally due to actuator and/or robotic extension construction relative to the handheld housing or assembly. The limiter may thus be considered a passive or an active movement limiter. In some robotic handheld devices and assemblies herein, at least some of the robotic functionality of the device may be incorporated into a component that is removable from the handheld housing, but which may be releasable secured thereto prior to the procedure. This may allow for some components to be reused more easily while others may be discarded after a procedure, for example. This may also allow some removable components to be able to be used with a variety of handheld devices, for example. This may also help and/or ease manufacturing of certain components. 12 and 13 illustrate exemplary assemblies with a removable component that includes at least some robotic control functionality. 12 illustrates handheld housing 1200 and removable robotic block 1270 also referred to herein as a removable introducer controller, which is shown releasably secured to the handheld housing 1200. Block as used in this context is not limiting to any particular structure or function, but refers generally to the removable functionality of the component. In this example, removable block 1270 includes introducer coupler 1222, robotic extension or actuating member 1240 at least partially within the housing of the block 1270, guide 1217 within block 1270 that may include any feature of any of the guides herein that allows for movement of the robotic extension 1240 but maintains the extension 1240 in the guide or associated with guide 1217. Actuator 1216 is also shown, which may include any of the functionality or features of any of the actuators herein , motor control to move extension 1240 and thereby move introducer 1250. In this example, handheld housing 1210 may include a block coupler 1272, which may also be referred to herein as an introducer controller coupler that is sized and configured to be releasably secured to the removable block. The block 1270 may similarly include any type of suitable corresponding mating structure that is sized and configured to releasably but securely interface with the block coupler 1272 on the housing 1210. The block coupler 1272 may include a wide variety of coupling features that allow for the block to be releasably secured to housing 1210 , press fit, male/female, . Once block 1270 is secured to housing 1210, and introducer 1250 is releasably secured to introducer coupler 1222, the device may be used in any other manner described herein , image processing/recognition, scope movement, light transmission, video capture, mechanical actuation, distal tip articulation, . Any of the removable blocks herein may also be referred to herein as an introducer controller, introducer controller, or second imaging members that include an introducer. 12 also shows removable cover 1260 which may include any cover or blade features or functions described herein with first and second channels that is releasably secured to housing 1210. Handheld housing 1210 in this embodiment may include an integrated or built-in display , touchscreen 1218, which may include any feature or functionality of any display herein, and may be used according to any method or step described herein , manual anatomical landmark recognition. Alternatively, the system may have a display integrated or removeable situated on a side of housing 1210 rather than on its top surface as is shown in 12. An aspect of the disclosure herein includes optionally utilizing pre-operative information about a patient during the intubation procedure, such as pre-operative imagery CT, X-ray, MRI, PET scan and/or 3D-reconstructed images of the airway or video that may help or be of assistance when navigating during the intubation procedure. For example only, it may be beneficial to utilize pre-operative endoscopic video exam for improved or enhanced navigation. Optionally, a patient characteristic or condition can be incorporated into the navigation procedure. For example, if the patient's disease and/or characteristic and/or condition all of which may be generally referred to herein as a condition can be generally described, the operator may be able to select an appropriate condition from the plurality of selectable conditions that match that ones of the patient before the procedure from software image library, and which the device , via AI and/or associated trained machine learning algorithms will then take into account during the intubation procedure to further improve the visualization, navigation and ETT placement, further improving the speed, success rate, accuracy and safety of intubation. For example only, such an interface may include a pull-down application menu presented on any of the displays or screens herein, including a mobile phone or computer or voice command to free up the use of a hand that will allow a selection of a condition personalized to this particular patient's medical care. For example, conditions or information about the procedure may include laryngeal cancer, base of the tongue tumor, or a nasal intubation, or airway exchange through the intubating LMA. Combinations of procedures and conditions may also be presented or otherwise selectable such as a nasal intubation for a patient with base of the tongue tumor. Image libraries may be retrieved for any of these conditions and/or procedures from the cloud or built-in device software, and optionally stored on the handheld and utilized as part of the navigation. In some embodiments, existing patient images may be delivered and stored on the handheld device , wirelessly and utilized during navigation , by AI. By way of example, these images may include one or more of preoperative videos, pictures of the patient's anatomy, or reconstructed images/videos from a CT/X-ray/MRI/PET scans and/or other imaging studies. Any of the handheld devices or assemblies herein may further include a video recording button or other type of actuator on the device to allow for recording of video from one or both of a VG or scope. Any information from any of the procedures , video and/or image data may also be transmitted to a central location , to the cloud, either automatically or manually. The data may be used to train AI to enhance the navigation capabilities of the system and other systems. Any information from any of the procedures , video and/or image data may also be transmitted either automatically wirelessly or manually to patient's electronic medical record EMR to document the intubation procedure for enhanced record keeping, for billing and/or for teaching purposes. In any of the examples herein, the tracheal tube , ETT may also be utilized as an additional manipulating tool for the flexible introducer. Moving the tracheal tube may be beneficial to help navigate the scope and/or orient the scope that is coupled to ETT. Rotating the tracheal tube and/or moving it forward distally may be helpful to enhance proper introducer alignment, actuation and advancement, either when the introducer is automatically robotically navigated , 9A or manually robotically navigated , 9B. For example, a tracheal tube may be moved forward and backward distally-proximally and/or axially rotated, optimizing visualization and/or navigation/movement of the introducer towards the target, improving the speed and first-pass success rate of intubation. In some uses, a tracheal tube may be moved , distally advanced and/or axially rotated before initiating an automatic introducer navigation sequence. In some uses, a tracheal tube may be moved , distally advanced and/or axially rotated during an automatic scope navigation sequence. In some examples, the assembly is adapted such that if the tracheal tube is manipulated during an automatic navigation sequence , 9A, the tracheal tube manipulation may automatically cause the robotic interface to stop. One aspect of the disclosure herein is related to integrated dual-video systems that include an introducer guide or guide features. As is set forth herein, the devices either in a handheld or a removable block portion may include an introducer guide that allows for introducer or robotic extension movement relative to the guide yet restricts the movement to a particular or known pathway , within the guide and preventing it from leaving the guide. As is partially described above, any of the guides herein may include a guiding rail for the proximal part of the introducer or robotic introducer extension within or outside of the device housing. Additionally, the guides may be sized and configured to prevent buckling of the proximal part of the introducer including a robotic extension. Additionally, a proximal region of an introducer or a robotic extension may have a stiffness adapted to prevent scope buckling, wherein the stiffness may be greater than a stiffness of a scope shaft that is disposed outside of the housing. Additionally, any of the guides may direct the introducer at a favorable angle and/or favorable trajectory and/or pathway within or outside of the device housing. Additionally, as described above, introducer guidance can be enhanced by positioning a distal region of the introducer inside the tracheal tube to keep the scope taught due to the frictional engagement between the introducer and the tracheal tube. One aspect of the disclosure is related to an introducer that is robotically movable distally, and optionally at least a certain distance once coupled to the handheld housing either to a handheld or coupling via a removable block. In some merely exemplary uses, a total introducer length with or without a robotic extension may be about from 25 cm to 70 cm. In some exemplary uses, the device is configured to allow for at least 5 cm of robotic distal introducer advancement, and optionally from 10 cm to 40, such as 10 cm to 25 cm. This may be facilitated by the travel allowed in the handheld by a robotic extension, if the system includes a robotic extension. In any of the embodiments herein, any of the introducer guides in the handheld housings may include a coating or other lubricious material , silicon that facilitates scope movement relative to the guide and helps prevent buckling or binding. In any of the embodiments herein, the introducer may include one or more sensors at a distal tip for avoiding excessive manipulation force during navigation , during tip deflection. In any of the embodiments herein, deflection or steering of the distal scope tip may be facilitated without the use of pull wires or other elongate tensioning structures. For examplepiezo, alloys bimetal, nitinol. In any of the embodiments herein, the introducer may have a stiffness that varies along its length. For example without limitation, the s introducers herein may comprises one or more polymeric materials , PEBAX with varying stiffness , different durometers along its length, and optionally providing for a distal tip that is more flexible or deflectable than proximal region of the scope. Concepts for varying polymeric elongate shaft stiffness are generally known, and any concepts related thereto may be incorporated into the embodiments herein. For example, a robotic extension may include a polymeric material with a higher durometer than an introducer material at a distal end of the introducer. Again, for example only, a proximal region of an introducer such as a region of an introducer that interfaces with a guide on the outside of the handheld, such as shown in 11 may have a higher durometer than a distal region of the introducer to help prevent buckling during distal movement. One aspect of the disclosure herein is that the described devices and systems may optionally yet advantageously be adapted and configured to be highly usable, handheld, portable, multi-functional dual-video airway management platforms that can be operated by a single hand of a single user. For example only, handheld housings herein may be adapted to be used in a variety of different airway management procedures and/or used with a variety of different introducers and/or a variety of different first image members , video laryngoscopes. In these embodiments, the handheld housings may be considered to be universal, or common, handheld housings to which a variety of different introducers and/or first imaging members may be interchangeably coupled and utilized. For example, it may be desirable to have a universal platform that can be coupled to introducers that are sized for pediatric patients as well as to introducers that sized for adult patients. Additionally or alternatively, it may be desirable to have a universal platform , handheld housings herein that can be used with introducers having different lengths, such as those that are relatively longer and which may be used in a bronchoscopy and other airway management procedures described herein, as well as relatively shorter introducers that can be used in an intubation or intubation-related airway management procedure. The platform technology can also be adapted to be able to robotically control different introducers that may be coupled to the platform, regardless of the airway management procedure, medical, diagnostic or surgical. The handheld housings herein may be adapted such that one or more of an introducer, a cover or blade, or a first imaging member , video laryngoscope may be releasably coupled to the handheld housing directly or indirectly to provide the multifunctionality. This disclosure thus includes devices and systems that may be adapted to provide multi-functionality and versatility not yet observed or provided in existing medical airway management approaches. Additionally, the devices and systems herein may be adapted such that an introducer may be temporarily or permanently removed or simply not coupled to the housing if it is not needed for a particular airway management procedure or part of a procedure. For example, if the introducer is not needed for a procedure or part of a procedure, the introducer may not be used, and may optionally be temporarily or permanently uncoupled from the housing if it has been previously coupled to the housing. Additionally, if the first imaging member is not needed for a procedure or part of a procedure, the first imaging member may not be used, and may optionally be temporarily or permanently uncoupled from the housing if it has been previously coupled to the housing. Additionally, a blade may be temporarily or permanently removed or not used if it is not needed for a particular procedure or part of a procedure , a bronchoscopy. The devices, assemblies, systems and methods herein thus can be adapted and configured to be multi-functional, universal, highly-usable, handheld, portable platforms that are adapted to be operated by a single user, to provide a great deal of functionality that is not observed and available with existing approaches, including being able to treat a variety of patient populations in a variety of clinical settings and locations, as well as being able to be used in a variety of airway management procedures, and optionally while robotically controlling , with AI, smart navigation, the movement of the introducer. Additionally, the devices herein can be quickly modified as needed , by temporarily or permanently removing and/or reattaching an introducer or first imaging member based on the needs of the procedure. While the disclosure herein describes some system and devices that may be used to generate and view two images either simultaneously on the same screen or by toggling between views, the disclosure herein also includes the use of a handheld assembly when only one image source is used. For example only, the methods and devices herein may be used without the introducer , scope and only with the first imaging member. For example, this may occur in a situation when the introducer module is either malfunctioning or ineffective, such as if blood and secretions interfere within the airway. The assembly may be adapted such that the operator can use the device as a regular video laryngoscope or VG, while the introducer disconnected or at least not in use. The assemblies may have a separate mode that includes the use of the video from the first imaging member only a VG. Another exemplary scenario in which only one video or imaging device may be used includes the use of only the introducer video, while the first imaging member is disconnected or at least not in use. In some instances, in which the first imaging member may be removed from the housing, this may include disconnecting the first imaging member from the housing, or at least deactivating the first imaging member. One exemplary scenario of an airway management in which the introducer could be used without a first imaging member is in a nasal flexible endoscope intubation, including the option to couple the first imaging member at a later time to facilitate endoscope manipulation when it reaches the oral cavity through the nose. Another exemplary scenario of the airway management in which the scope could be used without a first imaging member is with use of the endoscope to facilitate intubation through the supraglottic airway devices SGAs, which are small masks positioned inside the patient's mouth to provide ventilation/oxygenation of the lungs when connected to the ventilator. The SGAs the laryngeal mask airway, LMA, are frequently used to temporize a difficult airway situation when intubation is either deemed or encountered as difficult. Another exemplary scenario of the airway management in which the introducer can be used without a first imaging member is in a diagnostic upper airway endoscopy or bronchoscopy, in which the procedure may be performed through the indwelling tracheal tube or without it. Another exemplary scenario of an airway management in which the introducer can be used without a first imaging member is to confirm proper positioning of the indwelling ETT inside patient's trachea and/or optimize ETT positioning inside patient's trachea. Another exemplary scenario of an airway management in which the introducer can be used without a first imaging member is for facilitating placement and positioning of a double lumen tracheal tubea special type of ETT used, for example, for thoracic surgery, and which placement is frequently difficult. Another exemplary scenario of an airway management in which the introducer can be used without a first imaging member is for facilitating the endoscope-assisted exchange of the existing indwelling ETTs in the patient, optionally in the operating room, ICU, emergency department or other location. Another exemplary scenario of an airway management in which the introducer can be used without a first imaging member is for facilitating the extubation trial. In any of these exemplary airway management procedures that may be used without the first imaging member, a cover may be detached from the handheld housing if a blade is in fact already coupled to the handheld housing. In some alternative uses, the devices or assemblies herein may be used in one or more ENT airway management procedures, such as the biopsies or injections vocal cords injections. In some alternative uses, the devices herein may be used in one or more other medical and/or surgical endoscopic procedures. Additionally, while the applications herein are related to medical procedures, it is possible the devices and methods herein may be used in non-medical procedures. For example, devices herein may be used for industrial applications where the introducers may need to be snaked into the orifices for evaluation of poorly accessible machinery, for example. Any of the systems herein may be adapted such that it may optionally be used in a manner in which the first imaging member, such as a video laryngoscope which may be referred to herein as VL or VG is used, and the system may not be coupled to an introducer, or the introducer is not used as part of the procedure. Any of the systems herein may be used in a manner in which the introducer is used without using a first imaging member , video laryngoscope, or a first imaging member may be detached from a handheld housing and not used in the procedure, or a first imaging member may be reattached to a handheld housing at a later time during the procedure. Any of the systems herein may be adapted to be coupled to a plurality of covers, each of which may be at least one dimension that is different from a dimension of at least one other cover, which may allow for different sized covers for pediatrics and adults, for example. Any of the systems herein may be adapted to be coupled to a plurality of introducers, which may have different sizes and/or lengths, which may allow the handhelds herein to be used with adults and pediatrics, for example. Any of the systems herein may be adapted to be coupled to a plurality of deflectable introducers, which may be flexible or rigid, or partially flexible and partially rigid, and which may have different lengths and diameters. Some exemplary systems or assemblies herein include introducers , mechanical guides, flexible endoscopes, rigid endoscopes, that may be releasably secured to a handheld housing, whereby when the introducers are secured to the housing, the handheld housing is in operable communication with the introducer such that the handheld housing is adapted to provide controlled movement of the introducer, and the introducer is adapted to receive and respond to such control. One aspect of the disclosure herein is related to optionally disposable, or one-time use, introducers that are sized and configured to be releasably secured to the handheld housing. After being releasably secured directly or indirectly to the handheld housing, the movement of the introducer can be controlled using the handheld housing. After a procedure is over, or at time decided by the operator, the introducer may be released from the handheld housing, and optionally may be discarded. The handheld housing may be used again, either with the same patient or after clearing and/or sterilization. After the intubation system has been exposed to a patient, it generally must be cleaned and sterilized to be used again on a different patient. With some systems herein, it may be challenging to clean and/or sterilize one or more components that facilitate the axial movement of the introducer. As such, in some uses it may be desirable to have a disposable or one-time use introducer that can be removed after use, so that the handheld housing can be cleaned and sterilized and used with a new introducer in subsequent uses. It may thus be desirable to utilize introducers that are disposable yet affordable. Any of the introducers herein , flexible or rigid endoscopes, may be disposable and/or may be incorporated into a disposable introducer housing assembly, any of which may be sized and configured to be releasably secured to a handheld housing. The introducer housing assemblies herein may be referred to as second imaging members generally. 14A-14D illustrate an example of an intubation system that includes an exemplary handheld housing and an exemplary introducer housing assembly also referred to as a second imaging member, wherein the handheld housing and the introducer housing assembly are sized and configured such that the introducer housing assembly can be releasably secured to the handheld housing to create operable communication between the handheld housing and the introducer housing assembly. Introducer housing assemblies in this context may also be referred to simply as introducer assemblies or second imaging members. The introducer housing assemblies herein may be disposable, and can be easily released from the handheld housing following the procedure or as desired by the operator, such as in the case of introducer assembly failure. When the introducer housing assembly is releasably secured to the handheld housing, the two components together may also be referred to together as a handheld device or assembly. In this example, intubation system or assembly 1400 includes handheld housing 1410, which may include any other suitable feature described herein with respect to any other handheld housing, such as a display screen, one or more internal actuators , one or more motors, electronics, one or more computer executable methods, firmware, a microprocessor, a blade coupler, a laryngoscope or laryngoscope coupler, etc. In some examples, housing 1400 may include an integrated first imaging member , laryngoscope, and in some examples the housing 1400 includes a laryngoscope coupler that is adapted to be releasably secured to a laryngoscope. In some examples, the housing 1400 is coupled to or comprises a video device that is not necessarily considered a laryngoscope but is adapted to provide video images. In some examples, the handheld housings herein do not include and are not adapted to be coupled to a laryngoscope. In this example, system 1400 optionally includes a detachable cover 1460, which in this example include a first imaging member channel 1462 defining a lumen and a tracheal tube channel 1461. Any other suitable cover feature or description herein including in any claim may be incorporated into assembly 1400 in 14A-14D. Assembly 1400 includes an introducer assembly 1499 which may be referred to herein as a second imaging member, which includes an introducer housing 1490 secured to moveable introducer 1450, wherein the introducer housing 1490 is sized and configured to be releasably secured to handheld housing 1410, and wherein the introducer assembly 1499 may be disposable. In this example, handheld housing 1410 includes a plurality of electrical connections 14B that are adapted to be placed into electrical communication with corresponding electrical connections 1495 of the introducer housing 14C and 14D, which may facilitate at least one type of controlled movement of the introducer, which is described below. The handheld housing 1410 electrical connections are in communication with an on-board handheld housing controller that may be adapted to cause movement of the introducer by facilitation sending electrical signals to the electrical connections. For example, as described elsewhere herein, an on-board controller may include one or more processors and/or computer executable methods that facilitate the automated movement , via AI of the introducer based on automatic or manual image recognition methods. Any of the suitable features of any other handheld housing herein that facilitates controlled movement of an introducer may be incorporated into the handheld housing 1410. In this example, handheld housing 1410 includes one or more motors therein, wherein a motor is in rotational communication with or considered part of motor coupling 1413, which may comprise a motor shaft that is rotated when the motor rotates. In this example, the motor is adapted to, when activated, cause axial movement of the introducer, which is described below. 14C and 14D illustrate exemplary features of introducer housing 1490 and introducer 1450. In this embodiment, introducer housing 1490 includes an electrical coupling 1495 comprising a plurality of electrical connects as shown that are adapted and positioned to create electrical communication with the electrical connections of the handheld housing 14B when the introducer assembly 1499 is releasable secured to the handheld housing 1410. The electrical coupling between the handheld housing and the introducer housing may provide for one or more types of controlled movement of the introducer. It is of note that the electrical coupling is optional, and in alternative designs the electrical connections may be replaced with one or more motors in the handheld housing. Electrical coupling 1495 may also comprise a connection to communicate image data from the introducer to the handheld housing. The introducer housing 1490 is adapted to releasably couple to the handheld housing, wherein the coupling creates operable communication between one or more controllers in the handheld housing and the introducer. In this example, the coupling creates both electrical communication and mechanical communication, but in other examples it may be solely mechanical or solely electrical. In this example, a mechanical coupling is adapted to control axial movement of the introducer, and electrical coupling is adapted to cause one or more types of deflection of the articulating section of the introducer shown in 14C, which may facilitate 360 degrees of deflection. In this example, a motor coupling includes a motor in the handheld that is rotationally coupled to a shaft, as shown in 14B. The motor shaft is sized and configured to be disposed within motor coupling 1496 is the introducer housing, which is shown in 14C. The motor shaft may be sized and configured to interface with an inner surface of one of the rollers 1494a or 1494b, shown in 14D. Motor activation thus drives the rotation of one of the rollers 1494a or 1494B, both of which are adapted to rotate about an axis in this example. One end region of the introducer 1450 optionally an end of a flexible introducer shaft is fixed relative to the housing 1490 at location 1491, while the introducer and housing are adapted such that the introducer can move axially relative to the housing 1490 at regions distal to location 1491. Introducer 1450 is shown disposed through an aperture 1493 in housing body 1492 and through an aperture 1497 in housing body 1492. The handheld housings may be able to be used with introducers of different sizes. The motor shaft may, for example, be a common or universal driving element that may be sized and configured to fit within rollers of varying size, depending on the dimensions of the rollers and/or the introducer. This is an example of how a universal handheld may be able to be releasable secured to different introducers, which may have at least one difference is size/dimension , diameter. This may allow the handheld housing to be used in pediatrics as well as with adults. As used herein, an introducer housing may or may not completely surround or enclose the introducer. For example, an introducer housing may have one side that is open to the ambient environment and still be considered an introducer housing herein. Introducer 1450 is disposed between rollers or wheels 1494a and 1494b, as shown, such that rotation of the rollers in opposite directions towards the introducer causes axial movement of the introducer, as indicated by the arrow in 14D. Rotation of the rollers in a first manner may cause distal movement, and rotation in the opposite manner may cause proximal movement. Any other disclosure herein related to controlled axial movement , such as axial distances, , 10-40 cm may be incorporated into the disclosure of 14A-14D. One or both of the rollers may have a groove or other frictional interface, similar to that shown in top right in 14D, while in alternatives one or more rollers may have less pronounced teeth or grooves. In alternatives, one or more of the one or more rollers may be made out of a polymeric, plastic so as to reduce the likelihood of or avoid damage to the introducer shaft. The rollers may have a smooth interfacing surface or they may have non-smooth interfacing surfaces. The introducer housings herein may include a body such as housing body 1492, wherein the housing body may comprise one or more components, optionally two or more components that secured together , with adhesive, welding, to at least partially define the housing body. The introducer housing assemblies herein may include a plurality of electrical couplings 1495 , comprising pins and/or vias that may each be coupled to a wire three exemplary wires shown in 14D, wherein the individual wires may each extend through the introducer shaft or be in electrical communication with a wire that extends through the introducer shaft. For example, any of the wires , four-nine may communicate a video signal from a camera at a distal end region of the introducer , 14C, or the wire may facilitate deflection control of the articulating section of the introducer, such as with shape memory material , nitinol or bi-metal wires , 5-8 wires, for example. A general concept of communicating an electrical signal through a wire comprising shape memory material to cause a shaft to deflect has been described, and the basic concept may be used to incorporate one or more of such wires to cause deflection of the introducer articulating sections herein. For example, electrical signals may be communicated to one or more introducer wires to cause distal sections thereof to transition between shape memory states, which can be used to control the deflection of the introducer in one or more directions. Any of the wires herein may also comprise bi-metal wires, which may also be used to cause deflection of the introducer by communicating signals to and through the one or more wires. An additional electrical signal can be transmitted to the wire to cause a distal region of the wire to change shape back to an original, or non-deflected, or straightened shape. Electrical signals may be communicated to any combination of wires to create and cause any desired deflection or configuration of the articulating section, which may be controlled automatically by AI, for example. For example, deflection can be caused by changing the shape of any combination of the wires in the introducer, which can be used to create a variety of shapes and configuration of the deflectable section of the introducer. 14A-14D illustrate an example of a system that uses mechanical coupling to control axial movement of the introducer, and electrical coupling to cause tip distal end region deflection of an articulating section of the introducer, an example of which is shown in 14C. Any of the disclosure herein related to automatic and/or manual introducer control may be incorporated in system 1400. For example, the deflection of the distal region of the introducer using one or more of shape memory material , nitinol or bi-metals can be used to control the deflection of the introducer in one or more of any of the automatic modes and/or manual modes herein, including partial automatic and partial manual. As can be seen in 14D, the axial motion of the introducer is controlled , force applied at a location that is distal to the proximal end of introducer shaft which in this example is coupled to the introducer housing at location 1491. The forces applied to the shaft that cause the axial motion are applied where the introducer shaft interfaces with the rotatable rollers. While not shown in 14A, the introducer 1450 is generally adapted to guide the advancement of a tracheal tube, for which any relevant disclosure herein may be incorporated into this example. By controlling the axial motion of the introducer , force applied at a location that is closer to the tracheal tube, as in this example, axial movement of the introducer is less likely to cause buckling of the introducer. Applying force to the introducer at a location further from the tracheal tube may increase the likelihood of introducer shaft buckling, for example. An additional exemplary advantage of controlling the axial introducer movement at one or more particular locations that are relatively close to a tracheal tube is that better force feedback may be provided if desired or required. Any of the handheld housings herein , 1410 may comprise an encoder on a motor to control the axial distance that the introducer is moved. Additionally or alternatively, any of the introducers herein may include a plurality of visual markings thereon , axially spaced lines that may be used to control the axial travel of the introducer. Additionally or alternatively, any of the blades herein may include a RFID tag or EEPROM that may be used to control initial introducer advancement and/or to communicate to the system a blade type, since blade type can affect first visuals. Any of the components that may include an introducer housing , 1490 that can be releasably secured to a handheld housing may include a RFID tag or EEPROM, which may be adapted to notify to the system or control what type of introducer is being used, such as information related to introducer diameter, introducer length, video resolution, and/or being adapted to enable and disable certain features, such as reuse. In any of the examples herein, an introducer housing , 1490 may be adapted to be releasably secured to a back surface or a side surface of the handheld housing , 1410. In the embodiment in 14C, the section of the introducer referred to as extra loop in the figure is shown not encased in a housing until a portion of the introducer is fed through housing 1490. In alternative designs, this region of the introducer may be at least partially contained or disposed in a housing, which may provide protection to the introducer and possibly reduce the likelihood of objects being caught in or snagged by the loop region of the introducer. Such a housing may be fixed relative to housing 1490 or be a part of housing 1490 such that the introducer may move relative to the additional loop housing. For example, this loop region of the introducer may be disposed in a cylindrical housing that is in a fixed relationship relative to housing 1490. Alternatively, the loop region of the introducer may be disposed within a helical housing or container that is secured to housing 1490, wherein the introducer can be movable relative to the helical housing to facilitate axial movement. In the example in 14A-14D, deflection is caused by wires in the introducer that are adapted to change shape in response to signals communicated thereto. In alternatives, deflection of the introducer may instead be driven by additional motor couplings between the handheld housing and the introducer housing. In these alternatives, the system may include mechanical couplings for axial movement of the introducer as well as deflection of the introducer. In these alternatives, wires in the introducer can be in communication with one or more motors, wherein the motors can be used to tension the wires and cause deflection of the steerable section of the introducer. For example, the system may include two or four additional motors to facilitate deflection of the deflectable section. For example, the system may include four motors for tensioning pull wires or other elongate tensioning elements to cause deflection and/or transitioning back towards a straightened configuration. Including four additional motors may help maintain tension in pull wires when a pull wire at a different circumferential location is tensioned to deflect the housing, which can help with backlash by reducing slack in the other pull wires that were not tensioned for deflection. Having four motors, for example, may help maintain tension on all pullwires that aren't being tensioned to deflect in that particular location. Alternatively, the system may include two additional motors, wherein the additional motors may be adapted to deflect the deflectable section of the introducer. Any of the introducer housings herein may also include or house therein a lumen that me be adapted as a working lumen. The housing may include one or more ports in communication with one or more lumens to allow for the delivery of one or more substances and/or devices into the working lumen, for example. A schematic illustration of a port and lumen are shown in 14D. For example, housing 1490 may be adapted to couple to and receive a syringe to facilitate delivery of one or more substances, such as one or more therapeutics. It may be beneficial to have an agent delivery port and lumen in the housing, which may be disposable and may not need to be cleaned. Additionally, a port and lumen may allow for the delivery of one or more other devices, such as one or more diagnostic , ultrasound imaging and/or therapeutic devices, or other medical tools. One or more delivery ports may be on a side or region of the housing other than that shown in 14D, such as through a lower side or top side. Additionally, the general location 1491 where the introducer is fixed other fixation locations 1491 may be used may include a port into which a substance or device may be delivered. The fixed location may provide more functionality and options at that location since the introducer does not move relative to the housing at that location. Additionally, any of the introducers herein may thus also include a working channel or working lumen through which any device and/or substance may be delivered. As discussed above, an exemplary benefit of some embodiments herein is that a distal end of an introducer and a distal end of a first imaging member may be initially positioned in a patient as a coupled unit , both coupled to a handheld housing. This initial positioning of the introducer may allow the system to advance the introducer immediately sufficiently distally inside the patient's airway and in a such a manner that a substantially shorter distance would be required for the introducer to navigate to the desired location laryngeal inlet. This would be opposite to the standard situation when the introducer were not coupled to the handheld and had to be advanced distally from the starting point located further away from the desired location. For example, in some uses, the introducer may only be about 2-7 cm from a desired location when inserted into its initial position when coupled to the first imaging member and the assembly in general. The introducer may then optionally be guided to the desired location while utilizing imaging and the resultant robotic control from the first imaging member only, because the first imaging member is able to provide the reliable and stable anatomic landmarks by viewing a larger anatomical region, even in a difficult airway situation. One of the exemplary benefits with these examples is that the introducer and first imaging member may be coupled to a common assembly in a manner such that fields of view of the introducer and first imaging member are at least partially overlapping. An exemplary benefit of some of the embodiments herein that incorporate a common handheld housing to which an introducer and a first imaging member are coupled is that the overall footprint or envelope occupied by the structural devices when placed in the patient may be smaller or at least more reliably controlled, which in the case of tracheal intubation may further improve the first pass success rate, shorten intubation time and lower the risk of injury to the patient. Any of the image processing herein may take place inside or outside of the handheld housing. For example, image processing may at least partially take place in a device that is external to the handheld housing, such as a computer with a graphics processing unit, smartphone, etc. The handheld may be in communication wired or wireless with the external device, and optionally wherein information related to acquired optical information is communicated from the handheld to the external device for processing. The external device may also be adapted to communicate information to the handheld housing to facilitate control of the introducer, which is described in more detail herein. 17A-17G illustrate at least a portion of exemplary intubation system 1700, wherein 17A and 17B are unassembled views of exemplary integrated and handheld dual-video tracheal intubation assembly 1702, wherein this and other integrated assemblies herein may simply be referred to as an assembly. Any of the individual components of any of the assemblies herein may be assembled together prior to the procedure and thereby assemble and create the assembly. 17C-E illustrate the assembled assembly 1702. Assembly 1702 includes housing 1710, which includes a first imaging member coupler 1712, a second imaging member coupler 1714, and a cover coupler 1716. Housing 1702 is configured to be releasably coupled to cover 1780, first elongate imaging member 1730 an example of which is shown in 17F and 17G, and an optionally disposable second elongate imaging member 1740. Assembly 1702 which may include any suitable feature of assembly 1702, and vice versa includes first elongate imaging member 1730 with a first coupling region that is sized and configured to be releasably coupled to first imaging member coupler 1712, as shown in 17F and 17G. First elongate imaging member 1730 includes elongate flexible body 1732 and a first image sensor 1734 , a video camera disposed at a distal region 1736 of elongate body 1732. Assembly 1702 also includes second elongate imaging member 1740, which includes second coupling region 1742 that is sized and configured to be releasably coupled to second imaging member coupler 1714 of housing 1710. Second elongate imaging member 1740 includes a flexible and navigable elongate endotracheal tube introducer 1770 introducer, at least a portion of which is deflectable. Introducer 1770 is sized to be disposed within an endotracheal tube 1790 and to allow endotracheal tube 1790 to be moved axially over introducer 1770. Second imaging member 1740 includes second image sensor 1773 , a video camera disposed at a distal region 1772 of introducer 1770. As is described in more detail below, introducer includes a first end or first end region secured to a housing of second imaging member and a movable portion that is movable relative to the housing of the second imaging member. Assembly 1702 also includes cover 1780 with a cover coupling region 1782 that is sized and configured to be releasably coupled to the cover coupler 1716 of housing 1710. Cover includes an elongate channel defining an elongate lumen, the elongate channel sized and dimensions such that at least a portion of elongate body 1732 of first imaging member 1730 is disposed within the elongate lumen when the first coupling region of the first imaging member 1730 is releasably coupled to first imaging member coupler 1712 and when the cover coupling region 1782 is releasably coupled to cover coupler 1716. Cover 1780 further includes endotracheal tube channel 1784 disposed on a side of cover 1780 as shown, the endotracheal tube channel 1784 sized and dimensioned to interface with endotracheal tube 1790 and restrict movement of endotracheal tube 1790 relative to cover 1780 in at least one direction. Endotracheal tube channel 1784 is further configured to allow endotracheal tube 1790 to be laterally moved relative to endotracheal tube channel 1784. In this example channel 1784 includes a depression or trough formed in a side of cover 1780 that is dimensioned and configured to interface with a portion of the outer wall of endotracheal tube 1790, and in some embodiments channel 1784 may have a cross sectional configuration that includes a surface forming a partial circle to interface with a circular outer surface of endotracheal tube 1790. Endotracheal tube 1790 includes a lumen that is sized to movably receive introducer 1770 therein, as shown in 17C. Assembly 1720 further includes one or more actuators disposed within housing 1710 and configured and positioned such that when second coupling region 1742 of the disposable second elongate imaging member 1740 is releasably coupled to second imaging member coupler 1714 of housing 1710, the actuator is configured to facilitate the controlled robotic movement of introducer 1770 and second images sensor 1772 , a video camera relative to first image sensor 1734 , a video camera. System 1700 may also include one or more processors, which may be part of the assembly or which may not be part of the assembly. The one or more processors may be configured to receive as input information indicative of a signal received from the first image sensor , video camera when the first image sensor is disposed in an upper airway of a patient, and cause communication to an actuator within the housing , housing 1710 to control the robotic movement of the introducer and the second image sensor , video camera relative to the first image sensor and toward one or more upper airway anatomical landmarks. Details about exemplary methods of use are described in more detail elsewhere herein. As is described elsewhere herein, an exemplary advantage of assemblies herein is that, once assembled, they are sized and configured such that they can be held and moved as an assembly with a single hand of an operator. Assembly 1702 is an example of an assembly that is sized and configured such that when the first coupling region of first imaging member 1730 is releasably coupled to first imaging member coupler 1712, when second coupling region 1742 is coupled to second imaging member coupler 1714, when cover coupling region 1782 is releasably coupled to cover coupler 1716, and when endotracheal tube 1790 is releasably coupled to endotracheal tube channel 1784, the assembly, including the first image sensor and the second image sensor, is movable as an integrated unit with the single hand of the operator. 17C illustrates an assembled view of assembly 1702, referred to a bottom view in which the first and second image sensors can be seen. Horizontal distance between images sensors is measured in the direction H as labeled in 17C. 17D illustrates a side assembled view of assembly 1702. Relative axial distance between image sensors when the assembly is assembled as is described herein is measured in the axial direction A as shown in 17D. It is understood that the axial direction A depends on the orientation of the assembled assembly. For example, if the assembly in 17D were rotated counterclockwise 90 degrees, axial direction A would similarly be rotated 90 degrees. As is set forth herein, an exemplary advantage of some assemblies herein is that they are dimensioned and configured to be held and moved by a single hand of an operator. When assembled, the relative couplings between the housing, cover, first elongate imaging member, and endotracheal tube maintains the first image sensor at a distance from the second image sensor prior to actuation of the actuator, examples of which are shown in 17C-17E. In some uses it may be preferable that the first and second image sensors are axially aligned or as close to axially aligned as possible when the assembly is assembled. In some instances, the image sensors are axially aligned. In some instances they are substantially axially aligned. In some instances, the first and second image sensors are maintained to be axially A within 3 cm of each when the assembly is assembled. In any these instances, the horizontal distance H between first and second image sensors may be no greater than 2 cm when the assembly is assembled. The maintained proximity of the images sensors when assembled may help provide the overall smaller profile or footprint of the assembly, which helps make it easier and safer for a single operator to hold a dual-video intubation assembly in a single hand. 17E shows a top view of assembled assembly 1702 in which second imaging member 1740 is coupled to housing 1710, as shown. The first and second image sensors are facing down or into the page in this top view. 17F and 17G illustrate assembly 1702 that may be include any of the disclosure from assembly 1702. 17F and 17G show first imaging member 1730, which includes elongate flexible body 1732 and a first image sensor , video camera disposed at a distal region 1736 of flexible body 1732. First imaging member 1730 may, in some embodiments, be integrated into and considered part of the cover, wherein the first imaging member and cover are releasably coupled to housing 1710 as a subassembly. In some examples, first imaging member 1730 may be releasably coupled to the housing before use separately from the cover. In some examples, the housing , housing 1710 and first imaging member , imaging member 1730 may be an integrated unit and don't need to be coupled by an operator prior to the medical procedure. It may be advantageous to be able to remove the first imaging member from the housing and reuse the housing, such as if a differently sized first imaging member is needed for a subsequent procedure , between different patients or pediatric patient versus adult patient, advantages of which are described elsewhere herein. It is thus understood that any of the assemblies herein may include a housing and first imaging member that are integrated, a cover and first imaging member that are integrated and coupled to the housing as a unit, or a cover and a first imaging member that are separately releasably coupled to the housing. Any component in system 1702 that is not labeled or described is understood to be the same as the corresponding component or subassembly in assembly 1702. 18A-18J illustrate exemplary housing 1710. It is understood that any of housing features described with respect to housing 1710 may be incorporated into any of the housings herein, which may be part of any of the assemblies herein. Any of the labels to housing 1710 from 17A-17G may be included in 18A-18J even if not labeled in the figures. Housing 1710 includes communication region 1720 that is adapted to communicate with optionally disposable second imaging member 1740. Communication region 1720 may be configured to be in one or more of electrical, mechanical, optical and/or video communications with second imaging member 1740 when second imaging member 1740 is releasably coupled to housing 1710 see 17C-17E. In this example, communication region 1720 includes mechanical interface 1721 and mechanical interface 1722 that are adapted to mechanical interface with second imaging member 1740, described in more detail below. In this example, mechanical interfaces 1721 and 1722 are positioned and configured to mechanically interface with corresponding features on second imaging member 1740 and cause deflection of the introducer via one or more pull wire or other tensioning elements, as described below. Mechanical interfaces 1721 and 1722 may be actuated by one or motors, or example, that may be disposed in housing 1710. Motors in this context may be considered part of the robotic control mechanism that is adapted to robotically control the movement of the introducer, optionally in response to the image processing described elsewhere herein. Motors in this type of design may be considered to be actuators as that term is used herein, or one or more motors may be considered to be part of an actuator, wherein actuator may refer to one or more separate components operating together as a mechanical and/or electrical system, for example , motor and drive shaft, . The assemblies herein include one or more actuators 1761 see 20 that facilitate directly or indirectly the robotic controlled movement of the introducer. Motors within the housing 1710 as described in this example are an example of actuators 1761 within the assembly. The pullwires that are in this exemplary embodiment are also considered to be actuators, and in this example they are disposed in exemplary second imaging member 1740. Communication region 1720 may also further include mechanical interface 1723 that is positioned to mechanically interface with the second imaging member and cause or facilitate axial movement of the introducer, which is described in more detail below. One or more motors in the housing may cause the mechanical movement of mechanical interface. Communication region 1720 of the housing optionally includes optical sensor 1724 that is adapted to optically track the axial movement and/or axial position of the introducer as is moved axially relative to the optical sensor, and is described in more detail elsewhere below. Other sensors may be used to track the axial movement and/or position of the introducer. Communication region 1720 also optionally includes a plurality of electrical connectors 1725 , a plurality of electrical connects, which can be adapted to interface with a plurality of corresponding connectors on the second imaging member 1740 to receive data communicated from the second image sensor , video camera signal to the housing where it may be processed or communicated to an external device and/or display of the device for processing. Exemplary image processing and robotic control automatic and/or manual robotic control utilizing the second image sensor data is described elsewhere herein. 18G-18J illustrate housing 1710, with at least one housing body component removed , half of a shell, for example to illustrate exemplary internal housing components that may be disposed in any of the housings herein. 18G is a side view of housing 1710. 18H is a bottom perspective view of housing 1710. 18I is a bottom view of housing 1710. 18J is a top perspective view of housing 1710. In this example, housing includes camera control unit 1801, which may be a camera control unit for any of the first image sensors herein, and which may be adapted to translate a raw camera signal from the first image sensor into USB, for example. Housing 1710 further includes second camera control unit 1802, which may be a camera control unit for any of the second image sensors herein, and which may be adapted to translate a raw camera signal from the second image sensor into USB, for example. Location 1803 presents an exemplary location for an exemplary USB hub. 18G also illustrates exemplary axial drive motor 1804 which may be referred to herein as an axial actuation mechanism, which may comprise an axial drive servo, and which may be adapted to facilitate axial motion of the introducers herein, which is described in more detail elsewhere herein. For example, axial drive motor 1804 may cause rotation of mechanical interface 1723, which is described in more detail herein and shown in 18J. Axial drive motor 1804 is an example of an actuator as that term is used herein and in this example is an example of an actuator that is disposed in the housing. 18H illustrates merely exemplary first and second deflection or articulation drive motors 1805 which may be referred to herein as actuation mechanisms, which may comprise deflection servos, and which may be adapted to facilitate deflection of the introducer, which is described in more detail elsewhere herein , which may cause pullwires to be tensioned. For example, first and second deflection drive motors 1805 may cause rotation of mechanical interfaces 1721 and 1722, respectively, which are described in more detail herein and shown in 18J. First and second deflection or articulation drive motors 1805 are examples of actuators as that term is used herein and in this example are an example of actuators that are disposed in the housing. 18I illustrate an exemplary location for a microcontroller. 19A-19K illustrate exemplary second elongate imaging members and exemplary features thereof. 19A-19K are shown and described in the context of second imaging member 1740, but any suitable aspect of the second imaging member in 19A-19K may be incorporated with any other second elongate imaging member herein, and vice versa. For example, 14A herein illustrates an exemplary system 1400, which may also be considered to be an integrated assembly as that term is used herein. System 1400 or assembly 1400 in 14A includes an introducer assembly 1499 which may also be considered to be any of the second imaging members herein, which includes an introducer housing 1490 which may be considered to be any of the second imaging member housings herein secured to moveable introducer 1450, wherein introducer housing 1490 is sized and configured to be releasably secured to handheld housing 1410 which may be considered to be any of the housings herein, and wherein the introducer assembly 1499 may be disposable. 19A is view of the bottom of second imaging member 1740, which includes housing 1744 and introducer 1770. In this example, introducer 1770 is secured to housing 1744 at an end or an end region of introducer 1770, and movable relative to housing 1744 along at section of the introducer 1770 distal to where it is secured to housing 1744. This is similar to introducer 1450 shown in 14A-14D herein. Housing 1744 including a communication region 1746 that includes one or more communication elements that are adapted to communicate with the handle when the second imaging member is releasable coupled to the housing. The communication elements may optionally be positioned and configured to communicate mechanically, electrically, video and/or optically with the handle. The communication elements may be configured to communicate information , image data and/or operably communicate with the handle to cause movement of the introducer in one or more directions, as is described in more detail herein. In this exemplary embodiment, communication region 1746 of second imaging member 1740 includes mechanical interfaces 1741 and 1743 that are disposed relative to housing 1744 and configured to mechanically interface with mechanical interfaces 1721 and 1722 in the communication region 1720 of housing 1710 see 18C. The mechanical interfaces may include, as an example only, a geared interaction with teeth, wherein rotation of mechanical interfaces 1721 and 1722 of the housing 1710 , in response to activation of a housing motor causes rotation of mechanical interfaces 1741 and 1743, respectively. As described in more detail below, one or more tensioning elements , one or more pullwires may be operatively coupled to mechanical interfaces 1741 and 1743 such that rotation of the interfaces 1741 and 1743 causes tensioning of one or more pullwires, which may cause deflection of the introducer. In this example, communication region 1746 of second imaging member 1740 also includes mechanical interfaces 1747 that is disposed relative to housing 1744 and configured to mechanically interface with mechanical interface 1723 in the communication region 1720 of housing 1710 see 18C. The mechanical interface may include, as an example only, a geared interaction with teeth, wherein rotation of mechanical interface 1723 of the housing 1710 , in response to activation of a housing motor causes rotation of mechanical interface 1747. In this example, interface 1745 is configured to have a geared interaction with interface 1747 such that rotation of interface 1747 causes rotation of 1745 as well see 19F. Introducer passes through housing 1744 and is disposed between rotatable mechanical interfaces 1747 and 1745 such that their rotation causes axial translation distal or proximal of introducer 1770, wherein 19A illustrates axial direction A, which represents the axial movement of introducer 1770 relative to housing 1744. Additional details of communication region 1746 are described below. 19B illustrates a top of the second imaging member 1740, including endotracheal tube coupler 1750. 19C illustrates a side of view of second imaging member 1740. 19C illustrates an introducer with a curved configuration. When one or more system or assembly components are descried herein as having a length, the length generally refers to the length along the component , from a first end or a first location of the component to a second end or a second location. For example, the introducer in 19C may have a length that extends from a first end secured to housing 1742 and a distal most end of the introducer, wherein the length of the introducer in this example can be measured from the first secured end to the second end along the length of the introducer itself, even if there is one or more curved regions. 19D and 19E illustrate end views of second imaging member 1740. 19F illustrate a view of the top of exemplary second imaging member 1740 with a top surface of layer of the housing removed to show exemplary internal components in the communication region 1746 of housing 1744. The view in 19F is the same view as in 19B, but with a portion of the housing removed to illustrate components internal to housing 1744. 19A illustrates a view of the bottom of second imaging member 1740. 19F illustrates an exemplary geared interface between rotatable interfaces 1745 and 1747, which are described above. Mechanical interface 1747 may be in interfaced with rotating interface 1723 in the handle such that rotation of rotating interface 1723 , driven by a motor causes rotation of mechanical interface 1747. The geared interface between interfaces 1745 and 1747 causes interface 1745 to rotate in the opposite direction to the rotation of interface 1747, which axially moves introducer 1770 that is disposed between interfaces 1745 and 1747. The geared relationship between interfaces 1745 and 1747 may amplify forces on the introducer and help prevent slipping during axial movement of the introducer. 19F also illustrates a plurality of electrical connectors 1749, which are arranged and configured in the housing to interface with electrical connectors 1725 in the handle, and which may communicate image data from the image sensor at the distal end of the introducer to the housing 1710, which is described in more detail herein. 19F also illustrates window or opening 1748 aligned with and adjacent to a section of introducer 1770, as shown. The window 1748 is positioned to align with sensor 1724 , an optical sensor in the housing such that the housing sensor can track the movement or position of the introducer as it moves axially A. The introducer may have one or more markers on an outer surface thereof to facilitate position tracking or sensing by the housing sensor 1724. The housing sensor 1724 may communicate the sensed information to the processor directly or indirectly to track the axial movement and/or position of the introducer. 19F also illustrates mechanical interfaces 1741 and 1743, which are positioned and configured to interact with mechanical interfaces 1721 and 1722 in housing 1710. In this example, mechanical interfaces 1741 and 1743 rotate in response to rotation of mechanical interfaces 1721 and 1722 in housing 1710, which in this example is used to cause deflection of a distal region or distal tip of the introducer in a plurality of directions , one or more of up, down, left, right, or a combination thereof. 19F illustrates a mere example of using first and second motors within housing 1710 to cause rotation of interface 1741 and 1743, respectively. In this example, interfaces 1741 and 1743 are each coupled to first and second pullwires, such that when they are rotated in a first direction, one of the pullwires is tensioned to cause deflection of the introducer distal tip, and when rotated in the opposite direction, a second pullwire may be tensioned to cause deflection in a different direction , 180 degrees relative to the first direction. In an example, rotation of interface 1741 can cause deflection of the introducer in a first plane each direction 180 degrees relative to the other direction, for example, left and right, and rotation of interface 1743 can cause deflection of the introducer in a second plane transverse to the first plane each direction 180 degrees relative to the other direction, for example, up and down. The interfaces 1741 and 1743 may be controlled separately, such as with different motors in the housing, both in response to robotic control of the introducer movement either automatic or manual robotic control. In the example of 19F, pull wires , exemplary and illustrative first and second pull wires 1757 are shown for clarity may be attached to interfaces 1741 and 1743 at locations 1754 and 1753 respectively, using any of a variety of securing techniques or mechanisms. The pullwires extend from interfaces 1741 and 1743 through pull wire channels generally labeled 1752, and then extend into introducer 1770. The pull wires may extend through the introducer using existing pull wire routing techniques and approaches, and are secured in a distal region of the introducer to cause the defection of the distal tip of the introducer. In any alternative, a housing motor may be in operable communication with only one pull wire. For example, in a variation to that shown in 19F, the housing 1710 may include four motors therein, each causing an individual pull wire to be tensioned. 19G-19K illustrate exemplary second imaging member housings, and it is understood that any second imaging member housings herein may include any of the exemplary features of the exemplary housing 1744 shown in 19G-19K, and vice versa. 19G is a perspective view of the bottom of housing 1744. 19H is a bottom view of housing 1744. 19I is a top perspective view of housing 1744. 19J is a front end of housing 1744, illustrating first and second windows 1755 and 1756 through which introducer 1770 extends. 19K illustrates a back end view of housing 1744, including endotracheal tube coupler 1750. As set forth herein, a first end of end region of the introducer may be secured within housing 1710, and in 19F a first end of introducer 1770 is secured to element 1751 within housing 1751. From element 1751, introducer 1770 extends through housing opening or window 1756 see 19J and out of housing. Introducer 1770 then forms a loop configuration as shown , 17B, 17C, 17E, 17G, 19A, 19B, 19E, and then extends back into the housing through housing opening or window 1755 see 19J. Introducer then extend through optional endotracheal coupler 1750 of housing 1744, as can be seen in 19F. As shown, housing 1744, in this example, has a general rectangular configuration and in this example it has a generally square configuration, with width and height that are both greater than a thickness measured in the top-to-bottom direction. The corresponding mating region of housing 1710 has a similar rectangular configuration, as shown in the unassembled view of 17B. Second imaging member 1740 includes endotracheal tube coupler 1750 which in this example has a circular cross section and has a cylindrical configuration. As shown in 17A, endotracheal tube has a first end with a coupler region 1791 as shown that is dimensioned and sized to interface with endotracheal tube coupler 1750 on the housing 1744 of the second imaging member 1740. Endotracheal tube 1790 also has lumen that is sized to receive the introducer therein when the coupler region 1791 is interfaced with endotracheal tube coupler 1750 of housing 1744, such as is shown in 17C. As shown in 17A, body 1711 of housing 1710 also includes a side endotracheal tube channel 1713 that is positioned and configured such that when cover 1780 is releasably secured to housing 1710, channel 1713 of housing 1710 and the channel 1784 in the cover form a continuous channel for the endotracheal tube, as shown in 17D. The term continuous in this context includes some minimal axial gap , 0. 01 cm-2 cm between the two channels when cover 1780 is coupled to housing 1710, as long as endotracheal tube 1790 can be interfaced with housing channel 1713 and cover channel 1784. Both channels 1713 and 1784 are open on one side to allow the endotracheal tube to be removed from the channels by laterally or horizontally removing them therefrom. The housings , housing 1710 or integrated handheld assemblies herein may include one or more image processors, while is some embodiments data may be communicated to a component not integrated into the assembly where image processing occurs, such as an external computer and/or display of the system, for example. Any type of connection may be used for data transfer, such as a USB interface. Any of the assemblies may also include one or more microcontrollers, which may optionally be disposed in the housing , housing 1710. Any of the assemblies may also include one or more motor interfaces , part of the robotic control of the introducer movement, which may optionally be disposed in the housing , housing 1710. Any of the assemblies herein may also include an electronics control board, which may provide power to the one or more motors optionally disposed in the housing during the robotically movement of the introducer. As shown in 17D in combination with 17C, the first and second image sensors , video cameras are maintained an axial distance relative to one another when the assembly is assembled. In 17D the axial locations of the first and second image sensors is labeled L, and in this embodiment the images sensors are within 3 cm from each other axially, and may optionally be axially aligned. The image sensors are also at a horizontal distance H see 17C not greater than 2 cm from other. When assembled, the first and second image sensors are thus maintained at a relatively close distance both axially and horizontally to each other, the advantages of which are described herein. Assembly 1702 is thus an example of an integrated and handheld dual-video tracheal intubation assembly, wherein the assembly is configured such that when the first imaging member is coupled to the housing, when the second imaging member is coupled to the housing, when the cover is releasably coupled to the housing, and when the endotracheal tube is releasably coupled to assembly, the assembly maintains the first image sensor at a distance from the second image sensor prior to actuation of the actuator, an example of which is shown in 17C and 17D. In this example, the lengths and configurations of all of the endotracheal tube, the introducer, the cover, the flexible body 1732 of the first imaging member 1730, and the body 1711 of housing 1710 together and individually allow and cause the first and second image sensors to be at the maintained distance from one another, as well as contribute to the overall small profile and footprint of the assembly, which allows the assemblies to be held and moved by a single hand of an operator. Assembly 1702 is an example of an assembly that can be used in a tracheal intubation. One of the advantages is that the first and second image sensors, when the assembly is assembled, are maintained at a relatively close axial distance , not greater than 3 cm, and optionally axially aligned. When in use, the introducer generally needs to be able to be robotically axially moved approximately at 3-5 cm to move it to or towards the glottic opening. The assembly is also adapted to be able to axially move the introducer through the glottic opening and in the trachea. The assemblies herein are thus able to robotically cause axially movement of the introducer and second image sensor that is at least 3 cm relative to the first image sensor, which is maintained in the upper airway when in use for tracheal intubation. The assemblies herein may include a distal movement limiter that is configured to limit distal movement of the introducer relative to an initial position and relative to a first image sensor. For example, assembly 1720 includes a distal movement limiter that is configured to limit distal movement of introducer 1770 relative to an initial position , as shown in 17C and 17D and relative to a first image sensor of the first imaging member 1730. In this example, a first end of introducer is secured to housing 1744, as shown in 19F, while the introducer is allowed to be robotically moved axially with the axial movement mechanism inside housing 1744 , including mechanical interfaces 1745 and 1747. By securing a first end of introducer 1770, there is a limit on how far introducer 1770 may be advanced distally in this example. The length of the introducer outside of housing 1744 between openings 1755 and 1756 in housing 1744 can also influence how far distally the introducer may be advanced, and may thus be considered part of or influence the distal movement limiter. For example, if the length of introducer 1770 that is outside housing 1744 and between openings 1755 and 1756 is very short, the distal movement limiter will prevent much distal travel of the introducer relative to housing 1744. 20 illustrates schematically an exemplary integrated and handheld dual-video assembly 2002, which may incorporate any and all aspects of assembly 1702 and assembly 1702. Assembly 2002 includes housing 2010, which may incorporate any and all aspects of any of the housings herein , housing 1710. Assembly 2002 includes cover 2080, which may incorporate any and all aspects of any of the covers herein , cover 1780. Assembly 2002 includes optionally disposable second imaging member 2040, which may incorporate any and all aspects of any second imaging member herein second imaging member 1740. Assembly 2002 includes endotracheal tube 2090, which may incorporate any and all aspects of any of the ETT's herein , endotracheal tube 1790. Assembly 2002 includes one or more actuators 2061, which may incorporate any and all aspects of any one of the actuators herein , one or more actuators 1761 actuators may be within housing 2010 and/or within second imaging member 2040, for example. Any of the assemblies herein , 1702, 1702 may be generalized as shown in the schematic of 20. It is understood that any description including features or methods of use related to assembly 1702 may be integrated with assembly 1702, and vice versa. 21 illustrates an exemplary method of using any of the assemblies herein. In the method shown, an assembled assembly , assembly 1702 is positioned within an upper airway of a patient , such as shown in 15 at step 2110. The method may include causing a processor of a system to receive a signal indicative of image data from a first image sensor , a video camera of the assembly at step 2120. The method may include, at step, 2130, processing the signal indicative of the data from the first image sensor. The method may include, at step 2140, initiating automated or manual robotically controlled movement of an introducer of the assembly away from a first image source of the assembly and towards at least one upper airway anatomical landmark in response to the processing step while maintaining the first image sensor in the upper airway. The method may also include, at step 2150, causing the introducer to be moved within the endotracheal tube and advanced through and past the glottic opening and into a trachea while at least a portion of a cover of the assembly and the first image sensor remain in the upper airway and while the first image sensor receives video data from the first image sensor during automated movement of the introducer. Any additional method steps may be incorporated into the exemplary method shown in 21. Any of the second elongate imaging members , 1740 may also be referred to herein as a cartridge, in that the cartridge may be releasably secured to the housing , housing 1710. In variations to some embodiments herein, the systems are adapted such that a remote operator can remotely control the robotically controlled movement of the introducers herein. For example, a first operator would be with the patient and places the assembly into the patient's upper airway, for example. The first operator activate operation of the first and/or second image sensors. The image data can be communicated , wireless communication to the remote location, where a second operator can control the introducer remotely using a workstation. The second operator can have a display that shows video data from the first and/or second image sensors. This type of remote control arrangement can provide the advantages of the integrated dual-video intubation assemblies herein, with the exception that the first operator in these examples would be directly handling the assembly. Any of the processors herein may have stored thereon any number of computer executable methods , software, algorithms that may be adapted, alone or in combination, to receive one or more inputs that may be indicative of image or video data from one or more image sensors herein, and determine or contribute to determining a pathway in any number of directions for an introducer, which may include causing the movement of the introducer such that it is moved to or towards one or more anatomical landmarks. This type of determining or planning may be referred to herein as navigating or part of a navigation process. It is understood that any feature, component, or method step described herein in one example may be incorporated into any other suitable example herein unless the description indicates to the contrary. For example, any feature or method of use in any embodiment or aspect herein may be included with or incorporated into any other suitable embodiment or aspect herein. The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the inventions herein. However, it will be apparent to one skilled in the art that specific details may not be required in order to practice one or more of the inventions herein. Thus, the foregoing descriptions of specific embodiments of the inventions herein are presented for purposes of illustration and description. Even if not specifically indicated, one or more techniques or methods described in this disclosure may optionally be implemented, at least in part, in hardware, software, firmware or any combination thereof. For example, various aspects of the techniques or components may be implemented within one or more processors, including one or more microprocessors, digital signal processors DSPs, application specific integrated circuits ASICs, field programmable gate arrays FPGAs, programmable logic circuitry, or the like, either alone or in any suitable combination. The term processor or processing circuitry may generally refer to any of the foregoing circuitry, alone or in combination with other circuitry, or any other equivalent circuitry. Such hardware, software, or firmware may be implemented within the same device or within separate devices to support the various operations and functions described in this disclosure. In addition, any of the described units, modules or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware or software components. Rather, functionality associated with one or more modules or units may be performed by separate hardware or software components, or integrated within common or separate hardware or software components. When implemented in software, the functionality ascribed to the systems, devices and techniques described in this disclosure may optionally be embodied as instructions on a computer-readable medium such as random access memory RAM, read only memory ROM, non-volatile RAM NVRAM, electrically erasable programmable ROM EEPROM, Flash memory, and the like, for example. The instructions , methods may be executed by a processor to support one or more aspects of the functionality described in this disclosure.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/59/245/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9478657245635986,
                    "F_SPEC_PARAMS": [
                        "difficult to attack"
                    ],
                    "S_SPEC_PARAMS": [
                        "less effective"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Many types of nosocomial infections are difficult to attack with antimicrobials."
                    ],
                    "S_SENTS": [
                        "However, the drugs have been used so widely and for so long that the infectious organisms the antimicrobials are designed to kill have instead adapted to the antimicrobials, making the drugs less effective."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Strength"
                    ],
                    "F_SIM_SCORE": 0.5277632474899292,
                    "S_TRIZ_PARAMS": [
                        "Waste of Time"
                    ],
                    "S_SIM_SCORE": 0.5356082916259766,
                    "GLOBAL_SCORE": 1.6795514941215515
                },
                "sort": [
                    1.6795515
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11273555-20220315",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11273555-20220315",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-09-17",
                    "PUBLICATION_DATE": "2022-03-15",
                    "INVENTORS": [
                        "Bernard D. Casse",
                        "Clinton J. Smith",
                        "Christopher Lalau Keraly",
                        "Matthew E. Shaffer",
                        "Christopher A. Paulson"
                    ],
                    "APPLICANTS": [
                        "Robotik Innovations Inc.    ( Palo Alto , US )"
                    ],
                    "INVENTION_TITLE": "Multimodal sensor array for robotic systems",
                    "DOMAIN": "B25J 91694",
                    "ABSTRACT": "A multimodal sensing architecture utilizes an array of single sensor or multi-sensor groups superpixels to facilitate advanced object-manipulation and recognition tasks performed by mechanical end effectors in robotic systems. The single-sensors/superpixels are spatially arrayed over contact surfaces of the end effector fingers and include, , pressure sensors and vibration sensors that facilitate the simultaneous detection of both static and dynamic events occurring on the end effector, and optionally include proximity sensors and/or temperature sensors. A readout circuit receives the sensor data from the superpixels and transmits the sensor data onto a shared sensor data bus. An optional multimodal control generator receives and processes the sensor data and generates multimodal control signals that cause the robot system's control circuit to adjust control operations performed by the end effector or other portions of the robot mechanism and when the sensor data indicates non-standard operating conditions.",
                    "CLAIMS": "1. A method for controlling a robotic system including a robot mechanism having an end effector including a plurality of spatially arrayed superpixels, each said superpixel including an associated pressure sensor and an associated vibration sensor disposed over a corresponding contact surface portion of the end effector, the end effector being configured to grasp a target object, and the robot mechanism being configured to move the target object while being grasped by the end effector, the method comprising: controlling the pressure sensors of said plurality of spatially arrayed superpixels such that each said pressure sensor generates associated time-based localized static event data values in response to static forces applied to said corresponding contact surface portion on the end effector while the target object is grasped by the end effector; controlling the vibration sensors of said plurality of spatially arrayed superpixels such that said each said vibration sensor generates associated time-based localized dynamic event data values in response to vibrational forces applied to said corresponding contact surface portion on the end effector while the robot mechanism is moving the target object; while the robot mechanism is moving the target object, identifying a location of a non-standard operating condition using changes in at least one of said time-based localized static event data values and said time-based localized dynamic event data values; and adjusting the end effector in response to said identified non-standard operating condition location. 2. The method of claim 1, further comprising: using both said static event data and said dynamic event data to identify slipping of said target object relative to said end actuator, and conrolling the end effector to increase a grip force applied to the target object in response to said identified slipping. 3. The method of claim 1, wherein identifying said non-standard operating condition location includes using both said static event data and said dynamic event data to identify an undesirable contact location at which undesirable contact between said target object and a secondary object occurs, and wherein adjusting the end effector comprises controlling the robot mechanism to move the target object to a location that relative to the secondary object such that the undesirable contact is avoided. 4. The method of claim 1, wherein controlling the robotic system comprises performing an assembly process including the target object and a secondary object. 5. The method of claim 4, wherein the target object comprises a hollow structure having an open bottom end, and wherein performing the assembly process comprises mounting the target object over the secondary object such that at least a portion of the secondary object extends through the open bottom end into the hollow structure when the assembly process is successfully completed. 6. The method of claim 4, wherein identifying the location of the non-standard operating condition comprises detecting a misalignment between said target object and said secondary object by detecting changes in one or more time-based localized dynamic event data values generated by associated vibration sensors located adjacent to an impact location between said target object and said secondary object during said assembly process. 7. The method of claim 4, wherein identifying the location of the non-standard operating condition comprises detecting a misalignment between said target object and said secondary object by detecting changes in one or more time-based localized dynamic event data values generated by associated vibration sensors located adjacent to a scraping-type contact location between said target object and said secondary object during said assembly process. 8. The method of claim 4, further comprising using both said static event data and said dynamic event data to detect an expected final contact event that confirms a successful completion of said assembly process.",
                    "FIELD_OF_INVENTION": "This invention relates generally to robotic systems and more particularly to sensors utilized to control robot mechanisms.",
                    "STATE_OF_THE_ART": "Robotic systems typically integrate mechanical, electrical/electronic and computer science technologies in a way that produces autonomously controlled mechanisms that selectively perform a variety of different mechanical operations. For example, articulated robots are a class of industrial robotic systems in which an end effector , a hand or gripper mounted on a robot arm mechanism is utilized to perform repetitive tasks, such as picking up a target object at one location and moving the target object to a second location. The robot arm mechanism and end effector are generally controlled in accordance with a programmed series of movement operations that are based, for example, on a precise X-Y-Z starting location at which a target object will be reliably available for pick-up, and a precise X-Y-Z terminal location at which a receptacle is positioned to receive the target object when dropped off. While this programmed movement control approach is acceptable for use in highly ordered environments, erroneous and possibly dangerous situations can occur when minor variations arise, such as displacement of a target object from the expected starting location or a receptacle is displaced at the terminal location, whereby performance of the programmed movement operations can result in damage to one or both of the target objects and the end effector/gripper. To avoid such incidents, modern robotic systems often employ camera systems and single-modal sensors , pressure sensors that are mounted on the end effector and provide feedback information that allows the system's control circuit to recognize and adjust for minor variations. The lack of a rich end effector sensory feedback is one of the main limitations of modern robotic systems. That is, conventional single-modality sensors , pressure sensing only are unable to provide sufficient information to avoid many common industrial accidents and/or to perform complex assembly processes. For example, although single-modality pressure sensors provide sufficient data to verify that a predetermined gripping force is being applied by a hand-type end effector onto a target object, they lack the rich sensor feedback needed to recognize when the target object is slipping from the end effector's grasp. In addition, when mounting a canister-type object over a cylindrical object, single-modality pressure sensors provide insufficient data regarding excessive contact between the cannister and cylindrical objects when the canister and cylindrical objects are misaligned. Note that while camera-type feedback systems may be useful to identify and adjust for such occurrences in some cases, critical portions of the camera's field of view are often occluded by the end effector, which limits the functionality of camera-type feedback systems. In contrast to single-modality sensors, the human hand consists an unparalleled multimodal sensory system i. e. , mechanoreceptors sensing both pressure and vibration, and thermoreceptors sensing temperature, which largely contributes to its unprecedented dexterous manipulation. Specifically, the human multimodal sensing architecture provides fine-grained cues about contact forces, textures, local shape around contact points, and deformability, all of which are critical for evaluating an ongoing grasp, and to trigger force correction measures in case of instability. What is needed is a sensing architecture for robotic systems that overcomes the deficiencies of conventional single-modality sensors. In particular, what is needed is sensing architecture that mimics human-like tactile exploration to facilitate object-manipulation and recognition tasks that present problems to robotic systems using conventional single-modality sensors.",
                    "SUMMARY": [
                        "The present invention is generally directed to a multimodal sensing architecture that utilizes spatially arrayed multi-sensor groups superpixels to facilitate advanced object-manipulation and recognition tasks performed by mechanical end effectors , a robot gripper/hand attached to end of a robot arm mechanism in robotic systems. In a manner similar to sensory receptors found in human fingers, the superpixels are spatially arrayed over contact surfaces of the end effector , on the inward-facing surfaces of robot gripper fingers such that each superpixel generates localized multimodal sensor data e. g. , data respectively generated by two or more different sensor types, or two or more types of sensor measurement in response to stimuli applied or received at an associated contact surface portion i. e. , the region of the end effector's contact surface over which the superpixel is fixedly disposed. According to an aspect of the invention, each superpixel includes at least one pressure sensor, at least one vibration sensor, an optional proximity sensor and an optional temperature sensor that collect corresponding sensor data in response to corresponding stimuli, thereby providing data that may be used to determine events-of-interest occurring at each superpixel's associated contact surface portion. The pressure sensor of each superpixel , a strain gauge, a capacitive pressure sensor or a piezoelectric element is configured to generate pressure static event data in response to an amount of static force applied to the corresponding surface portion, and the vibration sensor of each superpixel , a piezoelectric sensor, a piezoresistive sensor or a MEMS accelerometer is configured to generate vibration dynamic event data in response to mechanical vibrations received at the corresponding surface portion. According to another aspect of the invention, a readout circuit receives the pressure data and vibration data generated by the spatially arrayed superpixels and operably transmits the received data to the robotic system's control circuit either directly , using a shared sensor data bus connected between the readout circuit and the controller circuit or indirectly , by way of an optional multimodal control generator that is configured to pre-process the raw sensor data before being passed to the controller circuit. By providing the control circuit with both static force and vibration data collected from the end effector in this manner, the multimodal sensing architecture enhanced robotic system control based on both static events that occur on the end effector's contact surface , the force by which an object is being gripped by the end effector, and also dynamic events that periodically occur on the end effector's contact surface , mechanical vibrations generated when the object is slipping from the grasp of a gripper, or mechanical vibrations generated by contact between a grasped primary object and a secondary object. That is, by providing each superpixel with both static and dynamic event data, the multimodal sensing architecture of the present invention greatly enhances a host robotic system's ability to quickly identify non-standard operating conditions , object slip or misaligned/misplaced objects and automatically implement a corrective operation , to adjust the gripping force applied by the end effector, or adjust the position of one object relative to an obstructive object. Even further enhancement of the multimodal sensing architecture's sensing capability may be achieved by way of utilizing proximity sensors in each superpixel to generate proximity data indicating distances between a target object and the multiple corresponding surface portions of the end effector, and/or by using temperature sensors to generate temperature data indicating the amount of thermal energy transferred to multiple corresponding surface portions of the end effector. By forming superpixels that include all four of these sensor types, the multimodal sensing architecture of the present invention enables robotic systems to utilize human-like tactile exploration i. e. , recognize vibrations, textures, and moments of contact with an object to facilitate object-manipulation and recognition tasks that greatly enhance the adaptability of robot mechanisms to a wide range of functional operations , automatically adjusting to random variations arising in repetitive tasks that present problems to robotic systems using conventional single-modality sensors. According to a practical embodiment of the present invention, a robotic system implements the multimodal sensing architecture by way of disposing two or more multimodal sensor arrays on associated contact surfaces provided on opposing end effector fingers. In this case, each multimodal sensor array includes an associated feedback circuit that is operably coupled to the robotic system's control circuit by way of associated sensor data buses that extend along the robot arm mechanism, where each feedback circuit is configured to receive sensor data from a large number of superpixels and to transmit the sensor data in a time multiplexed manner, whereby the large amount of sensor data is efficiently transmitted to the control circuit using a small number of signal lines. In one embodiment, the control circuit is customized to process the raw sensor data from the two or more multimodal sensor arrays that is transmitted on the sensor data buses. In another embodiment, the multimodal sensing architecture further includes a multimodal control generator that receives and processes the raw sensor data from one or more multimodal sensor arrays, and generates multimodal control signals that are then transmitted to the control circuit for use in controlling operations performed by the robot mechanism and the end effector. According to another embodiment of the present invention, a method for controlling a robotic system involves utilizing one or more sensors to generate both static event , pressure data and dynamic event , vibration data. As described above, the static event data is generated in response to static forces applied by a target object to corresponding contact surface portions on the end effector while grasping the target object, and the dynamic event , vibration data is generated in response to vibrational forces applied to the corresponding contact surface portions on the end effector while the robot mechanism is being actuated to move the target object from one location to another location. The method also includes utilizing both the static event data and the dynamic event data to identify non-standard operating conditions while the target object is being moved, and adjusting the operation of the robot mechanism and/or the end effector in response to the identified non-standard operating condition. For example, a combination of constant static event data and increasing dynamic event data is used to identify undesirable slipping of the target object due to insufficient friction between the end effector and the target object. In this case, operation of the end effector is adjusted in response to the identified slipping condition, for example, by way of causing the end effector to increase the gripping force applied to the target object, thereby preventing undesirable dropping of the target object. In another example, a combination of constant static event data and sharply increasing dynamic event data is used to identify undesirable impact-type or scraping-type contact between a transported primary object and a stationary secondary object resulting from an unscheduled misalignment of one or both objects. In this case, operation of the robot mechanism is adjusted in response to the identified contact condition, for example, by way of translating moving the primary object in a way that removes the misalignment with the stationary secondary object, whereby subsequent movement of the primary object relative to the secondary object produces acceptable sensor data. In a presently preferred embodiment, the static/dynamic event data is generated using the superpixel configuration described above. In an alternative embodiment, the static/dynamic event data is generated using an array of single multimodal sensors , piezoelectric sensors or piezoresistive sensors that are capable of detecting both static events and dynamic events. In this case, the associated readout circuit is modified to include signal processing circuitry , filters, configured to separate static event characteristics from dynamic event characteristics in each sensor's output signal, thereby enabling the above-mentioned identification of non-standard operating conditions and associated corrective adjustments using a smaller number of sensor nodes. According to another embodiment of the present invention, a topology of the multimodal sensing architecture includes embedded multi-node readout circuitry that is configured to extract sensor data generated by the sensor nodes , the pressure sensors and vibration sensors of each superpixel, and to coordinate periodic transmissions of the sensor data to the robotic system's control circuit over one or more shared signal lines, thereby greatly simplifying the process of integrating multimodal sensing capabilities into existing robot systems by minimizing the number of signal lines. The topology consists of two main integrated parts: a multimodal sensing platform sensor layer and a custom backplane integrated silicon readout circuit readout layer. The sensor layer includes a silicon or other substrate upon which the superpixel sensor structures are fabricated using CMOS fabrication technologies or PCB fabrication processes. The readout layer includes an array of addressable readout circuit portions pixels. In one embodiment each readout circuit portion includes a custom circuit that is capable of both reading voltage changes in analog sensor data signals and generating bias voltages or currents. In a specific embodiment, each readout circuit portion includes an analog front end with an analog-to-digital converter ADC and a digital-analog-converter DAC. Additionally, an optional protective matrix is formed over the sensor layer, and an optional support substrate is disposed under the readout layer. According to a presently preferred embodiment of the present invention, the multimodal sensing architecture is fabricated using a flexible substrate material such that the sensor arrays can be flexed without suffering any loss in performance. In a specific embodiment, the readout circuit of each array is fabricated using amorphous silicon a-Si thin-film transistor TFT elements.",
                        "These and other features, aspects and advantages of the present invention will become better understood with regard to the following description, appended claims, and accompanying drawings, where: 1 is a diagram depicting a multimodal sensing architecture implemented on a robotic system according to an embodiment of the present invention; 2 is simplified block diagram depicting the multimodal sensing architecture of 1; 3A, 3B, 3C and 3D are simplified side views depicting an operation performed by the robotic system of 1 using static and dynamic event data generated by the multimodal sensing architecture of 1 in accordance with an exemplary embodiment; 4A, 4B, 4C and 4D are simplified side views depicting an operation performed by the robotic system of 1 using static and dynamic event data generated by the multimodal sensing architecture of 1 in accordance with another exemplary embodiment; 5 is a simplified cross-sectional side view showing a sensor array of a multimodal sensing architecture according to a specific embodiment; 6 is an exploded perspective view showing an exemplary sensor array of a multimodal sensing architecture according to another specific embodiment; 7 is a simplified cross-sectional side view showing a sensor array of a multimodal sensing architecture according to another specific embodiment; 8 is a block diagram depicting a readout circuit of a sensor array of a multimodal sensing architecture according to another specific embodiment; 9 is a simplified circuit diagram depicting a partial readout circuit of a sensor array of a multimodal sensing architecture according to another specific embodiment; and 10 is a simplified cross-sectional side view showing a sensor array of a multimodal sensing architecture according to another specific embodiment."
                    ],
                    "DESCRIPTION": "The present invention relates to an improvement in sensing architectures utilized in robotic systems. The following description is presented to enable one of ordinary skill in the art to make and use the invention as provided in the context of a particular application and its requirements. As used herein, directional terms such as upper, lower, lowered, front and back, are intended to provide relative positions for purposes of description and are not intended to designate an absolute frame of reference. With reference to electrical connections between circuit elements, the terms coupled and connected, which are utilized herein, are defined as follows. The term connected is used to describe a direct connection between two circuit elements, for example, by way of a metal line formed in accordance with normal integrated circuit fabrication techniques. In contrast, the term coupled is used to describe either a direct connection or an indirect connection between two circuit elements. For example, two coupled elements may be directly connected by way of a metal line, or indirectly connected by way of an intervening circuit element , a capacitor, resistor, inductor, or by way of the source/drain terminals of a transistor. Various modifications to the preferred embodiment will be apparent to those with skill in the art, and the general principles defined herein may be applied to other embodiments. Therefore, the present invention is not intended to be limited to the particular embodiments shown and described, but is to be accorded the widest scope consistent with the principles and novel features herein disclosed. 1 shows an exemplary robotic system 200 that is modified to include a multimodal sensing architecture according to an exemplary embodiment of the present invention. Robot system 200 generally includes a robot mechanism 201 and a control circuit 203, and multimodal sensing architecture 100 includes sensor arrays 101-1 and 101-2 disposed on an end effector 250 of robot mechanism 201. As described in detail below, sensor arrays 101-1 and 101-2 are operably coupled to control circuit 203, and control circuit 203 is configured to control operations performed by robot mechanism 201 including end effector 250 in response to sensor data generated by sensor arrays 101-1 and 101-2. Referring to the upper portion of 1, robot mechanism 201 includes various mechanisms and structures that are operably configured in accordance with known techniques to manipulate a target object 90 by way of selectively actuating electrical motors. In the exemplary embodiment robot mechanism 201 includes a shoulder/base mechanism 210 that is fixedly attached to a work surface not shown by way of a fixed base 211, an upper arm structure 215 extending from the shoulder/base mechanism 210 to an elbow mechanism 220, a forearm structure 225 extending from the elbow mechanism 220 to a wrist mechanism 230, a wrist structure 235 extending from the wrist mechanism 230 to hand/axial rotation mechanism 240, and an end effector 250 operably connected to a terminal portion of the hand/axial rotation mechanism 240. End effector 250 is a hand/gripper-type mechanism having two gripper fingers 255-1 and 255-2 that open move away from each other or close move toward each other in accordance with the corresponding actuation of motors mounted inside the gripper structure. Robot mechanism 201 also includes an optional camera 270 that is mounted near end effector 250 and provides image data to controller 203. As mentioned above, robot mechanism 201 is merely introduced to provide a context for explaining the features and benefits of multimodal sensing architecture 100, and the specific configuration of robot mechanism 201 is not intended to limit the appended claims. Referring to the simplified block diagram located at the center right portion of 1, control circuit 203 includes a control signal generator 205 that is configured to control operations performed by robot mechanism 201 and end effector hand 250 in response to data received from several sources via wires not shown or other transmission medium. As described in the background section above control signal generator 205 receives a programmed series of movement operation/control instructions 207, and generates corresponding robot control signals RMC that are transmitted via wires not shown to specific electric motors disposed in robot mechanism 201, whereby target object 90 is manipulated in a programmed manner using end effector 250 , robot mechanism is actuated to move end effector toward object 90, and then end effector 250 is actuated such that gripper fingers 255 press against opposite sides of target object 90. In addition to programmed control instructions 207, control circuit 203 also receives image-type feedback data 271 that may be used to adjust the programmed operations in the manner described in the background section. However, as also explained in the background section and depicted in the upper left portion of 1, a region 277 of the camera's vision field 275 is typically occluded by portions of end effector 277, which limits the functionality of image data 271. Moreover, as explained in the background section, single-modality sensors fail to provide the information needed to avoid many common industrial accidents and/or to perform complex assembly processes. Accordingly, control circuit 203 is distinguished over conventional control circuits in that it also utilizes sensor-type feedback data generated by multimodal sensing architecture 100 in the manner described below. Referring to 1 and 2, multimodal sensing architecture 100 includes sensor arrays 101-1 and 101-2 that are respectively fixedly attached to opposing contact surfaces 257-1 and 257-2 of gripper fingers 255-1 and 255-2, where each sensor array 101-1 and 101-2 includes multiple superpixels 102 that are fixedly disposed over corresponding surface portions of contact surfaces 257-1 and 257-2 and operably connected to an associated readout circuit. For example, as indicated in the enlarged bubble region showing a tip portion 256 of gripper finger 255-1 in 1 and also in 2, sensor array 101-1 includes multiple super pixels 102, each disposed over a different portion areal region of contact surface 257. For example, superpixel 102-1 is disposed over contact surface portion 257-11 of contact surface 257-1, and superpixel 102-2 is disposed over contact surface portion 257-12 of contact surface 257-1. As further depicted in 2, superpixel 102-21 of array 101-2 is disposed over contact surface portion 257-21 of contact surface 257-2, and superpixel 102-2 is disposed over contact surface portion 257-22 of contact surface 257-2. Each superpixel 102 includes multiple sensor nodes S that measure an associated different stimuli applied to its corresponding contact surface portion. For example, as indicated in the block diagram provided in the lower left portion of 1, superpixel 102-1 includes a pressure sensor 103 configured to generate pressure static event data PSD in response to an amount of static force SF received at corresponding surface portion 257-1, a vibration sensor 104 configured to generate vibration dynamic event data VD in response to mechanical vibrations MV applied onto corresponding surface portion 257-1, an optional proximity sensor 105 configured to generate proximity data PXD in response to a detected air-gap proximity distance PXD between corresponding surface portion 257-1 and an adjacent object , target object 90, and an optional temperature sensor 106 configured to generate temperature data TD in response to a local temperature LT applied to corresponding surface portion 257-1. In exemplary embodiments, pressure sensor 103 of each superpixel 102 is implemented by a strain gauge, a capacitive pressure sensor, a piezoelectric sensor or a piezoresistive sensor, vibration sensor 104 of each superpixel 102 is implemented by a piezoelectric sensor, a piezoresistive sensor, or a micromechanical system MEMS accelerometer, proximity sensor 105 of each superpixel 102 is implemented using a capacitive-coupling-type sensing element, and temperature sensor 106 of each superpixel 102 is implemented using a resistive temperature detectors RTD or a thermoelectric element. Referring to 2, each array 101-1 and 101-2 also includes an associated readout circuit 107-1 and 107-2 configured to receive sensor data from all of the array's superpixels and to transmit the received sensor data onto an associated shared sensor data bus for transmission to control circuit 203. For example, as indicated in the bubble portion in 1, array 101-1 includes a first readout circuit portion 107-11 that collects sensor data from the various sensors of superpixel 102-1, and a second readout circuit portion 107-12 that collects sensor data from the various sensors of superpixel 102-2, where both readout portions 107-11 and 107-12 form part of readout circuit 107-1 1, which passes the collected sensor data onto shared sensor data bus 108-1. As indicated in 2, the sensor data collected by readout circuit 107-1 includes pressure data PSD-1 and PSD-2 generated by pressure sensors 103-1 and 103-2 of superpixels 102-1 and 102-2, and vibration data VD-1 and VD-2 from vibration sensors 104-1 and 104-2. Similarly, as shown in 2, readout circuit 107-2 of array 101-2 collects pressure data PSD-21 and PSD-22 generated by pressure sensors 103-21 and 103-22 and vibration data VD-21 and VD-22 from vibration sensors 104-21 and 104-22, and passes this collected data onto shared bus line 108-2 for transmission to control circuit 203. As indicated near the bottom of 1, raw sensor data PVPT transmitted on shared sensor bus 108-1 may either be transmitted directly to control circuit 203 i. e. , along the single-dot-dash line/arrow, or transmitted indirectly to control circuit 203 by way of an optional multimodal control generator circuit 109 i. e. , as indicated by the double-dot-dash arrows. That is, in the case of the single-dot-dash-arrow, embodiment raw sensor data PVPT i. e. , serially transmitted pressure, vibration, proximity and temperature data is transmitted directly to control signal generator 205. In this case, control signal generator 205 is modified to interpret sensor data PVPT and to generate appropriately responsive robot mechanism control signals RMC. In the alternative double-dot-dash embodiment, the required modification of control circuit 203 may be reduced by way of providing and configurating multimodal control generator circuit 109 to pre-process sensor data PVPT, and to generate multimodal control signals MCS that allow control signal generator 205 to generate the appropriately responsive robot mechanism control signals RMC with minimal processing time. For example, as indicated in 2, control circuit 203 is configured to control operations performed by end effector 250 in response to said multimodal control signals MC by way of transmitting robot mechanism control signals RMC225-1 and RMC225-2, which cause the end effector, , to increase or decrease forces FP1 and FP2 applied by gripper fingers 255-1 and 255-2 on an object not shown. In a practical embodiment, multimodal control generator circuit 109 is implemented using an application-specific integrated circuit ASIC or field programmable gate array FPGA that is configured to generate appropriate output signals in response to predetermined patterns occurring in sensor data PVPT. 3A to 3D depict gripper fingers 255-1 and 255-2 of robotic system 200 shown in 1 during a series of operations involving grasping and moving target object 90, and illustrate an exemplary method for controlling a robotic system to move an object that is grasped in its end effector. In particular, 3A to 3D illustrate an example of how the multimodal sensing architecture of the present invention is beneficially utilized to enhance the ability of robotic systems to quickly identify a slipping-type non-standard operating condition and to automatically implement an appropriate corrective operation by way of increasing the gripping force applied by end effector 250 on target object 90. 3A depicts target object 90 at an initial time t0 when target object is positioned at a pre-designated start position, and the robotic system positions gripper fingers 255-1 and 255-2 i. e. , by way of causing fingers 255-1 and 255-2 on opposite sides of target object 90. Note that in 3A all sensor output is assumed to be zero , PSD1=0 because there is no contact between superpixels 102-1, 102-2, 102-21 and 102-22 and the sides of target object 90. 3B depicts target object 90 at a subsequent time t1 when target object is grasped between fingers 255-1 and 255-2, which is achieved by way of actuating appropriate mechanisms of the robotic system to move gripper fingers 255-1 and 255-2 toward each other such that grip forces FP1t1 and FP2t1 are respectively applied by fingers 255-1 and 255-2 on the sides of target object 90. At this point, pressure sensors 103-1, 103-2, 103-21 and 103-22 are utilized to respectively generate static event data PSD-1, PSD-2, PSD-21 and PSD-22 in response to static forces applied by target object 90 to corresponding contact surface portions 257-1 and 257-2 on end effector fingers 255-1 and 255-2, respectively. Note that the recorded force value 1 is arbitrarily selected, and that vibration data values VD-1, VD-2, VD-21 and VD-22 are assumed to be zero to simplify the description. 3C depicts target object 90 at a subsequent time t2 while target object is grasped between fingers 255-1 and 255-2 and being transported by the robotic system from the pre-designated start position to a pre-designated destination position. For descriptive purposes it is assumed that target object 90 undergoes a slipping dynamic event in which object 90 slips an amount Z relative to fingers 255-1 and 255-2 at some point during transport. Note that the grip forces FP1t1 and FP2t1 respectively applied by fingers 255-1 and 255-2 on the sides of target object 90 have not changed from time t1, so all static event data PSD-1, PSD-2, PSD-21 and PSD-22 remains unchanged i. e. , equal to 1; that is, pressure sensors 103-1, 103-2, 103-21 and 103-22 are not able to detect the slipping event. However, slipping events of this type generate characteristic mechanical vibrations forces MV1 in fingers 255-1 and 255-2 that are detectable by vibration sensors 104-1, 104-2, 104-21 and 104-22. According to an aspect of the present invention, vibration sensors 104-1, 104-2, 104-21 and 104-22 are utilized to quickly identify the slipping event by way of respectively generating non-zero dynamic event data VD-1, VD-2, VD-21 and VD-22 in response to mechanical vibrations MV1. By configuring control circuit 203 to properly interpret the static and dynamic sensor data , by identifying that a slipping event is occurring when all static event data remains unchanged and all dynamic event data increases uniformly during transport of an object, the control circuit 203 is able to quickly implement a corrective action i. e. , adjust either the robot mechanism or the end effector in response to the identified slipping non-standard operating condition, thereby preventing further slipping and possible loss of target object 90. For example, as indicated in 3D, a suitable corrective action may involve actuating appropriate mechanisms of the robotic system to move gripper fingers 255-1 and 255-2 toward each other such that grip forces FP1t3 and FP2t3 applied by fingers 255-1 and 255-2 on the sides of target object 90 are higher than those applied at time t2. Note that successful application of the corrective action is also immediately detected by way of an expected increase in static event data , pressure data values PSD-1, PSD-2, PSD-21 and PSD-22 increase from 1 at time t3 to 2 at time t4 and a concomitant decrease in dynamic event data , vibration data values VD-1, VD-2, VD-21 and VD-22 decreased from 1 at time t3 to 0 at time t4. 4A to 4D depict gripper fingers 255-1 and 255-2 of robotic system 200 shown in 1 during a series of operations involving mounting a primary object 91 , a hollow cylinder with an open bottom end over a secondary object 92 , a solid cylinder, and illustrate an exemplary method for controlling a robotic system during a relatively complex assembly process. In particular, 4A to 4D illustrate examples of how multimodal sensing architecture of the present invention may be beneficially utilized to enhance the ability of robotic systems to adjust to various misalignments that may prevent completion of the assembly process if performed using conventional methods. 4A depicts a time t0 when primary object 91 is grasped between fingers 255-1 and 255-2 and moved into a pre-designated position for mounting over secondary object 92. Similar to the situation described above with reference to 3B, pressure sensors 103-1, 103-2, 103-21 and 103-22 are utilized to respectively generate static event data PSD-1, PSD-2, PSD-21 and PSD-22 in response to static forces applied by primary object 91, that the indicated pressure force value 1 is arbitrarily selected, and that vibration data values VD-1, VD-2, VD-21 and VD-22 are assumed to be zero for brevity. 4B indicates a first misalignment event occurring a time t1. In this case primary object 91 is displaced by a small distance X1 relative to secondary object 92, whereby a lower right edge portion of primary object 91 contacts an upper surface of secondary object 92 as the robotic mechanism lowers primary object 91 in the Z direction. Because the grip forces applied by fingers 255-1 and 255-2 on the sides of primary object 91 have not changed from time t0 to time t1, all static event data PSD-1, PSD-2, PSD-21 and PSD-22 remains unchanged i. e. , equal to 1. However, the impact between objects 91 and 92 generates mechanical vibrations forces MV2 that radiate through fingers 255-1 and 255-2 in a characteristic manner such that the point of impact may be determined by combining static event data with the dynamic event data collected by vibration sensors 104-1, 104-2, 104-21 and 104-22. For example, when the static event data remains constant and vibration data VD-22 from vibration sensor 104-22 is higher , 4 than vibration data VD-21 from vibration sensor 104-21 , 3, and both are higher than vibration data from vibration sensors 104-1 and 104-2, then an impact-type dynamic event may be identified and an impact location may be estimated. With this information, the robotic system's control circuit is able to automatically perform a corrective adjustment , by moving primary object 91 a small amount in the X direction, thereby achieving a suitable alignment between primary object 91 and secondary object 92 for the mounting process to continue. 4C indicates a second misalignment event occurring a time t2. In this case primary object 91 is positioned adequately to facilitate mounting over secondary object 92, but a minor displacement relative to secondary object 92 results in scraping rubbing contact between a portion of primary object 91 and secondary object 92. As in the previous example, static event data PSD-1 and PSD-2 remains unchanged i. e. , equal to 1, but the scraping-type contact between objects 91 and 92 generates characteristic mechanical vibrations forces MV3 that allow the vibration sensors to identify the location of the scraping-type contact. Additionally, the imbalance in forces from the lack of contact of object 92 and 255-1, but contact with object 92 and object 255-2, may create a detectible pressure change by PSD-21 and PSD-22. For example, when the static event data remains constant and vibration data VD-21 and VD-22 is higher , 2 than vibration data VD-1 and VD-2 , 1 during the assembly operation, then a scraping-type dynamic event is occurring and contact point is near finger 255-2. With this information, the robotic system's control circuit is able to automatically perform a corrective adjustment , by moving primary object 91 a small amount in the X direction, thereby achieving an optimal alignment between primary object 91 and secondary object 92 that facilitates a scraping-free mounting process. 4D depicts primary object 91 and secondary object 92 at the completion of the mounting process, which occurs at a time t3 when primary object 91 has been fully lowered over secondary object 92. In addition to utilizing the multimodal sensing architecture to detect impact-type or scraping-type contact for purposes of taking corrective action, the combination of dynamic event data and static event data may also be utilized to confirm the successful completion of an assembly operation by way of recording an expected final contact or non-contact event. For example, the successful mounting of primary object 91 on secondary object 92 may produce characteristic mechanical vibrations MV4 only when precise alignment between the objects has been achieved. Conversely, mechanical vibrations MV4 may only be generated when the assembly process was completed incorrectly. In either case, the ability to detect both static and dynamic event data allows the multimodal sensing architecture to provide information that cannot be obtained using cameras or single-modality sensors. Although the examples of 3A to 4D are described with reference to superpixels that include two separate sensor i. e. , a pressure sensor 103-x and a vibration sensor 104-x, the methodology utilized in these examples may be implemented using a single multimodal sensor in place of each superpixel, provided each single multimodal sensor is capable of detecting both static and dynamic events, and provided the readout circuit is configured to separately generate both static event data and dynamic event data from an output signal generated by each multimodal sensor. In alternative single-multimodal-sensor embodiments each superpixel of the array described above is replaced with either a piezoelectric sensor or a piezoresistive sensor; both of these sensor types qualify as multimodal sensors in that they generate sensor output signals including both static event characteristics , direct current magnitude and dynamic event characteristics , alternating current magnitude. By configuring the readout circuits to separate and measure the static/dynamic characteristics in the output signal generated by each multimodal sensor , by way of filters and other known signal processing techniques, the static/dynamic event data values described above with reference to 3A to 4B are made available for use by a robotic system's control circuit, thereby facilitating implementation of the associated automatically performed corrective adjustments. 5 is a simplified cross-sectional side view showing a sensor array 101A of a multimodal sensing architecture according to a specific embodiment of the present invention. Data readout from sensor networks or arrays of sensor nodes are often realized by using wires or electrical interconnects directly routed to the sensor nodes. However, though simplistic and convenient, these wires become cumbersome when dealing with a large number sensor network with thousands of nodes. Specifically, for application such as tactile sensing/exploration in robotic end effectors, this type of wiring, with interconnects directly in the sensor plane, becomes a real issue. With a large array of multimodal sensor nodes, this configuration is not only impractical both in terms of footprint and data acquisition, but can also introduces noise and cross-talk in the robotic tactile sensors. The topology implemented in sensor array 101A addresses this problem by way of providing a frontplane sensor layer 110A and a separate backplane readout layer 120A that are integrally connected in a way that minimizes wiring between the large number of sensors in sensor array 101A and a host robotic system control circuit. In one embodiment, the various sensors of each superpixel , pressure sensor 103A and vibration sensor 104A are fabricated or otherwise disposed on sensor layer 110A, and layout circuit 107A is disposed on readout layer 120A, where pressure sensor 103A and vibration sensor 104A are operably coupled to readout circuit 107A by way of via-type signal lines 115A-1 and 115A-2, respectively, that extend between sensor layer 110A and readout layer 120A. In one embodiment, the topology of sensor 101A further includes one or both of a protective layer/matrix 130A disposed over sensor layer 110A, and a base substrate 140A disposed under readout layer 120A. In practical embodiments, protective layer/matrix 130A comprises one of a flexible material , silicone, silicon or a hard shell material , aluminum, where appropriate such as around the perimeter, and has a thickness in the range of one micron to one millimeter. In other embodiments, optional base substrate 140A comprises one of silicon, glass, steel, plastic and aluminum, and has a thickness in the range of ten microns and one millimeter. 6 is an exploded perspective view showing an exemplary sensor array 101B according to another specific embodiment. Sensor array 101B has a topology similar to that of array 101A, including a sensor layer 110B, a readout layer 120B, an optional protective layer 130B and a base substrate 140B. In this case, sensor layer 110B includes a silicon substrate 111B on which superpixels 102 are fabricated using CMOS or microfabrication techniques such that each superpixel includes the various sensor types described above , superpixel 102B-1 includes a pressure sensor 103B, a vibration sensor 104B, a proximity sensor 105B and a temperature sensor 106B. In addition, readout layer 120B also includes a silicon substrate 121B with readout circuit 107B implemented thereon by way of CMOS fabrication techniques such that each superpixel 102B on substrate 101B is aligned with a corresponding readout circuit portion 124B , such that sensor nodes 103B to 106B of superpixel 102B-1 are aligned with input nodes provided in corresponding readout circuit portion 124B-1. During the manufacturing process, substrate 111B is fixedly attached to substrate 121B by a die-attach method such that pressure sensor 103B and vibration sensor 104B are operably coupled to corresponding input nodes 125B-1 and 125B-2 of readout circuit 107B by way of bumps 116B-1 and 116B-2 , indium bumps, solder bumps or polymer bumps, which respectively form at least a portion of signal paths 115B-1 and 115B-2 extending between substrates 111B and 121B. 7 is a simplified cross-sectional side view showing a sensor array 101C having a topology according to another specific embodiment. In this case, each superpixel 102C-1 and 102C-2 of sensor array 101C is respectively fabricated on a separate associated silicon island 114C-1 and 114C-2, and silicon islands 114C-1 and 114C-2 are mounted over a readout layer 120C , by way of an intervening silicon substrate 111C. The various sensors of superpixels 102C-1 and 102C-2 are operably coupled to associated readout circuit portions 107C-1 and 107C-2 by way of via-type signal lines 115C, which are produced using any of the relevant techniques mentioned herein. 8 is a block diagram depicting a readout circuit 107D of a sensor array according to another specific embodiment. Readout circuit 107D includes an array of readout portions pixels RCP-00 TO RCP-24 that are respectively configured and operably coupled to receive associated sensor data , pressure data values PSD-00 to PSD-24, respectively from corresponding pressure sensors not shown. Readout circuit 107D also includes readout control circuits , a row select circuit 810, a column select circuit 820 that are configured to sequentially transfer the sensor data , pressure data values PSD-00 to PSD-24 from each readout portion RCP-00 TO RCP-24 to a digital readout circuit 830, which in turn is configured to convert the sensor data into digital values for transfer to a control circuit or other circuit. 9 is a block diagram depicting a readout circuit portion 107E according to another specific embodiment. Readout circuit portion 107E comprises analog front-end 910 including an analog-to-digital converter ADC 913 and a digital-analog-converter DAC 915, digital circuitry 920 including digital logic and signal processing, and a digital communications interface 930. Analog circuitry 910 is configured to receive analog sensor data , pressure data value PSD-00A from corresponding sensor nodes of associate superpixels , sensor 103E, and configured to generate digital data values , digital pressure data value PSD-00D that is transmitted to digital circuitry 920. Digital circuitry 920 is configured to transfer the digital sensor data to a host robotic system's control circuit via the communications interface 930. 10 is a simplified cross-sectional side view showing a sensor array 101F according to another specific embodiment. Data acquisition from sensor network or arrays of sensor nodes, especially on flexible substrates, are often realized by the use of wires or electrical interconnects directly routed to the sensor nodes. The interconnects on polymer substrates typically lie on the same plane. While this approach simplifies the manufacturing process, these wires become cumbersome when dealing with a large number sensor network with thousands of nodes. Specifically, for application such as tactile sensing/exploration in robotic end effectors, this type of wiring, with interconnects directly in the sensor plane, becomes a real issue. With a large array of multimodal sensor nodes, this configuration is not only impractical both in terms of footprint and data acquisition but can also introduces noise and cross-talk in the robotic tactile sensors. Rigid silicon backplanes can also be impractical in some instances because they can break with any kind of bending, flexing or twisting. Accordingly, sensor array 101F includes a flexible electronics inspired topology with an amorphous silicon backplane integrated to a frontplane sensor architecture to address this problem. Referring to 10, array 101F includes a sensor layer 110F having one or more sensors , pressure sensor 103F formed on a first substrate 111F, and a readout layer 120F including second substrate 121F on which a readout circuit 107F is fabricated as an amorphous-silicon a-Si integrated circuit including a plurality of a-Si thin-film transistor TFT elements. In one embodiment, one or more sensor nodes , vibration sensor 104F is/are also fabricated on readout layer 120F using TFT elements. In one embodiment custom a-Si TFT readout circuit 107F is a pixel matrix with addressable pixels. Although the present invention has been described with respect to certain specific embodiments, it will be clear to those skilled in the art that the inventive features of the present invention are applicable to other embodiments as well, all of which are intended to fall within the scope of the present invention. For example, although the present invention is described with specific reference to articulated-type robotic systems that use two-finger end effectors, the multimodal sensing architecture disclosed herein may also be beneficially utilized in advanced robotic systems that utilize three, four or five finger end effectors , human-like robotic hands, and may also be utilized in other mechanical systems as well, such as on prosthetic limbs. In some embodiments a multimodal sensing architecture may comprise a single sensor array operably mounted on a single finger of a multi-finger end effector or on a probe-like end effector. Further, those skilled in the art will understand that the sensors and superpixels of the present invention can have different interrelated configurations, orientations, placement of nodes, sensors, array size, periodicity/aperiodicity, different circuit configurations inside the pixel array, etc. , while still embodying the spirit and scope of the inventive concept. For example, although the invention is described primarily with reference to CMOS or TFT-type sensors fabricated on associated silicon substrates, suitable sensor and superpixel arrays may also be produced by forming or placing sensors/superpixels on printed circuit boards PCBs using well-known PCB fabrication techniques.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/55/735/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9235923886299133,
                    "F_SPEC_PARAMS": [
                        "erroneous and possibly dangerous situations",
                        "damage"
                    ],
                    "S_SPEC_PARAMS": [
                        "avoid many common industrial accidents and/or to perform complex assembly processes"
                    ],
                    "A_PARAMS": [
                        "conventional single-modality sensors"
                    ],
                    "F_SENTS": [
                        "While this programmed movement control approach is acceptable for use in highly ordered environments, erroneous and possibly dangerous situations can occur when minor variations arise, such as displacement of a target object from the expected starting location or a receptacle is displaced at the terminal location, whereby performance of the programmed movement operations can result in damage to one or both of the target objects and the end effector/gripper."
                    ],
                    "S_SENTS": [
                        "That is, conventional single-modality sensors , pressure sensing only are unable to provide sufficient information to avoid many common industrial accidents and/or to perform complex assembly processes."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object"
                    ],
                    "F_SIM_SCORE": 0.600207507610321,
                    "S_TRIZ_PARAMS": [
                        "Manufacturability"
                    ],
                    "S_SIM_SCORE": 0.5093177556991577,
                    "GLOBAL_SCORE": 1.6783550202846527
                },
                "sort": [
                    1.678355
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11235359-20220201",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11235359-20220201",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-02-11",
                    "PUBLICATION_DATE": "2022-02-01",
                    "INVENTORS": [
                        "Joseph Andrew Bolton",
                        "Keith Daniel Humfeld"
                    ],
                    "APPLICANTS": [
                        "The Boeing Company    ( Chicago , US )"
                    ],
                    "INVENTION_TITLE": "Robotic laser and vacuum cleaning for environmental gains",
                    "DOMAIN": "B08B 70042",
                    "ABSTRACT": "Methods, systems, and apparatuses are disclosed for the selective and controlled removal of debris from specific areas of a substrate outer surface without adversely impacting the substrate outer surface, including substrate outer surface coatings, and returning an actual substrate outer surface profile containing affixed debris to a predetermined substrate outer surface profile by comparing a library of predetermined profiles to an actual substrate outer surface profile in real time.",
                    "CLAIMS": "1. A system comprising: a detector configured to determine the presence of detected debris on an actual substrate outer surface at a specified location on the actual substrate outer surface, said actual substrate outer surface comprising an ideal substrate outer surface profile, said ideal substrate outer surface profile comprising an ideal substrate outer surface profile thickness; a memory comprising characteristics of the ideal substrate outer surface profile, said memory in communication with the detector; a processor configured to access the ideal substrate outer surface profile from the memory, said processor in communication with the detector, said processor in further communication with the memory; at least one controller, said at least one controller in communication with the processor, and said at least one controller in communication with the memory; a positioning mechanism in communication with the controller; an energy source in communication with the at least one controller; a vacuum in communication with the at least one controller; and wherein the processor in combination with the detector and the memory is configured to determine a difference between the actual substrate outer surface profile and the ideal substrate outer surface profile, said ideal substrate outer surface profile accessed from the memory, said actual substrate outer surface profile detected by the detector, said processor configured to compare the ideal substrate surface outer profile to the actual substrate outer surface profile; and wherein the energy source is configured to release a predetermined amount of energy from the energy source, said predetermined amount of energy sufficient to only remove the detected debris from the actual substrate outer surface. 2. The system of claim 1, wherein the actual substrate outer surface profile further comprises an actual substrate outer surface coating; wherein the system is configured to effect a predetermined energy release from the energy source to remove debris from the actual substrate outer surface coating to return the actual substrate outer surface profile to closely approximate the ideal substrate outer surface profile. 3. The system of claim 1, wherein the energy source comprises a laser. 4. The system of claim 1, wherein the controller is configured to control the predetermined energy released from the energy source for a predetermined duration; and wherein the at least one controller is further configured to control movement of the positioning mechanism. 5. The system of claim 1, wherein the detector comprises at least one camera. 6. The system of claim 2, wherein the system is configured to direct the predetermined energy release to a predetermined location on the actual substrate outer surface for a predetermined amount of time. 7. The system of claim 1, wherein the actual substrate outer surface comprises a hybrid laminar flow control surface, said hybrid laminar flow control surface comprising a plurality of micro holes. 8. The system of claim 4, wherein the predetermined amount of energy released from the energy source for a predetermined duration ranges from 9 W to 2 kW. 9. A method comprising: accessing, by a processor, an ideal substrate outer surface profile from a memory; reading, by a detector, an actual substrate outer surface profile of an actual substrate outer surface at a specific actual substrate outer surface location; and comparing, by the processor, the ideal substrate outer surface profile of the specific substrate outer surface location to the actual substrate outer surface profile of the specific substrate outer surface location; determining, by the processor, a presence of debris on the actual substrate outer surface at a specific actual substrate outer surface location; determining, by a controller, an amount of energy required to dislodge the debris from the actual substrate outer surface at the specific actual substrate surface location; wherein the controller in communication with the processor, and the controller in communication with the memory activate, by the controller, an energy source; directing, by a positioning mechanism, the amount of energy from the energy source required to dislodge the debris at the specific actual substrate outer surface location; dislodging, by the energy source, the debris from the actual substrate outer surface at the specific actual substrate outer surface location to form an amount of dislodged debris; and after step of dislodging the amount of dislodged debris from the actual substrate outer surface, the method further comprising: vacuuming the dislodged debris from the actual substrate outer surface. 10. The method of claim 9, the actual substrate outer surface comprising an actual substrate outer surface coating, said method further comprising: dislodging the debris from the actual substrate outer surface coating, without dislodging the actual substrate outer surface coating. 11. The method of claim 9, further comprising: emitting an amount of energy from the energy source, said amount of energy ranging from about 9 W to about 2 kW, said amount of energy required to dislodge the debris at the specific actual substrate outer surface location. 12. A method comprising: accessing, by a processor, an ideal substrate outer surface profile of a specific ideal substrate outer surface location from a stored ideal substrate outer surface profile; reading, by a detector, an actual substrate outer surface profile of a specific actual substrate outer surface location; comparing, by the processor, the ideal substrate outer surface profile of the specific ideal substrate outer surface location to the actual substrate outer surface profile of the specific actual substrate outer surface location; determining, by the processor, a presence of debris on an actual substrate outer surface coating at the specific actual substrate outer surface location; determining, by a controller, an amount of energy required to dislodge the debris from the actual substrate outer surface coating at the specific actual substrate outer surface location; activating, by the controller, an energy source to emit the amount of energy required to dislodge the debris at the specific actual substrate outer surface location; wherein the controller in communication with the processor, and the controller in communication with the memory; directing, by a positioning mechanism, the amount of energy required to dislodge the debris at the specific actual substrate outer surface location from the energy source to the debris at the specific actual substrate outer surface location; and dislodging, by the energy source, the debris from the actual substrate outer surface coating at the specific actual substrate outer surface location without dislodging the actual substrate outer surface coating to form an amount of particulate debris; and removing, by a vacuum, the amount of particulate debris from the actual substrate outer surface coating. 13. The method of claim 12, the step of comparing the ideal substrate outer surface profile to the actual substrate outer surface profile, further comprising: determining a difference between the actual substrate outer surface profile and the ideal substrate outer surface profile. 14. The method of claim 12, wherein after the step of removing the amount of particulate debris from the actual substrate outer surface coating, further comprising: returning the actual substrate outer surface profile to an actual substrate outer surface profile that is substantially equivalent to the ideal substrate outer surface profile. 15. The method of claim 12, the actual substrate outer surface coating comprising: an actual substrate outer surface coating thickness; and wherein the actual substrate outer surface coating thickness is treated by the removal of accumulated debris to return the actual substrate outer surface thickness to the ideal substrate outer surface coating thickness after the step of removing the amount of particulate debris from the actual substrate outer surface coating. 16. The method of claim 12, wherein the actual substrate outer surface profile is configured to facilitate a laminar flow over the actual substrate outer surface. 17. The method of claim 12, the ideal substrate outer surface profile comprising; at least one recess in the ideal substrate outer surface profile. 18. The method of claim 17, the ideal substrate outer surface comprising: a hybrid laminar flow control surface, said hybrid laminar flow control surface comprising a plurality of micro holes.",
                    "FIELD_OF_INVENTION": "The present disclosure relates generally to the field of restoring surface profiles to an original profile. More specifically, the present disclosure relates to restoring performance of a substrate surface by restoring performance of a substrate surface profile including the removal of debris from substrate surface.",
                    "STATE_OF_THE_ART": "Certain substrate surfaces are established to provide a substrate surface having a designed profile capable of delivering a predetermined characteristic to the substrate surface; such as laminar flow of another material over the substrate surface. Laminar flow is important to substrate surfaces in terms of reducing amounts of drag on a substrate surface, and otherwise establishing the efficiency with which substances can contact and pass over or around substrate surfaces. Substances such as, for example, air found in airflow can adversely impact the efficiency of large objects designed to move through air. For example, when a large object such as, for example, an aircraft comprises an outer surface having a significantly large outer surface area, any variation from an original , new or as manufactured or clean outer substrate surface profile to a dirty outer substrate surface profile can result in a decrease in aircraft performance, and/or maneuverability, and can also impact factors such as, for example, aircraft fuel efficiency. Deviation in an original profile of an aircraft substrate surface can result from factors during use including, for example, damage from object impact during flight as well as the build-up of even small amounts of atmospheric debris , dust, dirt, etc. , that can build up on an outer substrate surface. In addition, Hybrid Laminar Flow Control HLFC technology has been employed to improve fuel efficiency by advantageously altering or improving laminar flow surfaces of aircraft. HLFC surfaces such as, for example, areas at the front , leading edge areas, of aircraft wings, can include surfaces perforated by a large number , up to and including millions, of micro perforations or holes that have micron level diameters. A predetermined portion of airflow over such surfaces passes through the perforations such that the airflow over the surfaces , wing surfaces, can be controlled to be laminar instead of turbulent. Such micro holes can fill up with debris or otherwise clog over time while in service. Such perforations can also beneficially take advantage of hot air , from engine operation, that is blown through such perforations before take-off, during take-off, and during landing, potentially reducing the need in cold-weather for manual anti-icing operations. Aircraft outer substrate surfaces, including HLFC surfaces that can include perforations are often cleaned or otherwise serviced at random or regular intervals in the hopes of improving aircraft efficiency. Such cleaning methods include applying vast amounts of liquids, including water, solvents, and water mixed with amounts of solvents. Such methods are time-consuming and inefficient; creating, for example, vast amounts of wastewater and other waste product. In addition, traditional cleaning methods further cannot reliably remove all debris from substrate surfaces, especially surfaces that contain relatively small recesses, including HLFC surfaces that can include perforations. That is, while an aircraft outer substrate surface may appear clean after being subjected to a cleaning protocol that is designed to achieve a cleaner laminar surface, such aircraft outer surfaces may retain amounts of adhered debris that may or may not be visible to the eye, and that can still adversely impact an ideal laminar airflow at an outer substrate surface of, for example, aircraft.",
                    "SUMMARY": [
                        "According to a present aspect, a system for removing debris from a substrate outer surface including without adversely affecting the substrate outer surface is disclosed, with the system including a detector for evaluating characteristics of an actual substrate outer surface of a an substrate at a specified location on the actual substrate outer surface, a memory comprising characteristics of a predetermined substrate outer surface, a processor for accessing characteristics of a predetermined substrate outer surface of a predetermined substrate from said memory, with the detector in communication with the processor, and with the processor configured to compare characteristics of the actual substrate outer surface with the characteristics of the predetermined substrate outer surface. The system further includes at least one controller, with the controller in communication with the processor and the memory. A positioning mechanism is in communication with the controller, and an energy source is in communication with the controller. The system further includes a vacuum in communication with the controller, wherein the system non-destructively removes debris from the actual substrate outer surface. According to another aspect, a method is disclosed, with the method including determining the presence of an amount of debris on a substrate outer surface at a specific substrate outer surface location, determining an amount of energy required to dislodge the debris from the substrate outer surface at the specific substrate surface location, activating an energy source, directing the amount of energy from the energy source to the debris at the specific substrate outer surface location, and dislodging the debris from the substrate outer surface at the specific substrate outer surface location to form an amount of dislodged debrisIn another aspect, a method is disclosed further including accessing a predetermined substrate outer surface profile, reading an actual substrate outer surface profile of the specific substrate outer surface location, and comparing the predetermined substrate outer surface profile of the specific substrate outer surface location to the actual substrate outer surface profile of the specific substrate outer surface location. In another aspect, a method further includes removing dislodged debris from the substrate outer surface. In a further aspect, a method further includes vacuuming the dislodged debris from the substrate outer surface. In another aspect, a method further includes treating the actual substrate outer surface profile to return the actual substrate outer surface profile to closely approximate the predetermined substrate outer surface profile. In another aspect, the present application discloses a method of removing debris from a substrate outer surface coating, including effecting such removal without adversely affecting the substrate outer surface coating, with the method including accessing a predetermined substrate outer surface profile from a substrate outer surface profile storage; reading an actual substrate outer surface profile of a specific substrate outer surface location; comparing the predetermined substrate outer surface profile of the specific substrate outer surface location to the actual substrate outer surface profile of the specific substrate outer surface location; determining the presence of an amount of debris on a substrate outer surface coating at a specific substrate outer surface location, with the substrate outer surface coating comprising an outer surface coating thickness; determining an amount of energy required to remove the debris from the substrate outer coating surface at the specific substrate outer surface location; activating an energy source; directing the amount of energy from the energy source to the debris at the specific substrate outer surface location; dislodging the debris from the substrate outer surface coating at the specific substrate outer surface location to form an amount of dislodged debris; and removing the particulate debris from the substrate outer surface coating. The features, functions and advantages that have been discussed can be achieved independently in various aspects or may be combined in yet other aspects, further details of which can be seen with reference to the following description and the drawings.",
                        "Having thus described variations of the disclosure in general terms, reference will now be made to the accompanying drawings, which are not necessarily drawn to scale, and wherein: 1 is a box diagram illustrating an aspect of the present disclosure; 2 is an illustration of an aspect of the present disclosure; 3 is an illustration of an aspect of the present disclosure; 4 is an illustration of an aspect of the present disclosure; 5A, 5B, and 5C are cross-sectional illustrations of a substrate outer surface being progressively treated according to aspects of the present disclosure; 5D is an illustration of an enlarged cross-sectional view of the substrate outer surface shown in 5A, 5B, and 5C; 6A, 6B, and 6C are illustrations of a substrate outer surface being progressively treated according to aspects of the present disclosure; 6D, 6E, and 6F are enlarged illustrations of the substrates shown in 6A, 6B, and 6C; 6G is an illustration of an enlarged cross-sectional view of the substrates outer surface shown at least in 6F; 7 is a block diagram outlining a method according to aspects of the present disclosure; 8 is a block diagram outlining a method according to aspects of the present disclosure; and 9 is a block diagram outlining a method according to aspects of the present disclosure."
                    ],
                    "DESCRIPTION": "According to present aspects, systems, methods, and apparatuses are disclosed for accurately detecting the presence of an amount of debris at specific and predetermined locations on a substrate outer surface, determining the amount of energy required to remove the debris, and removing such detected debris from the substrate outer surface at the precise and specific locations, when the presence of debris is detected at such locations. According to the present disclosure, the substrate outer surface can be a surface that is important to laminar flow. Contemplated outer substrate surfaces have an original, manufactured substrate outer surface profile referred to equivalently herein as a predetermined substrate outer surface profile or predetermined profile that is specific to a particular location on a particular part or particular component. Disclosed methods, systems, and apparatuses assess the condition of an actual substrate outer surface profile at specific locations along a part or component outer surface. A predetermined substrate outer surface profile for the corresponding area of the part or component outer surface is accessed from a library equivalently referred to herein as memory or storage, and the accessed predetermined substrate outer surface profile is then compared to the actual substrate outer surface profile. When the present systems and apparatuses detect a difference between actual and ideal substrate outer surface profiles, such a detected difference indicates that the actual substrate outer surface contains an amount of debris at a precise location on the substrate outer surface. According to present aspects, present methods, systems, and apparatuses detect the presence of debris, the amount of detected debris, and the specific location of detected debris to be removed from a specific area of a substrate outer surface. According to further aspects, present methods, systems, and apparatuses further determine the amount of energy required to be applied to a substrate outer surface to dislodge debris from the actual substrate outer surface. The present systems and apparatuses include an energy source that is activated. Integrated controllers control and/or direct a predetermined amount of energy , an amount of energy that present systems and apparatuses determine as being required for removing or dislodging the detected debris from the energy source to the detected debris-containing area of the substrate outer surface. As the predetermined energy from the energy source is absorbed by the debris, the debris becomes dislodged from the substrate outer surface. According to present aspects, the substrate outer surface can include one or more outer surface coating layers that, for example, can include a topcoat. In such instances, according to present aspects, a coating layer or layers present on the substrate, along with the substrate, together form the basis of the substrate outer surface profile. The present methods, systems, and apparatuses selectively dislodge debris that is present on the outermost substrate surface to return the actual substrate outer surface profile to a predetermined substrate outer surface profile. That is, when a topcoat or other coating is present on the outer substrate surface, the methods, systems and apparatuses described herein, in the process of accessing a known and predetermined substrate outer surface profile from a memory, the present systems, methods, and apparatuses recognize and account for the presence of the coating layers, such that only the debris is dislodged and removed from the substrate outer surface , no amount of coating layer is disturbed or removed. In situations where HLFC surfaces and HLFC features are present on the substrate outer surface, , HLFC features including depressions, recesses, holes, , according to present aspects, the terms substrate outer surface and substrate outer surface profile include such HLFC features, including scenarios where the HLFC features extend a distance or depth into the substrate outer surface. In such situations where debris removal from HLFC features present on substrate outer surfaces is desired, the predetermined substrate outer surface profile for the corresponding area of the part or component outer surface includes the HLFC features in the stored profile in the memory that is accessed according to present systems, methods, and apparatuses. The accessed predetermined substrate outer surface profile is then compared to the actual substrate outer surface profile. It is presently recognized that debris removal from HLFC features , HLFC holes can require a different amount and intensity of energy applied to the HLFC features to dislodge debris from such HLFC features. According to present aspects, the present systems detect debris at the location of the HLFC features, and an amount of energy required to dislodge debris from the HLFC features is directed to the HLFC feature to dislodge the detected debris to return the actual substrate outer surface profile to closely match the predetermined substrate outer surface profile, and without adversely impacting the substrate outer surface than can include a topcoat or other coatings. According to present systems, apparatuses, and methods the term return the actual substrate outer surface profile to a predetermined substrate outer surface profile means that the predetermined substrate outer surface profile and the actual substrate outer surface profile are identical, substantially identical, or close enough within measured tolerances such that laminar flow over the actual substrate outer surface in the actual state actual laminar flow very closely approximates the laminar flow over the predetermined substrate outer surface predetermined laminar flow. More specifically, while being bound to no particular theory, according to present aspects, complete or substantially complete debris removal from a substrate occurs when recognized ideal substrate surface tolerances are re-established. Such surface tolerances can include an actual substrate surface profile being returned to a predetermined substrate surface profile within about +/0. 030 in. In the case of re-establishing an ideal laminar flow to a treated substrate surface, according to present aspects, the present debris removal systems, methods, and apparatuses are said to completely or substantially completely remove debris from HLFC features when detected debris has been removed from at least about 85% of the HLFC features , HLFC openings, recesses, depressions, holes, . The present apparatuses, systems, and methods detect anomalies on a part surface by making use of known and stored parameters and schematics of an object's outer surface characteristics, including an object's substrate outer surface profile, by retrieving stored information about a specific surface location on a specific part or component and relaying such information to a sensor, controller and/or other system components for the purpose of comparing such stored surface characteristic information to an actual, assessed substrate outer surface profile of the same location on the same part type or same component type, preferably in real time. Detected disparities , deviations, between predetermined and actual outer surface characteristics equate to the presence of debris attached, affixed to, contained within HLFC features , HLFC openings, recesses, depressions, holes, or otherwise present and occurring on an actual part outer surface. Such a part outer surface that includes detected debris is then identified in real time by the present apparatuses, systems, and methods as requiring treatment and debris removal to restore a part surface's optimal outer surface profile , such that the treated area of the partthe substrate outer surface of the partwill again closely match the predetermined substrate outer surface profile of that particular part being evaluated at each specific location being evaluated on the substrate outer surface, . Present aspects contemplate precise positioning of disclosed detection systems and apparatuses at specific locations relative to an actual specific location on a specific substrate type, , specific location on a specific part or component and the ability to obtain real-time image processing on demand and feedback of characteristics of an actual specific substrate outer surface at a precise location on the actual substrate outer surfaces. Processors and memory in communication with one another, according to the present systems and apparatuses, substantially concurrently , in real time access and relay information values on the surface characteristics of a predetermined, or stock, specific substrate type corresponding to the same location on the predetermined substrate. Such information on the characteristics and values correspond to a part and part surface that is unused, new, and therefore in a clean state that will be debris-free. The present systems and apparatuses compare the surface characteristic values for the same location on both the actual substrate outer surface and the predetermined substrate outer surface. If the values differ, then debris is determined to be present on the actual substrate outer surface. The precise positioning, comparison of ideal or predetermined values with actual values in real time, and debris detection is followed by the automated selection of intensity, wavelength and dwell time of energy that is then emitted from an integrated energy source and directed to the debris residing on the substrate outer surface. That is, according to present aspects, the present systems, methods, and apparatuses further determine, in real time, the amount of energy required to be directed from an energy source and directed to a substrate surface , including HLFC features located on an evaluated substrate outer surface for the purpose of removing detected debris from a substrate outer surface. 1 shows a non-limiting representation of a system according to an aspect of the present disclosure. As shown in 1, a system 10 for removing debris from a substrate outer surface includes a debris removal device 12 in communication with a positioning mechanism 11. The positioning mechanism 11 can be an automated and/or robotic mechanism that is responsive to signals from a positioning program that can be programmed to move the debris removal device 12, as well as the system 10 relative to a substrate having a substrate outer surface. The system 10 can include a housing or other structure not shown for containing and positioning the debris removal device 12. The debris removal device 12 includes an energy source 14 capable of emitting a predetermined amount of energy. The energy source 14 can be a laser or other energy source than emit beams of energy that can be directed from the energy source to a target such as, for example, a substrate outer surface. Present aspects contemplate various lasers and laser assemblies able to generate, emit and direct energy beams to a substrate outer surface at an intensity ranging from about 9 W to about 2 kW 2000 W. Energy beams in this range are selected and directed to amounts of debris on a substrate outer surface for the purpose of sublimating, ablating, and/or otherwise removing a predetermined amount of debris present from such substrate outer surface. According to further aspects, if a selected energy does not completely dislodge an amount of detected debris, the energy can be increased, or the debris removal device can be directed over the debris repeatedly as needed to accomplish the debris removal. According to further aspects, the amount of energy provided , the intensity of the emitted energy beams in concert with the duration of the such emitted beams and emitted energy, or dwell time is selected to dislodge and/or remove a discovered amount of debris occurring on a substrate outer surface, without adversely impacting the substrate outer surface, including any substrate outer surface coatings that may exist on the substrate outer surface. The debris removal device 12 further includes at least one controller 16 that is in communication with the energy source 14 and the positioning mechanism 11. The controller 16 can control the activation of the energy source 14, and controller 16 can also control the movement and intensity of the of the beams emitted from the energy source 14. The controller 16 can be manually operated or can be remotely signaled to operate via operation of a unit not shown with the unit able to send signals from the unit to the controller 16 via, for example, wireless communication links, etc. Debris removal device 12 further includes at least one detector 18 capable of detecting an amount of and location of debris occurring on a substrate outer surface. Detector 18 can be in communication with a computer not shown, or can itself contain a microprocessor , a microprocessor with pertinent predetermined substrate outer surface characteristics in a, or in communication with a memory. and assess the presence of debris, and can assess a substrate outer surface actual profile via camera or other photographic means, x-ray means, etc. , and can create a digital or digitized image that can be a 3D image of the substrate outer surface actual profile. Detector 18 is further able to record, construct, or send an assessed substrate surface outer surface actual profile having substrate surface outer surface actual profile values to a memory 20 or other information storage device for the purpose of comparing the assessed substrate outer surface actual profile and actual substrate outer surface profile values with a substrate outer surface predetermined or stock profile that is accessible from the memory 20 , a library, electronic catalog, electronic storage, ; with the memory 20 providing known and specified predetermined substrate outer surface profiles of known and predetermined locations on such substrate outer surfaces, and with such predetermined substrate outer surface profiles having predetermined substrate outer surface profile values. In further aspects, system 10 further includes a processor 21 that, alone or with memory 20 also can be located remotely from system 10. Memory 20 may itself include a processor or, as shown in 1, processor 21 and memory 20 can be in communication and integrated into system 10. Together, detector 18, processor 21, memory 20, and controller 16 can be in communication for the purpose of sending and/or receiving signals and information regarding actual substrate surface profiles, retrieved or accessed predetermined substrate surface profile, disparities between the actual and predetermined substrate surface profiles, etc. , as well as determining a total amount of energy required to dislodge debris including amount of energy , total energy including the amount and intensity of energy to be released from energy source 14 and directed to a substrate surface in combination with dwell time of the debris removal device 12 and beams of energy from energy source 14 directed to a particular substrate outer surface required to dislodge debris from a location on a substrate outer surface. According to further aspects, when the detector 18 perceives and otherwise assesses a substrate outer surface actual profile and processor 21 and memory 20 determine , detect, sense, a disparity between the actual substrate surface outer profile and the substrate surface outer predetermined profile, the controller 16 is signaled for the purpose of activating and controlling energy source 14, substantially in real time, and on demand. System 10, as shown in 1 further optionally includes an integrated vacuum 22 although, according to alternative aspects, vacuum 22 can be located remotely from and operated independently of system 10. The term vacuum and the act of vacuuming include any device and the positioning of such device capable of establishing a pressure gradient at the substrate outer surface for the purpose of removing debris and debris particulate from the substrate outer surface. 2, 3, and 4 show the present apparatuses, methods and systems employed to dislodge and remove debris from an object for the purpose of detecting debris on an substrate outer surface actual profile, and then dislodging and removing such debris to return the substrate outer surface to more closely approach or indeed match a substrate outer surface profile predetermined profile. As shown, in non-limiting fashion, in 2 an object is an aircraft 30 having a fuselage section 31. A plurality of debris removal devices 32 are shown, with each debris removal device 32 attached or otherwise in communication with a mechanical arm 34 with both the mechanical arm 34 and the debris removal device in communication with a power cord 35. As shown in 2, mechanical arm 34 includes sections than can extend or retract, etc. The mechanical arm 34 can be any mechanical device that can be moveable is response to actuating the mechanical device. According to present aspects, the methods, systems, and apparatuses can be automated such that the control and movement of the mechanical arm 34 can be automated such that, for example, the mechanical arm 34 includes, without limitation, robotic arms, robotic hands, etc. Further, the debris removal device itself and the energy course itself can be automatically controlled via incorporated robotics and robotic devices. As further shown in 2, an operator cab 33 is dimensioned to house an operator, and is further in communication with and is operable to direct movement and location of the debris removal device 32 relative to a substrate outer surface 36 present on aircraft 30. 2 also shows a vacuum hose 37 located proximate to the debris removal device 32 and proximate to the substrate outer surface 36 when the debris removal device 32 is directed to a position proximate to the substrate outer surface 36. As shown in 2, the debris removal devices 32 are shown attached to fixed position mechanical devices that can be manually operated or automatically operated and even remotely operated for the purpose of positioning the debris removal device 32 to positions proximate to various areas along the fuselage 31. The systems and apparatuses shown in 2 can implement the systems and apparatuses shown in 1. According to present aspects, though not shown in 2, the debris removal device may be positioned relative to or proximate to a substrate surface outer surface by any mechanical positioning device able to precisely position and precisely locate the debris removal device at a specific location for the purpose of scanning the substrate outer surface to detect the presence or absence of debris on the substrate outer surface. According to further aspects, positioning mechanisms used to move and precisely position the overall apparatus as well as the devices used to maneuver and position the debris removal device with precision relative to the substrate outer surface can include, without limitation, robotics and other automated devices that can be controlled on-site or that can be controlled remotely with, for example, wireless technology and other technologies, including the use of hardware and software required to operate the positioning mechanisms. Such mechanical positioning devices can further include, for example, drones or other objects, etc. that can be directed to specified locations with a required amount of precision and that can be operated remotely and/or automatically, and that that can be operated and directed using robotics; , an entirely automated system. Such objects can incorporate, for example, one or more global positioning systems GPS working in concert with locators that can be, for example, incorporated into regions of the objects that are targeted for debris removal. As stated herein, to return an actual substrate outer surface profile to a desired predetermined substrate outer surface profile, a substrate outer surface is first scanned to assess and confirm whether or not the scanned substrate outer surface profile matches or adequately approximates a known stored substrate outer surface predetermine profile. The debris removal systems of the present disclosure incorporate the scanning, sensing, etc. functions that compare actual and predetermined outer substrate surface profiles. If a disparity between actual and predetermined profiles is detected, such disparity confirms the presence of unwanted debris on the substrate outer surface. If debris is detected on the substrate outer surface, an energy source integrated into or located proximate to and in communication with the debris removal device is engaged or activated, and a predetermined amount of energy is directed from the energy source to the substrate outer surface in a predetermined amount that is adequate to dislodge detected debris from the substrate outer surface. According to a present aspect, an integrated vacuum assembly can include the components required to generate a negative pressure including, for example, motors, hoses, pumps housings, assemblies, etc. that normally attend a vacuum assembly. The vacuum assembly not shown is activated during the debris dislodging/removal process to provide an area of negative pressure adjacent to the substrate outer surface where the debris is detected. As debris is dislodged/removed from the substrate outer surface, debris will leave the substrate outer surface in particulate debris form, ablated debris form, etc. collectively equivalently referred to herein as particulate debris or particulate. The particulate debris is then drawn into an airstream moving in a direction into an open end of the vacuum hose, with the vacuum created by the negative pressure of the vacuum. The particulate debris is directed into the vacuum hose 37 and away from both the substrate outer surface and the surrounding environment , including the region and environment immediately adjacent to the substrate outer surface, . According to present aspects, the incorporation of the vacuum into the present systems, apparatuses, and methods insures that unwanted debris that is dislodged from the substrate outer surface , by the application of a predetermined amount of energy from an energy source, will not re-settle or otherwise become re-deposited on the substrate outer surface. The present systems, apparatuses, and methods efficiently and selectively remove predetermined amounts of debris from specific regions of substrate outer surfaces of outer surfaces of large objects and vehicles aircraft, spacecraft, rotorcraft, . According to present aspects, the ability to avoid the re-deposition of debris onto substrate outer surfaces including, for example, laminar surfaces of aircraft is especially desirable. In addition, by applying a vacuum in the region of the debris removal, present aspects avoid the re-deposition of dislodged debris, as well as particulate debris into substrate outer surface recesses , holes for pitot tubes, seams, depressions, HLFC features, . Presently disclosed methods and systems therefore help to avoid issues that can otherwise occur with dislodged debris from cleaned surfaces that re-deposits into or onto important instrumentation at could affect, for example, correct airspeed readings of aircraft in flight from blocked or clogged pitot tubes, blocked or clogged static vents, . Additionally, according to present aspects, in the case of HLFC features impacting the establishment or improvement of laminar flow over a surface, eliminating the presence of dislodged particulate debris from the regions proximate to the area of the substrate outer surface from which debris has been dislodged and removed, eliminates or ameliorates the risk of particulate debris re-settling or otherwise becoming deposited onto and into surface structures that could foul or re-clog HLFC features , HLFC holes and interfere with performance of the components and parts that, for example, include the HLFC features in an substrate outer surface. 3 and 4 show another portion of aircraft 30 showing a wing 38 attached to a fuselage section 31. As shown in 3, in non-limiting fashion, debris removal device 32 is now attached to a unit 39 in communication with mechanical arm 34. Power cord 35 is shown in communication with debris removal device 32 and unit 39 and a vacuum assembly, including the vacuum hose 37, is integrated into unit 39. According to further aspect, a vacuum assembly including vacuum hose 37 can be provided to, and be considered a part of, a debris removal system even if a vacuum system is provided to the overall debris removal system separately from unit 39. In 3, debris removal device 32 is shown positioned proximate to a section or area of the lower wing outer surface 38a. 4 shows mechanical arm 34 now extended from unit 39 to a position to facilitate locating and positioning the debris removal device 32 proximate to a section or area of the upper wing outer surface 38b. The systems and apparatuses shown in 1 and 2 can implement the systems and apparatuses shown in 3 and 4. 5A, 5B, and 5C show non-limiting cross sectional views of upper wing outer surface 38b of wing 38 shown in 3 and 4. 5A, 5B, and 5C show upper wing outer surface comprising a wing substrate 40a onto which is deposited a wing coating layer 40b. 5D shows an enlarged cross-sectional view of upper wing outer surface region 40 shown in 5A of upper wing outer surface 38b. According to present aspects, as shown in detail in 5D, the wing coating layer 40b together with the wing substrate 40a form the upper wing outer surface 38b. 5A shows an amount of debris 44 present near a leading edge of upper wing outer surface 38b. According to present aspects, present systems, methods, and apparatuses are employed to scan not shown the upper wing outer surface and determine the presence of debris 44, with debris 44 potentially not being visible to the human eye. However, the non-visible amount of debris present and detected by present systems and methods on the upper wing outer surface can impact laminar flow disadvantageously , in terms of increasing drag and increasing fuel consumption of the aircraft in flight, . An energy source not shown within or in communication with debris removal device 32 has been activated, and a predetermined amount of energy at a predetermined intensity is emitted from debris removal device 32 and directed to upper wing outer surface 38b onto which an amount of debris 44 is attached. The emitted energy is shown as energy beam 42 in a series of broken lines extending from the debris removal device 32 and the upper wing outer surface 38b. Vacuum hose 37 is shown located proximate to the debris removal device 32 and the upper wing outer surface 38b and the amount of debris 44 attached to the upper wing outer surface 38b. As shown in 5B, enough energy has been directed to the debris 44 attached to the upper wing outer surface 38b to form debris particulate 44a. As debris particulate becomes dislodged from upper wing outer surface 38b, the formed debris particulate 44a leaves the upper wing outer surface 38b or is loosened to a degree considered to have dislodged or detached from the upper wing outer surface 38b, without disturbing the integrity or ideal profile of the upper wing outer surface. As shown in 5B, as the debris particulate becomes dislodged or, if desired, before debris particulate becomes dislodged, present systems activate a vacuum to create a pressure gradient in the vicinity of the debris particulate 44a that is dislodged from the upper wing outer surface such that the debris particulate is directed toward and into vacuum hose 37. The process continues until, as shown in 5C, none of the debris 44 or debris particulate 44a remains in the vicinity of the upper wing outer surface 38b. At the completion of the debris removal process, with the completed removal of debris, the upper wing outer surface actual profile has been returned to very closely or completely match the upper wing outer surface predetermined profile. According to present aspects, a post-processing scan of the treated upper wing outer surface can be conducted to ensure the removal of the debris without any unwanted impact on, or degradation of the upper wing outer surface , the substrate outer surface. Although not visible in 5A, 5B, and 5C, the upper wing outer surface 38b includes a plurality of HLFC holes shown as HLFC holes 50 in enlarged views 6D, 6E, 6F, 6G extending from the upper wing outer surface 38b, and extending through the substrate , the upper wing outer surface 38b, with the holes bounded by the substrate such that the substrate forms the wall of the HLFC holes. The systems and apparatuses shown in 5A, 5B, 5C, and 5D can implement the systems and apparatuses shown in 1, 2, 3, and 4. 6A, 6B, and 6C show non-limiting overhead plan views of upper wing outer surface 38b, and further illustrate the progressive processes conducted and shown in 5A, 5B, and 5C, respectively. As shown in 6A, 6B, and 6C, according to present aspects, the debris removal device 32 can be moved to multiple positions and locations , in the direction as indicated by arrow A in 6A during and throughout the process of emitting energy in response to signals sent to the debris removal device from, for example, a controller. That is, as debris is detected at a specific location on a substrate outer surface, the energy source is activated to release and direct a predetermined amount of energy required to dislodge the detected debris. During this energy release, the debris removal device itself can be moved or can remain stationary during energy release and beams of emitted energy can be directed to move to desired specific locations, so long as an adequate amount of energy calculated by the system to dislodge the debris is administered to the debris to be dislodged and removed. 6D, 6E, and 6F are enlarged views of the upper wing outer surface 38b shown in 6A, 6B, and 6C. As shown in 6D, the upper wing outer surface 38b includes a plurality of HLFC holes 50, extending a desired distance from the upper wing outer surface 38b, into the wing 38 and, if desired, extending through the substrate material of the wing 38. The HLFC holes 50 also referred to equivalently herein as micro holes or HLFC micro holes can extend through a substrate material , a wing substrate material, with the holes 50 bounded by the wing substrate material. 6G is an enlarged cross-sectional view of upper wing outer surface 38b 6G shows an enlarged cross-sectional view of upper wing outer surface 38b as shown in 6F and taken across line 6G-6G. According to present aspects, as shown in detail in 6G, the wing coating layer 40b together with the wing substrate 40a form the upper wing outer surface 38b. As shown in 6G, the HLFC holes 50 include a first opening 50a at the upper wing outer surface 38b and a second opening 50b at the inner surface of sing substrate 40a, with the hole tunneling through the substrate from first opening 50a to second opening 50b, , extending from the first opening to the second opening, and with the substrate serving as the walls bounding the tunneling holes, . The HLFC holes can have a diameter ranging from about 50 m to about 100 m. As shown in 6D, HLFC holes 50 can accumulate or otherwise contain and/or collect an amount of debris 44, with the debris 44 shown as clogging the openings of holes 50 at the upper wing outer surface 38b. As shown in 6D, 6E, and 6F, the substrate outer surface is illustrated as a representative upper wing outer surface 38b. The detection of debris is accomplished by a detector comparing feedback from the specific location on the actual upper wing surface profile to the predetermined substrate outer surface profile values of the same wing component type that are accessed from a memory housing such catalogued predetermined substrate outer surface profiles. Once the presence of debris is confirmed as existing at or in the HLFC holes. An energy source not shown within or in communication with debris removal device 32 , as shown in 6A and 6B is activated, and a predetermined amount of energy at a predetermined intensity is emitted from debris removal device 32 and directed to upper wing outer surface 38b onto which an amount of debris 44 is attached at the location of the upper wing outer surface 38b where the HLFC holes 50 are located. As described above and shown in in 6A and 6B, the emitted energy is shown as energy beam 42 in a series of broken lines extending from the debris removal device 32 to the upper wing outer surface 38b. Vacuum hose 37 is shown located proximate to the debris removal device 32 and the upper wing outer surface 38b and the amount of attached debris 44. As shown in 6E, enough energy has been directed to the debris 44 located at HLFC holes 50 on upper wing outer surface 38b to form debris particulate 44a that has been dislodged from the HLFC holes 50. As debris particulate 44a becomes dislodged from HLFC holes 50 in upper wing outer surface 38b, the debris particulate 44a leaves the HLFC holes 50 in upper wing outer surface 38b, or is loosened to a degree considered to be dislodged or detached from the upper wing outer surface 38b, without disturbing the integrity or ideal profile of the upper wing outer surface 38b. As shown in 6E, as the debris particulate becomes dislodged or, if desired, before debris particulate becomes dislodged, present systems activate a vacuum to create a pressure gradient in the vicinity of the dislodged debris particulate 44a. As shown in 6F, the debris particulate 44a leaves the surface of upper wing outer surface 38b and the debris particulate 44a is no longer present, having been removed from the HLFC holes 50 of upper wing outer surface 38b and directed toward and into vacuum hose 37. 6A-6F can implement the systems, methods, and apparatuses shown in 1, 2, 3, 4, 5A, 5B, 5C, and 5D. According to further aspects, the substrate outer surface can be monitored in real-time during energy release, and the emission of energy from the debris removal device can be terminated once the debris is dislodged. The debris removal device 32 can be maintained at a substantially fixed position, or can be moved to a desired distance from the substrate outer surface while it is moved to a new location relative to the substrate outer surface , in a scan-like fashion in response to signals from a processor to a controller in communication with the debris removal device, . The control of the movement of the debris removal device and the control of the dwell time over a specific substrate outer surface location in concert with the amount of energy released from the debris removal device can be calculated to deliver the amount of energy required to dislodge unwanted debris from a specific substrate outer surface location without any unwanted impact on, or degradation of the a substrate outer surface. That is, according to present aspects, the system can be activated and an energy release initiated only when debris is detected. In other words, according to present aspects, the present systems will not activate in the absence of detected debris. Further, the release of energy can be terminated once debris removal occurs. When no further debris is detected on an inspected area of a substrate outer surface, no further energy is released from the system or applied to the outer substrate surfaces. 7 is a flow diagram illustrating a non-limiting method according to present aspects. As shown in 7, an illustrative method 100 is outlined including determining 102 the presence of debris on a substrate outer surface followed by determining 103 the amount of energy required to dislodge the detected debris and activating 104 an energy source. The method 100 further includes directing 105 an amount of energy from an energy source to a substrate outer surface followed by dislodging 106 the detected debris from the substrate outer surface. The outlined method 100 further includes the optional vacuuming 107 dislodged debris from the substrate outer surface. 8 is a flow diagram illustrating a non-limiting method according to present aspects. As shown in 8, according to present aspects, a representative method 110 further describes the process of determining the presence of debris on a substrate outer surface. Method 110 includes accessing 111 a predetermined substrate outer surface profile , from a memory, storage, library etc. containing or facilitating access to such predetermined substrate outer surface profile corresponding to a particular part outer surface profile or particular component outer surface profile, reading 112 an actual substrate outer surface profile of the particular part or component outer surface. The method 111 further includes comparing 113 the predetermined substrate outer surface profile , predetermined profile values, to the actual substrate outer surface profile , actual profile values, . 9 is a flow diagram illustrating a non-limiting method according to present aspects. As shown in 9, illustrative method 120 includes accessing 111 a predetermined substrate outer surface profile , from a memory, storage, library etc. containing or facilitating access to such predetermined substrate outer surface profiles corresponding to a particular part outer surface or component outer surface, reading 112 an actual substrate outer surface profile of the particular part or component outer surface. The method 111 further includes comparing 113 the predetermined substrate outer surface profile , ideal profile values, to the actual substrate outer surface profile , actual profile values, . Method 120 as outlined further includes determining 102 the presence of debris on a substrate outer surface coating 114 followed by determining 103 the amount of energy required to dislodge the detected debris and activating 104 an energy source. The method 100 further includes directing 115 an amount of energy from an energy source to a substrate outer surface coating, followed by dislodging 118 detected debris from the substrate outer surface coating, removing 122 debris from the substrate outer surface coating, and optionally returning the actual substrate outer surface profile to the predetermined substrate outer surface profile. The methods outlined in 7, 8, and 9 can implement the systems and apparatuses shown in 1, 2, 3, 4, 5A, 5B, 5C, 5D, 6A, 6B, and 6C. Contemplated energy sources according to present systems, apparatuses, and methods include, for example, laser systems that can be adapted to generate a laser beam or beams with a power ranging from about 9 W to about 2 kW, and in particular with a power ranging from about 60 W to about 2 kW, and further in particular with a power ranging from about 9 W to about 95 W. Lasers that emit energy within the states ranges include, for example, gas lasers, solid-state lasers, semiconductor lasers, etc. The energy source can further comprise a wavelength adjuster that allows a wavelength to be adjusted or tuned to a desired wavelength in real time. By adjusting the wavelength in combination with real time monitoring of the substrate outer surface being treated, the debris can be dislodged from substrate outer surfaces, according to present systems and methods, without adversely impacting the substrate outer surface and any substrate outer surface coatings paint layers, primer layers, topcoat layers, . Present systems, methods, and apparatuses can employ software that can include an algorithm to determine a maximum and/or minimum energy power to be selected for a predetermined substrate outer surface, and then applied to a corresponding actual substrate outer surface. According to present aspects, based upon the livery of the composition of the substrate outer surface, a particular region of the substrate outer surface to be treated may not be able to withstand a particular energy intensity of applied energy as another region, or more energy may be required to cause the desired degree of removal , the complete removal, of debris from the actual substrate outer surface. Likewise, as explained herein, more energy may be required and delivered to remove debris from, and contained within, the areas of the Hybrid Laminar Flow Control HLFC recesses including, , HLFC holes, . According to further aspects, positioning mechanisms used to move the entire apparatus as well as the devices used to maneuver and position the debris removal device relative to the substrate outer surface can include, without limitation, robotics and other automated devices that can be controlled on-site or that can be controlled remotely with, for example, wireless technology and other technologies, including the use of hardware and software required to operate the positioning mechanisms. According to further aspects, such larger structures and objects that can include the actual substrate outer surfaces to be treated by the present methods, systems, and apparatuses can further include, for example and without limitation, manned and unmanned spacecraft, manned and unmanned aircraft, manned and unmanned hovercraft, manned and unmanned rotorcraft, manned and unmanned terrestrial vehicles, manned and unmanned surface watercraft, manned and unmanned sub-surface watercraft, manned and unmanned satellites, etc. , and combinations thereof. The flowcharts and block diagrams in the different depicted aspects illustrate the architecture, functionality, and operation of some possible implementations of apparatuses, systems, and methods in an illustrative aspect. In this regard, each block in the flowcharts or block diagrams may represent a module, segment function, and/or a portion of an operation or step. For example, one or more of the blocks can be implemented as program code, in hardware, or a combination of the program code and hardware. When implemented in hardware, the hardware can, for example, take the form of integrated circuits that are manufactured or configured to perform one or more operations in the flowcharts or block diagrams. In some alternative implementations of an illustrative aspect, the function or functions noted in the block of the flowcharts or block diagrams may occur out of the order noted in the s. For example, in some cases, two blocks shown in succession can be executed substantially concurrently depending upon the functionality involved. Further blocks can be added to the illustrated blocks in a flowchart or block diagram. Aspects of the present invention may, of course, be carried out in other ways than those specifically set forth herein without departing from essential characteristics of the invention. The present aspects are to be considered in all respects as illustrative and not restrictive, and all changes coming within the meaning and equivalency range of the appended claims are intended to be embraced therein.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/59/353/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.994562566280365,
                    "F_SPEC_PARAMS": [
                        "efficiency",
                        "damage"
                    ],
                    "S_SPEC_PARAMS": [
                        "time-consuming",
                        "inefficient;",
                        "reliably remove all debris",
                        "cleaner laminar surface,",
                        "ideal laminar airflow"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Substances such as, for example, air found in airflow can adversely impact the efficiency of large objects designed to move through air.",
                        "Deviation in an original profile of an aircraft substrate surface can result from factors during use including, for example, damage from object impact during flight as well as the build-up of even small amounts of atmospheric debris , dust, dirt, etc.",
                        ", that can build up on an outer substrate surface."
                    ],
                    "S_SENTS": [
                        "Such methods are time-consuming and inefficient; creating, for example, vast amounts of wastewater and other waste product.",
                        "In addition, traditional cleaning methods further cannot reliably remove all debris from substrate surfaces, especially surfaces that contain relatively small recesses, including HLFC surfaces that can include perforations.",
                        "That is, while an aircraft outer substrate surface may appear clean after being subjected to a cleaning protocol that is designed to achieve a cleaner laminar surface, such aircraft outer surfaces may retain amounts of adhered debris that may or may not be visible to the eye, and that can still adversely impact an ideal laminar airflow at an outer substrate surface of, for example, aircraft."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Productivity",
                        "Harmful Factors Acting on Object"
                    ],
                    "F_SIM_SCORE": 0.684137225151062,
                    "S_TRIZ_PARAMS": [
                        "Productivity",
                        "Waste of Energy",
                        "Waste of Substance",
                        "Harmful Factors Acting on Object"
                    ],
                    "S_SIM_SCORE": 0.5487378686666489,
                    "GLOBAL_SCORE": 1.677666779855887
                },
                "sort": [
                    1.6776668
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11254016-20220222",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11254016-20220222",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-08-30",
                    "PUBLICATION_DATE": "2022-02-22",
                    "INVENTORS": [
                        "Wai Man Lee",
                        "Tin Cheung Wong"
                    ],
                    "APPLICANTS": [
                        "GUANGDONG O-MATIC INTELLIGENT ROBOT LIMITED    ( Foshan , CN )"
                    ],
                    "INVENTION_TITLE": "Joint mechanism, method for controlling the same, multi-arm device, and robot",
                    "DOMAIN": "B25J 170258",
                    "ABSTRACT": "The present invention relates to a joint mechanism 100, a method for controlling the joint mechanism 100, a multi-arm device 200 including the joint mechanism 100, and a robot. The joint mechanism 100 comprises: a base 4 having a pivot shaft 41; a swinging arm 1 having a first end 11 mounted on the pivot shall 41; a first driving member 2 and a second driving member 3 mounted on the pivot shall 41 for interacting with the swinging arm 1 through magnetorheological fluid; and a first electromagnetic component 22 and a second electromagnetic component 32, configured to change phase state of the magnetorheological fluid. The first driving member 2 and the second driving member 3 can selectively drive tire swinging arm 1 to rotate along a first direction or a second direction.",
                    "CLAIMS": "1. A joint mechanism 100, comprising: a base 4 having a first and a second pivot shaft 41; a swinging arm 1 which is mounted on the pivot shaft 41 at a first end 11 thereof; a first driving member 2 and a second driving member 3, which are each mounted on one of the first and the second pivot shaft 41 for receiving a driving force from a driving source, wherein the first driving member 2 and the second driving member 3 interact with the swinging arm 1 through magnetorheological fluid, respectively; and a first electromagnetic component 22 and a second electromagnetic component 23, which are configured to change phase state of the magnetorheological fluid, so that the first driving member 2 and the second driving member 3 can selectively drive the swinging arm 1 to rotate along a first direction or a second direction, wherein the first direction is opposite to the second direction, wherein the first driving member 2 and the second driving member 3 define, together with the swinging arm 1, a closed, first liquid cavity 21 and a closed, second liquid cavity 31 respectively, which are both filled with the magnetorheological fluid, and further wherein the first end 11 of the swinging arm 1 is formed as a sleeve, which is divided into two recesses 13, 14 through a partition wall 12 extending along a radial direction, and wherein open ends of said two recesses 13, 14 are closed by the first driving member 2 and the second driving member 3 respectively, so as to form the first liquid cavity 21 and the second liquid cavity 31. 2. The joint mechanism according to claim 1, wherein the first driving member 2 and the second driving member 3 are each formed as a hollow shaft having an open end and a closed end, the open ends of the first driving member 2 and the second driving member 3 are configured so that the pivot shaft 41 can be mounted thereon, and the closed ends thereof extend into the recesses 13, 14 respectively, and each hollow shaft is provided on an outer periphery thereof with an annular shoulder 25, 35, which is sealingly connected with the open end of a respective recess 13, 14 through a bearing 26, 36, for closure of the first liquid cavity 21 or the second liquid cavity 31. 3. The joint mechanism according to claim 2, wherein the first electromagnetic component 22 and the second electromagnetic component 23 are configured as annular members, which are arranged in the first liquid cavity 21 and the second liquid cavity 31, respectively, and sleeved on the closed ends of the first driving member 2 and the second driving member 3, respectively. 4. The joint mechanism according to claim 2, wherein portions of the first driving member 2 and the second driving member 3 extending into the recesses are made of soft magnetic material. 5. The joint mechanism according to claim 1, wherein the first electromagnetic component 22 and the second electromagnetic component 32 are arranged at a side of the first liquid cavity 21 and the second liquid cavity 31, respectively. 6. The joint mechanism according to claim 1, wherein the driving source drives the first driving member 2 and the second driving member 3 to rotate in the first direction and the second direction respectively through a transmission mechanism, which is selected from a group consisting of a pulley, a gear, a sprocket and a belt. 7. The joint mechanism according to claim 6, wherein the driving source drives the first driving member 2 through a first belt and a first pulley, and drives the second driving member 3 through a second belt and a second pulley, and wherein the first belt adopts one of a cross-belt drive mode and an open-belt drive mode, while the second belt adopts the other of the cross-belt drive mode and the open-belt drive mode. 8. A multi-arm device 20, comprising: a first joint mechanism according to claim 1; and a second joint mechanism according to claim 1, which is arranged at a second end 19 of the swinging arm 1 of the first joint mechanism, wherein the base of the second joint mechanism is formed by the second end 19 of the swinging arm 1 of the first joint mechanism, and the driving source of the second joint mechanism is formed by the first driving member 2 and the second driving member 3 of the first joint mechanism. 9. A method of controlling the joint mechanism according to claim 1, comprising a step of: applying current to the first electromagnetic component 22 and/or the second electromagnetic component 23 to change the phase state of magnetorheological fluid, so that at least one of the first driving member 2 and the second driving member 3 can selectively drive the swinging arm 1 to rotate along the first direction or the second direction. 10. The method according to claim 9, wherein the method includes any one or more of operation modes as follows: a first operation mode, in which only a first current is applied to the first electromagnetic component 22, so that the magnetorheological fluid in the first liquid cavity changes its phase state, resulting in that the first driving member 2 can drive the swinging arm 1 to rotate in the first direction; a second operation mode, in which only a second current is applied to the second electromagnetic component 32, so that the magnetorheological fluid in the second liquid cavity changes its phase state, resulting in that the second driving member 3 can drive the swinging arm 1 to rotate in the second direction; a third operation mode, in which a third current and a fourth current are continuously applied to the first electromagnetic component 22 and the second electromagnetic component 32, respectively, so that the magnetorheological fluids in the first liquid cavity 21 and the second liquid cavity 31 both change their phase states, and a driving force exerted by the first driving member 2 on the swinging arm 1 is equal to that exerted by the second driving member 3 on the swinging arm 1, resulting in the swinging arm 1 being at a natural stationary state; a fourth operation mode, in which a fifth current and a sixth current are continuously applied to the first electromagnetic component 22 and/or the second electromagnetic component 32, respectively, so that the magnetorheological fluid in the first liquid cavity 21 and/or the second liquid cavity 31 changes its phase states, and a driving force exerted by the first driving member 2 and/or the second driving member 3 on the swinging arm 1 counteracts to a gravity of the swinging arm 1, resulting in the swinging arm 1 being at a stationary state, the fifth current and the sixth current being pulse current; and a fifth operation mode, in which a seventh current and an eighth current are alternately applied to the first electromagnetic component 22 and the second electromagnetic component 32, so that the magnetorheological fluids in the first liquid cavity 21 and the second liquid cavity 31 change their phase states in an alternate manner, and the first driving member 2 and the second driving member 3 drive the swinging arm 1 in an alternate manner, thus generating a reciprocating swing movement of the swinging arm 1.",
                    "STATE_OF_THE_ART": "Robots have been widely used in industrial fields. Among others, joint robots are particularly popular due to their many advantages, such as large operating range, flexible movement, compact structure, or the like. The joint of a robot usually includes a rotatable swinging arm, and a driving mechanism for driving the swinging arm into rotation. In current joint robots, each joint of a robot is directly driven by a motor, for example, a stepper motor, a DC servo motor, an AC servo motor, a hydraulic servo motor, etc. However, due to inherent characteristics of these motors, this design will cause a large inertia of the swinging arm of each joint of the robot, so that it is difficult to achieve high-speed reciprocal swing movement.",
                    "SUMMARY": [
                        "The present invention aims to provide a joint mechanism, winch is capable of realizing rapid changes of movement state of the joint mechanism by means of magnetorheological fluid. The present invention further aims to provide a method for controlling the joint mechanism, a multi-arm device including such a joint mechanism, and a robot including such a joint mechanism or a multi-arm device. According to a first aspect of the present invention, a joint mechanism is proposed, comprising: a base having a pivot shaft; a swinging arm which is mounted on the pivot shaft at a first end thereof; a first driving member and a second driving member, which are both mounted on the pivot shaft for receiving a driving force from a driving source, wherein the first driving member and the second driving member interact with the swinging arm through magnetorheological fluid, respectively; and a first electromagnetic component and a second electromagnetic component, which are configured to change phase state of the magnetorheological fluid, so that the first driving member and the second driving member can selectively drive the swinging arm to rotate along a first direction or a second direction, wherein the first direction is opposite to the second direction. The magnetorheological fluid is a kind of suspension formed by dispersing magnetically polarized particles having a size of micron range in a non-magnetic liquid for example, mineral oil, silicone oil, . When there is no magnetic field, the magnetorheological fluid behaves as a liquid of good flowability, and has a very small apparent viscosity. However, under a strong magnetic field, the apparent viscosity of the magnetorheological fluid will be increased by more than two orders of magnitude in a short time , in milliseconds, so drat the magnetorheological fluid exhibits solid-like characteristics. Moreover, the change of phase state is continuous and reversible. That is, the magnetorheological fluid will return to its original phase state after the magnetic field is withdrawn. Since the rheology of the magnetorheological fluid under a magnetic field is instantaneous and reversible, and the shear yield strength of the magnetorheological fluid after the rheology maintains a stable corresponding relationship with respect to the strength of the magnetic field, the magnetorheological fluid has been widely used as a smart material with excellent performances. The procedure in which the magnetorheological fluid changes its viscosity as the magnetic field strength changes is often referred to as the phase state change of the magnetorheological fluid. According to the present invention, in a joint mechanism, particularly a joint mechanism for a robot, the driving members interact i. e. , form a driving connection relationship with the swinging arm through the magnetorheological fluid, and enable the magnetorheological fluid to experience phase state change by means of the magnetic field generated by the electromagnetic components. When the electromagnetic components do not generate a magnetic field, the magnetorheological fluid behaves as a liquid of good flowability. In this case, no driving connection is formed between any of the driving members and the swinging arm. In other words, the driving members cannot drive the swinging arm into rotation. When any one of the electromagnetic components generates a magnetic field, the magnetorheological fluid experiences change of phase state and thus behaves like a semi-solid. In this case, a driving connection is formed between a corresponding driving member and the swinging arm. That is, the corresponding driving member can drive the swinging arm into rotation. Because the electromagnetic components can be switched on and off in a short time, and the response time of the magnetorheological fluid is also very fast, the driving connection between the driving member and the swinging arm can be established or disappeared in a short time. Therefore, through providing two driving members with different rotation directions and establishing a driving connection between one or two driving members and the swinging arm or not as required, the swinging arm can perform various movements as required, and achieve rapid turnarounds. In an embodiment, the first driving member and the second driving member define, together with the swinging arm, a closed, fast liquid cavity and a closed, second liquid cavity respectively, which are both filled with the magnetorheological fluid. In a specific embodiment, the first end of the swinging arm is formed as a sleeve, which is divided into two recesses through a partition wall extending along a radial direction. The open ends of said two recesses are closed by the first driving member and the second driving member respectively, so as to form the first liquid cavity and the second liquid cavity. In a specific embodiment, the first driving member and the second driving member are each formed as a hollow shaft having an open end and a closed end. The open ends of the first driving member and the second driving member are configured so that the pivot shaft can be mounted thereon, and the closed ends thereof extend into the recesses respectively. Each hollow shaft is provided on an outer periphery thereof with an annular shoulder, which is sealingly connected with the open end of a respective recess through a bearing, for closure of the first liquid cavity or the second liquid cavity. In an embodiment, the first electromagnetic component and the second electromagnetic component are configured as annular members, which are arranged in the first liquid cavity and the second liquid cavity, respectively, and sleeved on the closed ends of the first driving member and the second driving member, respectively. In a specific embodiment, portions of the first driving member and the second driving member extending into the recesses are made of soft magnetic material. In a specific embodiment, the first electromagnetic component and the second electromagnetic component are arranged at a side by the first liquid cavity and the second liquid cavity, respectively. In an embodiment, the driving source drives the first driving member and the second driving member to rotate in the first direction and the second direction respectively through a transmission mechanism, which is selected from a group consisting of a pulley, a gear, a sprocket and a belt. In a specific embodiment, the driving source drives the first driving member through a first belt and a first pulley, and drives the second driving member through a second belt and a second pulley. The first belt adopts one of a cross-belt drive mode and an open-belt drive mode, while the second belt adopts the other of the cross-belt drive mode and the open-belt drive mode. Alternatively, the first belt and the second belt both adopt a semi-cross-belt drive mode. In an embodiment, the first driving member and the second driving member are each provided with a sensor for detecting rotating angle of the swinging arm. According to a second aspect of the present invention, a multi-arm device is proposed, comprising: a first joint mechanism as mentioned above; and a second joint mechanism as mentioned above, which is arranged at the second end of the swinging arm of the first joint mechanism. The base of the second joint mechanism is formed by the second end of the swinging arm of the fast joint mechanism, and the driving source of the second joint mechanism is formed by the first driving member and the second driving member of the first joint mechanism. According to a third aspect of the present invention, a method of controlling the joint mechanism as mentioned above is proposed, comprising a step of applying current to the first electromagnetic component and/or the second electromagnetic component to change the phase state of the magnetorheological fluid, so that at least one of the first driving member and the second driving member can selectively drive the swinging arm to rotate along the first direction or the second direction. In an operation mode of the method, only a first current is applied to the first electromagnetic component, so that the magnetorheological fluid in the first liquid cavity changes its phase state. Accordingly, the first driving member can drive the swinging arm to rotate in the first direction. In another operation mode of the method, only a second current is applied to the second electromagnetic component, so that the magnetorheological fluid in the second liquid cavity changes its phase state. Accordingly, the second driving member can drive the swinging arm to rotate in the second direction. In a further operation mode of the method, a third current and a fourth current are continuously applied to the first electromagnetic component and the second electromagnetic component, respectively, so that the magnetorheological fluids in the first liquid cavity and the second liquid cavity both change their phase states. Accordingly, a driving force exerted by the first driving member on the swinging arm is equal to that exerted by the second driving member on the swinging arm, resulting in the swinging arm being at a natural stationary state. In still a further operation mode, a fifth current and a sixth current are continuously applied to the first electromagnetic component and/or the second electromagnetic component, respectively, so that the magnetorheological fluid in the first liquid cavity and/or the second liquid cavity changes its phase states. Accordingly, a driving force exerted by the first driving member and/or the second driving member on the swinging arm counteracts to a gravity of the swinging arm, resulting in the swinging arm being at a stationary state. The fifth current and the sixth current are both pulse current. In still a further operation mode, a seventh current and an eighth current are alternately applied to the first electromagnetic component and the second electromagnetic component, so that the magnetorheological fluids in the first liquid cavity and the second liquid cavity change their phase states in an alternate manner. Accordingly, the first driving member and the second driving member drive the swinging arm in an alternate manner, thus generating a reciprocal swing movement of the swinging arm. According to a fourth aspect of the present invention, there also provides a robot, which includes the joint mechanism as mentioned above, or the multi-arm device as mentioned above.",
                        "In the Following preferred embodiments of the present invention will be described by way of non-limiting examples with reference to the following drawings, in which: 1 shows a perspective view of a multi-arm device according to the present invention, which includes two joint mechanisms according to the present invention; and 2 shows a cross-sectional view of a joint mechanism according to the present invention."
                    ],
                    "DESCRIPTION": "The present invention will be described in detail below with reference to the drawings. 1 shows a perspective view of a multi-arm device 200 according to the present invention. As shown in 1, the multi-arm device 200 includes two joint mechanisms 100 according to the present invention connected to each other. It is easy to understand that the multi-arm device 200 may include more joint mechanisms 100 connected to each other. As shown in 1, the joint mechanism 100 includes a base 4 having a pivot shaft 41, and a swinging arm 1 mounted on the pivot shaft 41. The swinging arm 1 is mounted on the pivot shaft 41 through a first end 11 thereof see 2. As shown in 2, the first end 11 of the swinging arm 1 is formed as a sleeve-shaped structure, the center of which is provided with a partition wall 12 extending in a radial direction. The sleeve-shaped first end 11 is divided into two recesses adjacent to each other in an axial direction by the partition wall 12, namely a first recess and a second recess 14. In the present invention, the terms axial and radial are defined with respect to the pivot shaft 41. In this case, the pivot shaft 41 actually includes two pivot portions, which are respectively mounted in the first recess 13 and the second recess 14. According to the present invention, the joint mechanism 100 further includes a first driving member 2 and a second driving member 3, which are mounted side by side on the pivot shaft 41. Both the first driving member 2 and the second driving member 3 can rotate under a driving force from a driving source not shown. However, the rotation directions of the first driving member 2 and the second driving member 3 are set to be different from each other. In the embodiment as shown in 1, the first driving member 2 can rotate in a clockwise direction, while the second driving member 3 can rotate in a counterclockwise direction. The driving source may be, for example, a motor. As shown in 2, the first driving member 2 is formed as a hollow shaft, one end of which is open and the other end is closed. In this manner, the pivot shaft 41 can extend into the open end of the first driving member 2, so that the first driving member 2 can be mounted on the pivot shaft 41 to rotate about the pivot shaft 41. In addition, the closed end of the first driving member 2 extends into the first recess 13 of the first end 11 of the swinging arm 1, so that the first driving member 2 and the swinging arm 1 together define a closed first liquid cavity 21, which is filled with magnetorheological fluid. In the embodiment as shown in 2, the first driving member 2 is further provided on the outer periphery thereof with an annular shoulder 25 extending radially outward. The annular shoulder 25 is connected to the first end 11 of the swinging arm 1 through a bearing 26. To this end, a bearing stop 15 for mounting the bearing 26 is provided at the free end of the first recess 13 of the first end 11 of the swinging arm 1. Accordingly, the swinging arm 1 can be rotated under the driving force of the first driving member 2. For sealing purposes, a sealing element not shown is provided at the bearing 26. It can be known from the foregoing that the first driving member 2 and the swinging arm 1 can interact with each other through the magnetorheological fluid in the first liquid cavity 21, so as to establish a driving connection relationship. According to the illustrated embodiment of the present invention, a first electromagnetic component 22 is provided in the first liquid cavity 21. The first electromagnetic component 22 can be configured, for example, as a ring-shaped member, which is sleeved on a portion 28 of the first driving member 2 extending into the first recess 13. In an embodiment not shown, the first electromagnetic component 22 is not disposed in the first liquid cavity 21, but out of the first liquid cavity 21, for example, at a side by the first liquid cavity 21. It is easy to understand that as long as the first electromagnetic component 22 can act on the magnetorheological fluid in the first liquid cavity 21 for changing its phase state, the position thereof can be arbitrarily selected by those skilled in the art according to the needs of the specific structure. However, the arrangement of the first electromagnetic component 22 in the first fluid cavity 21 can enable the structure of the entire joint mechanism 100 more compact, and the effect of the first electromagnetic component 22 on the magnetorheological fluid is more direct. Therefore, this arrangement is preferred. In one embodiment, the first electromagnetic component 21 may be formed by a stator of a motor, which can be directly sleeved on the portion 28 of the first driving member 2 extending into the first recess 13, so that the assembly is very simple. In addition, ready-made, high-quality stator of the motor can be obtained directly in markets. Therefore, it is unnecessary to design the stator independently, thus saving a large part of the cost. Of course, a shaftless motor stator can also be used. The first driving member 2 and the second driving member 3 have similar structures. Therefore, it is easy to understand that the description on the structure of the first driving member 2 can be similarly applicable to the second driving member 3. Specifically, the second driving member 3 is also formed as a hollow shaft, with an open end and a closed end. The pivot shaft 41 extends into the open end of the second driving member 3, so that the second driving member 3 can be mounted on the pivot shaft 41 to rotate about the pivot shaft 41. In addition, the closed end of the second driving member 3 extends into the second recess 14, so that the second driving member 3 and the swinging arm 1 together define a closed second liquid cavity 31, which is filled with magnetorheological fluid. In the embodiment as shown in 2, the second driving member 3 is further provided on the outer periphery thereof with an annular shoulder 35 extending radially outward. The annular shoulder 35 is connected to the first end 11 of the swinging arm 1 through a bearing 36. To this end, a bearing stop for mounting the bearing 36 is provided at the free end of the second recess 14 of the first end 11 of the swinging arm 1. Accordingly, the swinging arm 1 can be rotated under the driving force of the second driving member 3. For sealing purposes, a sealing element not shown is provided at the bearing 36. Accordingly, the second driving member 3 and the swinging arm 1 can interact with each other through the magnetorheological fluid in the second liquid cavity 31, so that a driving connection relationship can be established. According to the illustrated embodiment of the present invention, a second electromagnetic component 32 is provided in the second liquid cavity 31. The second electromagnetic component 32 can be configured as a ring-shaped member, which is sleeved on a portion of the second driving member 3 extending into the second recess 14. It is easy to understand that as long as the second electromagnetic component 32 can act on the magnetorheological fluid in the second liquid cavity 31 to change its phase state, it can be arranged at another position. Similarly, the second electromagnetic component 31 may be formed by a stator of a motor. According to the present invention, when the first electromagnetic component 31 or the second electromagnetic component 32 is not energized so that no magnetic field is generated, the magnetorheological fluid in the first liquid cavity 21 or the second liquid cavity 31 will act as liquid with excellent flowability. In this case, no driving connection will be established between the first driving member 2 or the second driving member 3 and the swinging arm 1. That is, even if the first driving member 2 or the second driving member 3 rotates, the swinging arm 1 will not be driven into rotation. However, when the first electromagnetic component 31 or the second electromagnetic component 3 is energized to generate a magnetic field, the magnetorheological fluid in the first liquid cavity 21 or the second liquid cavity 31 will act as solid under the influence of the magnetic field. In this case, a driving connection will be established between the first driving member 2 or the second driving member 3 and the swinging arm 1. That is, if the first driving member 2 or the second driving member 3 rotates, the swinging arm 1 is driven into rotation. As shown in 1, in the case where a driving connection is established between the first driving member 2 and the swinging arm 1, the swinging arm 1 will rotate clockwise when the first driving member 2 rotates clockwise. In the case where a driving connection is established between the second driving member 3 and the swinging arm 1, the swinging arm 1 will rotate counterclockwise when the second driving member 3 rotates counterclockwise. Since the magnetorheological fluid has a very fast response speed, the swinging arm 1 can follow the first driving member 2 or the second driving member 3 very quickly to rotate in two different directions of rotation. Therefore, through appropriately controlling the rotation directions of the first driving member 2 and the second driving member 3 and the ON/OFF states of the first electromagnetic component 22 and the second electromagnetic component 32, the joint mechanism 100 according to the present invention can realize forward rotation and reverse rotation of the swinging arm 1, as well as fast switching between two different rotation directions. The specific movement modes of the joint mechanism 100 according to the present invention will be described in detail below. In a preferred embodiment, portions of the first driving member 2 and the second driving member 3 that extend into the first recess 13 and the first recess 14 respectively may be made of soft magnetic material, so as to facilitate the transmission of magnetic force. In another preferred embodiment, an encoder and a code reader may be provided on the first driving member 2 and the second driving member 3, so that the rotation angle of the swinging arm can be read and controlled. According to the present invention, the driving source is connected to the first driving member 2 or the second driving member 3 through a transmission mechanism, so that the driving force can be transmitted to the first driving member 2 or the second driving member 3 to drive the first driving member 2 or the second driving member 3 into rotation. Preferably, the transmission mechanism may be a pulley, a gear, a sprocket, a timing belt, or the like. Therefore, the inertia of the entire driving structure is very small, which is favorable for the swinging arm 1 to switch its moving direction rapidly. In the illustrated embodiment, the transmission mechanism is a pulley and a transmission belt. Specifically, the driving source drives the first driving member 2 through a first driving belt 24 and a first driving pulley 27 that cooperates therewith, and drives the second driving member 3 through a second driving belt 34 and a second driving pulley 37 that cooperates therewith. In one embodiment, as shown in 1, the first driving belt 24 adopts a cross-belt drive, while the second driving belt 34 adopts open-belt drive, so that the rotation directions of the first driving member 2 and the second driving member 3 are opposite to each other. With this design, two side-by-side pulleys can be arranged on the rotating shaft of the driving source, so that no additional steering mechanism is necessary to ensure the rotation direction of the first driving member 2 is opposite to that of the second driving member 3. This design has a simple structure, low cost, and small footprint. If is easy to understand that the first transmission belt 24 can adopt open-belt drive while the second transmission belt 34 adopt cross-belt drive, which can also achieve the above-mentioned effect. In another embodiment, the first driving belt 24 adopts a semi-cross-belt drive, and the second driving belt 34 also adopts a semi-cross-belt drive, thereby ensuring that the rotation direction of the first driving member 2 is opposite to that of the second driving member 3. With this arrangement, the rotation shaft of the driving source can be allowed to be set in any direction, and the problem of self-friction caused by the cross-belt drive of the transmission belt can also be avoided. In another embodiment, the transmission mechanism is a gear set. One skilled in the art can easily understand that by appropriately selecting the number of gears in the gear set, the rotation direction of the first driving member 2 can be opposite to that of the second driving member 3. This design also provides great flexibility for the arrangement of the rotation shaft of the drive source. In the following the method of controlling the joint mechanism 100 is described. As described above, by applying a current to the first electromagnetic component 22 and/or the second electromagnetic component 32, the magnetorheological fluid in the first liquid cavity 21 and/or the second liquid cavity 31 will change its phase state, so that the first driving member 2 and/or the second driving member 3 can selectively drive the swinging arm 1 to rotate in the first direction and the second direction, respectively. The joint mechanism 100 according to the present invention is particularly capable of operating in several operation modes as described below. In a first operation mode, only a first current is applied to the first electromagnetic component 22, so that the magnetorheological fluid in the first liquid cavity 21 is in a phase state of high viscosity. In this way, a driving connection is established only between the first driving member 2 and the swinging arm 1. Accordingly, the first driving member 2 can drive the swinging arm 1 to rotate in a first direction, i. e. , the clockwise direction in the illustrated embodiment. In a second operation mode, only a second current is applied to the second electromagnetic component 32, so that the magnetorheological fluid in the second liquid cavity 31 is in a phase state of high viscosity. In this way, a driving connection is established only between the second driving member 3 and the swinging arm 1. Accordingly, the second driving member 3 can drive the swinging arm 1 to rotate in the second direction, i. e. , the counterclockwise direction in the illustrated embodiment. In a third operation mode, a third current and a fourth current are continuously applied to the first electromagnetic component 22 and the second electromagnetic component 32, respectively. In this way, the magnetorheological fluids in the first liquid cavity 21 and the second liquid cavity 31 are each in a phase state of high viscosity, so that driving connections are established between the first driving member 2 and the swinging arm 1, and also between the second driving member 3 and the swinging arm 1. By appropriately selecting the magnitudes of the third current and the fourth current, the driving force exerted by the first driving member 2 on the swinging arm 1 can be made equal to that exerted by the second driving member 3 on the swinging arm 1, resulting in the swinging arm 1 being at a natural stationary state. Preferably, the third current is less than or equal to the first current, and the fourth current is less than or equal to the second current. In the fourth operation mode, a fifth current and a sixth current are continuously applied to the first electromagnetic component 22 and or the second electromagnetic component 32, respectively. In this way, the magnetorheological fluids in the first liquid cavity 21 and the second liquid cavity 31 are each in a phase state of high viscosity, so that driving connections are established between the first driving member 2 and the swinging arm 1, and also between the second driving member 3 and the swinging arm 1. By appropriately selecting the magnitudes of the fifth current and the sixth current, the driving force exerted by the first driving member 2 and/or the second driving member 3 on the swinging arm 1 can counteract to the gravity of the swinging arm 1, resulting in the swinging arm 1 being at a stationary state. Preferably, the fifth current and the sixth current are both pulse current. In the fifth operation mode, a seventh current and an eighth current are alternately applied to the first electromagnetic component 22 and the second electromagnetic component 32. In this way, the magnetorheological fluids in the first liquid cavity 21 and the second liquid cavity 31 present a highly viscous state alternatively, so that driving connections are established between the first driving member 2 and the swinging arm 1 and between the second driving member 3 and the swinging arm 1 in an alternate manner. Therefore, the first driving member 2 and the second driving member 3 can alternately drive the swinging arm 1 to rotate in different directions. That is, a reciprocating swing movement of the swinging arm 1 is realized. One or more of the above operation modes may be selected in sequence according to actual needs, so that various functions of the swinging arm 1 as required can be achieved. The multi-arm device 200 according to the present invention may include a plurality of joint mechanisms 100 connected to each other. In the embodiment as shown in 1, two joint mechanisms 100 are connected to each other. In order to simplify the structure, the second end 19 of the swing arm 1 of an upstream joint mechanism 100 is formed as the base of a downstream joint mechanism 100. In addition, the transmission mechanism of the downstream joint mechanism 100 is connected to the first driving member 2 and the second driving member 3 of the upstream joint mechanism 100. That is, the first driving member 2 and the second driving member 3 of the upstream joint mechanism 100 act as the driving source of the downstream joint mechanism 100. When the multi-arm device 200 includes a plurality of joint mechanisms 100, these joint mechanisms 100 are connected one by one in sequence as described above. An arm 110 serving as an end effector may be arranged on the last joint mechanism 100 i. e. , the upper one in 1. With this design, the multi-arm device 200 has a very compact structure, and can achieve a more complex movement. In addition, only one driving source that provides a driving force for the first joint mechanism 100 is required to drive all the joint mechanisms. Therefore, the cost of the multi-arm device 200 is cut down, and the power consumption thereof is also greatly reduced. According to another aspect of the present invention, there also provides a robot, which includes the joint mechanism 100 according to the present invention, or the multi-arm device 200 according to the present invention. The robot may be a single-joint robot, or a multi-joint robot. Finally, it should be noted that the above descriptions are merely preferred embodiments of the present invention, and do not restrict the present invention in any manner. Although the present invention has been described in detail with reference to the foregoing embodiments, those skilled in the art can still modify the technical solutions described in the foregoing embodiments or substitute equivalent technical features therein. Any modification, equivalent replacement, or improvement made within the spirit and principle of the present invention shall fall within the protection scope of the present invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/16/540/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.95123291015625,
                    "F_SPEC_PARAMS": [
                        "operating range, flexible movement, compact structure,"
                    ],
                    "S_SPEC_PARAMS": [
                        "inertia",
                        "high-speed reciprocal swing movement"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "<s> Robots have been widely used in industrial fields.",
                        "Among others, joint robots are particularly popular due to their many advantages, such as large operating range, flexible movement, compact structure, or the like."
                    ],
                    "S_SENTS": [
                        "However, due to inherent characteristics of these motors, this design will cause a large inertia of the swinging arm of each joint of the robot, so that it is difficult to achieve high-speed reciprocal swing movement."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Durability of Moving Object"
                    ],
                    "F_SIM_SCORE": 0.4759191870689392,
                    "S_TRIZ_PARAMS": [
                        "Force Torque"
                    ],
                    "S_SIM_SCORE": 0.5684356689453125,
                    "GLOBAL_SCORE": 1.6734103381633758
                },
                "sort": [
                    1.6734103
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11439461-20220913",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11439461-20220913",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-02-20",
                    "PUBLICATION_DATE": "2022-09-13",
                    "INVENTORS": [
                        "Patrick Codd",
                        "Kimberly Hoang",
                        "David Britton",
                        "Westin Hill",
                        "Weston Ross"
                    ],
                    "APPLICANTS": [
                        "Duke University    ( Durham , US )"
                    ],
                    "INVENTION_TITLE": "Automated surgical robot",
                    "DOMAIN": "A61B 1820",
                    "ABSTRACT": "An automated laser-surgery system for performing a closed-loop surgical procedure is disclosed. The procedure includes forming a post-procedural goal based on a three-dimensional 3D image of a surgical site, planning a path for a surgical laser signal based on the post-procedural goal, performing a procedural pass by steering the surgical laser signal along the path, measuring the surface of the surgical site after the procedural pass, updating a model based on the measured effect at the surgical site, and evaluating the success of the procedural pass based on the surface measurement and the post-procedural goal. If necessary, a new path is planned based on the post-procedural goal and the surface measurement a new pass based on that path is performed, and the surface is again measured to evaluate the success of the new pass. These operations are repeated as a closed-loop sequence as many times as necessary to achieve success.",
                    "CLAIMS": "1. A surgical system for manipulating tissue at a surgical site, the system comprising: a first laser operative for providing a first laser signal, the first laser signal being operative for manipulating tissue at the surgical site; a beam scanner that is configured to controllably direct the first laser signal to points within a first region of the surgical site; a surface profiler that is configured to measure a range for points within the first region; and a processing circuit that is configured to: 1 plan a first path for the first laser signal within the first region, wherein the first path is based on a post-procedural goal and a first estimate of at least one tissue parameter selected from the group consisting of threshold radiant exposure, tissue density, ablation enthalpy, absorption coefficient, optical scattering coefficient, reduced scattering coefficient, refractive index, and scattering anisotropy coefficient, and wherein the first estimate is based on a model of the reaction of the tissue to the laser signal derived from a topological map of a first feature formed at the surgical site using the first laser signal; 2 control the first laser signal along the first path while simultaneously controlling at least one characteristic of the first laser signal, the at least one characteristic being selected from the group consisting of beam shape, focus, focal depth, spot size, and orientation; 3 generate a first intra-procedural model of the surgical site based on a measured range for each of a first plurality of points within the first region, wherein the range for each point of the first plurality thereof is measured after the first laser signal has traversed the first path; and 4 perform a first comparison of the post-procedural goal and the first intra-procedural model. 2. The system of claim 1 wherein the processing circuit is further configured to: 5 plan a second path for the first laser signal within the first region based on the first comparison; 6 control the first laser signal along the second path; 7 generate a second intra-procedural model of the surgical site based on a measured range for each of a second plurality of points within the first region, wherein the range for each point of the second plurality thereof is measured after the first laser signal is controlled along the second path; and 8 perform a second comparison of the post-procedural goal and the second intra-procedural model. 3. The system of claim 2 wherein the processing circuit is further configured to: 9 generate a second estimate of the at least one tissue parameter based on a difference between the first intra-procedural model and a third intra-procedural model that is based on a measured range for each of a third plurality of points within the first region, wherein the range for each point of the third plurality thereof is measured before the first laser signal is controlled along the first path; and 10 plan the second path such that it is further based on the second estimate. 4. The system of claim 1 wherein the processing circuit is further configured to 5 generate the first estimate such that it is based on an interaction between the first laser signal and a first point within the first region. 5. The system of claim 1 wherein the post-procedural goal is based on a second intra-procedural model of the surgical site, wherein the second intra-procedural model is based on a measured range for each of a second plurality of points within the first region, the range for each point of the second plurality thereof being measured before the first laser signal is controlled along the first path. 6. The system of claim 1 wherein the post-procedural goal is based on a first image of the surgical site, wherein the first image is a three-dimensional 3D image taken before the first laser signal is controlled along the first path. 7. The system of claim 1 further comprising a second laser operative for providing a second laser signal, wherein the beam scanner is configured to controllably direct the second laser signal to any point within the first region. 8. The system of claim 7 wherein the surface profiler includes the second laser. 9. The system of claim 8 wherein the surface profiler includes a laser triangulation sensor, the laser triangulation sensor including the second laser. 10. The system of claim 1 wherein the beam scanner comprises a two-axis galvanometer. 11. The system of claim 1 wherein the beam scanner comprises a two-axis MEMS mirror. 12. The system of claim 1 further comprising a graphical user interface GUI that enables a user to specify a classification for tissue in at least one region of the surgical site. 13. A method for manipulating tissue at a surgical site, the method comprising: generating a model of the reaction of the tissue to the laser signal from a topological map of a first feature formed at the surgical site using the first laser signal; generating a first estimate of at least one tissue parameter based on the model, the at least one tissue parameter being selected from the group consisting of threshold radiant exposure, tissue density, ablation enthalpy, absorption coefficient, optical scattering coefficient, reduced scattering coefficient, refractive index, and scattering anisotropy coefficient; planning a first path for the first laser signal within a first region of a surgical site, wherein the first path is based on a post-procedural goal and the first estimate; controlling at least one characteristic of the first laser signal, the at least one characteristic being selected from the group consisting of beam shape, focus, focal depth, spot size, and orientation; generating a first intra-procedural model of the surgical site based on a measured range for each of a first plurality of points within the first region, wherein the range for each point of the first plurality thereof is measured after the first laser signal has traversed the first path; and performing a first comparison of the post-procedural goal and the first intra-procedural model. 14. The method of claim 13 further comprising: planning a second path for the first laser signal within the first region, wherein the second path is based on the first comparison; controlling the first laser signal along the second path; generating a second intra-procedural model of the surgical site based on a measured range for each of a second plurality of points within the first region, wherein the range for each point of the second plurality thereof is measured after the first laser signal is directed along the second path; and performing a second comparison of the post-procedural goal and the second intra-procedural model. 15. The method of claim 14 further comprising: generating a second estimate of the at least one tissue parameter based on a difference between the second intra-procedural model and a third intra-procedural model that is based on a measured range for each of a third plurality of points within the first region, wherein the range for each point of the third plurality thereof is measured before the first laser signal is directed along the first path; and planning the second path such that it is further based on the at least one tissue parameter and the second estimate. 16. The method of claim 13 further comprising: controlling the first laser signal to create the first feature; and generating the first estimate based on the first feature and a model of the first feature. 17. The method of claim 13 further comprising measuring the range for each of the first plurality of points, wherein the range is measured via a laser triangulation sensor. 18. The method of claim 13 wherein the first laser signal is controlled along the first path by a two-axis beam scanner. 19. The method of claim 18 wherein the beam scanner comprises a two-axis galvanometer. 20. The method of claim 18 wherein the beam scanner comprises a two-axis MEMS mirror. 21. The method of claim 13 wherein the post-procedural goal is generated by operations comprising: generating a first pre-procedural model based on a first image of the surgical site, the first pre-procedural model being a 3D model, and the first image being generated before the first laser signal is controlled along the first path; and specifying a first classification for at least one region within the first pre-procedural model. 22. The method of claim 21 wherein the post-procedural goal is generated by operations further comprising: generating a first surface scan of the first region, the first surface scan being based on a range for each of a second plurality of points measured before the first laser signal is controlled along the first path; and adapting the first pre-procedural model based on the first surface scan.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to surgical systems in general, and, more particularly, to robotic surgery.",
                    "STATE_OF_THE_ART": "Laser surgery has become a critical procedure in the treatment of many conditions, such as brain cancer, skin cancer, and urinary-tract conditions, among others. In addition, laser surgery has been directed to the treatment of many non-life-threatening ailments, such as tattoo removal and the like. Unfortunately, in many instances, a life-threatening condition is deemed inoperable because its surgical treatment is beyond the ability of even state-of-the-art laser treatments. Furthermore, some procedures require a greater accuracy or precision than is possible within the limitations of the human hand and physiologic tremor, such as dissecting a tumor from a cranial nerve or critical vascular structure, removing a tumor from a highly eloquent or functional brain tissue that controls a delicate function, or manipulating tissue on the brainstem. Operability, extent of resection, and complication rates are inexorably linked to limitations in current surgical procedures and medical imaging that can be applied. For example, brain tumors can be treated by resecting the cancerous tissue using a hand-held laser tool, which is often invisible to the human eye. For even the most-skilled surgeons, such an approach taxes their ability to controllably manipulate tissue where desired without impacting tissue that should not be disturbed. As a result, such procedures introduce the risk of inadvertent delivery of injury to delicate neurovascular tissues with risk of neurologic injury or death. While robotic-assisted surgery has become relatively commonplace in other realms of surgery, it is not currently widely used in neurosurgery; however, the demands and limitations of current practice might be addressed through its implementation. To date, the robotic applications in clinical neurosurgery have largely been limited to improving the positional accuracy during stereotaxy or the positioning of optical devices for use in surgery. Neurosurgeons continue to rely on mechanical dissection where instruments including the bipolar electrocautery, suction, and probe dissector are used to manipulate sensitive neurologic structures under direct visualization; consequently, the precision of these tools continues to be limited to that of their human operator. While incremental improvements in patient outcomes can be realized by refining neurosurgical techniques, more meaningful advances must be achieved through advancing robotic assistance and imaging in the operating room. In addition, there is a fundamental cognitive-flow limitation as ever increasing modalities for imaging, intra-procedural monitoring, and multispectral data recordings are integrated into care but are not fully utilized in the surgical setting, since integrating such vast quantities of information in real time by a single surgeon/operator, or even an entire care team, is difficult, if not impossible. Furthermore, even with the advanced level of robotic assistance currently available in the operating room, a surgeon cannot perform at the desired level of precision while constrained by a timeframe so heavily weighted by cost. Some of the basic surgical decisions must be yielded to the assisting robot, thereby giving it a higher level of automation than previously demonstrated in commercially available surgical robots. A laser-surgery approach that reduces the rate of complication, decreases the percentage of conditions deemed inoperable, increases accuracy and precision of intervention, and/or reduces operating-room time and cost would be a welcome advance in the state of the art",
                    "SUMMARY": [
                        "Embodiments of the present disclosure enable automated surgical procedures without some of the costs and disadvantages of the prior art. Systems and methods in accordance with the present disclosure employ a laser signal to manipulate tissue at a surgical site, where the path of the laser signal, as well as its orientation, power and speed, are automatically controlled based on a pre-procedural model and a post-procedural goal. After a procedural pass of the surgical procedure has been performed at the surgical site, an intra-procedural assessment of the results of that pass is performed to determine how accurately the post-procedural goal has been achieved. In cases where additional tissue manipulation is warranted, a path for another pass of the laser through the surgical site is planned and performed via the automated surgical system. The planning of the surgical path, the execution of a procedural pass based on the planned path, and the performance of an intra-procedural assessment of that pass based on the post-procedural goal collectively define a closed-loop sequence that can be repeated as many times as needed to realize the desired surgical outcome. Furthermore, in some embodiments, its implementation enables real-time adaptation of the surgical procedure based on the actual response to the tissue at a surgical site to the surgical laser signal. Some embodiments of the present disclosure are particularly well-suited for use in applications in neurosurgery , brain surgery, , dermatology , tattoo removal, treatment of skin cancers, , and eye surgery, among others. An illustrative embodiment is an automated surgical system comprising a processing circuit, a surgical laser, a two-axis beam scanner, and a surface profiler, where the processing circuit plans a path for a first pass of a surgical laser signal through a surgical site based on a pre-procedural three-dimensional image of the field, an intra-procedural image of the field, and the desired surgical outcome. The processing circuit executes a procedural pass by controlling the beam scanner to steer the surgical laser signal over the desired path while simultaneously controlling one or more of its characteristics , power, beam shape, focus, focal depth, spot size, trajectory, . The surface profiler performs an intra-procedural surface analysis of the surgical site and provides it to the processing circuit as real-time feedback on the effectiveness of the first pass relative to the desired outcome of the surgical procedure. If the result of the first pass does not satisfy the desired outcome, a second pass of the surgical laser signal through the surgical site is planned and performed, and the results relative to the desired outcome are assessed. In some embodiments, the second pass is planned based on a model derived from the measured response of the tissue at the surgical site to the first pass of the surgical laser signal. In the illustrative embodiment, the surface profiler is a laser triangulation sensor whose interrogation signal is provided to the beam scanner, which directs it over the surgical site. A portion of the interrogation signal is reflected from the surgical site and provided back to the laser triangulation sensor via the beam scanner. A determination of the range i. e. , depth for each of a plurality of points along the path is achieved by correlating the instantaneous output of the laser triangulation sensor, the instantaneous reflected signal received at the triangulation sensor, and the instantaneous position of the beam scanner at each point. In some embodiments, the interrogation signal is combined with the surgical laser signal to form a single composite beam that is scanned over the surgical site by the beam scanner. In some embodiments, a guidance signal comprising visible light is combined with the surgical laser signal to enable a surgeon to easily monitor the position of the surgical laser signal during a procedure in real time. In some embodiments, one or more of a variety of types of surface profilers, such as an interferometer, optical coherence tomography OCT device, high-resolution visible light stereo-vision imaging system ultrasound imaging system, etc. , are used to determine the range for each of the plurality of points in the surgical site. In some embodiments, one or more additional feedback modalities , other topological sensors, video imaging, multi-spectral imaging, optical coherence tomography, magnetic resonance imaging MRI, computed tomography CT, are used to evaluate the intra-procedural and post-procedural condition of the surgical site. In some embodiments, automated surgical path planning is employed to generate the desired path of the surgical laser signal through the surgical site. In some of these embodiments, automated path planning includes generating an estimate of the tissue parameters at the surgical site by creating a test feature, analyzing the surface of the test feature to form a topological map of its structure, and creating a model of the reaction of the tissue to the surgical laser signal based on this topological map. In some embodiments, a measured response of the tissue at the surgical site to a previous pass of the surgical laser is used to create or update a model of the reaction of the tissue for use in a subsequent pass. In some embodiments, validation of the intended surgical path by a surgeon is required before execution of a procedural pass is enabled. An embodiment in accordance with the present disclosure is a surgical system for manipulating tissue at a surgical site, the system comprising: a first laser operative for providing a first laser signal, the first laser signal being operative for manipulating tissue at the surgical site; a beam scanner that is configured to controllably direct the first laser signal to points within a first region of the surgical site; a surface profiler that is configured to measure a range for points within the first region; and a processing circuit that is configured to: 1 plan a first path for the first laser signal within the first region, wherein the first path is based on a post-procedural goal; 2 control the first laser signal along the first path; 3 generate a first intra-procedural model of the surgical site based on a measured range for each of a first plurality of points within the first region, wherein the range for each point of the first plurality thereof is measured after the first laser signal has traversed the first path; and 4 perform a first comparison of the post-procedural goal and the first intra-procedural model. Another embodiment in accordance with the present disclosure is a method comprising: planning a first path for the first laser signal within a first region of a surgical site, wherein the first path is based on a post-procedural goal; controlling the first laser signal along the first path; generating a first intra-procedural model of the surgical site based on a measured range for each of a first plurality of points within the first region, wherein the range for each point of the first plurality thereof is measured after the first laser signal has traversed the first path; and performing a first comparison of the post-procedural goal and the second intra-procedural model.",
                        "1 depicts a schematic drawing of an illustrative embodiment of an automated surgical system in accordance with aspects of the present disclosure. 2 depicts the stages of an exemplary automated laser surgery method in accordance with the illustrative embodiment. 3 depicts an exemplary implementation of a pre-procedural stage in accordance with the illustrative embodiment. 4 depicts an exemplary implementation of a preparation stage in accordance with the illustrative embodiment. 5 depicts an exemplary implementation of an intra-procedural stage in accordance with the illustrative embodiment. 6 depicts an exemplary sub-method suitable for estimating parameters for tissue of a surgical site and planning a desired path through the surgical site, in accordance with the illustrative embodiment. 7 depicts an example of a representative model fitting in accordance with the illustrative embodiment. 8 depict a series of cross sections of planned and realized serial cutting paths through a surgical site, respectively, in accordance with the illustrative embodiment."
                    ],
                    "DESCRIPTION": "1 depicts a schematic drawing of an illustrative embodiment of an automated surgical system in accordance with aspects of the present disclosure. System 100 is a laser-surgery system operative for manipulating tissue at a surgical site. For the purposes of this Specification, including the appended claims, the term tissue manipulation is defined as interacting with a tissue to effect a change, such as ablation, inducing necrosis in the tissue, inducing a chemical change to an ink or other foreign substance in the tissue, inducing coagulation, cutting, heating, illuminating, disrupting, and the like. Furthermore, the term surgical, as used herein, is intended to include any surgical or non-surgical medical procedure that can be performed within or outside of an operating room. System 100 includes processing circuit 102, surgical laser 104, guidance laser 106, beam combiner 108, surface profiler 110, beam scanner 112, and graphical user interface 114. In the depicted example, system 100 is specifically configured to controllably ablate soft brain matter along a desired surgical path at surgical site 116. Although the illustrative embodiment is a laser-surgery system configured to resect brain tumors via ablation, some systems in accordance with the present disclosure are well suited for use in other medical procedures, such as tattoo removal via laser-induced chemical breakdown of tattoo inks, laser-induced thermal necrosis for treatment of skin-cancer cells, removal of other soft pathological tissues, coagulation or ablation of vascular lesions and vascular structures, removal of both benign and malignant tissues, among others. Processing circuit 102 comprises processing circuitry, control circuitry, memory, and the like, and is configured to, among other things, provide control signals 118-1, 118-2, 118-3, and 118-4 to surgical laser 104, guidance laser 106, surface profiler 110, and beam scanner 112, respectively, receive measurement data from surface profiler 110, generate a desired path through surgical site 116 for surgical laser signal 120, store one or more pre-generated three-dimensional 3D maps of surgical site 116, utilize and tune a tissue manipulation model simulator, and generate an assessment of the success of a surgical procedure based on a comparison of the measurement data received from surface profiler and a stored 3D map of the surgical site. In the embodiment of 1, the processing circuit is implemented as a single, discrete component within system 100. In various other embodiments, the processing circuit can be distributed, at least in part, among multiple components of system 100, implemented, in part or in full, in a remote or cloud-based computing system, or otherwise implemented in a suitable arrangement for carrying out the functions described herein. Surgical laser 104 is a conventional carbon-dioxide CO2 surgical laser, which provides laser signal 120 to beam combiner 108. Laser signal 120 has a wavelength of approximately 10. 6 microns and is operative for ablating biological material to perform tissue resection. In some embodiments, surgical laser 104 is a different laser, such as a neodymium-doped yttrium aluminum garnet Nd:YAG laser, a Q-switched laser suitable for removal of tattoo ink, a pulsed-dye laser for treating basal cell carcinoma, and the like. The choice of source for surgical laser 104 is based on several factors, such as intended application, the material properties of the tissue to be manipulated, location of the tissue to be manipulated. Myriad lasers can be used in surgical laser 104 without departing from the scope of the present disclosure. Guidance laser 106 is a conventional visible-light laser, which provides laser signal 122. In the depicted example, guidance laser 106 is a helium-neon laser that emits red light having a wavelength of approximately 632. 8 nm; however, any laser that emits light at a visible wavelength can be used in guidance laser 106. In some embodiments, guidance laser 106 is not included. Beam combiner 108 is a conventional dichroic beam splitter that receives laser signal 120 from surgical laser 104 and laser signal 122 from guidance laser 106 and combines them into composite light signal 126. The addition of a visible-light signal to the surgical laser signal provides a visual indicator of where surgical laser signal 120 is incident on surgical site 116, thereby enabling a surgeon or other user to visually monitor the progress of the surgical procedure and intercede, if desired. Surface profiler 110 is a conventional laser triangulation sensor suitable for determining the range position along the z-direction of points within surgical site 116. Surface profiler 110 includes solid-state laser light source 134 and detector array 136 typically a CMOS/CCD detector array. Light source 134 provides interrogation signal 128, which is directed to surgical site 116 via conventional mirror 130 and beam scanner 112. A portion of interrogation signal 128 is reflected back from surgical site 116 as reflected signal 132, which is directed to surface profiler 110 via beam scanner 112 and mirror 130 and focused onto detector array 136 via suitable focusing optics. The position at which reflected signal 132 strikes the detector array is a function of the range i. e. , depth of the point on surgical site 116 on which interrogation signal 128 is incident. As the tissue at this point is ablated, the range at the point increases and the position of reflected signal 132 on the detector array shifts commensurately. In the depicted example, interrogation signal 128 has a wavelength of approximately 670 nm; however, myriad wavelengths can be used for interrogation signal 128. In some embodiments, all of laser signals 120 and 122 and interrogation signal 128 are combined into a single composite laser signal via conventional beam combiners. Beam scanner 112 is a conventional two-axis scanning-mirror system for steering composite signal 126 and interrogation signal 128 in two dimensions. In the depicted example, beam scanner 112 is a two-axis galvanometer mirror system; however, there are many two-axis beam steering systems suitable for use in beam scanner 112. Beam scanners suitable for use in embodiments in accordance with the present disclosure include, without limitation, two-axis gimbal-mounted mirrors, pairs of single-axis turning mirrors, MEMS beam-steering mirrors, and the like. In use, the beam scanner is positioned in close proximity to the surgical site, while bulky laser sources, processing circuits, computing systems, etc. can be located remotely. This mitigates sterilization issues and improves the visibility of the surgical site for the surgeon. Graphical User Interface GUI 114 is a hardware and/or software system that enables a user, such as a surgeon, to interact with path-planning software routines executed by processing circuit 102. GUI 114 is configured to enable the user to specify a classification for at least one region within a 3D model of the surgical site, such as classifying tissue in a region for special consideration , removal, protection, and the like. It should be noted that system 100 can also include various optical elements for manipulating and/or shaping light signals, such as collimating optics, focusing optics, spatial and spectral filtering optics, and the like. 2 depicts the stages of an exemplary automated laser surgery method in accordance with the illustrative embodiment. Method 200 can be implemented using system 100 of 1, and reference is made thereto in following description of 2-8B for the sake of clarity. However, other systems suitable for implementation of method 200 are also contemplated by the present disclosure. Method 200 is a surgery method suitable for resection of a brain tumor and includes three stages: pre-procedural planning stage 201, in which a post-procedural goal for surgical site 116 is developed; site preparation stage 202, in which surgical site 116 is prepared for surgery and system 100 is aligned relative to the surgical site to enable the desired procedure; and intra-procedural stage 203, in which one or more procedural passes of the surgical laser signal through the surgical site are performed and, after each pass, the surgical site is analyzed to determine whether the post-procedural goal has been achieved. Each stage of method 200 includes a series of operations, as discussed below. Pre-procedural stage 201 includes a set of operations that enable the development of a computer model representative of the desired outcome of a surgical procedure. 3 depicts an exemplary implementation of a pre-procedural stage in accordance with the illustrative embodiment. Pre-procedural stage 201 begins with operation 301, wherein a pre-procedural 3D image of surgical site 116 is transformed into a 3D model by processing circuit 102. For the purposes of this Specification, including the appended claims, the term pre-procedural is defined as referring to an operation performed before the surgical site is prepared for surgery. For example, a pre-procedural image is an image taken of a surgical site before the site has been exposed , via removal of a portion of a skull, and prepared for surgery. After pre-procedural stage 201, therefore, some change in the state of the surgical site might be expected due to site preparation. In the depicted example, the 3D model is based on a pre-procedural image of surgical site 116, which is generated using magnetic resonance imaging MRI. In some embodiments, a different conventional imaging modality is used to create a pre-procedural 3D image of the surgical site. Examples of alternative imaging modalities suitable for use in embodiments in accordance with the present disclosure include, without limitation, computed tomography CT, optical coherence tomography OCT, x-ray imaging, ultrasound, spectroscopy, microscopy, endoscopy, visible light camera, and the like. At optional operation 302, the 3D model is manipulated by a user , a surgeon, through GUI 114. This manipulation can include, for example, identification of important anatomical features in the model, such as blood vessels, nerves, etc. , delineation of critical functional regions within the surgical site, such as speech, motor function, sensory function, etc. that are to be protected from damage during a surgical procedure, confirmation or correction of computer generated mapping of the surgical site and boundaries, and the like. It should be noted that functional imaging, such as functional MRI, magnetoencephalography, electroencephalography, and the like, is sometimes used to identify functional areas with the surgical site. At operation 303, the surgeon employs GUI 112 to specify classifications for one or more tissue regions within surgical site 116. Such tissue classifications include, without limitation, tissue to be manipulated at the surgical site; tissue to be removed at the surgical site; tissue that should remain after surgery; tissue that should not be disturbed during the surgical procedure; healthy tissue that can be removed, if necessary, to access tissue designated for removal, to provide a safety margin, etc. ; and the like. At operation 304, a trajectory for surgical laser signal 120 is selected for each region of tissue to be removed, where the trajectory is based on the tissue identification performed in operation 303. In some cases, more than one trajectory is required at one or more regions in the surgical site in order to effectively remove unwanted tissue. The trajectory is defined as both a function of the location of the distal end of the beam scanner 112 relative to the surgical site 116 and also the position of the beam scanner actuators mirrors. At operation 305, processing circuit 102 generates a post-procedural goal that is a three-dimensional model of the desired state of surgical site 116 after the surgical procedure is complete. At operation 306, the user accepts or rejects the post-procedural goal. It should be noted that, typically, the user has the opportunity to modify the post-procedural goal at virtually any point in method 200. If the goal is rejected, pre-procedural stage 201 returns to operation 304 where a different trajectory is selected for at least one portion of surgical site 116. Using the different trajectory or trajectories, operations 305 and 306 are repeated and a new post-procedural goal is generated and evaluated. Once an acceptable post-procedural goal is generated, method 200 continues with site preparation stage 202. 4 depicts an exemplary implementation of a preparation stage in accordance with the illustrative embodiment. Preparation stage 202 begins with operation 401, wherein surgical site 116 is prepared for surgery. In the depicted example, site preparation enabling access to the surgical site , by removing a portion of the patient's skull, making an incision, creating an access hole, , removal of incidental tissue from the surgical site , opening the lining that surrounds the brain, , and the like. It should be noted that, in some embodiments , dermatological applications, nothing needs to be removed to expose the surgical site for interaction with the surgical laser signal. At operation 402, system 100 is positioned relative to surgical site to enable achievement of the post-procedural goal based on the anatomy of the patient. In some cases, system 100 must be calibrated based on input from the user and/or anatomical landmarks. Once surgical site 116 is prepared and system 100 is in place, method 200 continues with intra-procedural stage 203. 5 depicts an exemplary implementation of an intra-procedural stage in accordance with the illustrative embodiment. Intra-procedural stage 203 begins with operation 501, wherein a first intra-procedural scan of surgical site 116 is performed. For the purposes of this Specification, including the appended claims, the term intra-procedural refers to an operation at a surgical site that is performed after the surgical site is prepared for surgery. In fact, typically, an intra-procedural operation is performed while the surgical site is accessible to a surgical tool, such as a surgical laser signal. For example, an intra-procedural scan is a surface scan taken of the surgical site after the site has been exposed , via removal of a portion of a skull, and prepared for surgery. An intra-procedural operation, therefore, is performed at a time after which little or no change in the state of the surgical site would be expected due to site preparation. In the depicted example, the first intra-procedural scan of surgical site 116 is performed using the laser triangulation sensor of surface profiler 110 and beam scanner 112 to determine the range for a plurality of points within surgical site 116. To develop a surface image of surgical site 116, processing circuit 102 issues control signals 118-3 and 118-4 to surface profiler 110 and beam scanner 112, respectively, to generate interrogation signal 128 and sweep it over the plurality of points. At each point, light is reflected back to detector array 136, through beam scanner 112 and mirror 130, as reflection signal 132. The position at which the reflection signal hits the detector array is dependent upon the range i. e. , depth into surgical site 116 of the point from which it reflects. Processing circuit 102 receives range signal 138, which includes triangular-sensor data for each of the plurality of points. In order convert the triangular-sensor data into a range value for each point, each distance sample is correlated with the angular position of beam scanner 112 at the instant that sample was measured. In some embodiments, an intra-procedural scan of the surgical site is performed using any of a variety of alternative imaging modalities, including, without limitation, ultrasound, CT, MRI, 3D imaging, 3D surface scanning , via a non-contact surface profiler, , interferometry, conoscopic holography, visible light cameras, computer vision systems, and the like. It should be noted, however, that the use of a laser triangulation sensor affords embodiments in accordance with the present disclosure particular advantages over automated surgery systems of the prior art. For example, although an imaging modality such as intra-procedural MRI would enable full 3D image generation, it typically requires a prohibitive image-acquisition time and/or requires too large a footprint in the operating room. Furthermore, the speed at which intra-procedural MRI can image a surgical site is too typically slow for practical use. Three-dimensional scanning using a laser triangulation sensor, however, provides a low-cost imaging method, as well as reduced intra-procedural impact while still providing an acceptable level of precision. At operation 502, a 3D intra-procedural model of surgical site 116 is developed based on the first intra-procedural scan generated in operation 501. At operation 503, processing circuit 102 compares the intra-procedural model to the post-procedural goal generated in operation 305 to determine any differences between them. This enables the user to intra-procedurally delineate areas of the surgical site that contain unwanted tissue while accounting for any tissue shift or deformation that might have occurred since the post-procedural goal was generated , due to the act of opening the cranial cavity thus allowing the internal volume to change, movement of a surgical site 116 relative to the system 100 due to patient movement, . Thus, the intra-procedural model and pre-procedural model are aligned. At operation 504, a desired path for surgical laser signal 120 through surgical site 116 is planned based on the differences between the intra-operative model and the post-procedural goal. The desired path includes the set of points through which surgical laser signal 120 is to pass, as well as power levels and scanning speed of the laser signal as it progresses along the desired path. A laser trajectory to achieve each point in the path on the surgical site 116 is determined by processing circuit 102 and actuated by the beam scanner 112. In the depicted example, the desired path is determined via an automated path-planning sub-method, which calculates the path based on the post-procedural goal, the intra-procedural model, input from the surgeon via GUI 114, and an estimate of the reaction of the tissue at the surgical site to surgical laser signal 120. Tissue response to irradiation with a known laser energy, wavelength, and power profile is a function of many tissue parametersincluding, but not limited to, threshold radiant exposure, th, tissue density, , ablation enthalpy, habl, absorption coefficient, a, optical scattering coefficient, s, reduced scattering coefficient, s, refractive index, n, and scattering anisotropy coefficient, g. Furthermore, a descriptive model of laser ablation or, more generally, laser manipulation of tissue, may take into account any of such parameters. In order to plan a suitable path for surgical laser signal 120 through surgical site 116, therefore, it is necessary to have an understanding of the values of these parameters. Although the path for the surgical laser signal can be planned based on generally accepted estimates of the reaction of the tissue to be manipulated, improved performance and surgical success rates are expected for surgical procedures based on tissue parameters determined for the actual surgical site. 6 depicts an exemplary sub-method suitable for estimating parameters for tissue of a surgical site and planning a desired path through the surgical site, in accordance with the illustrative embodiment. Process 600 can be an implementation of step 504. In the depicted example, the path is planned based on a measured response of the tissue in surgical site 116 to irradiation with a known surgical laser signal. Operation 504 begins with sub-operation 601, wherein a model for a test feature formed at surgical site 116 is provided. In the depicted example, the model describes the material removed during a pass of surgical laser signal 120 through the surgical site. In some embodiments, the model is based on a different effect that can be achieved at the surgical site, such as a color change , for tattoo removal, and the like. Specifically, in the depicted example, the model predicts the surface profile of a test feature which is formed by ablating tissue while the surgery laser signal is stationary, thereby generating an ablation crater. A suitable model for an ablation crater profile can be described as: fit r = A * - dt E 0 e - 2 r 2 o 2 , 1 where =*habl, dt is the period of irradiation, A* is corresponds to a theoretical surface height of the tissue if the tissue radiant threshold were zero, r is the distance from the center of the beam, E0 is the peak irradiance value of the beam, and 0 is the 1/e2 spot size of the laser signal. This approach is based on the assumptions that the true value of the tissue radiant threshold, th, of surgical site 116, which is not zero, can be derived from parameters fit to Equation 1 of a theoretical tissue with zero radiant threshold and that the laser signal 120 is a Gaussian beam. It should be noted that Equation 1 includes two fitting parameters: A* and . Subsequent calculations based on these fitting parameters allows for accurate estimation of tissue parameters pertinent to a model. It should be noted that, while Equation 1 is one suitable model for an ablation crater, other suitable models are known and can be used in accordance with the present disclosure. Still further, a test feature can be formed using laser paths that are not stationary and therefore produce test features other than an ablation crater. At sub-operation 602, a test feature is formed at surgical site 116 using a known power level for surgical laser signal 120. At sub-operation 603, a cross-sectional profile of cutest feature is measured by performing an intra-procedural scan of surgical site 116 via surface profiler 110. At sub-operation 604, the cross-sectional profile is fit to the model of the test feature i. e. , Equation 1. At sub-operation 605, pertinent tissue parameters are extracted from the fit. In the depicted example, the tissue parameters extracted are and th. 7 depicts an example of a representative model fitting in accordance with the illustrative embodiment. Plot 700 shows the relationship between fitted model 702 and measured surface scan 704. Points 706 denote the points used for the fit of the measured data to the model represented by Equation 1. Values for and A*can be readily derived via the model fit depicted in plot 700. Using the values of and A*determined above, the radiant threshold of the surgical site, th, is calculated based on relative tissue surface heights from the model fit and the actual surgical site, which is given by: th = A * - A dt 2 where A is the initial surface height of the surgical site 116. At sub-operation 606, using the extracted values for and th, the model of the formed feature is updated to more accurately represent the ablation properties of surgical site 116. The model, in turn, enables development of a desired path for surgical laser signal 120 through surgical site 116 to more accurately form desired features. At sub-operation 607, the desired path is planned based upon the updated model. It should be noted that the sub-method described above is merely one suitable approach for extracting tissue parameters and planning a path for a surgical laser signal through a surgical site. Another suitable sub-method includes planning a desired path based on tissue parameters extracted via analysis of the difference between the surgical site before and after a procedural pass is performed. For example, in some embodiments, a new path is planned based on a comparison of the first intra-procedural scan developed in operation 502 and the second intra-procedural scan developed in operation 507 after the procedural pass performed in operation 506. Furthermore, in some embodiments, a desired path for the surgical laser signal is generated without employing an automated path planning process and/or extracting tissue parameters. For example, in some embodiments, a desired path is planned based on the difference between the intra-procedural model and the post-procedural goal or specifically designed by the user, such as a surgeon operator who plans the path manually or in semi-automated fashion wherein the path is generated automatically based on information about the tissue at the surgical site that has been gathered and then approved or altered by the surgeon operator. It should be further noted that sub-methods for extracting tissue parameters for the tissue of a surgical site and planning a procedural path based on those tissue parameters are suitable for use in applications outside the scope of automated or robot-assisted medical procedures. For example, an assessment of tissue parameters at a surgical site can be used to inform a path planned for a manually performed medical procedure, assist a surgeon in selecting the proper laser power, spot size, etc. , to employ for a surgical laser signal used to during a procedure, and the like. Returning now to intra-procedural stage 203, at optional operation 505, the planned path must be approved by a user, such as a surgeon, before moving on to operation 506. If the user does not approve the planned path, intra-procedural stage 203 returns to operation 502 and a new intra-procedural model is developed. At operation 506, processing circuit 102 steers surgical laser signal 120 as part of composite signal 126 along the desired path through surgical site 116 to perform a first procedural pass. At operation 507, a second intra-procedural scan of surgical site 116 is performed, as described above and with respect to operation 501. At operation 508, the second intra-procedural scan is compared with the post-procedural goal. If the second intra-procedural scan shows that the post-procedural goal has been satisfactorily achieved, surgery is deemed complete. Typically, a surgical goal is considered achieved if the results are within a pre-defined margin of acceptable error and the surgeon/operator is satisfied with the result. If, on the other hand, the second intra-procedural scan indicates that the post-procedural goal has not been achieved , unwanted tissue remains at the surgical site, regions of a tattoo remain untreated, untreated vascular regions remain, , intra-procedural stage 203 returns to operation 502 and operations 502 through 508 are repeated. In some cases, one or more changes are made to the trajectories used to access unwanted tissue when the second surgical procedure is performed. In some embodiments, each new procedural pass is based on a path that is generated based on the measured results of its preceding procedural pass. In such embodiments, the actual response of the tissue at the surgical site to a known laser signal informs the path planning process. This approach represents a closed-loop process that improves the accuracy with which a complete surgical procedure can be performed and improves the likelihood of procedural success without subjecting the patient to repeated, discontinuous procedures. Furthermore, the use of surgical planning software in conjunction with surgeon input through GUI 114 enables more accurate positioning of the surgical laser signal as compared with hand-held surgical methods, which reduces collateral tissue damage. It also enables more rapid and precise positioning of the surgical laser energy, which reduces surgical times and increases treatment accuracy, thereby reducing stress on the patient and freeing valuable operating room time for other patients. In some embodiments, the second intra-procedural scan is compared with the first intra-procedural scan so that the actual tissue response to the first surgical procedure can inform the path planning performed in operation 504. This enables the processing circuit to account for unexpected characteristics of the tissue as manifested during the first surgery. In some embodiments, a user is afforded oversight of the surgical process through a visual camera feed of the surgical site and tool operation, a digital representation of the surgical site and tool paths, or some combination of supervision modalities. 8 depicts a series of cross sections of planned and realized serial cutting paths through a surgical site, respectively, in accordance with the illustrative embodiment. Each iteration of the surgical plan removes one layer of tissue via a pass of the surgical laser signal along a path. Model 800 represents a cross section of an exemplary series of planned cutting passes through surgical site 116, where the planned approach includes four passes producing four cuts within a surgical site. The depth of the cut at the surgical site 116 is increased with each cut by approximately 0. 5 mm. This illustrates that with each iterative pass, tissue is removed. The specific path, cut dimensions, and pattern shown in 8 are merely exemplary and any practical treatment region, cutting path, pattern, and cutting dimensions can be used. Each pass within the series of cutting passes is established for surgical laser signal 120 set to an output continuous wave at a fixed 85% duty cycle, with a power maximal power of 11 W8% and a 1/e2 spot size of approximately 1. 75 mm at the surgical site located approximately 6 inches from the center of beam scanner 112. Continuous velocity control of composite signal 126 is achieved by providing discrete position commands that were closely spaced 0. 5 micron relative to the spot size of the laser signal. In the depicted example, the position commands are updated at a rate of 20 kHz. In the depicted example, each successive cut and laser duty cycle setting is tailored based on the difference between the depth of the intra-operative model of the tissue following the prior cut and the current target depth. Although the figures show a specific order of method steps, the order of the steps can differ from what is depicted. Also two or more steps can be performed concurrently or with partial concurrence. Such variation will depend on the software and hardware systems chosen and on designer choice. All such variations are within the scope of the disclosure. Likewise, software implementations could be accomplished with standard programming techniques with rule based logic and other logic to accomplish the various connection steps, calculation steps, processing steps, comparison steps, and decision steps. The construction and arrangement of the systems and methods as shown in the various exemplary embodiments are illustrative only. Although only a few embodiments have been described in detail in this disclosure, many modifications are possible , variations in sizes, dimensions, structures, shapes and proportions of the various elements, values of parameters, mounting arrangements, use of materials, colors, orientations, . For example, the position of elements can be reversed or otherwise varied and the nature or number of discrete elements or positions can be altered or varied. Accordingly, all such modifications are intended to be included within the scope of the present disclosure. The order or sequence of any process or method steps can be varied or re-sequenced according to alternative embodiments. Other substitutions, modifications, changes, and omissions can be made in the design, operating conditions and arrangement of the exemplary embodiments without departing from the scope of the present disclosure. As used herein, the term circuit can include hardware structured to execute the functions described herein. In some embodiments, each respective circuit can include machine-readable media for configuring the hardware to execute the functions described herein. The circuit can be embodied as one or more circuitry components including, but not limited to, processing circuitry, network interfaces, peripheral devices, input devices, output devices, sensors, etc. In some embodiments, a circuit can take the form of one or more analog circuits, electronic circuits , integrated circuits IC, discrete circuits, system on a chip SOCs circuits, , telecommunication circuits, hybrid circuits, and any other type of circuit. In this regard, the circuit can include any type of component for accomplishing or facilitating achievement of the operations described herein. For example, a circuit as described herein can include one or more transistors, logic gates , NAND, AND, NOR, OR, XOR, NOT, XNOR, , resistors, multiplexers, registers, capacitors, inductors, diodes, wiring, and so on. The circuit can also include one or more processors communicably coupled to one or more memory or memory devices. In this regard, the one or more processors can execute instructions stored in the memory or can execute instructions otherwise accessible to the one or more processors. In some embodiments, the one or more processors can be embodied in various ways. The one or more processors can be constructed in a manner sufficient to perform at least the operations described herein. In some embodiments, the one or more processors can be shared by multiple circuits , circuit A and circuit B can comprise or otherwise share the same processor which, in some example embodiments, can execute instructions stored, or otherwise accessed, via different areas of memory. Alternatively or additionally, the one or more processors can be structured to perform or otherwise execute certain operations independent of one or more co-processors. In other example embodiments, two or more processors can be coupled via a bus to enable independent, parallel, pipelined, or multi-threaded instruction execution. Each processor can be implemented as one or more general-purpose processors, application specific integrated circuits ASICs, field programmable gate arrays FPGAs, digital signal processors DSPs, or other suitable electronic data processing components structured to execute instructions provided by memory. The one or more processors can take the form of a single core processor, multi-core processor , a dual core processor, triple core processor, quad core processor, , microprocessor, etc. In some embodiments, the one or more processors can be external to the apparatus, for example the one or more processors can be a remote processor , a cloud based processor. Alternatively or additionally, the one or more processors can be internal and/or local to the apparatus. In this regard, a given circuit or components thereof can be disposed locally , as part of a local server, a local computing system, or remotely , as part of a remote server such as a cloud based server. To that end, a circuit as described herein can include components that are distributed across one or more locations. The present disclosure contemplates methods, systems and program products on any machine-readable media for accomplishing various operations. The embodiments of the present disclosure can be implemented using existing computer processors, or by a special purpose computer processor for an appropriate system, incorporated for this or another purpose, or by a hardwired system. Embodiments within the scope of the present disclosure include program products comprising machine-readable media for carrying or having machine-executable instructions or data structures stored thereon. Such machine-readable media can be any available media that can be accessed by a general purpose or special purpose computer or other machine with a processor. By way of example, such machine-readable media can comprise RAM, ROM, EPROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to carry or store desired program code in the form of machine-executable instructions or data structures and which can be accessed by a general purpose or special purpose computer or other machine with a processor. Combinations of the above are also included within the scope of machine-readable media. Machine-executable instructions include, for example, instructions and data which cause a general purpose computer, special purpose computer, or special purpose processing machines to perform a certain function or group of functions.",
                    "IMAGE": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAEBAMgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKxP7P1NNQ1KVZlkhuJFeBTOymMeWqMBwcchmGO5/GqraRrjeFp9La+Q3eQIbsSMrEb8/NgcHaMZHB9BVeTSfFsLtFZavCbaEKLc3B3SPiNgTIdnJ3FT+H4VbsbHxKmrpLfanBLZI7HykUKWUhgAfl9dp6+tdFWJeDxJHeSNZGwltmcbFlDBkHGc47dT6+1WSmqi/MnmRG1JOIweQCq4529Qwf6hh6VRtrbxMskf2m9tnQMu7YADjK5z8nJwHHGOvbt0FYl1F4kGou9ncWDWhPyxzq24DjuOvQ/n7VPbprnlxtcS2fmjeXWMNtPyjaOeeGzn2plpDrouLZ7y6tTHmQzxxqeQQNgUkZ4Oc59a16KxvsmtfbA4vY/JF0XKkZ3QnHy9OCOec//WintfEPl3S295BvkSRYWk6REu5RsbecKUX8M894GsPFC/ami1OAmaN1hSRciFicq2cZbHT8R6cyx2XiQ2V2k2oWxuXgZYXjXCpJuYhsY9CB+H41c0W31S3ilGqXKTyHbtZWyOFAJxtGMnJxzWpRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRVS41CK2vLe1aOYvOcKyodg69W6dunWrdFFFFFFFFFFFFFFFFFFFFFFFYOs3LrrukQRSShi5Z0jbAK5Ayw7jr/ng67C4jBYSRuBzh12/qP8KQXkRg83J6Z2fxfSq/8AbVh0eYxn0kjZf5ipP7W0/GftsA+rgUw6zZZISR5cd4omcfmBij+1ov4be5P/AGzx/OrEV3HLGGOYz/dfANSq6uMqwI9qdRRRRRRRRRRRRRRRWFq6vJrmmDfAio4ZSzkO2TyoH0A/zzW7TZJEiXdI6ovqxwKqTajH5Ti0ZLi42ny40OQW7ZIzge9ZdsmrL4wE13dK1q+nRobWJTsjm3Es+T64wPoK6GiiiiiiiiiiiiiiiiiiisDWIHm8QaSQGZI2LHCZC8jnOOM/0+lb9Z8FzZXeqTokvmzwDaUKHEfrjPc/4VoVzmravaad4s0mzn8zz787IAnTKhyd3thvzxXR0UUUUUUUUUUUUUUUUVGtxA8zQrNGZVGWQMNw+oqSiisfTpbca3qFvG5aZTvf93gDPOM5OevtWxWBraf8TK1kEZcoAwUHBJB45+pFb9FFFZ11qoj1A6daxie9WITtCX2YjJKhgTwfmGMVo0UUUUUUUUUm5d23Iz6ZpaKw7Oyii8UXlx5sfnMmdgb5tp28kY6cdv8A9W5RRWBpiQxeJdRQGfziNzbkAUgnIwR1xn9a36xdaQPPGGXKmP5h7ebHn9Ca1bZi1rCzKFYoMqGzg46Z71LRWdd6xHa3sVqlpd3LPnc1vGHWLA/jOeM9qitZmn1uSZoJYFa3WNfOAUsQzE4GecAitaiiiiiiiiisS2swniq6ui9uHaMDarDzCuByRjpkHmtuiuet8N44um2HcLYLuwRjkfh3+tdDRRXP2Uco8YX5DkJsBKkZBBC9Dn5Tkfj+FdBWTrAO5iOos5yPqChH8qu2OBbbVVFCuwAVsgDccfpjirNFRR20EUskscMaSS4MjqoBfHqe9NubYXCZUqk6giObYGaMnqRmnwJJHCqSzGZx1cqAT+A4qSiiiiiiiiuf2EeOPN3Jta1KYEgJyCDyuc9+tdBRWLBBL/wlM9wUZFaIoRxggbdp988/Stqiiqtvptpa3MtzDFtllJLtuJzk5PX3q1VC8QSahbof44Zl/PbTdHfdaAkICyxyfKeTlF5PvkH8q0aKKKKKKKKKKKKKwBcmPxoYDcusbwf6pj8pb2z3wDxW7JLHCheV1RB1ZjgClR1kQOjBlYZDA5BFc9byA+OrpEcEfZRvAHQ5HfP6VvySxwpvlkRF/vMcCnKyuoZSCpGQQcgilooqnccanZH1Eg/Qf4VX0n5AEPlj5Cg/vHY7L+XI/OtSmSSxwxtJK6oijJZjgCljkSVA8bq6noynIp1FFFFFFFFFFYawTP4tac2zLCISm9sEMRjBHp1xWhqlmb6waAAEllOC5XoQeoBI/CnabEINNt41feBGMN6/oP5UxbW2h1bzY7aJZZY2ZpQvzHBXv+P6VHrdiNQsPKKqVVw53MVGB34B6dcVas4xDY28QbcEjVd3rgdanooqndcX9gf+mjj/AMcP+FV7Rtl0VJjGLiaPkfMd3z4B/WtSquooZNOuFBAOwkE54xzngE/pTdLEP9nRGBo3jOWDR9Dkkk/nVyiiiiiiiiiqk180d0beO1mncIHbYUAAJIH3mHoartcXTXkcv9l3O1EYffi6kj/b9qkbUmjdVmsp4d4bazlCMhS2PlYnoDVmzTy7KBD1WNR+lUru6aHUWSJQ0xgBXccJGMnczHsOn1/Mh8dzM+kTTSrllRtrBdvmADhsds+lXo08uJE/uqBTqKKqXf8Ax92B/wCmzD/yG9YOuapc6Zcj7ElvLM+owb4pOXMTKFcoB1YDJ+ma09O1Ca6v5EMsEiBcukbA+S3Zcj7xx19Ku6hk6bdY6+S/8jVLwzs/4R60EYwoDDG0jBDHPBJrWooooooooorNmla21aSVoJ3jeBFDRxluQzEjj6ihtbtluUt2huhNIjOkZgbLKpAJHsNy/mKhu8apc2cXk3kaJIzO21o8DYw6/UiqWvaRBJZ29sL/AFKB2lUxvHfPGAVGfmbPTAPHU/rVezkN1rzaNJbXhszZi6N6+Qbh923DHtgAED0refSYZEZGuLwqwwR9pfkfnUMkFlDOkEl9dLI+Nqm5fnJwO/c1M2mxKpZrq7CgZJNy+B+tczres2mjrbXU17eR280yxQxB5XknBIBfA5VQOc45/EV0BsJHLCM3i4YDdLdsAR3IAJP54rG1D+zFvYrSTXLia6Sf5raG8PmqDG20BQd3p19fSs2OzPhPS0FjDf3sonht5HaUzPAruFIBPV8MST24HoD3AsLUQpD5CbE+6CMke+eufeq19aJHp10VnljHlNksTIAMc/KTzxRoaImjwCOR5FO4hpBhjlieeTzWjRRRRRRRRRRWPc2FrJ4u0++eIG5is50STJ4BaPIx071NqWpy2NzBGkAkSTG5vm4ywXsCO/eppXeVCk2ntJHkHBKN05BwTVWGcSeInJRo8Wg+/jn5z6Gtas270lbvVLe8YoDCuFOG3A5zxzgfkaoSs11etCZJZ9jlBbuwHmEfxOAAAg4PT5vyBpeFLe81S2n1LVRPBeySvFIqTjojsAPk6DGOM+tdD/ZNkTmSHzT/ANNnaT/0Imq8nh/T/tQu7WCKzu+jTwRKrsuMEE49OnpTtShjt9MhiiUKi3NuAB/12StOqmps6aZclInlYxlQiDJOeKqeGoJbfw/axTRGKRQ25CpGPmPY1rUUUUUUUUUUVQm/5D9n/wBes/8A6FFVmW1gmmjlkiV5I87GIztzjp+QqHV/+QLff9e8n/oJrCktLNfFf2IafZG3a08zb9nTIbJ9s1u6V/yB7H/r3j/9BFXKbtRWaTChiOWxzgf5NZfhzYdGSRFVRJNNIdpyCTIxzmtaiqGsf8eKf9fNv/6OSp729h0+1e5uC4iTliiFyPwANTqwdFYZwRkZGDS0UUUUUUUUUUVQm/5D9n/16z/+hRVJqOoQaXZtdXG/y1IB2jJ5qnqOo20uh3TbmTzLVyodSucoeM9CfpUUrofFCxsjlxbllf5ABkYx03HpnrWjpX/IHsf+veP/ANBFR3+qw6dNDHKjsZeF2lfUDoTk9e1Lc30DQtCsyJLIRGglQjJJx0I5qnpSXX9ltHEITGZJlUZKlR5jfn+lP8PWE2n2LxzB1LPuCuQWHyjOSCR1z0rXrO1rd/Z67CA32m3wWGR/rkqDWIb2505oSSu51Ba2LEsO+Rjp+NalupS2iVmZmCAEt1Jx396koooooooooooqhN/yH7P/AK9Z/wD0KKpr2zS+t/JkZlG4NlMZ4+oNQ6jEsHh+7iQYRLV1UewU1luz/wDCbY6KbMjGOo9fbmreyWTwrbrD5nmGCLHl9ei+4qeLTkkt7QztMXhXGGb73IPzdc8gd6kmuoG1SCxFzELgIZ2gJBdkHy5x2GSOfamaJ/yDB/12m/8ARjVoUVQ1j/jxT/r5t/8A0clX6KKKKKKKKKKKKKoTf8h+z/69Z/8A0KKr9VtRiefTLuGJd0kkLqq5xklSBXPfay3ipg1veNdC2Mi2x8rcsZO3P+sxjIPbPNX9QSe28HPF5eZUtFR1BHHygN7cc1a0Mt/YVmWcu3lDJbqTXIrqMDazDeCa4GqyQRQSXBji2iPO9lUkccsf0z0rZ8M6ktzdSW0Etw0CIzkXAj3Bi+eNnY5J+mK2dU1KPSrI3UkbyKGC7Uxnn6mrFtOtzaxTqCFkQOAeoyM1V1j/AI8U/wCvm3/9HJV+iiiiiiiiiiiiiqE3/Ifs/wDr1n/9Ciq/RWQLS2/4S9rzyI/tP2ER+dtG/bvJ259M9qm12PzdEu1Mjxr5ZLFF3EgckYyM5HHXvSWF9F/ZtufLmCGMciAjHtgZx9Kw4NKuLvTdPmWzO5VjkBa6IGMLnAxxkD860NNtGtdednhaNpLbktN5hbBUZzita+ijktWMjMqp+8yrEEY56ipLddttEuzZhANuc446VjRxanq+l20zXlrEJfLnCi1Y4IYOBnzOemKuQzXsWqR2tzNBKkkLyAxwlCCrKO7HP3v0rRqK5uY7S3eeUOUQZOxC5/IAmlgmS4t45492yRQy7lKnB9QelSUUUUUUUUUVQm/5D9n/ANes/wD6FFV+is4f8jG3/XoP/QzTtbWR9EvFidUcxEAt0Pt+PSm6CFGg2YTdt8oY3dazL7VLrw1pditzHbOjSwWSMrPksxCgkbeB3P0q+2mS3t8s9+EVY4yirBNIpJJBySMelSNodi6lWW4KkYIN1LyP++qrX3hTStRWFbhbsiGUTJtvZl+YZx0b3qhcXs2m+CtPuLeQxukcQzwcjb054rVuZXTXbNlheXNrNkIRx80fqRUF1c3z6xaJAJ44XCl1KcYDHdng4yMd/wAq0721F7atAzBQxByVDdCD0II7UWRBs49owoGBwBkA4zwAOasUUUUUUUUUVQm/5D9n/wBes/8A6FFV+isy5jvItVF1b2yzo0HlkeaEIO7Pce9MuGvLuBobjR0kjbGVa4GDj8KRbq8s44oY9IVELbVAuF4PJ9KLpr+7g8p9NKjcrBkugCCCCO3qKpXCapHEGWK7B3oP+P1e7Af3fepvI1P/AJ43f/gav/xNHkan/wA8bv8A8DV/+JrPvQT4F0xTkMVhx8+Dnb6/5z7dRvP/AMh60/69Jv8A0KKtCsto3udSuw93PFHEqYEbhQMgknpUU9t9jso5ba+uWVJI1UGUMpG9QR09K2aKKKKKKKKYZY1lWIuokYEqhPJAxkgfiPzqtd2L3FxDcRXUlvJGjJlFVshipOcg/wB0VH9hvv8AoLz/APfmP/4mj7Dff9Bef/vzH/8AE1D9m1A3nkjVptoj3MfJj6k4H8Psam+w33/QXn/78x//ABNV7iyvTcWqnVpuXJ/1UfZT/s1Y+w33/QXn/wC/Mf8A8TVe7sr4CEHVpsNKoP7mP1z/AHfUCrH2G+/6C8//AH5j/wDiaPsN9/0F5/8AvzH/APE1U07TYb7QdJM5k3wRRujq205wOeKi16xvtQ1C3g0/VZNMnNtKRcRxJIcb4+MMMVdl0/UHhdE1qdHZSFfyYztPr92qGnWN9bWmo2l1fNqd55KKZpY0jMh2nqqjbToLSay8O+TOmxvtKEDjgGVccDgfQV0FFFFFFFQPPIt0sS2sjIRkygrtX25Of0ovbpLKzluXUssYyQvU1n6ba2F7fHxCkDC8mh+z736rGGztx/vDPr+Va9FFZN3emDXbWAfaP3q4ISNSmOerYz+HpzWtWP4guntLeGSN51cuQpijD8kdTn05+ta6/dHXp3rO12drbTTKpmBDDHkqGJPbr0Ge9XLRi9nA7MzFo1JLjBPHcdjU1UP7D0nP/INtP+/K/wCFS22m2NnIZLa0ghcjaWjjCkj04q1VF7S6W9mnt7iFBKFBWSEtgjPcMKZNZ39ygjlurby96s2y3YE7WBwCXPpWjRRRRRRWHfSXD+JLS3jmYRhVkdA6gAAnOVJy2ePpgVr3Nul1bSQSFgki7TtODS28EdrAsMQwi9BnNSUUVWeyjkvUuizB0XaAMY6/TP61Zqtd2aXnlb2ZfLbcNuOTgjuD69qs1Bd2qXkIidmUbg2Vxng57g1KihEVBkhRjmnUUUUUUUUUUUUVE9tbvOs7wRtMgwshQFh9DUtFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf/9k=",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/61/394/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.967348575592041,
                    "F_SPEC_PARAMS": [
                        "life-threatening",
                        "accuracy",
                        "precision"
                    ],
                    "S_SPEC_PARAMS": [
                        "cannot perform at the desired level of precision",
                        "cost",
                        "higher level of automation"
                    ],
                    "A_PARAMS": [
                        "advanced level of robotic assistance"
                    ],
                    "F_SENTS": [
                        "Unfortunately, in many instances, a life-threatening condition is deemed inoperable because its surgical treatment is beyond the ability of even state-of-the-art laser treatments.",
                        "Furthermore, some procedures require a greater accuracy or precision than is possible within the limitations of the human hand and physiologic tremor, such as dissecting a tumor from a cranial nerve or critical vascular structure, removing a tumor from a highly eloquent or functional brain tissue that controls a delicate function, or manipulating tissue on the brainstem."
                    ],
                    "S_SENTS": [
                        "Furthermore, even with the advanced level of robotic assistance currently available in the operating room, a surgeon cannot perform at the desired level of precision while constrained by a timeframe so heavily weighted by cost.",
                        "Some of the basic surgical decisions must be yielded to the assisting robot, thereby giving it a higher level of automation than previously demonstrated in commercially available surgical robots.",
                        "While incremental improvements in patient outcomes can be realized by refining neurosurgical techniques, more meaningful advances must be achieved through advancing robotic assistance and imaging in the operating room."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Side Effects",
                        "Accuracy of Measurement"
                    ],
                    "F_SIM_SCORE": 0.5920113921165466,
                    "S_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Productivity",
                        "Level of Automation"
                    ],
                    "S_SIM_SCORE": 0.6555087566375732,
                    "GLOBAL_SCORE": 1.671108649969101
                },
                "sort": [
                    1.6711086
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11338442-20220524",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11338442-20220524",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-06-12",
                    "PUBLICATION_DATE": "2022-05-24",
                    "INVENTORS": [
                        "Takayuki Ogawara"
                    ],
                    "APPLICANTS": [
                        "CANON KABUSHIKI KAISHA    ( Tokyo , JP )"
                    ],
                    "INVENTION_TITLE": "Robot apparatus, control method for robot apparatus, article manufacturing method using robot apparatus, and storage medium",
                    "DOMAIN": "B25J 91694",
                    "ABSTRACT": "A robot apparatus which does assembly by tilting a workpiece which is to be a fitting component, and bringing a part of a surface of the workpiece which is the fitting component and a part of a surface of a workpiece which is to be a fitted component, into contact with each other a plurality of times, when inserting the workpiece which is the fitting component into the workpiece which is the fitted component.",
                    "CLAIMS": "1. A robot system that performs a task while maintaining contact between a first object and a second object, the robot system comprising: a robot; and a control device configured to control the robot system, wherein the control device is configured to operate the first object by the robot, and configured to, after bringing a part of a second surface of the first object and a part of a second surface of the second object into contact with each other by moving the first object in a direction in which the part of the second surface of the first object and the part of the second surface of the second object come close to each other while maintaining a state where a part of a first surface of the first object is along a part of a first surface of the second object, bring the part of the second surface of the first object along the part of the second surface of the second object by executing a rotation to rotate the first object around an axis extending in a direction along with the second surface of the first object or the second object while pressing the part of the second surface of the first object against the part of the second surface of the second object. 2. The robot system according to claim 1, wherein the control device inserts the first object into the second object, while maintaining a state where the part of the first surface of the first object and the part of the first surface of the second object are in contact with each other, and the part of the second surface of the first object and the part of the second surface of the second object are in contact with each other. 3. The robot system according to claim 1, wherein the control device is configured to rotate the first object when bringing the part of the first surface of the first object and the part of the first surface of the second object into contact with each other. 4. The robot system according to claim 1, wherein the control device is configured to tilt the first object by a predetermined amount by operating the robot, and when the first object is tilted by the predetermined amount, the first object is tilted so as to bring a predetermined corner portion of the first object to a position closest to the second object. 5. The robot system according to claim 1, wherein the part of the first surface of the first object is a two-dimensional area, and wherein the part of the first surface of the second object is a two-dimensional area. 6. The robot system according to claim 1, wherein the first object is operated by the robot, and the part of the second surface of the first object and the part of the second surface of the second object are brought into contact with each other by moving the first object in a direction in which the part of the second surface of the first object and the part of the second surface of the second object come close to each other while maintaining a state where the part of the first surface of the first object is along the part of the first surface of the second object and not rotating the first object. 7. The robot system according to claim 1, wherein the control device is configured to determine that a contact interference occurs between the first object and the second object in a case where an amount of change in posture of the first object does not reach a threshold even if the rotation is executed a predetermined number of times, and separate the first object from the second object. 8. The robot system according to claim 7, wherein the control device is configured to stop the robot in a case where the first object cannot be moved to a predetermined position within a predetermined time when the control device is separating the first object from the second object. 9. The robot system according to claim 1, further comprising: a force detection unit configured to detect a force, wherein the control device is configured to bring the part of the second surface of the first object and the part of the second surface of the second object into contact with each other by moving the first object in a direction in which the part of the second surface of the first object and the part of the second surface of the second object come close to each other while maintaining a state where the part of the first surface of the first object is along the part of the first surface of the second object based on a detection value of the force detection unit. 10. The robot system according to claim 1, wherein the second object includes an opening, and the first surface and the second surface of the second object are surfaces to form the opening. 11. The robot system according to claim 1, wherein the robot includes a robot arm and an end effector. 12. A control method for a robot system that performs a task while maintaining contact between a first object and a second object, the robot system including a robot, and a control device configured to control the robot system, the control method comprising: operating the first object by the robot, and after bringing a part of a second surface of the first object and a part of a second surface of the second object into contact with each other by moving the first object in a direction in which the part of the second surface of the first object and the part of the second surface of the second object come close to each other while maintaining a state where a part of a first surface of the first object is along a part of a first surface of the second object, bringing the part of the second surface of the first object along the part of the second surface of the second object by executing a rotation to rotate the first object around an axis extending in a direction along with the second surface of the first object or the second object while pressing the part of the second surface of the first object against the part of the second surface of the second object. 13. The control method according to claim 12, wherein, in the second contact, the control device is configured to rotate the first object after the part of the first surface of the first object and the part of the first surface of the second object are brought into contact with each other. 14. The control method according to claim 12, wherein the control device is configured to tilt the first object by a predetermined amount by operating the robot, and when performing the tilting, the first object is tilted to bring a predetermined corner portion of the first object to a position closest to the second object. 15. A non-transitory computer-readable storage medium that stores the control method according to claim 12. 16. An article manufacturing method using a robot system that performs a task while maintaining contact between a first object and a second object, the robot system including: a robot; and a control device configured to control the robot system, the article manufacturing method comprising: operating the first object by the robot, and, after bringing a part of a second surface of the first object and a part of a second surface of the second object into contact with each other by moving the first object in a direction in which the part of the second surface of the first object and the part of the second surface of the second object come close to each other while maintaining a state where a part of a first surface of the first object is along a part of a first surface of the second object, bringing the part of the second surface of the first object along the part of the second surface of the second object by executing a rotation to rotate the first object around an axis extending in a direction along with the second surface of the first object or the second object while pressing the part of the second surface of the first object against the part of the second surface of the second object; and inserting the first object into the second object. 17. The article manufacturing method according to claim 16, wherein, in the inserting process, the control device inserts the first object into the second object, while maintaining a state where the part of the first surface of the first object and the part of the first surface of the second object are in contact with each other, and the part of the second surface of the first object and the part of the second surface of the second object are in contact with each other. 18. A robot system that performs a task while maintaining contact between a first object and a second object, the robot system comprising: a robot; and a control device configured to control the robot system, wherein the control device is configured to operate the first object by the robot, and configured to, before bringing a part of a second surface of the first object and a part of a second surface of the second object into contact with each other by moving the first object in a direction in which the part of the second surface of the first object and the part of the second surface of the second object come close to each other while maintaining a state where a part of a first surface of the first object is along a part of a first surface of the second object, bring the part of the first surface of the first object along the part of the first surface of the second object by executing a rotation to rotate the first object around an axis extending in a direction along with the first surface of the first object or the second object while pressing the part of the first surface of the first object against the part of the first surface of the second object. 19. The robot system according to claim 18, wherein the control device is configured to determine that a contact interference occurs between the first object and the second object in a case where an amount of change in posture of the first object does not reach a threshold even if the rotation is executed a predetermined number of times, and separate the first object from the second object. 20. The robot system according to claim 19, wherein the control device is configured to stop the robot in a case where the first object cannot be moved to a predetermined position within a predetermined time when the control device is separating the first object from the second object. 21. A control method for a robot system that performs a task while maintaining contact between a first object and a second object, the robot system including a robot, and a control device configured to control the robot system, the control method comprising: operating the first object by the robot, and, before bringing a part of a second surface of the first object and a part of a second surface of the second object into contact with each other by moving the first object in a direction in which the part of the second surface of the first object and the part of the second surface of the second object come close to each other while maintaining a state where a part of a first surface of the first object is along a part of a first surface of the second object, bringing the part of the first surface of the first object along the part of the first surface of the second object by executing a rotation to rotate the first object around an axis extending in a direction along with the first surface of the first object or the second object while pressing the part of the first surface of the first object against the part of the first surface of the second object.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to a robot apparatus.",
                    "STATE_OF_THE_ART": "In recent years, in order to meet diversified needs of consumers, production automation at a production site requires a robot apparatus that can produce an article by holding various objects and assembling one object to another object. However, in a case where an article is assembled by a robot apparatus, the relative positions of an object to be operated on by the robot apparatus and an object to be assembled thereto need to be accurately managed in order to increase a chance of assembly success. Therefore, a positioning device that accurately determines the position of the object to be assembled is necessary, but this leads to a cost increase. It is therefore desirable that the chance of assembly success can be increased without using such a special apparatus. Given these circumstances, Japanese Patent Application Laid-Open 2009-125904 discusses a related technology. In this technology, a fitting component held by a hand is tilted by a predetermined amount, and one corner portion of the fitting component is inserted into an opening portion of a component to be fitted. Subsequently, the corner portion is brought into contact with an inner surface of the component to be fitted by moving the fitting component in a Y direction. The fitting component is then moved in an X direction therefrom. The corner portion of the fitting component and a corner portion of the opening portion of the component to be fitted can thereby coincide with each other. The tilted fitting component is then returned to the original position, with reference to the coinciding corner portions of both components. This makes it possible to align the position of an inserted surface of the fitting component and the position of the opening portion without using a positioning device, so that the chance of assembly success can be increased. However, in the technology discussed in Japanese Patent Application Laid-Open 2009-125904, the relative positions of the fitting component and the component to be fitted are aligned by making the corner portions of the respective components coincide with each other. Accordingly, no issue arises if the corner portions of the fitting component and the component to be fitted are angular. However, it is sometimes difficult to align the corner portions of the respective components if the corner portions are round.",
                    "SUMMARY": [
                        "The present disclosure is directed to a robot apparatus that can increase the chance of article assembly success without using a positioning device, even when objects take various shapes. According to an aspect of the present disclosure, a robot apparatus that performs a task while maintaining contact between a first object and a second object, includes a robot arm, a force detection unit configured to detect a force, and a control device configured to control the robot apparatus. The control device is configured to tilt the first object by a predetermined amount by operating the robot arm, bring the first object and the second object into contact with each other, bring a part of a first surface of the first object and a part of a first surface of the second object into contact with each other based on a detection value of the force detection unit, and bring a part of a second surface of the first object and a part of a second surface of the second object into contact with each other in a state where the part of the first surface of the first object and the part of the first surface of the second object are in contact with each other. Further features of the present disclosure will become apparent from the following description of exemplary embodiments with reference to the attached drawings.",
                        "1A, 1B, and 1C are schematic diagrams illustrating a robot system according to a first exemplary embodiment. 2 is a schematic diagram illustrating a robot arm main body according to the first exemplary embodiment. 3 is a control block diagram illustrating the robot system according to the first exemplary embodiment. 4 is a control flow chart according to the first exemplary embodiment. 5A to 5I are state diagrams illustrating steps in 4."
                    ],
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/42/384/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9800339937210083,
                    "F_SPEC_PARAMS": [
                        "accurately managed",
                        "chance of assembly success"
                    ],
                    "S_SPEC_PARAMS": [
                        "difficult to align the corner portions of the respective components if the corner portions are round"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "However, in a case where an article is assembled by a robot apparatus, the relative positions of an object to be operated on by the robot apparatus and an object to be assembled thereto need to be accurately managed in order to increase a chance of assembly success."
                    ],
                    "S_SENTS": [
                        "However, it is sometimes difficult to align the corner portions of the respective components if the corner portions are round."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Productivity",
                        "Reliability"
                    ],
                    "F_SIM_SCORE": 0.5411771982908249,
                    "S_TRIZ_PARAMS": [
                        "Shape"
                    ],
                    "S_SIM_SCORE": 0.5702700614929199,
                    "GLOBAL_SCORE": 1.669090956946214
                },
                "sort": [
                    1.669091
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11419616-20220823",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11419616-20220823",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2020-03-17",
                    "PUBLICATION_DATE": "2022-08-23",
                    "INVENTORS": [
                        "Hayden Cameron",
                        "Spiros Mantzavinos",
                        "Neil R. Crawford",
                        "Sanjay Joshi",
                        "Norbert Johnson"
                    ],
                    "APPLICANTS": [
                        "GLOBUS MEDICAL, INC.    ( Audubon , US )"
                    ],
                    "INVENTION_TITLE": "System for neuronavigation registration and robotic trajectory guidance, robotic surgery, and related methods and devices",
                    "DOMAIN": "A61B 171695",
                    "ABSTRACT": "A system of robotic surgery includes components capable of drilling a bore in the cranium of a patient in connection with craniotomy and other cranial surgeries. A perforator associated with such system is controlled by suitable computer-implemented instructions to maintain the perforator tip along a desired trajectory line while moving the perforator bit at locations proximal to such perforator tip in a circular motion, thereby imparting a conical oscillation to the perforator bit relative to the trajectory line. The angle at which the perforator bit is oscillated relative to such trajectory line results in the bore formed in the cranium having a diameter larger than the bit diameter, and the larger diameter and related conical oscillation is selected so as to reduce frictional force opposing withdrawal of the bit from the situs of the bore, thereby reducing the risk of jamming of the bit during its associated operations.",
                    "CLAIMS": "1. A surgical robot system for drilling a hole in a cranium of a patient in connection with cranial surgery, the system comprising: a surgical robot; a robot arm connected to the surgical robot; an end-effector connected to the robot arm and orientable to oppose the cranium so as to be in operative proximity thereto; a perforator connectable to the end-effector and configured to be advanced or withdrawn along a trajectory line relative to the cranium, the perforator having an elongated perforator bit terminating in a sharp, perforator tip, the bit having a bit diameter, the perforator having a clutch operable to engage rotation of the bit in response to resistance during advancement of the bit and further operable to stop bit rotation in response to detection of penetration past an internal wall of the cranium; a processor circuit; a memory accessible by the processor circuit and comprising machine-readable instructions; wherein the machine-readable instructions, when executed, cause the perforator to maintain the perforator tip along the trajectory line, and further cause the perforator bit to move in a conical oscillation relative to the trajectory line during the advancement of the bit into the cranium, to form a substantially circular bore having a diameter larger than the bit diameter by an amount to reduce frictional force opposing withdrawal of the bit from the bore after the penetration of the inner wall of the cranium, whereby the risk of jamming of the bit during cranium perforation is reduced. 2. The system of claim 1, wherein the machine-readable instructions include instructions which, when executed, cause the conical oscillation of the perforator to occur at an angle relative to the trajectory line ranging from about 1 to about 3. 3. The system of claim 1, wherein the machine-readable instructions include instructions which, when executed, cause the conical oscillation of the perforator to occur at an angle relative to the trajectory line of about 2. 4. The system of claim 1, wherein the system includes a user interface and machine-readable instructions comprising a perforator mode, and wherein the machine-readable instructions for causing the conical oscillation of the perforator are executable in response to user selection of the perforator mode through the user interface. 5. The system of claim 1, wherein the end-effector is connected to the perforator and wherein the end-effector is configured to be manually advanceable down the trajectory line by an amount sufficient to cause the perforator to penetrate the cranium and perforate the inner wall thereof, and wherein the end-effector is manually withdrawable up the trajectory line after perforation of the inner wall. 6. The system of claim 5, wherein the robot arm includes a robot wrist and a load cell operatively connected to the robot wrist, the load cell configured to sense reactive force corresponding to the perforator bit engaging the cranium, and further configured to detect reduction of the reactive force by a predetermined amount, the system further comprising machine-readable instructions for generating a user-perceptible signal when the perforator is being advanced manually and the load cell detects the reduction of the reactive force by the predetermined amount. 7. The system of claim 1, wherein the machine-readable instructions include instructions, executable in response to user activation, to cause the perforator to move down the trajectory line to perforate the cranium and to cause the perforator to move up the trajectory line after the perforation. 8. The system of claim 7, wherein the end-effector comprises a robot wrist and a load cell operatively connected to the robot wrist, and wherein the load cell is configured to sense reactive force corresponding to the perforator bit engaging the cranium. 9. The system of claim 8, wherein the robot wrist is accessible to the user, and the load cell is further configured to sense manual application of forward and rearward force to at least one of the end-effector, the robot arm, and the robot wrist, wherein the machine-readable instructions to cause the perforator to move down the trajectory line are executed in response to the sensing of the manually applied forward force, and wherein the machine-readable instructions to cause the perforator to move up the trajectory line are executed in response to the sensing of the manually applied rearward force. 10. The system of claim 8, further comprising machine-readable instructions to control forward advancement of the perforator in response to input from the load cell corresponding to the reactive force of the cranium. 11. The system of claim 10, wherein the machine-readable instructions for controlling advancement of the perforator includes instructions, when executed, for ceasing advancement of the perforator by the robot arm in response to the load cell sensing reduction of the reactive force by a predetermined amount. 12. The system of claim 1, wherein the perforator is secured relative to the robot arm and wherein the system includes machine-readable instructions to move the robot arm in the conical oscillation. 13. The system of claim 1, further comprising a linear slide interconnecting the perforator and the robot arm; wherein the machine-readable instructions include instructions to cause the robot arm to impart the conical oscillation to the linear slide, the perforator connected to the linear slide so as to move in the conical oscillation when the robot arm is moved in the conical oscillation. 14. A surgical robot system for drilling a hole in a cranium of a patient in connection with cranial surgery, the system comprising: a surgical robot; a robot arm connected to the surgical robot; an end-effector connected to the robot arm and orientable to oppose the cranium so as to be in operative proximity thereto; a perforator connectable to the end-effector and configured to be advanced or withdrawn relative to the cranium, the perforator having an elongated bit terminating in a sharp, perforator tip, the bit having a bit diameter, the perforator having a clutch operable to rotate the bit in response to detection by the surgical robot of resistance during advancement of the bit and further operable to stop bit rotation in response to detection by the surgical robot of penetration past an internal wall of the cranium; a processor circuit; a memory accessible by the processor circuit and comprising machine-readable instructions; wherein the machine-readable instructions, when executed, cause the perforator to maintain the perforator tip along the trajectory line, and, while maintaining a perforator tip along the trajectory line, further causing the perforator bit to move in a conical oscillation at an angle ranging from about 1 to about 3 relative to the trajectory line during the advancement of the bit into the cranium, to form a substantially circular bore having a diameter larger than the bit diameter by an amount sufficient to reduce frictional force opposing withdrawal of the bit from the bore after the penetration of the inner wall of the cranium, whereby the risk of jamming of the bit during cranial perforation is reduced; wherein the system includes a user interface and machine-readable instructions comprising a perforator mode, and wherein the machine-readable instructions for causing the conical oscillation of the perforator are executable in response to user selection of the perforator mode through the user interface; wherein the robot arm includes a robot wrist and a load cell operatively connected to the robot wrist, the load cell configured to sense reactive force corresponding to the perforator bit engaging the cranium, and further configured to detect reduction of the reactive force by a predetermined amount, the system further comprising machine-readable instructions for generating a user-perceptible signal when the perforator is being advanced manually and the load cell detects the reduction of the reactive force by the predetermined amount; wherein the machine-readable instructions for controlling advancement of the perforator include instructions, when executed, for ceasing advancement of the perforator by the robot arm in response to the load cell sensing the reduction of the reactive force by the predetermined amount; wherein the system further comprises a linear slide interconnecting the perforator and the robot arm; and wherein the machine-readable instructions include instructions to cause the robot arm to impart the conical oscillation to the linear slide, the perforator connected to the linear slide so as to move in the conical oscillation when the robot arm is moved in the conical oscillation. 15. A computer program product, stored on a non-transitory machine-readable medium, the computer program product having instructions to cause a microprocessor to control movement of a perforator of a surgical robot system, the perforator having an elongated perforator bit terminating at a distal end in a sharp, perforator tip, the bit having an associated bit diameter, the instructions comprising the steps of: maintaining the perforator tip along a trajectory line during rotation and advancement of the perforator bit corresponding to a cranial perforation; imparting a conical oscillation to the perforator bit during the advancement into the cranium by orbiting the perforator bit at a pre-determined radial distance from the trajectory line while maintaining the perforator tip aligned with the trajectory line during the advancement, the conical oscillation imparted by the instructions defining an angle relative to the trajectory line sufficient to form a bore upon perforation of the cranium having a diameter greater than the bit diameter by an amount sufficient to reduce frictional force opposing withdrawal of the bit from the bore. 16. The computer program product of claim 15, wherein the step of imparting the conical oscillation further includes causing the perforator bit to be displaced by an angle ranging from about 1 to about 3 relative to the trajectory line. 17. The computer program product of claim 16, the instructions further comprising the step of imparting the conical oscillation to the perforator in response to user actuation of a perforator mode. 18. The computer program product of claim 16 for use in conjunction a robot arm and a linear slide, the perforator being secured to the linear slide and the linear slide being movably mounted relative to the robot arm, the robot arm being positionable in a first position relative to a cranial surgical site, the linear slide connected to the robot arm so as be movable relative to the surgical site independent of movement of the robot arm, the instructions further comprising the step of causing the perforator to move relative to the surgical site by advancement of the linear slide relative to the position of the robot arm. 19. The computer program product of claim 15, wherein the instructions further comprise the steps of: receiving input corresponding to reactive force generated by contact between the perforator bit and the cranium during advancement of the bit through the cranium; and in response to signals corresponding to a reduction of the reactive force, ceasing advancement of the perforator. 20. The computer program product of claim 19, the instructions further comprising the step of causing withdrawal of the perforator from the bore previously formed by advancement thereof, the withdrawal being caused in response to detection of the signals corresponding to the reduction of the reactive force by a predetermined amount.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to medical devices and systems, and more particularly, systems for neuronavigation registration and robotic trajectory guidance, robotic surgery, and related methods and devices.",
                    "STATE_OF_THE_ART": "Position recognition systems for robot assisted surgeries are used to determine the position of and track a particular object in 3-dimensions 3D. In robot assisted surgeries, for example, certain objects, such as surgical instruments, need to be tracked with a high degree of precision as the instrument is being positioned and moved by a robot or by a physician, for example. Position recognition systems may use passive and/or active sensors or markers for registering and tracking the positions of the objects. Using these sensors, the system may geometrically resolve the 3-dimensional position of the sensors based on information from or with respect to one or more cameras, signals, or sensors, etc. These surgical systems can therefore utilize position feedback to precisely guide movement of robotic arms and tools relative to a patients' surgical site. Thus, there is a need for a system that efficiently and accurately provide neuronavigation registration and robotic trajectory guidance in a surgical environment. End-effectors used in robotic surgery may be limited to use in only certain procedures, or may suffer from other drawbacks or disadvantages. Cranial surgery such as electrode placement for deep brain stimulation DBS typically involves drilling a hole in the cranium using a specialized drill called a perforator, the perforator having an associated elongated perforator bit. The cranial perforator has a clutch that causes its sharp bit to engage rotate when there is resistance to forward thrust and to stop rotating once penetration past the internal wall of the skull is achieved, preventing the drill from damaging the brain once the craniotomy hole has been made. An unwanted consequence of the halting of rotation of the perforator is that the bit may become jammed in the hole that it has created, and may be difficult to remove. A surgical technique for preventing jamming of the perforator is to apply slight conical rotation to the perforator during drilling. The hole that is created while using this technique is slightly conically shaped instead of being cylindrical but still allows good attachment of the housing of the electrode holder. Poor surgical technique when applying conical motion during perforator usage could lead to poor outcome. Too large of a conical motion could cause the hole to be larger than desired. Too small of a conical motion could cause the hole to be too cylindrical and therefore not effectively prevent jamming of the bit. Uneven conical motion could lead to an oddly shaped hole that does not hold the electrode housing well.",
                    "SUMMARY": [
                        "According to some implementations, a surgical robot system is configured for surgery on an anatomical feature of a patient, and includes a surgical robot, a robot arm connected to such surgical robot, and an end-effector connected to the robot arm. According to some embodiments of inventive concepts, a system includes a processor circuit and a memory coupled to the processor circuit. The memory includes machine-readable instructions configured to cause the processor circuit to determine, based on a first image volume comprising an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a first plurality of fiducial markers that are fixed with respect to the registration fixture, determine, for each fiducial marker of the first plurality of fiducial markers, a position of the fiducial marker relative to the image volume. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined positions of the first plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature. The machine-readable instructions are further configured to cause the processor circuit to, based on a data frame from a tracking system comprising a second plurality of tracking markers that are fixed with respect to the registration fixture, determine, for each tracking marker of the second plurality of tracking markers, a position of the tracking marker. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined positions of the second plurality of tracking markers, a position and orientation of the registration fixture with respect to a robot arm of a surgical robot. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined position and orientation of the registration fixture with respect to the anatomical feature and the determined position and orientation of the registration fixture with respect to the robot arm, a position and orientation of the anatomical feature with respect to the robot arm. The machine-readable instructions are further configured to cause the processor circuit to control the robot arm based on the determined position and orientation of the anatomical feature with respect to the robot arm. According to some other embodiments of inventive concepts, a computer-implemented method is disclosed. The computer-implemented method includes, based on a first image volume comprising an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a first plurality of fiducial markers that are fixed with respect to the registration fixture, determining, for each fiducial marker of the first plurality of fiducial markers, a position of the fiducial marker. The computer-implemented method further includes determining, based on the determined positions of the first plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature. The computer-implemented method further includes, based on a tracking data frame comprising a second plurality of tracking markers that are fixed with respect to the registration fixture, determining, for each tracking marker of the second plurality of tracking markers, a position of the tracking marker. The computer-implemented method further includes determining, based on the determined positions of the second plurality of tracking markers, a position and orientation of the registration fixture with respect to a robot arm of a surgical robot. The computer-implemented method further includes determining, based on the determined position and orientation of the registration fixture with respect to the anatomical feature and the determined position and orientation of the registration fixture with respect to the robot arm, a position and orientation of the anatomical feature with respect to the robot arm. The computer-implemented method further includes controlling the robot arm based on the determined position and orientation of the anatomical feature with respect to the robot arm. According to some other embodiments of inventive concepts, a surgical system is disclosed. The surgical system includes an intraoperative surgical tracking computer having a processor circuit and a memory. The memory includes machine-readable instructions configured to cause the processor circuit to provide a medical image volume defining an image space. The medical image volume includes an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a plurality of fiducial markers that are fixed with respect to the registration fixture. The machine-readable instructions are further configured to cause the processor circuit to, based on the medical image volume, determine, for each fiducial marker of the plurality of fiducial markers, a position of the fiducial marker with respect to the image space. The machine-readable instructions are further configured to cause the processor circuit to determine, based on the determined positions of the plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature. The machine-readable instructions are further configured to cause the processor circuit to provide a tracking data frame defining a tracking space, the tracking data frame comprising positions of a first plurality of tracked markers that are fixed with respect to the registration fixture. The machine-readable instructions are further configured to cause the processor circuit to, based on the tracking data frame, determine a position of the anatomical feature with respect to the first plurality of tracked markers in the tracking space. The surgical system further includes a surgical robot having a robot arm configured to position a surgical end-effector. The surgical robot further includes a controller connected to the robot arm. The controller is configured to perform operations including, based on the tracking data frame, determining a position of the robot arm with respect to the tracking space. The controller is configured to perform operations including determining, based on the determined position and orientation of the anatomical feature with respect to the tracking space and the determined position and orientation of the robot arm with respect to the tracking space, a position and orientation of the anatomical feature with respect to the robot arm. The controller is configured to perform operations including controlling movement of the robot arm based on the determined position and orientation of the anatomical feature with respect to the robot arm to position the surgical end-effector relative to a location on the patient to facilitate surgery on the patient. In accordance with one possible implementation, a surgical robot system is provided for performing various operations on a patient, such as brain surgery, such brain surgery including the possibility of drilling a hole in a cranium of a patient in connection with such cranial surgery. The robot system includes certain features to address various potential drawbacks and disadvantages associated with perforation of the skull and craniotomy. The surgical system includes a surgical robot and a robot arm connected to such surgical robot. The surgical robot system further includes an end-effector in any number of suitable configurations, such end-effector being orientable so as to oppose the cranium to be operated upon, the end-effector is positioned in operative proximity to the cranium. In such implementations, a perforator may be connected to the end-effector and the perforator is adapted or otherwise configured to be advanced or withdrawn relative to the cranium by suitable features. The perforator has an elongated perforator bit which terminates in a sharp, perforator tip. The bit has an associated bit diameter, and the perforator has a clutch operable to rotate the bit in response to detection by the surgical robot of resistance during advancement of the bit and further operable to stop bit rotation in response to detection by the surgical robot of penetration past an internal wall of the cranium. Suitable electronic or computer implemented instructions are associated with the robot system, including, for example, a processor circuit and a memory accessible by the processor circuit and having machine-readable instructions associated therewith. Accordingly, various operations of the surgical robot system in connection with the drilling of a hole in a cranium may be controlled or otherwise executed by suitable machine-readable instructions. For example, in certain implementations, machine-readable instructions may cause the perforator to maintain the perforator tip along a trajectory line associated with the perforation. While maintaining the perforated tip along such trajectory line, the machine-readable instructions further cause the perforator bit itself to move in a conical oscillation relative to the trajectory line during the advancement of the bit into the cranium. In other words, the elongated perforator bit is angled relative to such trajectory line so as to trace a cone whose tip corresponds substantially to the perforator tip. By virtue of such conical oscillation, a substantially circular bore is formed in the cranium through which the bit has been advanced, but such bore, by virtue of the conical oscillation, has a diameter larger than the bit diameter. The conical oscillation and associated angle are selected or determined so that the resulting bore diameter is larger than the bit diameter by an amount to reduce frictional force opposing withdrawal of the bit from the bore after penetration of the inner wall of the cranium. In this way, the risk of jamming of the perforator bit during cranium perforation is reduced. In certain suitable implementations, the machine-readable instructions, when executed, cause the conical oscillation of the perforator to occur at an angle relative to the trajectory line ranging from about 1 to about 3, and in certain implementations, an angle of about 2 has been found suitable. In accordance with still other implementations, the above-described conical oscillation is associated with a perforator mode, and such perforator mode may be associated with machine-readable instructions which are user selectable through an associated user interface. Still other implementations of a robot system capable of drilling a hole in a cranium may include a robot arm with a robot wrist and a load cell operatively associated with such robot wrist. In this implementation, the load cell senses reactive, that is, opposing force which corresponds to the perforator bit engaging the cranium. The load cell may be further configured so as to detect a reduction in such reactive force by a predetermined amount. That predetermined amount corresponds generally to the force reduction corresponding to achieving perforation, such as by penetrating the inner wall of the cranium. Upon such detection, the system may generate either a user-perceptible signal so as to warn the user or operator to cease manual advancement or an input to the system so that machine-readable instructions cease automatic advancement of the perforator and potentially withdraw such perforator away from the cranium. While certain implementations of the system may impart the conical oscillation by movement of the robot arm or the robot wrist itself, other implementations may include a linear slide which can be angled so as to define the appropriate cone for conical oscillation, rotated, that is, orbited about a selected trajectory line, and advanced by translation of the linear slide toward the cranium being operated upon, and subsequently withdrawn upon completion or perforation of the desired procedure. Other methods and related devices and systems, and corresponding methods and computer program products according to embodiments will be or become apparent to one with skill in the art upon review of the following drawings and detailed description. It is intended that all such devices and systems, and corresponding methods and computer program products be included within this description, be within the scope of the present disclosure, and be protected by the accompanying claims. Moreover, it is intended that all embodiments disclosed herein can be implemented separately or combined in any way and/or combination.",
                        "The accompanying drawings, which are included to provide a further understanding of the disclosure and are incorporated and constitute a part of this application, illustrate certain non-limiting embodiments of inventive concepts. In the drawings: 1A is an overhead view of an arrangement for locations of a robotic system, patient, surgeon, and other medical personnel during a surgical procedure, according to some embodiments; 1B is an overhead view of an alternate arrangement for locations of a robotic system, patient, surgeon, and other medical personnel during a cranial surgical procedure, according to some embodiments; 2 illustrates a robotic system including positioning of the surgical robot and a camera relative to the patient according to some embodiments; 3 is a flowchart diagram illustrating computer-implemented operations for determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments; 4 is a diagram illustrating processing of data for determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments; 5A-5C illustrate a system for registering an anatomical feature of a patient using a computerized tomography CT localizer, a frame reference array FRA, and a dynamic reference base DRB, according to some embodiments; 6A and 6B illustrate a system for registering an anatomical feature of a patient using fluoroscopy fluoro imaging, according to some embodiments; 7 illustrates a system for registering an anatomical feature of a patient using an intraoperative CT fixture ICT and a DRB, according to some embodiments; 8A and 8B illustrate systems for registering an anatomical feature of a patient using a DRB and an X-ray cone beam imaging device, according to some embodiments; 9 illustrates a system for registering an anatomical feature of a patient using a navigated probe and fiducials for point-to-point mapping of the anatomical feature, according to some embodiments; 10 illustrates a two-dimensional visualization of an adjustment range for a centerpoint-arc mechanism, according to some embodiments; 11 illustrates a two-dimensional visualization of virtual point rotation mechanism, according to some embodiments; 12 is an isometric view of one possible implementation of an end-effector according to the present disclosure; 13 is an isometric view of another possible implementation of an end-effector of the present disclosure; 14 is a partial cutaway, isometric view of still another possible implementation of an end-effector according to the present disclosure; 15 is a bottom angle isometric view of yet another possible implementation of an end-effector according to the present disclosure; 16 is an isometric view of one possible tool stop for use with an end-effector according to the present disclosure; 17 and 18 are top plan views of one possible implementation of a tool insert locking mechanism of an end-effector according to the present disclosure; 19 and 20 are top plan views of the tool stop of 16, showing open and closed positions, respectively. 21 and 22 are schematic views of further implementations of the robotic system disclosed herein, including a surgical tool comprising a perforator."
                    ],
                    "DESCRIPTION": "It is to be understood that the present disclosure is not limited in its application to the details of construction and the arrangement of components set forth in the description herein or illustrated in the drawings. The teachings of the present disclosure may be used and practiced in other embodiments and practiced or carried out in various ways. Also, it is to be understood that the phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of including, comprising, or having and variations thereof herein is meant to encompass the items listed thereafter and equivalents thereof as well as additional items. Unless specified or limited otherwise, the terms mounted, connected, supported, and coupled and variations thereof are used broadly and encompass both direct and indirect mountings, connections, supports, and couplings. Further, connected and coupled are not restricted to physical or mechanical connections or couplings. The following discussion is presented to enable a person skilled in the art to make and use embodiments of the present disclosure. Various modifications to the illustrated embodiments will be readily apparent to those skilled in the art, and the principles herein can be applied to other embodiments and applications without departing from embodiments of the present disclosure. Thus, the embodiments are not intended to be limited to embodiments shown, but are to be accorded the widest scope consistent with the principles and features disclosed herein. The following detailed description is to be read with reference to the figures, in which like elements in different figures have like reference numerals. The figures, which are not necessarily to scale, depict selected embodiments and are not intended to limit the scope of the embodiments. Skilled artisans will recognize the examples provided herein have many useful alternatives and fall within the scope of the embodiments. According to some other embodiments, systems for neuronavigation registration and robotic trajectory guidance, and related methods and devices are disclosed. In some embodiments, a first image having an anatomical feature of a patient, a registration fixture that is fixed with respect to the anatomical feature of the patient, and a first plurality of fiducial markers that are fixed with respect to the registration fixture is analyzed, and a position is determined for each fiducial marker of the first plurality of fiducial markers. Next, based on the determined positions of the first plurality of fiducial markers, a position and orientation of the registration fixture with respect to the anatomical feature is determined. A data frame comprising a second plurality of tracking markers that are fixed with respect to the registration fixture is also analyzed, and a position is determined for each tracking marker of the second plurality of tracking markers. Based on the determined positions of the second plurality of tracking markers, a position and orientation of the registration fixture with respect to a robot arm of a surgical robot is determined. Based on the determined position and orientation of the registration fixture with respect to the anatomical feature and the determined position and orientation of the registration fixture with respect to the robot arm, a position and orientation of the anatomical feature with respect to the robot arm is determined, which allows the robot arm to be controlled based on the determined position and orientation of the anatomical feature with respect to the robot arm. Advantages of this and other embodiments include the ability to combine neuronavigation and robotic trajectory alignment into one system, with support for a wide variety of different registration hardware and methods. For example, as will be described in detail below, embodiments may support both computerized tomography CT and fluoroscopy fluoro registration techniques, and may utilize frame-based and/or frameless surgical arrangements. Moreover, in many embodiments, if an initial preoperative registration is compromised due to movement of a registration fixture, registration of the registration fixture and of the anatomical feature by extension can be re-established intraoperatively without suspending surgery and re-capturing preoperative images. Referring now to the drawings, 1A illustrates a surgical robot system 100 in accordance with an embodiment. Surgical robot system 100 may include, for example, a surgical robot 102, one or more robot arms 104, a base 106, a display 110, an end-effector 112, for example, including a guide tube 114, and one or more tracking markers 118. The robot arm 104 may be movable along and/or about an axis relative to the base 106, responsive to input from a user, commands received from a processing device, or other methods. The surgical robot system 100 may include a patient tracking device 116 also including one or more tracking markers 118, which is adapted to be secured directly to the patient 210 , to a bone of the patient 210. As will be discussed in greater detail below, the tracking markers 118 may be secured to or may be part of a stereotactic frame that is fixed with respect to an anatomical feature of the patient 210. The stereotactic frame may also be secured to a fixture to prevent movement of the patient 210 during surgery. According to an alternative embodiment, 1B is an overhead view of an alternate arrangement for locations of a robotic system 100, patient 210, surgeon 120, and other medical personnel during a cranial surgical procedure. During a cranial procedure, for example, the robot 102 may be positioned behind the head 128 of the patient 210. The robot arm 104 of the robot 102 has an end-effector 112 that may hold a surgical instrument 108 during the procedure. In this example, a stereotactic frame 134 is fixed with respect to the patient's head 128, and the patient 210 and/or stereotactic frame 134 may also be secured to a patient base 211 to prevent movement of the patient's head 128 with respect to the patient base 211. In addition, the patient 210, the stereotactic frame 134 and/or or the patient base 211 may be secured to the robot base 106, such as via an auxiliary arm 107, to prevent relative movement of the patient 210 with respect to components of the robot 102 during surgery. Different devices may be positioned with respect to the patient's head 128 and/or patient base 211 as desired to facilitate the procedure, such as an intra-operative CT device 130, an anesthesiology station 132, a scrub station 136, a neuro-modulation station 138, and/or one or more remote pendants 140 for controlling the robot 102 and/or other devices or systems during the procedure. The surgical robot system 100 in the examples of 1A and/or 1B may also use a sensor, such as a camera 200, for example, positioned on a camera stand 202. The camera stand 202 can have any suitable configuration to move, orient, and support the camera 200 in a desired position. The camera 200 may include any suitable camera or cameras, such as one or more cameras , bifocal or stereophotogrammetric cameras, able to identify, for example, active or passive tracking markers 118 shown as part of patient tracking device 116 in 2 in a given measurement volume viewable from the perspective of the camera 200. In this example, the camera 200 may scan the given measurement volume and detect the light that comes from the tracking markers 118 in order to identify and determine the position of the tracking markers 118 in three-dimensions. For example, active tracking markers 118 may include infrared-emitting markers that are activated by an electrical signal , infrared light emitting diodes LEDs, and/or passive tracking markers 118 may include retro-reflective markers that reflect infrared or other light , they reflect incoming IR radiation into the direction of the incoming light, for example, emitted by illuminators on the camera 200 or other suitable sensor or other device. In many surgical procedures, one or more targets of surgical interest, such as targets within the brain for example, are localized to an external reference frame. For example, stereotactic neurosurgery may use an externally mounted stereotactic frame that facilitates patient localization and implant insertion via a frame mounted arc. Neuronavigation is used to register, , map, targets within the brain based on pre-operative or intraoperative imaging. Using this pre-operative or intraoperative imaging, links and associations can be made between the imaging and the actual anatomical structures in a surgical environment, and these links and associations can be utilized by robotic trajectory systems during surgery. According to some embodiments, various software and hardware elements may be combined to create a system that can be used to plan, register, place and verify the location of an instrument or implant in the brain. These systems may integrate a surgical robot, such as the surgical robot 102 of 1A and/or 1B, and may employ a surgical navigation system and planning software to program and control the surgical robot. In addition or alternatively, the surgical robot 102 may be remotely controlled, such as by nonsterile personnel. The robot 102 may be positioned near or next to patient 210, and it will be appreciated that the robot 102 can be positioned at any suitable location near the patient 210 depending on the area of the patient 210 undergoing the operation. The camera 200 may be separated from the surgical robot system 100 and positioned near or next to patient 210 as well, in any suitable position that allows the camera 200 to have a direct visual line of sight to the surgical field 208. In the configuration shown, the surgeon 120 may be positioned across from the robot 102, but is still able to manipulate the end-effector 112 and the display 110. A surgical assistant 126 may be positioned across from the surgeon 120 again with access to both the end-effector 112 and the display 110. If desired, the locations of the surgeon 120 and the assistant 126 may be reversed. The traditional areas for the anesthesiologist 122 and the nurse or scrub tech 124 may remain unimpeded by the locations of the robot 102 and camera 200. With respect to the other components of the robot 102, the display 110 can be attached to the surgical robot 102 and in other embodiments, the display 110 can be detached from surgical robot 102, either within a surgical room with the surgical robot 102, or in a remote location. The end-effector 112 may be coupled to the robot arm 104 and controlled by at least one motor. In some embodiments, end-effector 112 can comprise a guide tube 114, which is able to receive and orient a surgical instrument 108 used to perform surgery on the patient 210. As used herein, the term end-effector is used interchangeably with the terms end-effectuator and effectuator element. Although generally shown with a guide tube 114, it will be appreciated that the end-effector 112 may be replaced with any suitable instrumentation suitable for use in surgery. In some embodiments, end-effector 112 can comprise any known structure for effecting the movement of the surgical instrument 108 in a desired manner. The surgical robot 102 is able to control the translation and orientation of the end-effector 112. The robot 102 is able to move end-effector 112 along x-, y-, and z-axes, for example. The end-effector 112 can be configured for selective rotation about one or more of the x-, y-, and z-axis such that one or more of the Euler Angles , roll, pitch, and/or yaw associated with end-effector 112 can be selectively controlled. In some embodiments, selective control of the translation and orientation of end-effector 112 can permit performance of medical procedures with significantly improved accuracy compared to conventional robots that use, for example, a six degree of freedom robot arm comprising only rotational axes. For example, the surgical robot system 100 may be used to operate on patient 210, and robot arm 104 can be positioned above the body of patient 210, with end-effector 112 selectively angled relative to the z-axis toward the body of patient 210. In some embodiments, the position of the surgical instrument 108 can be dynamically updated so that surgical robot 102 can be aware of the location of the surgical instrument 108 at all times during the procedure. Consequently, in some embodiments, surgical robot 102 can move the surgical instrument 108 to the desired position quickly without any further assistance from a physician unless the physician so desires. In some further embodiments, surgical robot 102 can be configured to correct the path of the surgical instrument 108 if the surgical instrument 108 strays from the selected, preplanned trajectory. In some embodiments, surgical robot 102 can be configured to permit stoppage, modification, and/or manual control of the movement of end-effector 112 and/or the surgical instrument 108. Thus, in use, in some embodiments, a physician or other user can operate the system 100, and has the option to stop, modify, or manually control the autonomous movement of end-effector 112 and/or the surgical instrument 108. Further details of surgical robot system 100 including the control and movement of a surgical instrument 108 by surgical robot 102 can be found in co-pending Patent Publication 2013/0345718, which is incorporated herein by reference in its entirety. As will be described in greater detail below, the surgical robot system 100 can comprise one or more tracking markers configured to track the movement of robot arm 104, end-effector 112, patient 210, and/or the surgical instrument 108 in three dimensions. In some embodiments, a plurality of tracking markers can be mounted or otherwise secured thereon to an outer surface of the robot 102, such as, for example and without limitation, on base 106 of robot 102, on robot arm 104, and/or on the end-effector 112. In some embodiments, such as the embodiment of 3 below, for example, one or more tracking markers can be mounted or otherwise secured to the end-effector 112. One or more tracking markers can further be mounted or otherwise secured to the patient 210. In some embodiments, the plurality of tracking markers can be positioned on the patient 210 spaced apart from the surgical field 208 to reduce the likelihood of being obscured by the surgeon, surgical tools, or other parts of the robot 102. Further, one or more tracking markers can be further mounted or otherwise secured to the surgical instruments 108 , a screw driver, dilator, implant inserter, or the like. Thus, the tracking markers enable each of the marked objects , the end-effector 112, the patient 210, and the surgical instruments 108 to be tracked by the surgical robot system 100. In some embodiments, system 100 can use tracking information collected from each of the marked objects to calculate the orientation and location, for example, of the end-effector 112, the surgical instrument 108 , positioned in the tube 114 of the end-effector 112, and the relative position of the patient 210. Further details of surgical robot system 100 including the control, movement and tracking of surgical robot 102 and of a surgical instrument 108 can be found in Patent Publication 2016/0242849, which is incorporated herein by reference in its entirety. In some embodiments, pre-operative imaging may be used to identify the anatomy to be targeted in the procedure. If desired by the surgeon the planning package will allow for the definition of a reformatted coordinate system. This reformatted coordinate system will have coordinate axes anchored to specific anatomical landmarks, such as the anterior commissure AC and posterior commissure PC for neurosurgery procedures. In some embodiments, multiple pre-operative exam images , CT or magnetic resonance MR images may be co-registered such that it is possible to transform coordinates of any given point on the anatomy to the corresponding point on all other pre-operative exam images. As used herein, registration is the process of determining the coordinate transformations from one coordinate system to another. For example, in the co-registration of preoperative images, co-registering a CT scan to an MR scan means that it is possible to transform the coordinates of an anatomical point from the CT scan to the corresponding anatomical location in the MR scan. It may also be advantageous to register at least one exam image coordinate system to the coordinate system of a common registration fixture, such as a dynamic reference base DRB, which may allow the camera 200 to keep track of the position of the patient in the camera space in real-time so that any intraoperative movement of an anatomical point on the patient in the room can be detected by the robot system 100 and accounted for by compensatory movement of the surgical robot 102. 3 is a flowchart diagram illustrating computer-implemented operations 300 for determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments. The operations 300 may include receiving a first image volume, such as a CT scan, from a preoperative image capture device at a first time Block 302. The first image volume includes an anatomical feature of a patient and at least a portion of a registration fixture that is fixed with respect to the anatomical feature of the patient. The registration fixture includes a first plurality of fiducial markers that are fixed with respect to the registration fixture. The operations 300 further include determining, for each fiducial marker of the first plurality of fiducial markers, a position of the fiducial marker relative to the first image volume Block 304. The operations 300 further include, determining, based on the determined positions of the first plurality of fiducial markers, positions of an array of tracking markers on the registration fixture fiducial registration array or FRA with respect to the anatomical feature Block 306. The operations 300 may further include receiving a tracking data frame from an intraoperative tracking device comprising a plurality of tracking cameras at a second time that is later than the first time Block 308. The tracking frame includes positions of a plurality of tracking markers that are fixed with respect to the registration fixture FRA and a plurality of tracking markers that are fixed with respect to the robot. The operations 300 further include determining, for based on the positions of tracking markers of the registration fixture, a position and orientation of the anatomical feature with respect to the tracking cameras Block 310. The operations 300 further include determining, based on the determined positions of the plurality of tracking markers on the robot, a position and orientation of the robot arm of a surgical robot with respect to the tracking cameras Block 312. The operations 300 further include determining, based on the determined position and orientation of the anatomical feature with respect to the tracking cameras and the determined position and orientation of the robot arm with respect to the tracking cameras, a position and orientation of the anatomical feature with respect to the robot arm Block 314. The operations 300 further include controlling movement of the robot arm with respect to the anatomical feature, , along and/or rotationally about one or more defined axis, based on the determined position and orientation of the anatomical feature with respect to the robot arm Block 316. 4 is a diagram illustrating a data flow 400 for a multiple coordinate transformation system, to enable determining a position and orientation of an anatomical feature of a patient with respect to a robot arm of a surgical robot, according to some embodiments. In this example, data from a plurality of exam image spaces 402, based on a plurality of exam images, may be transformed and combined into a common exam image space 404. The data from the common exam image space 404 and data from a verification image space 406, based on a verification image, may be transformed and combined into a registration image space 408. Data from the registration image space 408 may be transformed into patient fiducial coordinates 410, which is transformed into coordinates for a DRB 412. A tracking camera 414 may detect movement of the DRB 412 represented by DRB 412 and may also detect a location of a probe tracker 416 to track coordinates of the DRB 412 over time. A robotic arm tracker 418 determines coordinates for the robot arm based on transformation data from a Robotics Planning System RPS space 420 or similar modeling system, and/or transformation data from the tracking camera 414. It should be understood that these and other features may be used and combined in different ways to achieve registration of image space, i. e. , coordinates from image volume, into tracking space, i. e. , coordinates for use by the surgical robot in real-time. As will be discussed in detail below, these features may include fiducial-based registration such as stereotactic frames with CT localizer, preoperative CT or MRI registered using intraoperative fluoroscopy, calibrated scanner registration where any acquired scan's coordinates are pre-calibrated relative to the tracking space, and/or surface registration using a tracked probe, for example. In one example, 5A-5C illustrate a system 500 for registering an anatomical feature of a patient. In this example, the stereotactic frame base 530 is fixed to an anatomical feature 528 of patient, , the patient's head. As shown by 5A, the stereotactic frame base 530 may be affixed to the patient's head 528 prior to registration using pins clamping the skull or other method. The stereotactic frame base 530 may act as both a fixation platform, for holding the patient's head 528 in a fixed position, and registration and tracking platform, for alternatingly holding the CT localizer 536 or the FRA fixture 534. The CT localizer 536 includes a plurality of fiducial markers 532 , N-pattern radio-opaque rods or other fiducials, which are automatically detected in the image space using image processing. Due to the precise attachment mechanism of the CT localizer 536 to the base 530, these fiducial markers 532 are in known space relative to the stereotactic frame base 530. A 3D CT scan of the patient with CT localizer 536 attached is taken, with an image volume that includes both the patient's head 528 and the fiducial markers 532 of the CT localizer 536. This registration image can be taken intraoperatively or preoperatively, either in the operating room or in radiology, for example. The captured 3D image dataset is stored to computer memory. As shown by 5B, after the registration image is captured, the CT localizer 536 is removed from the stereotactic frame base 530 and the frame reference array fixture 534 is attached to the stereotactic frame base 530. The stereotactic frame base 530 remains fixed to the patient's head 528, however, and is used to secure the patient during surgery, and serves as the attachment point of a frame reference array fixture 534. The frame reference array fixture 534 includes a frame reference array FRA, which is a rigid array of three or more tracked markers 539, which may be the primary reference for optical tracking. By positioning the tracked markers 539 of the FRA in a fixed, known location and orientation relative to the stereotactic frame base 530, the position and orientation of the patient's head 528 may be tracked in real time. Mount points on the FRA fixture 534 and stereotactic frame base 530 may be designed such that the FRA fixture 534 attaches reproducibly to the stereotactic frame base 530 with minimal i. e. , submillimetric variability. These mount points on the stereotactic frame base 530 can be the same mount points used by the CT localizer 536, which is removed after the scan has been taken. An auxiliary arm such as auxiliary arm 107 of 1B, for example or other attachment mechanism can also be used to securely affix the patient to the robot base to ensure that the robot base is not allowed to move relative to the patient. As shown by 5C, a dynamic reference base DRB 540 may also be attached to the stereotactic frame base 530. The DRB 540 in this example includes a rigid array of three or more tracked markers 542. In this example, the DRB 540 and/or other tracked markers may be attached to the stereotactic frame base 530 and/or to directly to the patient's head 528 using auxiliary mounting arms 541, pins, or other attachment mechanisms. Unlike the FRA fixture 534, which mounts in only one way for unambiguous localization of the stereotactic frame base 530, the DRB 540 in general may be attached as needed for allowing unhindered surgical and equipment access. Once the DRB 540 and FRA fixture 534 are attached, registration, which was initially related to the tracking markers 539 of the FRA, can be optionally transferred or related to the tracking markers 542 of the DRB 540. For example, if any part of the FRA fixture 534 blocks surgical access, the surgeon may remove the FRA fixture 534 and navigate using only the DRB 540. However, if the FRA fixture 534 is not in the way of the surgery, the surgeon could opt to navigate from the FRA markers 539, without using a DRB 540, or may navigate using both the FRA markers 539 and the DRB 540. In this example, the FRA fixture 534 and/or DRB 540 uses optical markers, the tracked positions of which are in known locations relative to the stereotactic frame base 530, similar to the CT localizer 536, but it should be understood that many other additional and/or alternative techniques may be used. 6A and 6B illustrate a system 600 for registering an anatomical feature of a patient using fluoroscopy fluoro imaging, according to some embodiments. In this embodiment, image space is registered to tracking space using multiple intraoperative fluoroscopy fluoro images taken using a tracked registration fixture 644. The anatomical feature of the patient , the patient's head 628 is positioned and rigidly affixed in a clamping apparatus 643 in a static position for the remainder of the procedure. The clamping apparatus 643 for rigid patient fixation can be a three-pin fixation system such as a Mayfield clamp, a stereotactic frame base attached to the surgical table, or another fixation method, as desired. The clamping apparatus 643 may also function as a support structure for a patient tracking array or DRB 640 as well. The DRB may be attached to the clamping apparatus using auxiliary mounting arms 641 or other means. Once the patient is positioned, the fluoro fixture 644 is attached the fluoro unit's x-ray collecting image intensifier not shown and secured by tightening clamping feet 632. The fluoro fixture 644 contains fiducial markers , metal spheres laid out across two planes in this example, not shown that are visible on 2D fluoro images captured by the fluoro image capture device and can be used to calculate the location of the x-ray source relative to the image intensifier, which is typically about 1 meter away contralateral to the patient, using a standard pinhole camera model. Detection of the metal spheres in the fluoro image captured by the fluoro image capture device also enables the software to de-warp the fluoro image i. e. , to remove pincushion and s-distortion. Additionally, the fluoro fixture 644 contains 3 or more tracking markers 646 for determining the location and orientation of the fluoro fixture 644 in tracking space. In some embodiments, software can project vectors through a CT image volume, based on a previously captured CT image, to generate synthetic images based on contrast levels in the CT image that appear similar to the actual fluoro images i. e. , digitally reconstructed radiographs DRRs. By iterating through theoretical positions of the fluoro beam until the DRRs match the actual fluoro shots, a match can be found between fluoro image and DRR in two or more perspectives, and based on this match, the location of the patient's head 628 relative to the x-ray source and detector is calculated. Because the tracking markers 646 on the fluoro fixture 644 track the position of the image intensifier and the position of the x-ray source relative to the image intensifier is calculated from metal fiducials on the fluoro fixture 644 projected on 2D images, the position of the x-ray source and detector in tracking space are known and the system is able to achieve image-to-tracking registration. As shown by 6A and 6B, two or more shots are taken of the head 628 of the patient by the fluoro image capture device from two different perspectives while tracking the array markers 642 of the DRB 640, which is fixed to the registration fixture 630 via a mounting arm 641, and tracking markers 646 on the fluoro fixture 644. Based on the tracking data and fluoro data, an algorithm computes the location of the head 628 or other anatomical feature relative to the tracking space for the procedure. Through image-to-tracking registration, the location of any tracked tool in the image volume space can be calculated. For example, in one embodiment, a first fluoro image taken from a first fluoro perspective can be compared to a first DRR constructed from a first perspective through a CT image volume, and a second fluoro image taken from a second fluoro perspective can be compared to a second DRR constructed from a second perspective through the same CT image volume. Based on the comparisons, it may be determined that the first DRR is substantially equivalent to the first fluoro image with respect to the projected view of the anatomical feature, and that the second DRR is substantially equivalent to the second fluoro image with respect to the projected view of the anatomical feature. Equivalency confirms that the position and orientation of the x-ray path from emitter to collector on the actual fluoro machine as tracked in camera space matches the position and orientation of the x-ray path from emitter to collector as specified when generating the DRRs in CT space, and therefore registration of tracking space to CT space is achieved. 7 illustrates a system 700 for registering an anatomical feature of a patient using an intraoperative CT fixture ICT and a DRB, according to some embodiments. As shown in 7, in one application, a fiducial-based image-to-tracking registration can be utilized that uses an intraoperative CT fixture ICT 750 having a plurality of tracking markers 751 and radio-opaque fiducial reference markers 732 to register the CT space to the tracking space. After stabilizing the anatomical feature 728 , the patient's head using clamping apparatus 730 such as a three-pin Mayfield frame and/or stereotactic frame, the surgeon will affix the ICT 750 to the anatomical feature 728, DRB 740, or clamping apparatus 730, so that it is in a static position relative to the tracking markers 742 of the DRB 740, which may be held in place by mounting arm 741 or other rigid means. A CT scan is captured that encompasses the fiducial reference markers 732 of the ICT 750 while also capturing relevant anatomy of the anatomical feature 728. Once the CT scan is loaded in the software, the system auto-identifies through image processing locations of the fiducial reference markers 732 of the ICT within the CT volume, which are in a fixed position relative to the tracking markers of the ICT 750, providing image-to-tracking registration. This registration, which was initially based on the tracking markers 751 of the ICT 750, is then related to or transferred to the tracking markers 742 of the DRB 740, and the ICT 750 may then be removed. 8A illustrates a system 800 for registering an anatomical feature of a patient using a DRB and an X-ray cone beam imaging device, according to some embodiments. An intraoperative scanner 852, such as an X-ray machine or other scanning device, may have a tracking array 854 with tracking markers 855, mounted thereon for registration. Based on the fixed, known position of the tracking array 854 on the scanning device, the system may be calibrated to directly map register the tracking space to the image space of any scan acquired by the system. Once registration is achieved, the registration, which is initially based on the tracking markers 855 gantry markers of the scanner's array 854, is related or transferred to the tracking markers 842 of a DRB 840, which may be fixed to a clamping fixture 830 holding the patient's head 828 by a mounting arm 841 or other rigid means. After transferring registration, the markers on the scanner are no longer used and can be removed, deactivated or covered if desired. Registering the tracking space to any image acquired by a scanner in this way may avoid the need for fiducials or other reference markers in the image space in some embodiments. 8B illustrates an alternative system 800 that uses a portable intraoperative scanner, referred to herein as a C-arm scanner 853. In this example, the C-arm scanner 853 includes a c-shaped arm 856 coupled to a movable base 858 to allow the C-arm scanner 853 to be moved into place and removed as needed, without interfering with other aspects of the surgery. The arm 856 is positioned around the patient's head 828 intraoperatively, and the arm 856 is rotated and/or translated with respect to the patient's head 828 to capture the X-ray or other type of scan that to achieve registration, at which point the C-arm scanner 853 may be removed from the patient. Another registration method for an anatomical feature of a patient, , a patient's head, may be to use a surface contour map of the anatomical feature, according to some embodiments. A surface contour map may be constructed using a navigated or tracked probe, or other measuring or sensing device, such as a laser pointer, 3D camera, etc. For example, a surgeon may drag or sequentially touch points on the surface of the head with the navigated probe to capture the surface across unique protrusions, such as zygomatic bones, superciliary arches, bridge of nose, eyebrows, etc. The system then compares the resulting surface contours to contours detected from the CT and/or MR images, seeking the location and orientation of contour that provides the closest match. To account for movement of the patient and to ensure that all contour points are taken relative to the same anatomical feature, each contour point is related to tracking markers on a DRB on the patient at the time it is recorded. Since the location of the contour map is known in tracking space from the tracked probe and tracked DRB, tracking-to-image registration is obtained once the corresponding contour is found in image space. 9 illustrates a system 900 for registering an anatomical feature of a patient using a navigated or tracked probe and fiducials for point-to-point mapping of the anatomical feature 928 , a patient's head, according to some embodiments. Software would instruct the user to point with a tracked probe to a series of anatomical landmark points that can be found in the CT or MR image. When the user points to the landmark indicated by software, the system captures a frame of tracking data with the tracked locations of tracking markers on the probe and on the DRB. From the tracked locations of markers on the probe, the coordinates of the tip of the probe are calculated and related to the locations of markers on the DRB. Once 3 or more points are found in both spaces, tracking-to-image registration is achieved. As an alternative to pointing to natural anatomical landmarks, fiducials 954 i. e. , fiducial markers, such as sticker fiducials or metal fiducials, may be used. The surgeon will attach the fiducials 954 to the patient, which are constructed of material that is opaque on imaging, for example containing metal if used with CT or Vitamin E if used with MR. Imaging CT or MR will occur after placing the fiducials 954. The surgeon or user will then manually find the coordinates of the fiducials in the image volume, or the software will find them automatically with image processing. After attaching a DRB 940 with tracking markers 942 to the patient through a mounting arm 941 connected to a clamping apparatus 930 or other rigid means, the surgeon or user may also locate the fiducials 954 in physical space relative to the DRB 940 by touching the fiducials 954 with a tracked probe while simultaneously recording tracking markers on the probe not shown and on the DRB 940. Registration is achieved because the coordinates of the same points are known in the image space and the tracking space. One use for the embodiments described herein is to plan trajectories and to control a robot to move into a desired trajectory, after which the surgeon will place implants such as electrodes through a guide tube held by the robot. Additional functionalities include exporting coordinates used with existing stereotactic frames, such as a Leksell frame, which uses five coordinates: X, Y, Z, Ring Angle and Arc Angle. These five coordinates are established using the target and trajectory identified in the planning stage relative to the image space and knowing the position and orientation of the ring and arc relative to the stereotactic frame base or other registration fixture. As shown in 10, stereotactic frames allow a target location 1058 of an anatomical feature 1028 , a patient's head to be treated as the center of a sphere and the trajectory can pivot about the target location 1058. The trajectory to the target location 1058 is adjusted by the ring and arc angles of the stereotactic frame , a Leksell frame. These coordinates may be set manually, and the stereotactic frame may be used as a backup or as a redundant system in case the robot fails or cannot be tracked or registered successfully. The linear x,y,z offsets to the center point i. e. , target location 1058 are adjusted via the mechanisms of the frame. A cone 1060 is centered around the target location 1058, and shows the adjustment zone that can be achieved by modifying the ring and arc angles of the Leksell or other type of frame. This figure illustrates that a stereotactic frame with ring and arc adjustments is well suited for reaching a fixed target location from a range of angles while changing the entry point into the skull. 11 illustrates a two-dimensional visualization of virtual point rotation mechanism, according to some embodiments. In this embodiment, the robotic arm is able to create a different type of point-rotation functionality that enables a new movement mode that is not easily achievable with a 5-axis mechanical frame, but that may be achieved using the embodiments described herein. Through coordinated control of the robot's axes using the registration techniques described herein, this mode allows the user to pivot the robot's guide tube about any fixed point in space. For example, the robot may pivot about the entry point 1162 into the anatomical feature 1128 , a patient's head. This entry point pivoting is advantageous as it allows the user to make a smaller burr hole without limiting their ability to adjust the target location 1164 intraoperatively. The cone 1160 represents the range of trajectories that may be reachable through a single entry hole. Additionally, entry point pivoting is advantageous as it allows the user to reach two different target locations 1164 and 1166 through the same small entry burr hole. Alternately, the robot may pivot about a target point , location 1058 shown in 10 within the skull to reach the target location from different angles or trajectories, as illustrated in 10. Such interior pivoting robotically has the same advantages as a stereotactic frame as it allows the user to approach the same target location 1058 from multiple approaches, such as when irradiating a tumor or when adjusting a path so that critical structures such as blood vessels or nerves will not be crossed when reaching targets beyond them. Unlike a stereotactic frame, which relies on fixed ring and arc articulations to keep a target/pivot point fixed, the robot adjusts the pivot point through controlled activation of axes and the robot can therefore dynamically adjust its pivot point and switch as needed between the modes illustrated in 10 and 11. Following the insertion of implants or instrumentation using the robot or ring and arc fixture, these and other embodiments may allow for implant locations to be verified using intraoperative imaging. Placement accuracy of the instrument or implant relative to the planned trajectory can be qualitatively and/or quantitatively shown to the user. One option for comparing planned to placed position is to merge a postoperative verification CT image to any of the preoperative images. Once pre- and post-operative images are merged and plan is shown overlaid, the shadow of the implant on postop CT can be compared to the plan to assess accuracy of placement. Detection of the shadow artifact on post-op CT can be performed automatically through image processing and the offset displayed numerically in terms of millimeters offset at the tip and entry and angular offset along the path. This option does not require any fiducials to be present in the verification image since image-to-image registration is performed based on bony anatomical contours. A second option for comparing planned position to the final placement would utilize intraoperative fluoro with or without an attached fluoro fixture. Two out-of-plane fluoro images will be taken and these fluoro images will be matched to DRRs generated from pre-operative CT or MR as described above for registration. Unlike some of the registration methods described above, however, it may be less important for the fluoro images to be tracked because the key information is where the electrode is located relative to the anatomy in the fluoro image. The linear or slightly curved shadow of the electrode would be found on a fluoro image, and once the DRR corresponding to that fluoro shot is found, this shadow can be replicated in the CT image volume as a plane or sheet that is oriented in and out of the ray direction of the fluoro image and DRR. That is, the system may not know how deep in or out of the fluoro image plane the electrode lies on a given shot, but can calculate the plane or sheet of possible locations and represent this plane or sheet on the 3D volume. In a second fluoro view, a different plane or sheet can be determined and overlaid on the 3D image. Where these two planes or sheets intersect on the 3D image is the detected path of the electrode. The system can represent this detected path as a graphic on the 3D image volume and allow the user to reslice the image volume to display this path and the planned path from whatever perspective is desired, also allowing automatic or manual calculation of the deviation from planned to placed position of the electrode. Tracking the fluoro fixture is unnecessary but may be done to help de-warp the fluoro images and calculate the location of the x-ray emitter to improve accuracy of DRR calculation, the rate of convergence when iterating to find matching DRR and fluoro shots, and placement of sheets/planes representing the electrode on the 3D scan. In this and other examples, it is desirable to maintain navigation integrity, i. e. , to ensure that the registration and tracking remain accurate throughout the procedure. Two primary methods to establish and maintain navigation integrity include: tracking the position of a surveillance marker relative to the markers on the DRB, and checking landmarks within the images. In the first method, should this position change due to, for example, the DRB being bumped, then the system may alert the user of a possible loss of navigation integrity. In the second method, if a landmark check shows that the anatomy represented in the displayed slices on screen does not match the anatomy at which the tip of the probe points, then the surgeon will also become aware that there is a loss of navigation integrity. In either method, if using the registration method of CT localizer and frame reference array FRA, the surgeon has the option to re-attach the FRA, which mounts in only one possible way to the frame base, and to restore tracking-to-image registration based on the FRA tracking markers and the stored fiducials from the CT localizer 536. This registration can then be transferred or related to tracking markers on a repositioned DRB. Once registration is transferred the FRA can be removed if desired. Referring now to 12-18 generally, with reference to the surgical robot system 100 shown in 1A, end-effector 112 may be equipped with components, configured, or otherwise include features so that one end-effector may remain attached to a given one of robot arms 104 without changing to another end-effector for multiple different surgical procedures, such as, by way of example only, Deep Brain Stimulation DBS, Stereoelectroencephalography SEEG, or Endoscopic Navigation and Tumor Biopsy. As discussed previously, end-effector 112 may be orientable to oppose an anatomical feature of a patient in the manner so as to be in operative proximity thereto, and, to be able to receive one or more surgical tools for operations contemplated on the anatomical feature proximate to the end-effector 112. Motion and orientation of end-effector 112 may be accomplished through any of the navigation, trajectory guidance, or other methodologies discussed herein or as may be otherwise suitable for the particular operation. End-effector 112 is suitably configured to permit a plurality of surgical tools 129 to be selectively connectable to end-effector 112. Thus, for example, a stylet 113 13 may be selectively attached in order to localize an incision point on an anatomical feature of a patient, or an electrode driver 115 14 may be selectively attached to the same end-effector 112. With reference to the previous discussion of robot surgical system 100, a processor circuit, as well as memory accessible by such processor circuit, includes various subroutines and other machine-readable instructions configured to cause, when executed, end-effector 112 to move, such as by GPS movement, relative to the anatomical feature, at predetermined stages of associated surgical operations, whether pre-operative, intra-operative or post-operative. End-effector 112 includes various components and features to either prevent or permit end-effector movement depending on whether and which tools 129, if any, are connected to end-effector 112. Referring more particularly to 12, end-effector 112 includes a tool-insert locking mechanism 117 located on and connected to proximal surface 119. Tool-insert locking mechanism 117 is configured so as to secure any selected one of a plurality of surgical tools, such as the aforesaid stylet 113, electrode driver 115, or any other tools for different surgeries mentioned previously or as may be contemplated by other applications of this disclosure. The securement of the tool by tool-insert locking mechanism 117 is such that, for any of multiple tools capable of being secured to locking mechanism 117, each such tool is operatively and suitably secured at the predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient, such that multiple tools may be secured to the same end-effector 112 in respective positions appropriate for the contemplated procedure. Another feature of the end-effector 112 is a tool stop 121 located on distal surface 123 of end-effector 112, that is, the surface generally opposing the patient. Tool stop 121 has a stop mechanism 125 and a sensor 127 operatively associated therewith, as seen with reference to 16, 19, and 20. Stop mechanism 125 is mounted to end-effector 112 so as to be selectively movable relative thereto between an engaged position to prevent any of the tools from being connected to end-effector 112 and a disengaged position which permits any of the tools 129 to be selectively connected to end-effector 112. Sensor 127 may be located on or within the housing of end-effector 112 at any suitable location 12, 14, 16 so that sensor 127 detects whether stop mechanism 125 is in the engaged or disengaged position. Sensor 127 may assume any form suitable for such detection, such as any type of mechanical switch or any type of magnetic sensor, including Reed switches, Hall Effect sensors, or other magnetic field detecting devices. In one possible implementation, sensor 127 has two portions, a Hall Effect sensor portion not shown and a magnetic portion 131, the two portions moving relative to each other so as to generate and detect two magnetic fields corresponding to respective engaged and disengaged position. In the illustrated implementation, the magnetic portion comprises two rare earth magnets 131 which move relative to the complementary sensing portion not shown mounted in the housing of end effector 112 in operative proximity to magnets 131 to detect change in the associated magnetic field from movement of stop mechanism 125 between engaged and disengaged positions. In this implementation the Hall Effect sensor is bipolar and can detect whether a North pole or South pole of a magnet opposes the sensor. Magnets 131 are configured so that the North pole of one magnet faces the path of the sensor and the South pole of the other magnet faces the path of the sensor. In this configuration, the sensor senses an increased signal when it is near one magnet for example, in disengaged position, a decreased signal when it is near the other magnet for example, in engaged position, and unchanged signal when it is not in proximity to any magnet. In this implementation, in response to detection of stop mechanism 125 being in the disengaged position shown in 13 and 19, sensor 127 causes the processor of surgical robot system 100 to execute suitable instructions to prevent movement of end-effector 112 relative to the anatomical feature. Such movement prevention may be appropriate for any number of reasons, such as when a tool is connected to end-effector 112, such tool potentially interacting with the anatomical feature of the patient. Another implementation of a sensor 127 for detecting engaged or disengaged tool stop mechanism 125 could comprise a single magnet behind the housing not shown and two Hall Effect sensors located where magnets 131 are shown in the preferred embodiment. In such a configuration, monopolar Hall Effect sensors are suitable and would be configured so that Sensor 1 detects a signal when the magnet is in proximity due to the locking mechanism being disengaged, while Sensor 2 detects a signal when the same magnet is in proximity due to the locking mechanism being engaged. Neither sensor would detect a signal when the magnet is between positions or out of proximity to either sensor. Although a configuration could be conceived in which a sensor is active for engaged position and inactive for disengaged position, a configuration with three signals indicating engaged, disengaged, or transitional is preferred to ensure correct behavior in case of power failure. End-effector 112, tool stop 121, and tool-insert locking mechanism 117 each have co-axially aligned bores or apertures such that any selected one of the plurality of surgical tools 129 may be received through such bores and apertures. In this implementation end-effector has a bore 133 and tool stop 121 and tool-insert locking mechanism 117 have respective apertures 135 and 137. Stop mechanism 125 includes a ring 139 axially aligned with bore 133 and aperture 135 of tool stop 121. Ring 139 is selectively, manually rotatable in the directions indicated by arrow A 16 so as to move stop mechanism 125 between the engaged position and the disengaged position. In one possible implementation, the selective rotation of ring 139 includes features which enable ring 139 to be locked in either the disengaged or engaged position. So, for example, as illustrated, a detent mechanism 141 is located on and mounted to ring 139 in any suitable way to lock ring 139 against certain rotational movement out of a predetermined position, in this case, such position being when stop mechanism 125 is in the engaged position. Although various forms of detent mechanism are contemplated herein, one suitable arrangement has a manually accessible head extending circumferentially outwardly from ring 139 and having a male protrusion not shown spring-loaded axially inwardly to engage a corresponding female detent portion not shown. Detent mechanism 141, as such, is manually actuatable to unlock ring 139 from its engaged position to permit ring 139 to be manually rotated to cause stop mechanism 125 to move from the engaged position 20 to the disengaged position 19. Tool stop 121 includes a lever arm 143 pivotally mounted adjacent aperture 135 of tool stop 121 so end of lever arm 143 selectively pivots in the directions indicated by arrow B 16, 19 and 20. Lever arm 143 is operatively connected to stop mechanism 125, meaning it closes aperture 135 of tool stop 121 in response to stop mechanism 125 being in the engaged position, as shown in 20. Lever arm 143 is also operatively connected so as to pivot back in direction of arrow B to open aperture 135 in response to stop mechanism 125 being in the disengaged position. As such, movement of stop mechanism 125 between engaged and disengaged positions results in closure or opening of aperture 135, respectively, by lever arm 143. Lever arm 143, in this implementation, is not only pivotally mounted adjacent aperture 135, but also pivots in parallel with a distal plane defined at a distal-most point of distal surface 123 of end-effector 112. In this manner, any one of the surgical tools 129, which is attempted to be inserted through bore 133 and aperture 135, is stopped from being inserted past the distal plane in which lever arm 143 rotates to close aperture 135. Turning now to tool-insert locking mechanism 117 13, 17, 18, a connector 145 is configured to meet with and secure any one of the surgical tools 129 at their appropriate height, angle of orientation, and rotational position relative to the anatomical feature of the patient. In the illustrated implementation, connector 145 comprises a rotatable flange 147 which has at least one slot 149 formed therein to receive there through a corresponding tongue 151 associated with a selected one of the plurality of tools 129. So, for example, in 14, the particular electrode driver 115 has multiple tongues, one of which tongue 151 is shown. Rotatable flange 147, in some implementations, may comprise a collar 153, which collar, in turn, has multiple ones of slots 149 radially spaced on a proximally oriented surface 155, as best seen in 12. Multiple slots 147 arranged around collar 153 are sized or otherwise configured so as to receive there through corresponding ones of multiple tongues 151 associated with a selected one of the plurality of tools 129. Therefore, as seen in 13, multiple slots 149 and corresponding tongues 151 may be arranged to permit securing of a selected one of the plurality of tools 129 only when selected tool is in the correct, predetermined angle of orientation and rotational position relative to the anatomical feature of the patient. Similarly, with regard to the electrode driver shown in 14, tongues 151 one of which is shown in a cutaway of 14 have been received in radially spaced slots 149 arrayed so that electrode driver 115 is received at the appropriate angle of orientation and rotational position. Rotatable flange 147 has, in this implementation, a grip 173 to facilitate manual rotation between an open and closed position as shown in 17 and 18, respectively. As seen in 17, multiple sets of mating slots 149 and tongues 151 are arranged at different angular locations, in this case, locations which may be symmetric about a single diametric chord of a circle but otherwise radially asymmetric, and at least one of the slots has a different dimension or extends through a different arc length than other slots. In this slot-tongue arrangement, and any number of variations contemplated by this disclosure, there is only one rotational position of the tool 129 or adapter 155 discussed later to be received in tool-insert locking mechanism 117 when rotatable flange 147 is in the open position shown in 17. In other words, when the user of system 100 moves a selected tool 129 or tool adapter 155 to a single appropriate rotational position, corresponding tongues 151 may be received through slots 149. Upon placement of tongues 151 into slots 149, tongues 151 confront a base surface 175 within connector 145 of rotatable flange 147. Upon receiving tongues 151 into slots 149 and having them rest on underlying base surface 175, dimensions of tongues 151 and slots 149, especially with regard to height relative to rotatable flange 147, are selected so that when rotatable flange 147 is rotated to the closed position, flange portions 157 are radially translated to overlie or engage portions of tongues 151, such engagement shown in 18 and affixing tool 129 or adapter 155 received in connector 145 at the desired, predetermined height, angle of orientation, and rotational position relative to the anatomical feature of the patient. Tongues 151 described as being associated with tools 129 may either be directly connected to such tools 129, and/or tongues 151 may be located on and mounted to the above-mentioned adapter 155, such as that shown in 12, 17 and 18, such adapter 155 configured to interconnect at least one of the plurality of surgical tools 129 with end-effector 112. In the described implementation, adapter 155 includes two operative portionsa tool receiver 157 adapted to connect the selected one or more surgical tools 129, and the second operative part being one or more tongues 151 which may, in this implementation, be mounted and connected to the distal end of adapter 155. Adapter 155 has an outer perimeter 159 which, in this implementation, is sized to oppose an inner perimeter 161 of rotatable flange 147. Adapter 155 extends between proximal and distal ends 163, 165, respectively and has an adapter bore 167 extending between ends 163, 165. Adapter bore 167 is sized to receive at least one of the plurality of surgical tools 129, and similarly, the distance between proximal and distal ends 163, 165 is selected so that at least one of tools 129 is secured to end-effector 112 at the predetermined, appropriate height for the surgical procedure associated with such tool received in adapter bore 167. In one possible implementation, system 100 includes multiple ones of adapter 155, configured to be interchangeable inserts 169 having substantially the same, predetermined outer perimeters 159 to be received within inner perimeter 161 of rotatable flange 147. Still further in such implementation, the interchangeable inserts 169 have bores of different, respective diameters, which bores may be selected to receive corresponding ones of the tools 129 therein. Bores 167 may comprise cylindrical bushings having inner diameters common to multiple surgical tools 129. One possible set of diameters for bores 167 may be 12, 15, and 17 millimeters, suitable for multiple robotic surgery operations, such as those identified in this disclosure. In the illustrated implementation, inner perimeter 161 of rotatable flange 147 and outer perimeter 159 of adapter 155 are circular, having central, aligned axes and corresponding radii. Slots 149 of rotatable flange 147 extend radially outwardly from the central axis of rotatable flange 147 in the illustrated implementation, whereas tongues 151 of adapter 155 extend radially outwardly from adapter 155. In still other implementations, end-effector 112 may be equipped with at least one illumination element 171 14 and 15 orientable toward the anatomical feature to be operated upon. Illumination element 171 may be in the form of a ring of LEDs 177 14 located within adapter 167, which adapter is in the form of a bushing secured to tool locking mechanism 117. Illumination element 171 may also be a single LED 179 mounted on the distal surface 123 of end-effector 112. Whether in the form of LED ring 177 or a single element LED 179 mounted on distal surface of end-effector 112, or any other variation, the spacing and location of illumination element or elements 171 may be selected so that tools 129 received through bore 133 of end-effector 112 do not cast shadows or otherwise interfere with illumination from element 171 of the anatomical feature being operated upon. The operation and associated features of end-effector 112 are readily apparent from the foregoing description. Tool stop 121 is rotatable, selectively lockable, and movable between engaged and disengaged positions, and a sensor prevents movement of end-effector 112 when in such disengaged position, due to the potential presence of a tool which may not be advisably moved during such disengaged position. Tool-insert locking mechanism 117 is likewise rotatable between open and closed positions to receive one of a plurality of interchangeable inserts 169 and tongues 151 of such inserts, wherein selected tools 129 may be received in such inserts 169; alternately, tongues 151 may be otherwise associated with tools 129, such as by having tongues 151 directly connected to such tools 129, which tongue-equipped tools likewise may be received in corresponding slots 149 of tool-insert locking mechanism 117. Tool-insert locking mechanism 117 may be rotated from its open position in which tongues 151 have been received in slots 149, to secure associated adapters 155 and/or tools 129 so that they are at appropriate, respective heights, angles of orientation, and rotational positions relative to the anatomical feature of the patient. For those implementations with multiple adapters 155, the dimensions of such adapters 155, including bore diameters, height, and other suitable dimensions, are selected so that a single or a minimized number of end-effectors 112 can be used for a multiplicity of surgical tools 129. Adapters 155, such as those in the form of interchangeable inserts 169 or cylindrical bushings, may facilitate connecting an expanded set of surgical tools 129 to the end-effector 112, and thus likewise facilitate a corresponding expanded set of associated surgical features using the same end-effector 112. 21 and 22 illustrate in schematic form yet another possible implementation of a robot system 100 as hereinbefore described, with like reference numbers referring to like components. In this implementation, surgical robot system 100 includes features for drilling a hole or bore in a cranium of a head 128 2 of a patient 210 1B, such as in connection with cranial surgery. Surgical robot system 100 includes some or all of the various features and functionalities discussed with reference to the previous embodiments, including, for example, a surgical robot 102, robot arm 104 connected to surgical robot 102 and an end-effector 112 connected to robot arm 104 and orientable to oppose the cranium of the patient 210 1B, such end-effector 112 thereby being in sufficient proximity to perform the contemplated operation. One of the surgical tools 129 suitable for use with end-effector 112 is a perforator 222 connectable to end-effector 112. Perforator 222 is shown schematically in this disclosure, for purposes of illustrating its conical oscillations and other related operations and features. Accordingly, it will be appreciated that the dimensions of the perforator 222 are not to scale, and that the diameter of perforator 222, as well as the perforator bit and perforator tip discussed herein are often larger than as illustrated in 21 and 22, both in absolute terms and also relative to other features shown. Similarly, perforator length and other dimensions of those of components associated with perforator 222, may likewise vary, depending on the application. For certain cranial applications, perforator bit diameters ranging from 1 cm to 3 cm have been found suitable, and still other diameters are contemplated within the scope and spirit of this disclosure. End-effector 112, in certain implementations, may comprise a robot wrist 226 or a similar arrangement of moveable components so that the perforator 222 is connectable to end-effector 112 and configured to be advanced or withdrawn along a trajectory line L relative to the cranium of patient's head 128. Perforator 222 has an elongated perforator bit 224 terminating in a sharp perforator tip 228. Perforator bit 224 has a corresponding bit diameter as well as an internal mechanical clutch not shown operable to selectively engage or disengage rotation of bit 224 so as to drill a bore in a cranium for contemplated surgical procedures, including any number of craniotomies, such as DBS electrode placement and other associated operations associated with cranial surgery. The clutch mechanism is spring-loaded longitudinally, so that the spring forces interfacing portions apart from each other when there is no forward force present opposing the bit. With the clutch mechanism separated, although the inner shaft of the drill continues to spin, the bit is not engaged and does not spin. With forward thrusting longitudinal force, as the spring is overcome, the portions of the clutch mechanism are forced together, allowing the rotating shaft to engage the bit and cause the bit to rotate with the shaft. In one suitable mode of operation, clutch engages and rotates bit 224 as interfacing portions of the clutch are forced together by resistance or similar opposing or reactive force during advancement of bit 224, and clutch disengages rotation when interfacing portions move apart after the leading edge of the bit penetrates past an internal wall of the cranium, such penetration associated with insufficient force to keep the portions of the clutch engaged. Certain operations of perforator 222 may be performed manually, such as manual advancement or withdrawal of perforator bit 224 relative to the cranium. This and other operations may be performed by various computer implementations, including a processor circuit associated with such computerized implementation, a memory accessible by the processor circuit and comprising machine-readable instructions for performing various steps associated with perforator 222. In one possible implementation, the machine-readable instructions, when executed, cause perforator 222 not only to maintain perforator tip 228 along the trajectory line L during advancement of bit 224 toward the cranium, but they further cause the perforator bit to move in a conical oscillation relative to trajectory line L. 21 shows such conical oscillation movement at one point during the orbiting of perforator bit 224 about trajectory line L, this conical motion evident with the leading tip of the perforator coincident with line L while the trailing end of the perforator is slightly offset from line L, such orbiting and the associated advancement being accomplished by suitable machine-readable instructions for moving robot arm 104, and components comprising or secured to robot arm 104. Movements to achieve suitable conical oscillation may include computer-directed movements in the directions of arrows A, B, and C. As a result of moving perforator bit 224 in a conical oscillation while also advancing it through the cranium, the bore formed in the cranium is generally circular and has a diameter larger than the bore diameter which would have been formed without such conical oscillation movement during advancement of the perforator tip. As such, a bore is formed in the cranium having a diameter not only larger than the bit diameter, but the oscillation is selected so that the resulting bore of larger diameter reduces frictional force which might otherwise oppose withdrawal of perforator bit 224 from the bore formed in the cranium after penetration of the inner wall of such cranium. Such frictional force is otherwise associated with jamming of the perforator bit 224 in the cranium and therefore the larger bore accomplished by conical oscillation reduces the risk of such jamming. In certain implementations, orienting the perforator bit at an angle B relative to trajectory line L ranging from about 1 to about 3 has been found suitable, and in certain other implementations, an angle of about 2 relative to trajectory line L has been found suitable. The above-described conical oscillation movement may be initiated, controlled, or otherwise implemented in association with either manual or automatic steps in any number of ways. For example, oscillation may be imparted by the robotic system 100 including a suitable user interface and machine-readable instructions corresponding to a perforator mode. As such, the conical oscillation may be initiated, caused or controlled by user selection of such perforator mode through the user interface. In other possible implementations, end-effector 112 in the form of robot wrist 226 may be advanced by manually moving robot arm 104 or other components associated with advancement of end-effector 112 and similarly, withdrawing up trajectory line L after perforation of the cranium may be performed manually. In still other implementations, a load cell 232 may be operatively connected to robot wrist 226. Load cell 232 is configured to sense reactive force corresponding to perforator bit 224 engaging the cranium. Load cell 232 likewise is able to detect a reduction in such reactive force by a predetermined amount. Such predetermined amount is selected or corresponds to the drop-off of reactive force corresponding to completion of perforation of the cranium. As such, the system 100 may include further machine-readable instructions responsive to the load cell detecting such reduction in reactive force by the predetermined amount. One set of instructions in response to detection of reduction in the reactive force may result in generation of a user perceptible signal or alarm, especially useful if the perforator 222 is being advanced manually, so that the user may cease manual advancement upon receiving the signal generated by system 100. The perforator mode of system 100 may include more automated control of perforator 222, such that, upon load cell 232 detecting the reduction in reactive force corresponding to completing perforation, the automatic advancement accomplished by system 100 of perforator 222 is automatically ceased. Control of advancement and withdraw of the perforator may be further automated so that, upon detection by the load cell of the requisite reduction in reactive force, not only is advancement ceased, but the robot arm, wrist, or perforator are withdrawn along trajectory line L thereafter. In still another possible variation of control of perforator 222 by suitable machine-readable instructions, robot wrist 226 may be located so as to be accessible to the user and load cell 232 is configured to sense manual application of forward and rearward force to either the end-effector 112, robot arm 102, or robot wrist 226. Such manual force may be a tap forward or a tap back. In response to detecting such force, machine-readable instructions may cause perforator 222 to move down trajectory line L in response to sensing a forward tap and may cause the perforator 222 to be moved up the trajectory line L in response to sensing a rearward tap. In view of the foregoing, system 100 may, in certain implementations include suitable instructions to inter-relate signals from load cell 232, corresponding to the presence and amount of reactive force experienced by perforator bit 224 with features of a clutch or motor controller 230 with electronic signal inputs related to selective rotation of perforator bit 224. In this way, system 100 may control not only advancement or withdrawal of perforator 222 by means of load cell 232, but may also interact with or replace clutch 230 to start, stop or vary speed of rotation of perforator bit 224. Such interactions may be a function of the amount of reactive force sensed by load cell 232, including cessation of such rotation upon fall-off of reactive force sensed by the load cell 232 by one or more predetermined amounts. Although the implementations illustrated and described above with reference to 21 contemplate movement of one or more components of robot arm 104 to impart the necessary conical oscillation, it is within the spirit and scope of this disclosure to control perforator 222 and its associated conical oscillation by controlled movements of other components of robot 102. For example, in conjunction with robot arm 104 or in place thereof, a linear slide 234, as shown in 22, may be used to impart the desired conical oscillation, as well as to advance or withdraw perforator 222 and its associated perforator bit 224 during the drilling of a hole or bore in a cranium. Linear slide 234 includes inner and outer cylindrical assemblies, which may be oriented concentrically so that each cylinder's central axis corresponds to the directions of advancement and withdrawal along trajectory line L. A pair of tracks 236 extends vertically at locations relative to trajectory line L and is defined in circular members 238 approximately 180 degrees from each other. The inner cylindrical assembly includes a ring 240 with tongues slideably received within grooves 236. Radially inwardly from ring 240, a bracket 244 is connected to a guide tube 248 within which perforator 222 is suitably connected. A rotary bearing 242 rotatably connects bracket 244 to ring 240. The movement vertically of ring 240 relative to grooves 236 corresponds to advancement or withdrawal of perforator 222, rotary movement of rotary bearing 242 in a circular fashion corresponds to conical oscillation, as it allows bracket 244 to spin within ring 240 while swivel perforator bit 224 is held at an angle to bracket 244. Bracket 244 includes offset slots 246 at upper and lower ends of bracket 244. Offset slots 246 provide a track within which cylindrical bumpouts on guide tube 248 move, permitting perforator tip 228 to be maintained at adjustable positions near or on trajectory line L, while also allowing perforator tail end to be positioned at adjustable offsets near or on trajectory line L that may be different than offsets of perforator tip 228. To that end, the radius of the cone associated with the conical oscillation may vary depending on the location of guide tube 248 within bracket 244, and location of perforator bit 224 and perforator tail end relative to line L. The operation of linear slide 234 is apparent from the foregoing description. The appropriate angle of perforator bit 224 may be set by manual or automatic movement of guide tube 248 within offset slots 246. Suitable moveable connections and associated motors are actuatable in response to programmable logic controllers and associated machine-readable instructions to impart rotation associated with the conical oscillation of perforator bit 224 by rotary bearing 242 being rotated within ring 240 by suitable instructions and associated rotation means. As perforator bit 224 is orbited at its angle relative to trajectory line L, perforator tip 228 is maintained along trajectory line L and advanced by relative movement of ring 240 within grooves 236 near slide 234. Alternately, it will be appreciated that linear slide 234 may be adjusted so that guide tube 248 is coaxial with trajectory line L. Such coaxial alignment would occur when guide tube 248 is moved to points within offsets 246 corresponding to such axial alignment. In such axial alignment, both rotation and advancement of perforator bit 224 may occur without conical oscillation associated therewith. During operations of linear slide 234, robot wrist 226 and robot arm 104 may remain stationary during all or part of advancement or withdrawal of perforator bit 224 relative to the cranium and the bore being formed therein. Operations of the implementations disclosed with reference to 21 and 22, and its attendant advantages are likewise readily appreciated from the foregoing description. The machine-readable instructions associated with control of perforator 222 may be in the form of a computer program product stored on a non-transitory machine-readable medium and machine instructions will perform various steps associated with drilling a bore in a cranium. For example, in response to execution of suitable machine-readable instructions, perforator tip 228 will be maintained along trajectory line L during rotation of the perforator bit 224 and its advancement corresponding to a desired cranial perforation. A conical oscillation will be imparted to the perforator bit 224 by suitable computer control during advancement into the cranium by orbiting the perforator bit 224 at a predetermined radial distance from trajectory line L while maintaining the perforator tip 228 aligned with trajectory line L during such advancement. The resulting conical oscillation, that is, orbiting of the perforator bit and maintaining the perforator tip aligned with its trajectory lines will define an angle relative to such trajectory line and such angle has been selected so that the bore formed upon perforation has a greater diameter than the bit diameter itself. The increased diameter is selected to be sufficient to reduce frictional force opposing withdraw of the bit from the bore. The above-described orbiting of the perforator bit and maintenance of perforator tip in alignment with the trajectory line may be selectively combined with any number of related operations of robot 102 and its system 100, as well as manual operations associated with cranial procedures. In one possible workflow, while in perforator mode, the robot generates the continuously cycling conical motion as described above while still allowing the operator to cause an associated end-effector 112 to advance or withdraw along the pathway through the center of the cone defined by such conical motion. As such, with robot or automatic control of the aforesaid conical motion, a surgeon operating system 100 can concentrate on causing perforator 222 to advance as desired to create the planned craniotomy, and the surgeon's operations may be similar to those associated with advancement without such conical oscillation. In this workflow, then, the complexity of the surgeon's manipulations of robot 102 is reduced by removing the need to manually oscillate the perforator bit while also advancing it. In another possible workflow, a patient's anatomical coordinate system is registered to the coordinate system of robot 102 or tracking cameras or other related coordinate systems discussed in this application with reference to the implementations of 1-19. A trajectory, including trajectory line L, is planned for entry into the cranium by suitable manipulation of medical images. Robot 102 and its associated end-effector 112 may be automatically or manually positioned in operative proximity to the cranium so as to be located and oriented along desired trajectory line L. User activation may be used through a suitable interface to start perforator mode at which point robot 102 imparts the conical oscillation to perforator bit 224, namely, keeping perforator tip 228 located at a suitable point along the desired trajectory line L while moving more perforator bit 224 proximal points away from perforator tip 228 in a circular motion at some radius spaced from the desired trajectory line L. Movement of end-effector 112, including components of robot wrist 226, may be effectuated by any suitable manual or automatic means. Force forward to advance end-effector 112 and perforator 222 secured thereto may be applied manually, either as an initial tap or continuously, in either event such forward thrust is sensed by a load cell and in response thereto, the end-effector 112 and robot wrist move so as to advance down the trajectory line. Suitable machine-readable instructions of software may alternately be used so that the perforator 222 is advanced down the trajectory line and feedback from the load cell 232 may be used to control such forward movement, the reduction of reactive force signaling the successful perforation of the cranium and ceasing further advanced movement in such automatic mode. Once the cranium has been penetrated the end-effector 112 and its associated perforator 222 are moved back up the trajectory line either manually using reverse operations of those described previously or automatically through suitable machine-readable instructions. Upon completion of the desired perforation or at any other suitable moment during the foregoing procedures, the perforator mode and its associated conical oscillation may be deactivated either in response to certain conditions as mentioned above or in response to user input. In those procedures where the robot and its perforator 222 are being moved manually, system 100 may include a graphical display of the force sensed by load cell 232 which may serve as a useful guide to the surgeon as to when such surgeon should stop applying manual force during the perforation procedure. In those circumstances where linear slide 234 is used, rather than oscillating the entire robot arm, in certain circumstances, quicker disengagement of perforator 222 may be accomplished because withdrawal of such perforator can be accomplished by the linear slide 234 while robot arm 104 remains in its position for other related procedures. As can be understood from the foregoing description, the hole drilled using either coordinated robot arm movement or advancement of a linear slide 234 would be conical in shape, wherein the deeper the penetration, the larger the hole radius becomes as a wider portion of the fixed conical pattern of drill bit movement cuts bone. In some applications, however, it may be desirable to drill a first hole to the depth of the cranial bone that is then expanded radially without advancing the drill deeper and damaging brain tissue. In other applications, it may be desirable to limit the amount of bone dust created while precisely fitting an implantable surgical device by using a smaller diameter cutting bit and continuously adjust the offset of the tip and trailing end of the perforator so that it cuts with a perfectly conical path at all positions longitudinally down trajectory line L. In other words, the cutting pattern at the tip would start as a sweeping circular pattern on first penetration and gradually narrow its path of travel until the tip is at or close to a fixed point at final penetration, leaving a conical cut through bone that has a radius larger than the radius of the drill bit. To achieve an expanded hole after a first hole is completed and to cut while narrowing the path of travel during advancement, the offsets of tip and tail of the perforator relative to trajectory line L would be adjusted automatically or manually at or after different points during drill advancement. Adjustment of the cutting path may involve adjustment of the oscillation imparted to perforator 222 by either robot wrist 226 or linear slide 234. Such adjustment would be a function of where perforator tip 228 and perforator tail are positioned relative to trajectory line L, as described previously with reference to 22 or when not using a linear slide, by altering the commands driving the robot arm to perform its oscillations. In certain implementations, then, one or more sensors, such as Hall Effect sensors, optical tracking, LVDT linear variable differential transformer, resistive wiper, and the like may be used to continuously or periodically monitor location of perforator tip 228 and perforator tail relative to the location of linear slide 234 within grooves 236 22 and relative to the cranium. Location of perforator tip 228 determined by the foregoing sensors will allow suitable instructions of system 100 to calculate and apply the necessary offsets at offset slides 246 to create oscillation that is conical or cylindrical with appropriate oscillation radius while appropriately locating perforator tip 228 and tail along the desired trajectory and in turn angling the balance of perforator bit 224 proximally to achieve a perforation bore of the desired diameter to avoid potential jamming. In other implementations, tracking of the robot arm through similar sensors may be used to continuously or periodically monitor location of robot arm relative to cranium. Location of perforator tip 228 determined by the foregoing sensors will allow suitable instructions of system 100 to calculate and apply the necessary robotic movement to create oscillation that is conical or cylindrical with appropriate oscillation radius while appropriately locating perforator tip 228 and tail along the desired trajectory and in turn angling the balance of perforator bit 224 proximally to achieve a perforation bore of the desired diameter to avoid potential jamming. In the above-description of various embodiments of present inventive concepts, it is to be understood that the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of present inventive concepts. Unless otherwise defined, all terms including technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which present inventive concepts belong. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of this specification and the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein. When an element is referred to as being connected, coupled, responsive, or variants thereof to another element, it can be directly connected, coupled, or responsive to the other element or intervening elements may be present. In contrast, when an element is referred to as being directly connected, directly coupled, directly responsive, or variants thereof to another element, there are no intervening elements present. Like numbers refer to like elements throughout. Furthermore, coupled, connected, responsive, or variants thereof as used herein may include wirelessly coupled, connected, or responsive. As used herein, the singular forms a, an and the are intended to include the plural forms as well, unless the context clearly indicates otherwise. Well-known functions or constructions may not be described in detail for brevity and/or clarity. The term and/or includes any and all combinations of one or more of the associated listed items. It will be understood that although the terms first, second, third, etc. may be used herein to describe various elements/operations, these elements/operations should not be limited by these terms. These terms are only used to distinguish one element/operation from another element/operation. Thus a first element/operation in some embodiments could be termed a second element/operation in other embodiments without departing from the teachings of present inventive concepts. The same reference numerals or the same reference designators denote the same or similar elements throughout the specification. As used herein, the terms comprise, comprising, comprises, include, including, includes, have, has, having, or variants thereof are open-ended, and include one or more stated features, integers, elements, steps, components or functions but does not preclude the presence or addition of one or more other features, integers, elements, steps, components, functions or groups thereof. Furthermore, as used herein, the common abbreviation , which derives from the Latin phrase exempli gratia, may be used to introduce or specify a general example or examples of a previously mentioned item, and is not intended to be limiting of such item. The common abbreviation i. e. , which derives from the Latin phrase id est, may be used to specify a particular item from a more general recitation. Example embodiments are described herein with reference to block diagrams and/or flowchart illustrations of computer-implemented methods, apparatus systems and/or devices and/or computer program products. It is understood that a block of the block diagrams and/or flowchart illustrations, and combinations of blocks in the block diagrams and/or flowchart illustrations, can be implemented by computer program instructions that are performed by one or more computer circuits. These computer program instructions may be provided to a processor circuit of a general purpose computer circuit, special purpose computer circuit, and/or other programmable data processing circuit to produce a machine, such that the instructions, which execute via the processor of the computer and/or other programmable data processing apparatus, transform and control transistors, values stored in memory locations, and other hardware components within such circuitry to implement the functions/acts specified in the block diagrams and/or flowchart block or blocks, and thereby create means functionality and/or structure for implementing the functions/acts specified in the block diagrams and/or flowchart blocks. These computer program instructions may also be stored in a tangible computer-readable medium that can direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable medium produce an article of manufacture including instructions which implement the functions/acts specified in the block diagrams and/or flowchart block or blocks. Accordingly, embodiments of present inventive concepts may be embodied in hardware and/or in software including firmware, resident software, micro-code, that runs on a processor such as a digital signal processor, which may collectively be referred to as circuitry, a module or variants thereof. It should also be noted that in some alternate implementations, the functions/acts noted in the blocks may occur out of the order noted in the flowcharts. For example, two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order, depending upon the functionality/acts involved. Moreover, the functionality of a given block of the flowcharts and/or block diagrams may be separated into multiple blocks and/or the functionality of two or more blocks of the flowcharts and/or block diagrams may be at least partially integrated. Finally, other blocks may be added/inserted between the blocks that are illustrated, and/or blocks/operations may be omitted without departing from the scope of inventive concepts. Moreover, although some of the diagrams include arrows on communication paths to show a primary direction of communication, it is to be understood that communication may occur in the opposite direction to the depicted arrows. Although several embodiments of inventive concepts have been disclosed in the foregoing specification, it is understood that many modifications and other embodiments of inventive concepts will come to mind to which inventive concepts pertain, having the benefit of teachings presented in the foregoing description and associated drawings. It is thus understood that inventive concepts are not limited to the specific embodiments disclosed hereinabove, and that many modifications and other embodiments are intended to be included within the scope of the appended claims. It is further envisioned that features from one embodiment may be combined or used with the features from a different embodiments described herein. Moreover, although specific terms are employed herein, as well as in the claims which follow, they are used only in a generic and descriptive sense, and not for the purposes of limiting the described inventive concepts, nor the claims which follow. The entire disclosure of each patent and patent publication cited herein is incorporated by reference herein in its entirety, as if each such patent or publication were individually incorporated by reference herein. Various features and/or potential advantages of inventive concepts are set forth in the following claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/16/196/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9724670648574829,
                    "F_SPEC_PARAMS": [
                        "need to be tracked with a high degree of precision"
                    ],
                    "S_SPEC_PARAMS": [
                        "poor outcome",
                        "hole to be too cylindrical",
                        "prevent jamming",
                        "oddly shaped hole that does not hold the electrode housing well",
                        "preventing jamming",
                        "good attachment of the housing of the electrode holder"
                    ],
                    "A_PARAMS": [
                        "small of a conical motion",
                        "conical rotation to the perforator"
                    ],
                    "F_SENTS": [
                        "<s> Position recognition systems for robot assisted surgeries are used to determine the position of and track a particular object in 3-dimensions 3D. In robot assisted surgeries, for example, certain objects, such as surgical instruments, need to be tracked with a high degree of precision as the instrument is being positioned and moved by a robot or by a physician, for example."
                    ],
                    "S_SENTS": [
                        "Poor surgical technique when applying conical motion during perforator usage could lead to poor outcome.",
                        "Too small of a conical motion could cause the hole to be too cylindrical and therefore not effectively prevent jamming of the bit.",
                        "Uneven conical motion could lead to an oddly shaped hole that does not hold the electrode housing well.",
                        "A surgical technique for preventing jamming of the perforator is to apply slight conical rotation to the perforator during drilling.",
                        "The hole that is created while using this technique is slightly conically shaped instead of being cylindrical but still allows good attachment of the housing of the electrode holder."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement"
                    ],
                    "F_SIM_SCORE": 0.6500094532966614,
                    "S_TRIZ_PARAMS": [
                        "Reliability",
                        "Shape"
                    ],
                    "S_SIM_SCORE": 0.47284001111984253,
                    "GLOBAL_SCORE": 1.6672251303990682
                },
                "sort": [
                    1.6672251
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10895086-20210119",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10895086-20210119",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-06-19",
                    "PUBLICATION_DATE": "2021-01-19",
                    "INVENTORS": [
                        "Max Roumagnac"
                    ],
                    "APPLICANTS": [
                        "KOKIDO DEVELOPMENT LIMITED    ( Kowloon , HK )"
                    ],
                    "INVENTION_TITLE": "Autonomous pool cleaning robot",
                    "DOMAIN": "E04H 41654",
                    "ABSTRACT": "A pool cleaning robot that has a water jet electrohydraulic motorized propulsion/pump unit and a waste-collecting body having a battery for powering the unit, the unit and the battery being contained inside a rotary and sealed turret, external to the body of the robot. The robot advantageously has an automatic direction reversing device having a vane secured to the turret with a first end stop and second end stops.",
                    "CLAIMS": "1. Swimming-pool cleaning robot 1 comprising a water-jet electro-hydraulic propulsion/pump unit 31, 34, 35, a waste-collecting body 2, a power supply battery 32 for said unit and a rotary turret 3 outside the body 2 of the robot, the turret 3 being mounted on the body 2 of the robot by way of a rotary connection, the robot comprising an automatic direction reversal device comprising a vane 5, a first stop 52 and second stops 41, 42. 2. Robot 1 according to claim 1, wherein the vane 5 is secured to the turret 3. 3. Robot 1 according to claim 2, wherein the vane 5 has a density less than the density of the water to rise towards a vertical position in the absence of a flow of water, thereby freeing the stops and allowing free rotation of the turret 3. 4. Robot 1 according to claim 3, wherein the hydrodynamic thrust created by the rotation of the turret 3 acts, from the very start of said rotation, on the vane 5, which tilts towards a horizontal position, thereby positioning the first stop 52 in a position of contact with one of the second stops 41, 42 and causing the rotation to stop. 5. Robot 1 according to claim 1, wherein the vane 5 is articulated on a pin 53, bears said first stop 52, which acts as a retractable stop, and comprises, on a side remote from the first stop 52 with respect to the pin 53, a widened part 50 which allows the vane to turn about the pin 53 so as to cause the vane to descend again under the action of the hydrodynamic thrust that is brought about by the rotation of the turret and then by the movement of the robot and is applied to the vane. 6. Robot 1 according to claim 5, wherein the pin for receiving the vane is fixed in the lower part of the turret such that, when the vane is inclined towards the horizontal on account of a rotary movement of the turret or a movement of the robot, the first stop 52 comes into abutment against one of the second stops 41, 42 and such that the first stop is away from the second stops when the vane is in a vertical position with the robot and turret at a standstill. 7. Robot 1 according to claim 1, wherein the body 2 is circular and the turret 3 is centred in the middle of said body. 8. Robot 1 according to claim 1, wherein the unit comprises an electric motor 31 and a turbine 35, coupled to the electric motor by coupling means 34, for sucking in water that enters the body through a mouth 24 under the robot and passes through a filter 21, and for delivering this water through an ejection nozzle 36 that leads out of the turret 3. 9. Robot 1 according to claim 8, wherein the nozzle is positioned so as to deliver the sucked-in water in a direction substantially parallel to the bottom of the swimming pool in order for the robot to be propelled by means of the nozzle 36. 10. Robot 1 according to claim 8, wherein the electric motor is a motor with a power of less than or equal to 50 W. 11. Robot 1 according to claim 1, wherein the turret comprises a leaktight closure 33 for accessing the battery. 12. Robot 1 according to claim 1, wherein the rotary connection comprises an annular collar 25 on the body 2 around a hole for receiving an annular base 37 of the turret.",
                    "FIELD_OF_INVENTION": "The present invention relates to an autonomous swimming-pool cleaning robot.",
                    "STATE_OF_THE_ART": "In order to clean swimming pools and other pools, there exist hydraulic robots which operate using the energy of the swimming-pool filtration unit. These robots are connected either to the delivery side or to the suction side of the filtration pump by a floating line measuring 8 to 12 m. These robots only operate correctly if the filtration installation has sufficient power. They reduce the original filtration performance and the handling and then storage of the lines is impractical. In order to avoid these drawbacks, independent electric filtration robots that are powered by a floating electric cable have been developed. The main advantage of this type of robot, which is delivered with a low-voltage security transformer, is the ease of installation thereof since they are connected to a standard electrical socket. These autonomous robots have the advantage of operating immediately and without adjustment, this representing a clear sales argument. A robot of this type, but cable-powered, is described by 5A and 5B of the document FR2 896 005. According to this design, a member for preventing the rotation of the turret to which the cable is connected, said member being fixed to the front of the rotary turret, is activated by the movement of the robot. One of the main hazards that is encountered with electric robots in general is the tangling of the cable, it being possible, however, for this phenomenon to be limited by the trajectories of the robot being programmed, this requiring traction engines with sophisticated control electronics, however, and/or by a rotating connector that connects the electric cable to the robot or to the electricity supply of the robot. The drawbacks with this type of robot are the handling of the floating cable, which generally measures 8 to 18 m depending on the size of the swimming pool, and the apprehension of some users with regard to the use of electricity in water. In order to remedy these drawbacks, battery-operated wireless robots have been developed. These robots are either powered by a floating battery, as known from document EP 1 122 382 A1, or by on-board batteries that are rechargeable out of the water, as known for example from the document EP 1 689 957 A1, or are rechargeable in the water by induction, as described in the document EP 2 669 450 A1. These robots are often adaptations of cable-powered electric models and their cost is greater than that of the base models from which they are derived. Moreover, electric robots are not actually very suitable for battery operation on account of the fact that some use a programmed or programmable electronic guide system with a gyroscope, inclination sensors, wall detectors and several motors: a pump motor for suction and one or two traction motors. This multiplication of the equipment consumes energy and involves high-capacity batteries. Other robots with a more simple design use a single motor with water-jet propulsion, the direction of which is reversed by a timer, as known for example from the documents EP 2 484 847 A1 or EP 1 022 411 A1. In this case, the robot, which moves randomly, can remain stationary against a wall for a non-negligible period of its cycle while waiting for the reversal in direction. This operation thus consumes energy, this once again involving a high-capacity battery. In order to remedy this problem, the system provided in the document FR2 896 005 A1 provides a cable-powered electric robot in which the movement of the robot is not capable of immobilizing the turret systematically since this movement only takes place after the latter has been immobilized, meaning that the propulsion jet can sometimes rotate permanently and in this case the robot does not move. Another principle known from the abovementioned document FR2 896 005 A1 proposes a robot powered by a floating cable that is propelled by a rotary jet, the direction reversal of which takes place when a tilting bell cover frees a stop. This design results in a bulky appliance since the rotary jet is contained entirely in this bell cover. This type of appliance has high hydrodynamic resistance to movement, and this would involve a powerful pump and thus a high-capacity battery. The invention proposes remedying these various drawbacks by proposing a battery-powered robot having a simple design with a single motor and without on-board electronics, with low hydrodynamic resistance and provided with a system that allows instantaneous reversal of the direction of movement.",
                    "DESCRIPTION": [
                        "To this end, the present invention proposes a swimming-pool cleaning robot comprising, according to a first aspect of the invention, a water-jet electro-hydraulic propulsion unit/pump and a waste-collecting body, said robot comprising a power supply battery for said unit, the unit and the battery being contained in a leaktight rotary turret outside the body of the robot. The unit preferably comprises an electric motor and a turbine, coupled to the electric motor by coupling means, for sucking in water that enters the body through a mouth under the robot and passes through a filter, and for delivering this water through an ejection nozzle that leads out of the turret. The turret advantageously comprises a leaktight closure for accessing the battery. According to an advantageous embodiment, the nozzle is positioned so as to deliver the sucked-in water in a direction substantially parallel to the bottom of the swimming pool in order for the robot to be propelled by means of the nozzle. The turret is advantageously mounted on the body of the robot by way of a rotary connection which comprises an annular collar on the body around a hole for receiving an annular base of the turret. According to a particular embodiment, the rotary connection comprises protuberances for clip-fastening the turret to the body. The suction turbine is preferably of the centrifugal turbine type and comprises an inlet to the interface between the turret and the body. According to a particular embodiment, the inlet to the turbine at the body/turret interface is provided with a funnel-like profile. According to a particularly advantageous embodiment, the motor is a motor with a power of less than or equal to 50 W. According to a second aspect of the invention, the invention provides a robot comprising an automatic direction reversal device comprising a vane secured to the turret and comprising a first stop and second stops. The vane is advantageously articulated on a pin, bears said first stop, which acts as a retractable stop, and comprises, on a side remote from the first stop with respect to the pin, a widened part which allows the vane to turn about the pin so as to cause the vane to descend again under the action of the hydrodynamic thrust that is brought about by the rotation of the turret and then by the movement of the robot and is applied to the vane. The rising of the vane is obtained either as a result of its buoyancy with the robot at a standstill or, with the turret rotating, by the force exerted between the stops under the effect of the rotary torque of the turret. The pin for receiving the vane is preferably fixed in the lower part of the turret such that, when the vane is inclined towards the horizontal on account of a rotary movement of the turret or a movement of the robot, the first stop comes into abutment against one of the second stops and such that the first stop is away from the second stops when the vane is in a vertical position with the robot and turret at a standstill. According to a particular embodiment, the second stops are movable, an offset of one or both stops by an angle on the body of the robot with respect to the axis of movement defined by the wheels making it possible to skew the flow of water exiting the nozzle to a greater or lesser extent with respect to the axis of movement defined by the orientation of the wheels and to bend the trajectory of the robot to a greater or lesser extent. The nozzle is advantageously off-centre on the turret such that the thrust force is exerted along an axis that forms an angle with a main axis of the robot defined by the orientation of the wheels of the robot. According to a particular embodiment, the robot comprises a circular body in the middle of which the turret is centred. The robot can notably comprise three wheels that point in parallel directions. Alternatively, the robot can comprise two wheels and a roller. In order to avoid a situation in which the robot is immobilized on a break in the gradient of the pool bottom, the bottom of the robot can comprise at least one relief that is positioned under the robot on the axis of movement of the robot. The front roller or wheel can also be mounted on a pivoting axle. According to a particular embodiment, the robot can comprise a floating solar panel for recharging the battery, said solar panel being connected to the propulsion unit by an electric cable with a length slightly greater than the depth of the swimming pool.",
                        "Further features and advantages of the invention will become apparent from reading the following description of a nonlimiting exemplary embodiment of the invention with reference to the drawings, in which: 1 shows a cross-sectional side view of a robot according to a first aspect of the invention; 2 shows a top view of the robot from 1; 3 shows a perspective view of a turret of a robot according to the invention; 4 shows a bottom view of the turret from 3; 5 shows a perspective top view of a robot body according to one particular embodiment; 6A and 6B show top views of the movement of a robot according to the invention; 7A and 7B show side views of a turret according to one embodiment of the invention in two operating phases; 8 shows a bottom view of a variant of the robot according to the invention; 9 shows a side view of an embodiment of the robot according to the invention on a swimming pool bottom with a break in gradient."
                    ],
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/86/950/108/0.pdf",
                    "CONTRADICTION_SCORE": 0.9646226167678833,
                    "F_SPEC_PARAMS": [
                        "tangling of the cable,"
                    ],
                    "S_SPEC_PARAMS": [
                        "consumes energy,",
                        "high-capacity battery",
                        "suitable for battery operation",
                        "consumes energy",
                        "high-capacity batteries",
                        "cost"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "One of the main hazards that is encountered with electric robots in general is the tangling of the cable, it being possible, however, for this phenomenon to be limited by the trajectories of the robot being programmed, this requiring traction engines with sophisticated control electronics, however, and/or by a rotating connector that connects the electric cable to the robot or to the electricity supply of the robot."
                    ],
                    "S_SENTS": [
                        "This operation thus consumes energy, this once again involving a high-capacity battery.",
                        "Moreover, electric robots are not actually very suitable for battery operation on account of the fact that some use a programmed or programmable electronic guide system with a gyroscope, inclination sensors, wall detectors and several motors: a pump motor for suction and one or two traction motors.",
                        "This multiplication of the equipment consumes energy and involves high-capacity batteries.",
                        "In this case, the robot, which moves randomly, can remain stationary against a wall for a non-negligible period of its cycle while waiting for the reversal in direction.",
                        "These robots are often adaptations of cable-powered electric models and their cost is greater than that of the base models from which they are derived."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Tension Pressure"
                    ],
                    "F_SIM_SCORE": 0.4999447464942932,
                    "S_TRIZ_PARAMS": [
                        "Waste of Energy",
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.6383286863565445,
                    "GLOBAL_SCORE": 1.6670926665266355
                },
                "sort": [
                    1.6670927
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11333899-20220517",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11333899-20220517",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2020-12-04",
                    "PUBLICATION_DATE": "2022-05-17",
                    "INVENTORS": [
                        "Bernhard A. Fuerst",
                        "Pablo Garcia Kilroy",
                        "Joan Savall",
                        "Anette Lia Freiin Von Kapri"
                    ],
                    "APPLICANTS": [
                        "Verb Surgical Inc.    ( Santa Clara , US )"
                    ],
                    "INVENTION_TITLE": "Systems and methods for three-dimensional visualization during robotic surgery",
                    "DOMAIN": "G02B 3026",
                    "ABSTRACT": "An autostereoscopic three-dimensional display system for surgical robotics has an autostereoscopic three-dimensional display configured to receive and display video from a surgical robotics camera, and a first sensor assembly and a second sensor assembly. A processor is configured to detect and track an eye position or a head position of a user relative to the display based on processing output data of the first sensor assembly, and to detect and track a gaze of the user based on processing output data of the second sensor assembly. The processor further is configured to modify or control an operation of the display system based on the detected gaze of the user. A spatial relationship of the display also can be automatically adjusted in relation to the user based on the detected eye or head position of the user to optimize the user's visualization of three-dimensional images on the display.",
                    "CLAIMS": "1. A three-dimensional display system for use in a surgical robotics system, comprising: an autostereoscopic three-dimensional 3D display configured to receive and display video from a surgical robotics camera to facilitate a user's visualization of three-dimensional images on the 3D display without use of 3D glasses; a plurality of sensor assemblies that include a first sensor assembly configured to capture information related to eyes of the user, and a second sensor assembly that illuminates an eye of the user and is configured to capture information that tracks a gaze of the user; and a processor that detects and tracks the eyes of the user relative to the 3D display based on processing output data of the first sensor assembly and automatically adjusts a spatial relationship which maintains a prescribed distance between the eyes of the user and the 3D display and centers the eyes of the user relative to the 3D display, including adjusting a position or an orientation of a seat on which the user is to sit, based on the detected eyes of the user to affect the user's visualization of the three-dimensional images on the 3D display, and the processor that detects and tracks the gaze of the user based on processing output data of the second sensor assembly. 2. The display system of claim 1, wherein the 3D display comprises a panel display and one or more layers at least partially positioned over the panel display, the one or more layers include at least one of polarizing filters, a pattern retarder, or dynamic shutters. 3. The display system of claim 1 wherein the processor communicates a signal to a controller of a surgical robotic system to pause an operation of the surgical robotic system when it detects that the tracked gaze of the user is not directed towards the 3D display. 4. The display system of claim 2, wherein the 3D display further comprises a protective layer covering the one or more layers positioned on the panel display. 5. The display system of claim 4, wherein the protective layer includes surgical grade glass that is bonded to the one or more layers or to the panel display by an optically clear adhesive. 6. The display system of claim 1, wherein the 3D display comprises a panel display and one or more layers at least partially positioned over the panel display, the one or more layers includes a plurality of micro-lenses positioned to facilitate the user's visualization of three-dimensional images on the 3D display without the use of glasses or other wearable components. 7. The display system of claim 1, wherein the first sensor assembly and the second sensor assembly are physically attached to or integrated with the 3D display. 8. The display system of claim 1, wherein the first sensor assembly comprises an infrared camera, and wherein the second sensor assembly further comprises a high-speed camera. 9. The display system of claim 1, further comprising: a seat assembly having the seat in which the user is to sit, and a power adjustable seat support assembly connected to the seat and configured to change the position or the orientation of the seat based on receiving a signal from the processor. 10. The display system of claim 1, further comprising: a power adjustable display support assembly connected to and supporting the 3D display, and configured to change a position or an orientation of the 3D display based on receiving a signal from the processor, wherein the processor signals the power adjustable display support assembly to automatically change the position or orientation of the 3D display based on the detected eyes of the user. 11. The display system of claim 1, wherein a control panel having a plurality of icons is displayed on the 3D display, and wherein an application related to a displayed icon of the plurality of icons is invoked when the processor determines that the detected gaze of the user is directed at the displayed icon. 12. The display system of claim 1, wherein a position or orientation of the surgical robotics camera is changed based upon the detected gaze of the user. 13. The display system of claim 1, wherein three-dimensional image data received from the surgical robotics camera is displayed on the 3D display, and wherein when the processor determines that the detected gaze of the user is directed at an area or point on the three dimensional image data that is spaced apart from a center of the 3D display, the processor generates and sends a signal to the surgical robotics camera to adjust a position or an orientation of the surgical robotics camera such that the area or point on the three-dimensional image data is moved so as to be centered along the 3D display. 14. A method performed by a digital programmed processor executing instructions stored in a computer readable memory, the method comprising: receiving a video signal from a surgical robotics camera and rendering images on an autostereoscopic three-dimensional 3D display based upon the received video signal to facilitate a user's visualization of 3D images on the 3D display without use of 3D glasses; detecting and tracking eyes of the user relative to the 3D display based on processing output data from a first sensor; detecting and tracking a gaze of the user based on processing output data from a second sensor, via illumination of an iris of the user; and automatically signaling an actuator subsystem to adjust a spatial relationship of the 3D display in relation to the user which maintains a prescribed distance between the eyes of the user and the 3D display and centers the eyes of the user relative to the 3D display, based on the detected eyes of the user to optimize the user's visualization of three-dimensional images on the 3D display, by signaling the actuator subsystem to adjust a position or orientation of a seat assembly where the user is seated. 15. The method of claim 14 further comprising signaling to a controller of a surgical robotic system to pause an operation of the surgical robotic system when it is detected that the tracked gaze of the user is not directed towards the 3D display. 16. The method of claim 15 wherein a control panel having a plurality of icons is displayed on the 3D display, the method further comprising invoking an application related to a displayed icon of the plurality of icons in response to the detected gaze of the user. 17. The method of claim 16, wherein the application is a timer application that is started or stopped based on the detected gaze of the user. 18. The method of claim 16, wherein the application is an x-ray viewing tool that is initiated or controlled based on the detected gaze of the user. 19. The method of claim 15 further comprising determining that the gaze of the user is directed at an area or point on three dimensional image data which is shown on the 3D display that is spaced apart from a center of the 3D display; and adjusting a position or an orientation of the surgical robotics camera based on the gaze of the user such that the area or the point on the three-dimensional image data becomes centered along the 3D display. 20. The method of claim 16, wherein a power adjustable display support assembly is connected to and supporting the 3D display, and configured to change a position or an orientation of the 3D display based on the detected eyes of the user, in response to a signal from the digital programmed processor.",
                    "FIELD_OF_INVENTION": "This disclosure relates generally to the field of surgical robotics and, more particularly, to display systems for use with surgical robotic systems for visualizing the surgical site.",
                    "STATE_OF_THE_ART": "Minimally-invasive surgery MIS, such as laparoscopic surgery, involves techniques intended to reduce tissue damage during a surgical procedure. For example, laparoscopic procedures typically involve creating a number of small incisions in the patient , in the abdomen, and introducing one or more tools and at least one endoscopic camera through the incisions into the patient. The surgical procedures are then performed by using the introduced tools, with the visualization aid provided by the camera. Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery. In some embodiments, MIS may be performed with surgical robotic systems that include one or more robotic arms for manipulating surgical instruments based on commands from an operator. For example, an operator may provide commands for manipulating surgical instruments, while viewing an image that is provided by a camera and displayed on a display to the user. However, conventional display systems fall short in enabling effective control of the display systems or of surgical robotic systems. Furthermore, conventional display systems generally provide two-dimensional 2-D surgical image data to the user, and current three-dimensional 3-D displays typically require the user to wear glasses or additional, similar wearable components , with polarizing filters or dynamic shutters for visualization of three-dimensional images. Such glasses and additional wearable components, however, may be problematic to use and handle in surgical or sterile environments. Thus, there is a need for improved 3-D display systems that enable a user to better visualize the surgical site during robotic surgery.",
                    "SUMMARY": [
                        "Generally, a three-dimensional display system for use with a surgical robotic system can include a three-dimensional display configured to receive and display video from a surgical robotics camera, such as an endoscopic camera. The display system can include a plurality of sensor assemblies having a first sensor assembly and a second sensor assembly. The first sensor assembly and the second sensor assembly can be coupled to or integrally formed with the display. The display system can include a processor or controller configured to detect and track an eye position or a head position of a user relative to the display based on processing output data of the first sensor assembly. The processor or controller also can be configured to detect and track a gaze of the user based on processing output data of the second sensor assembly. The processor or controller further is configured to modify or control an operation of the display system based on the detected and tracked gaze of the user, for example, to facilitate control of the display system with the user's eyes or eye motions. In addition, a spatial relationship of the display can be automatically adjusted in relation to the user based on the detected eye or head position of the user. For example, a distance or orientation between the detected eye or head position and the display can be automatically , without requiring deliberate user input updated to adjust the user's visualization of three-dimensional image data from the surgical robotics camera on the display. In some variations, the display can include a panel display or monitor, such as an LCD, LED, plasma, or other suitable flat, curved, or otherwise shaped panel display or monitor, having a plurality of pixels for displaying two or three-dimensional images. The display further can include one or more layers at least partially positioned over the panel display and configured to facilitate a user's visualization of three-dimensional images on the panel display. The one or more layers can include a polarizing filter, a pattern retarder, or dynamic shutters that allow users to uses glasses or other wearable components to view or visualize the three-dimensional images on the panel display. Alternatively, the one or more layers can include layers of micro-lenses that can at least partially cover the plurality of pixels of the panel display. The layers of micro-lenses further can be positioned or disposed in relation to the plurality of pixels to facilitate or otherwise allow the user's visualization or perception of three-dimensional images on the panel display, without the use of three-dimensional glasses or other additional wearable or similar components worn by a user. The display further can include a protective layer at least partially disposed over or sealing off the layers of the panel display. The protective layer can be bonded to layers or the panel display using an adhesive, such as an optically clear adhesive or other suitable adhesive. An additional protective layer can be provided on the display panel, , between the one or more layers including micro-lenses and the display panel. In some variations, the first sensor assembly can include at least one camera, such as a stereo camera, an infrared camera, or other suitable camera that does not filter infrared light, , to allow for detection and tracking of a head or eye position of a user , an xyz position of the user's head or eye position in relation to an origin or original position or to the display. The second sensor assembly can include one or more cameras and a plurality of strobes or strobe lights, , to allow for illumination of and detection and tracking of an iris or irises of the user's eyes. In addition, a seat assembly can be provided with the display system. The seat assembly can have a seat in which a user is to sit or otherwise engage, while the user is viewing the display. The seat assembly also can include a movable or adjustable seat support assembly that is connected to and at least partially supports the seat. The processor or controller can automatically generate and send one or more signals or other output data to an actuator subsystem of the seat support assembly to adjust or update a position or orientation of the seat based upon received output data from the first or second sensor assemblies. For example, the position or orientation of the seat can be adjusted based on the detected and tracked eye or head position of the user to optimize the user's visualization of three-dimensional images on the display. The display system also can include a movable or adjustable display support assembly connected to and supporting the display. The processor or controller can automatically generate and send one or more signals or other output data to an actuator subsystem of the movable or adjustable display support assembly to adjust or update a position or an orientation of the display based upon received output data from the first or second sensor assemblies. For example, the position or orientation of the display can be adjusted based on the detected and tracked eye or head position of the user to optimize the user's visualization of three-dimensional images from the surgical robotics camera on the display. In one example, the position or orientation of the seat or the display can be automatically adjusted or changed such that the user's head or eyes are located at a predetermined distance from or orientation in relation to the display. In some variations, the processor or controller can be in communication with the surgical robotic system. The processor further can be operable to send a signal or other output data to the surgical robotic system, , to a controller thereof, for control of the surgical robotic system based on received output data from the first or second sensor assemblies. For example, when the gaze of the user is not directed towards the display, , for a predetermined time interval, control of one or more operations of the surgical robotic system , operations of robotic arms or surgical instruments may be paused or otherwise disabled. Additionally, an endoscopic image or other suitable image of a surgical site from the surgical robotics camera may be displayed on the display, , as part of a GUI or display window on the display. Control panels or side panels having a plurality of icons or images additionally or alternatively can be displayed on the display. For example, control or side panels can be positioned to the left and right of the primary display or window on the display. The plurality of icons or images can be related to applications for the display system or the surgical robotic system. The detected and tracked gaze of the user further can be used to initiate or control the applications in the control/side panels. For example, a user can focus their gaze on the images or icons shown the control or side panels to trigger application interactions , to start and stop a timer application, initiate or control an x-ray viewing tool, enlarge a view, or to initiate or control other suitable applications. In some variations, a position or orientation of the surgical robotics camera can be dynamically or continuously updated based on the detected and tracked gaze of the user. For example, the position of the surgical robotics camera can be automatically updated such that an area or point substantially focused on by the user's gaze, , an area or point within the primary display or window showing the endoscopic image, is substantially centered on the display. In one embodiment, when the processor or controller determines that the detected gaze of the user is directed at an area or point that is spaced apart from the center of the display, the processor or controller generates and sends a signal to the surgical robotics camera to adjust the position or orientation of the surgical robotics camera such that the area or point at which the user's gaze is directed or focused on is moved to the center of the display. Furthermore, a method for three-dimensional visualization during robotic surgery can be provided. The method can be performed by a digital programmed processor executing instructions stored in a computer readable memory. The method can include receiving and displaying video from a surgical robotics camera on a three-dimensional display. The method further can include detecting and tracking a head position or an eye position of a user relative to the display based on processing output data of a first sensor assembly, and detecting and tracking a gaze of the user based on processing output data of a second sensor assembly. The detected and tracked gaze of the user can be used to facilitate control or modify operations of a display system or a surgical robotic system. In addition, the method can include automatically , without requiring deliberate user input signaling an actuator subsystem to adjust or update a spatial relationship of the display in relation to the user based on the detected eye or head position of the user to optimize the user's visualization of three-dimensional images from the surgical robotics camera on the display. In some variations, a position or orientation of the display or a seat assembly, which is configured to be sat in or otherwise engaged by the user when viewing the display, can be automatically adjusted or modified based upon the detected and tracked head or eye position of the user. In further variations, an operations of the surgical robotic system or display system also can be modified or otherwise controlled based on the detected and tracked gaze of the user. For example, the processor or controller can automatically signal an actuator subsystem of the surgical robotics camera to update or alter a position of a lens of the surgical robotics camera based on the gaze of the user. More specifically, the position or orientation of the surgical camera can be automatically altered or updated such that the point or area focused on by the user's gaze is substantially centered on/along the display. Further, when the detected and tracked gaze of the user is directed at an image or icon that is related to an application, , an image or icon of a control or side panel displayed on the display, the application can be initiated or otherwise controlled. Still further, when the detected and tracked gaze of the user is not directed at the display, , for a predetermined time interval, an operation of the surgical robotic system can be disabled.",
                        "1 is a pictorial view of an example surgical robotic system in an operating arena. 2 shows a schematic view of an exemplary console for use with the surgical robotics system. 3 shows an exploded side view of an exemplary display or monitor. 4 shows an exemplary display system for use with the surgical robotics system."
                    ],
                    "DESCRIPTION": "Non-limiting examples of various aspects and variations of the invention are described herein and illustrated in the accompanying drawings. Referring to 1, this is a pictorial view of an example surgical robotic system 1 in an operating arena. The robotic system 1 includes a user console 2, a control tower 3, and one or more surgical robotic arms 4 at a surgical robotic platform 5, , a table, a bed, etc. The system 1 can incorporate any number of devices, tools, or accessories used to perform surgery on a patient 6. For example, the system 1 may include one or more surgical tools 7 used to perform surgery. A surgical tool 7 may be an end effector that is attached to a distal end of a surgical arm 4, for executing a surgical procedure. Each surgical tool 7 may be manipulated manually, robotically, or both, during the surgery. For example, the surgical tool 7 may be a tool used to enter, view, or manipulate an internal anatomy of the patient 6. In one embodiment, the surgical tool 7 is a grasper that can grasp tissue of the patient. The surgical tool 7 may be controlled manually, by a bedside operator 8; or it may be controlled robotically, via actuated movement of the surgical robotic arm 4 to which it is attached. The robotic arms 4 are shown as a table-mounted system, but in other configurations the arms 4 may be mounted in a cart, ceiling or sidewall, or in another suitable structural support. Generally, a remote operator 9, such as a surgeon or other operator, may use the user console 2 to remotely manipulate the arms 4 or the attached surgical tools 7, , teleoperation. The user console 2 may be located in the same operating room as the rest of the system 1, as shown in 1. In other environments, however, the user console 2 may be located in an adjacent or nearby room, or it may be at a remote location, , in a different building, city, or country. The user console 2 may comprise a seat 10, foot-operated controls 13, one or more handheld user input devices, UID 14, and at least one user display 15 that is configured to display, for example, a view of the surgical site inside the patient 6. In the example user console 2, the remote operator 9 is sitting in the seat 10 and viewing the user display 15 while manipulating a foot-operated control 13 and a handheld UID 14 in order to remotely control the arms 4 and the surgical tools 7 that are mounted on the distal ends of the arms 4. In some variations, the bedside operator 8 may also operate the system 1 in an over the bed mode, in which the beside operator 8 user is now at a side of the patient 6 and is simultaneously manipulating a robotically-driven tool end effector as attached to the arm 4, , with a handheld UID 14 held in one hand, and a manual laparoscopic tool. For example, the bedside operator's left hand may be manipulating the handheld UID to control a robotic component, while the bedside operator's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the bedside operator 8 may perform both robotic-assisted minimally invasive surgery and manual laparoscopic surgery on the patient 6. During an example procedure surgery, the patient 6 is prepped and draped in a sterile fashion to achieve anesthesia. Initial access to the surgical site may be performed manually while the arms of the robotic system 1 are in a stowed configuration or withdrawn configuration to facilitate access to the surgical site. Once access is completed, initial positioning or preparation of the robotic system 1 including its arms 4 may be performed. Next, the surgery proceeds with the remote operator 9 at the user console 2 utilising the foot-operated controls 13 and the UIDs 14 to manipulate the various end effectors and perhaps an imaging system, to perform the surgery. Manual assistance may also be provided at the procedure bed or table, by sterile-gowned bedside personnel, , the bedside operator 8 who may perform tasks such as retracting tissues, performing manual repositioning, and tool exchange upon one or more of the robotic arms 4. Non-sterile personnel may also be present to assist the remote operator 9 at the user console 2. When the procedure or surgery is completed, the system 1 and the user console 2 may be configured or set in a state to facilitate post-operative procedures such as cleaning or sterilization and healthcare record entry or printout via the user console 2. In one embodiment, the remote operator 9 holds and moves the UID 14 to provide an input command to move a robot arm actuator 17 in the robotic system 1. The UID 14 may be communicatively coupled to the rest of the robotic system 1, , via a console computer system 16. The UID 14 can generate spatial state signals corresponding to movement of the UID 14, position and orientation of the handheld housing of the UID, and the spatial state signals may be input signals to control a motion of the robot arm actuator 17. The robotic system 1 may use control signals derived from the spatial state signals, to control proportional motion of the actuator 17. In one embodiment, a console processor of the console computer system 16 receives the spatial state signals and generates the corresponding control signals. Based on these control signals, which control how the actuator 17 is energized to move a segment or link of the arm 4, the movement of a corresponding surgical tool that is attached to the arm may mimic the movement of the UID 14. Similarly, interaction between the remote operator 9 and the UID 14 can generate for example a grip control signal that causes a jaw of a grasper of the surgical tool 7 to close and grip the tissue of patient 6. The surgical robotic system 1 may include several UIDs 14, where respective control signals are generated for each UID that control the actuators and the surgical tool end effector of a respective arm 4. For example, the remote operator 9 may move a first UID 14 to control the motion of an actuator 17 that is in a left robotic arm, where the actuator responds by moving linkages, gears, etc. , in that arm 4. Similarly, movement of a second UID 14 by the remote operator 9 controls the motion of another actuator 17, which in turn moves other linkages, gears, etc. , of the robotic system 1. The robotic system 1 may include a right arm 4 that is secured to the bed or table to the right side of the patient, and a left arm 4 that is at the left side of the patient. An actuator 17 may include one or more motors that are controlled so that they drive the rotation of a joint of the arm 4, to for example change, relative to the patient, an orientation of an endoscope or a grasper of the surgical tool 7 that is attached to that arm. Motion of several actuators 17 in the same arm 4 can be controlled by the spatial state signals generated from a particular UID 14. The UIDs 14 can also control motion of respective surgical tool graspers. For example, each UID 14 can generate a respective grip signal to control motion of an actuator, , a linear actuator, that opens or closes jaws of the grasper at a distal end of surgical tool 7 to grip tissue within patient 6. In some aspects, the communication between the platform 5 and the user console 2 may be through a control tower 3, which may translate user commands that are received from the user console 2 and more particularly from the console computer system 16 into robotic control commands that transmitted to the arms 4 on the robotic platform 5. The control tower 3 may also transmit status and feedback from the platform 5 back to the user console 2. The communication connections between the robotic platform 5, the user console 2, and the control tower 3 may be via wired or wireless links, using any suitable ones of a variety of data communication protocols. Any wired connections may be optionally built into the floor or walls or ceiling of the operating room. The robotic system 1 may provide video output to one or more displays, including displays within the operating room as well as remote displays that are accessible via the Internet or other networks , the robotic system 1 can include one or more endoscopic cameras that provide video output or other suitable image data to the displays. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. 2 shows a schematic view of an exemplary user console 2. As shown in 2, a display system 140 can be provided for use with the user console 2 and the surgical robotic system 1. The display system 140 includes an autostereoscopic, three-dimensional monitor or display 142 configured display three-dimensional 3D or two-dimensional 2D information to a user. The monitor 142 may display various information associated with the surgical procedure , an endoscopic camera view of the surgical site, static images, GUIs, or surgical robotic system , status, system settings, or other suitable information in the form of 2D and 3D video, image data, text, graphical interfaces, warnings, controls, indicator lights, etc. The monitor 142 as described herein further may enable the user to interact with displayed content using eye movements or other suitable gestures of the user for control of the display system and operation of other instruments such as those in the surgical robotic system. 2 additionally shows that the display system 140 includes a plurality of sensor assemblies 144 and 146 including a first or head or eye tracking sensor assembly 144, and a second or gaze tracking sensor assembly 146. The first and second sensor assemblies 144 and 146 can be attached to, or in some variations integrally formed with, the monitor 142. For example, the first sensor assembly 144 can be connected to an upper or top portion of the monitor 142 and the second sensor assembly 146 can be connected to a lower or bottom portion of the monitor 142, as generally shown in 2. However, in the alternative, the second sensor assembly 146 can be attached to the top portion of the monitor 142 and the first sensor assembly 144 can be attached to the bottom portion of the monitor 142, or both the first and second sensor assemblies 144 and 146 can be attached to the top or bottom portions of the monitor 142, or the first or second sensor assemblies 144 and 146 can be attached to side portions of the monitor 142. The first or second sensor assemblies 144 and 146 also can be coupled to or incorporated with other suitable components or parts of or near the console 2, without departing from the scope of the present disclosure. As further shown in 2, the monitor 142 may be supported by a power adjustable monitor support assembly 150. The monitor 142 may be positioned proximate or near the seat 10 to enable a user to view the monitor while the user is seated in or otherwise engaged by the seat 10. For example, the support assembly 150 may have a support or column 152 positioned in front or forward of the seat 10, which support or column 152 at least partially supports the monitor 142. In one variation, the monitor 142 is connected to the support 152 by an adjustable mount assembly 154 including an actuator subsystem with one or more actuators 158, 162, 166 that enable automatic adjustment of a position or orientation of the monitor 152 , based upon output data received from the first or second sensor assemblies 144 or 146. The monitor 142 further can include one or more sensors , position sensors, motion sensors, accelerometers, attached to the monitor 142 that facilitate the detection and tracking of positions or orientations of the monitor. The mount assembly 154 can enable translation or rotational movement of the monitor 142 for up to six degrees of freedom including, , tilt, yaw, rotation, front-to-back movement, side-to-side movement, and up-and-down movement. For example, the mount assembly 154 can include a slidable support portion or member 156 coupled to the monitor 142. The slidable support portion 156 further can be driven by one or more actuators 158 , motors, hydraulic actuators, pneumatic actuators, for up-down and side-to-side translation of the monitor 142. The mounting assembly 154 further can include one or more telescoping portions or sections 160 or other suitable portions or components that are driven by one or more actuators 162 to enable forward and backward movement of the monitor 142 i. e. , movement of the monitor 142 towards and away from the seat 10, , to vary a distance between the seat 10 and the monitor 142. The telescoping portions 160 can connect the monitor 142 to the slidable support portion 156. The mounting assembly 154 also can include a pivotable connection 164 , a swivel fixture, ball joint, pivoting feature, connecting the monitor 142 to the telescoping portions 160. Movement of the monitor 142 about the pivotable connection 164 can be driven by an actuator 166 , motors, hydraulic actuators, pneumatic actuators, to enable tilt, yaw, and rotation of the monitor 142. The mounting assembly 154 further can allow for manual adjustment of the position or orientation of the monitor 142. 2 further shows that the seat 10 can be supported by a power adjustable seat support assembly 165. The power adjustable seat support assembly 165 can have an actuator subsystem including actuators 169/171 that drive movement of the seat 10. As shown in 2, the seat support assembly 165 includes seat support 167 having a single pillar at least partially supporting the seat 10, but in other examples, the seat support 167 may comprise two or more pillars. The seat support 167 can be angled posteriorly in relation to the monitor 142, but in other variations, may be angled vertically straight upward or tilted anteriorly. In some variations, the seat 10 can be moveably or adjustably mounted to the seat support 167. For example, the seat 10 can rotate, tilt, recline, etc. in relation to the support 167 to enable adjustment of a position or orientation of the seat 10 in relation to the monitor 142 , such that a position of the user's head or eyes can be automatically adjusted in relation to the monitor 142 to optimize visualization or perception of three-dimensional images thereon. The seat assembly 165 further can have one or more actuators 169 , motors, hydraulic actuators, pneumatic actuators, for automatically driving rotation, tilting, reclining, etc. of the seat 10 , in response to output data from the first or second sensor assemblies 144/146. In some variations, the seat 10 further is moveable along the support 167 , to move the seat 10 up and down and forward and backward in relation to the monitor 142. For example, an actuator 171 , a motor, a hydraulic actuator, a pneumatic actuator, can drive movement of the seat 10 along the support 167 , in response to output data from the first or second sensor assemblies 144/146. In addition, or in the alternative, the seat support 167 may be configured to change its angle or orientation, or to translate in the forward or rearward directions or in the lateral directions. In some further variations, the seat support 167 may be configured to telescope or otherwise extend or retract longitudinally or generally vertically. The seat support assembly 165 further may allow for manual adjustment the position or orientation of the seat 10. 3 shows a cross-sectional view of the display or monitor 142. The display or monitor 142 can include a flat, curved, or otherwise shaped panel display 170, such as an LCD, LED, plasma, or other suitable panel display, having a plurality of pixels for displaying two or three-dimensional images. The display 142 further can include one or more layers 172/174 at least partially overlaying, or otherwise disposed/positioned over, the display 142. The layers 172/174 are configured to facilitate a user's visualization of three-dimensional images on the display 142. In one embodiment, the layers 172/174 can include micro-lenses that can be at least partially positioned over the plurality of pixels of the panel display 170 to facilitate or otherwise allow for the user's visualization or perception of three-dimensional images on the panel display 170. In one embodiment, the pixels of the panel display 170 can display a left eye image and a right eye image that are continuously and/or dynamically interleaved, and the layers 172 or 174 can enable the user to visualize or perceive the left eye image and the right eye image as a single three-dimensional image, without the use of three-dimensional glasses or other additional wearable or similar components on the user. In the alternative, the one or more layers 172/174 can include polarizing filters, a patterned retarder, or dynamic shutters, and a user may use three-dimensional glasses or other similar wearable components to view or visualize three-dimensional images on the display. The display 142 further can include a protective layer 176 at least partially covering or sealing off the layers 172/174 or the panel display 170. The protective layer 176 may seal off and protect the layers 172/174 and panel display 170 such that the monitor 142 is suitable for use in a surgical environment. For example, the protective layer 176 may allow for sterilization or cleaning of the display , with cleaning chemicals, such as alcohol-based or chlorine-based cleaners without damage to the micro-lenses 172/174. In one embodiment, the protective layer 176 can include surgical-grade glass or other surgical-grade materials , surgical-grade plastics or other suitable composite materials. The protective layer 176 further can have a thickness in the range of approximately 1 mm to approximately 3. 0 mm, such as approximately 2. 0 mm or other suitable integer and non-integer numbers therebetween. Thicknesses of less than 1. 5 mm or greater than 3. 0 mm can be employed, however, without departing from the scope of the present disclosure. Additionally, or in the alternative, at least one additional protective layer 177 can be provided on the panel display 170 , between the panel display 170 and the layers 170. The additional protective layer 177 can have a thickness of up to 1. 0 mm, such as approximately 0. 3 mm, and can be formed from plastic, glass, or other suitable material. The protective layer 176 can be bonded to one or both of the layers 172/174 or the panel display 170 using an adhesive 178 , an optically clear adhesive or other suitable adhesive or glue. One or more spacers 179 further may be provided between the protective layer 176 and the layers 172/174 or the panel display 170. The spacers 179 can be positioned along a boundary of the protective layer 176 at equally spaced intervals, though in some variations the spacers 179 can be disposed intermittently or sporadically about the protective layer 176. The spacers 179 can prevent damage to the layers 174/176 during formation of the monitor, , during application and bonding of the protective layer 176. As shown in 4, in some variations, the first sensor assembly 144 can include one or more sensors 200. The sensors 200 can include a stereo cameras, an infrared cameras, or other suitable cameras 202 that does not filter infrared light. In one embodiment, the cameras 202 can include one Intel Real Sense Camera as provided by Intel Corp. of Santa Clara, Ca. The sensors 200 additionally or alternatively can include other types of cameras, , color cameras, or other suitable sensing devices, without departing from the scope of the present disclosure. Signals or output information from the first sensor assembly 144 can be received and processed, , by a controller or processor in communication with the first sensor assembly 144, to facilitate or otherwise allow for detection and tracking of a head or eye position of a user. For example, the first sensor assembly 144 can be used for detecting and tracking an xyz position of a user's head, eye, or eyes, , in relation to an origin or an original position, such that a position, , a distance, of the user's head or eyes can be continuously determined in relation to the monitor 142. In addition, in some variations, the second sensor assembly 146 includes one or more sensors 210, such as one or more cameras 212, and one or more strobes or strobe lights 214, , that flash light to facilitate detection and tracking of a gaze of a user by the cameras 212. The gaze of the user is detected based on a position or a movement of at least one iris of the user's eyes and includes an area or point at which the user is looking or substantially focused , an area or point on the monitor or an area or point off/away from the monitor. In one embodiment, the strobes 214 can be configured to provide multiple flashes of light per second, , flashes of light at a frequency in the range of approximately 80 Hz to approximately 100 Hz, such as approximately 90 Hz or other suitable frequency. The camera 212 includes a high-speed camera that is configured to capture the illuminated , by the strobes 214 and unilluminated irises of the user , such that the processor receiving and processing output data from the camera 212 can detect and track a user's irises to determine a point or area at which the user is looking or substantially focused on. The light flashes from the strobes 214 further may assist in perception of the user's eye or head position with the first sensor assembly 144, , during low light conditions. It should be understood that although some specific examples of sensor types, sensor locations, and sensor functions in the display system have been discussed above, a wide variety of other sensors and sensor types may additionally or alternatively be located throughout the various components of the display system in order to capture information about the user or for receiving user input as interactive user controls. An example of a graphical user interface GUI to be displayed on the monitor 142 is shown in 4. For example, the GUI may display a display portion or display window 180 showing endoscopic image or other suitable surgical image data , from an endoscopic camera or other suitable surgical robotics camera placed inside the patient. The GUI may further include control panels or side panels 182 including one or more images or icons 184 related to one or more applications related to the surgical robotic system 1 , a timer application, an x-ray imaging tool, . The control panels 182 also can include other suitable information, such as one or more medical images , pre-operative images of patient tissue, patient data , name, medical record number, date of birth, various suitable notes, , tool information , a left or right tool number, a left or right tool name, a left or right tool function, . Other suitable GUIs or other display content may appear on the monitor without departing from the present disclosure. Various user interactions , the user's gaze or eye or head movements also may cause changes to the displayed content type, as well as interaction with the applications as further described below. The display system 140 generally includes or is in communication with a processor or controller configured to detect and track a head position or an eye position of a user relative to the monitor 142 based on processing output data of the first sensor assembly 144. In some variations, a spatial relationship between the monitor 142 and a user , a user sitting in seat 10 can be adjusted based on the detected eye or head position of the user, , to optimize the user's visualization or perception of three-dimensional image data from the endoscopic or surgical robotics camera on the monitor 142. A user's perception of three-dimensional images on the monitor 142 may be optimal when the user's eyes are substantially centered with respect to the monitor 142 and spaced at a prescribed distance therefrom , approximately 70 cm to approximately 90 cm, such as approximately 80 cm from the monitor. Thus, the position or orientation of the seat 10 or the monitor 142 can be automatically , without requiring a deliberate user input adjusted or changed to ensure that the user's head or eyes are located and positioned at an optimal orientation or viewing distance in relation to the monitor. In one embodiment, the processor or controller can be in communication with the seat actuators 169 and 171 or the monitor actuators 158, 162, 166 and can automatically provide signals or information to seat actuators 169/171 or monitor 158, 162, 166 to adjust a position or orientation of the seat 10 or monitor 142 based upon processing output signals from the first or second sensor assemblies 144/166. For example, the processor or controller can determine a position of the user's head or eyes in relation to the monitor 142, and the processor can automatically generate and send a signals to the seat actuators 169 or 171 or the monitor actuators 158, 162, or 166 to adjust or change the position or orientation of the seat or monitor , the seat can be reclined, tilted, rotated, moved up or down, moved side to side, etc. or the monitor can be tilted, yawed, rotated, moved front-to-back, moved side-to-side movement, moved up-and-down, based on the determined position of the user's head or eyes, , to optimize the user's visualization of three-dimensional images from the surgical robotics camera on the monitor. For example, the position or orientation of the seat or monitor can be adjusted such that the user's head or eyes is substantially centered with respect to the monitor and is at a prescribed distance from the monitor for optimal viewing of three-dimensional images. The processor or controller additionally, or alternatively, can generate and send signals to the monitor 142 to display instructions thereon for manual adjustment of the monitor 142 or the seat 10 to optimize the user's perception or visualization of three dimensional image data on the display. Furthermore, the controller or processor is configured to detect the track the gaze of the user based on processing output data of the second sensor assembly 146, and in some variations, operations of the display system 140 or the surgical robotic system 1 can be modified or controlled based on the detected gaze of the user , to facilitate control of the display system with the user's eyes or eye gestures or to stop or pause operations of the display system or surgical robotic system when the detected gaze of the user is directed away from the monitor. In some variations, the processor or controller can be in communication with the surgical robotic system 1, and when the processor or controller determines that the gaze of a user is not directed at the monitor 142 , for a predetermined time period, such as approximately 3 seconds or up to approximately 5 seconds or more, the processor or controller is operable to automatically send a signals or other output data to the surgical robotic system 1 or the display system 140 to activate or disable one or more operations thereof , to disable or freeze operation of one or more subsystems of the surgical system, such as the robotic arms 4 or the surgical tools 7, or to generate an alarm with the display system. In one embodiment, when the processor or controller determined that the user's gaze is not directed at the monitor 142, , for a prescribed time period, such as when the user is distracted, falls asleep, etc. , the processor or controller automatically generates and sends one or more signals to the surgical system 1 to freeze or pause operation of the robotic arms 4 or the surgical tools 7, , to prevent injury to a patient being operated on. Further, when the processor or controller determines that the user's gaze has returned to the monitor 142, the processor or controller may automatically generate and send one or more signals to the surgical system to resume operation of the robotic arms 4 or the surgical tools 7. However, the processor or controller may require a specific user input , selection of an icon, a gesture, prior to sending the signals for resuming operation of the robotic arms or surgical tools 7. Additionally, or in the alternative, when the processor or controller determines that the user's gaze is not directed at the monitor 142, the processor or controller may generate and send a signals to the display system 140 to activate one or more alarms or notifications to get the attention of the user or other suitable entity , a speaker of the display system may play one or more audio sounds, the monitor may display one or more images indicating that the user's gaze is not directed at the monitor, or one or more vibrations or haptics of the seat or UIDs may be activated. In some variations, the detected and tracked gaze of the user also can be used to initiate or control the applications on the control/side panels 182. For example, a user can look at or focus on the one or more images 184 on the control or side panels 182 to trigger application interactions. The user can initiate or close the applications, open the applications in one or more new windows or pop-up windows, control features or operations of the applications, etc. by focusing on or looking at one or more areas or points on the GUI or using other suitable eye motions. In one example, the user can focus on or look at an image associated with a timer application shown on the control/side panels, , to start and stop the timer. In another example, the user can focus on or look at an image associated with an x-ray imaging tool to initiate the x-ray imaging tool , to open the x-ray imaging tool on one or more secondary or popup windows on the display. The user's gaze further can be used to close the x-ray image tool , when the user looks away or focuses on a close icon or image or other suitable feature. Additionally, a position or orientation of the surgical robotics camera also can be updated or adjusted based on the detected and tracked gaze of the user. In some variations, the position or orientation of the surgical robotics camera can be continuously or dynamically updated , the controller or processor can automatically generate and send signals to an actuator subsystem of the surgical robotics camera to tilt, rotate, or otherwise translate a lens of the surgical robotics camera such that an area or point on the monitor 142 that is being focused on by the user is substantially centered along the monitor 142 , centered in relation to the horizontal axis and the vertical axis of the monitor where perception or visualization of three-dimensional image data is optimal. That is, each time a user focuses on an area or point of the three-dimensional image data displayed in the display window 180 that is not substantially centered along the display window 180 , based on the user's detected gaze, the position or orientation of the surgical robotics camera can be updated or changed such that the area or point of the three-dimensional image data on which the user is focused is moved or otherwise adjusted along the display window 180 so as to be substantially centered therealong. For example, the user may initially focus on a point or area of the three-dimensional image data that is substantially centered within the display window 180, and when the user changes their focus or otherwise redirects their gaze to a new area or point on the image data shown in the display window 180 , the user looks at or focuses on an area or point that is proximate to or near an edge or corner of the display window 180 or the user looks at or focuses on an area or point that is otherwise spaced apart from the original point or area in the center of the display window, the processor or controller may generate and send one or more signals to the surgical robotics camera or a controller thereof to automatically adjust the position or orientation of the surgical robotics camera such that the new area or point of the three-dimensional image data that is focused on by the user is moved or adjusted so as to be substantially centered within the display window 180. In this way, the position or orientation of the surgical robotics camera can be continuously or dynamically adjusted or otherwise updated based upon the determined gaze of the user such that the user's focus is directed to be and remains generally centered along the display window to facilitate optimal three-dimensional perception or visualization of the three-dimensional image data displayed therein. The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the invention. However, it will be apparent to one skilled in the art that specific details are not required in order to practice the invention. Thus, the foregoing descriptions of specific embodiments of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed; obviously, many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications, and they thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/99/338/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9853561520576477,
                    "F_SPEC_PARAMS": [
                        "reduced patient scarring, less patient pain, shorter patient recovery periods,",
                        "lower medical treatment costs"
                    ],
                    "S_SPEC_PARAMS": [
                        "problematic to use and handle in surgical or sterile environments"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery."
                    ],
                    "S_SENTS": [
                        "Such glasses and additional wearable components, however, may be problematic to use and handle in surgical or sterile environments."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Side Effects"
                    ],
                    "F_SIM_SCORE": 0.4541338384151459,
                    "S_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object"
                    ],
                    "S_SIM_SCORE": 0.5087361335754395,
                    "GLOBAL_SCORE": 1.6667911380529403
                },
                "sort": [
                    1.6667911
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10895757-20210119",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10895757-20210119",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-02-27",
                    "PUBLICATION_DATE": "2021-01-19",
                    "INVENTORS": [
                        "Bernhard A Fuerst",
                        "Pablo Garcia Kilroy",
                        "Joan Savall",
                        "Anette Lia Freiin Von Kapri"
                    ],
                    "APPLICANTS": [
                        "Verb Surgical Inc.    ( Mountain View , US )"
                    ],
                    "INVENTION_TITLE": "Systems and methods for three-dimensional visualization during robotic surgery",
                    "DOMAIN": "G02B 3026",
                    "ABSTRACT": "An autostereoscopic three-dimensional display system for surgical robotics has an autostereoscopic three-dimensional display configured to receive and display video from a surgical robotics camera, and a first sensor assembly and a second sensor assembly. A processor is configured to detect and track an eye position or a head position of a user relative to the display based on processing output data of the first sensor assembly, and to detect and track a gaze of the user based on processing output data of the second sensor assembly. The processor further is configured to modify or control an operation of the display system based on the detected gaze of the user. A spatial relationship of the display also can be automatically adjusted in relation to the user based on the detected eye or head position of the user to optimize the user's visualization of three-dimensional images on the display.",
                    "CLAIMS": "1. A three-dimensional display system for use in a surgical robotics system, comprising: an autostereoscopic three-dimensional display configured to receive and display video from a surgical robotics camera, the display including one or more layers at least partially positioned over a panel display and configured to facilitate a user's visualization of three-dimensional images on the display; a plurality of sensor assemblies that include a first sensor assembly configured to capture information related to a head position of a user, and a second sensor assembly including a plurality of strobes configured to capture information that tracks a gaze of the user including illumination of an iris of the user with the plurality of strobes; and a processor that detects and tracks the head position of the user relative to the display based on processing output data of the first sensor assembly, and detects and tracks the gaze of the user based on processing output data of the second sensor assembly, the processor further modifies an operation of the display system based on the detected gaze of the user, and automatically adjusts a spatial relationship between the user and the display based on the detected head position of the user to affect the user's visualization of the three-dimensional images on the panel display, wherein the processor communicates a signal to a controller of a surgical robotic system to pause an operation of the surgical robotic system when it is detected that the tracked gaze of the user is not directed towards the display. 2. The display system of claim 1, wherein the one or more layers include at least one of polarizing filters, a pattern retarder, or dynamic shutters. 3. The display system of claim 2 wherein the user is to use glasses or other wearable components to view or visualize the three-dimensional images on the display. 4. The display system of claim 2, wherein the display further comprises a protective layer covering the one or more layers positioned on the panel display. 5. The display system of claim 4, wherein the protective layer includes surgical grade glass that is bonded to the one or more layers or to the panel display by an optically clear adhesive. 6. The display system of claim 1, wherein the one or more layers includes a plurality of micro-lenses positioned to facilitate the user's visualization of three-dimensional images on the display, without the use of glasses or other wearable components. 7. The display system of claim 1, wherein the first sensor assembly and the second sensor assembly are physically attached to or integrated with the display. 8. The display system of claim 1, wherein the first sensor assembly comprises an infrared camera, and wherein the second sensor assembly further comprises a high-speed camera. 9. The display system of claim 1, further comprising: a seat assembly having a seat in which the user is to sit, and a power adjustable seat support assembly connected to the seat and configured to change a position or an orientation of the seat based on receiving a signal from the processor, wherein the processor signals the power adjustable seat support assembly to automatically change the position or orientation of the seat based on the detected head position of the user. 10. The display system of claim 1, further comprising: a power adjustable display support assembly connected to and supporting the display, and configured to change a position or an orientation of the display based on receiving a signal from the processor, wherein the processor signals the power adjustable display support assembly to automatically change the position or orientation of the display based on the detected head position of the user. 11. The display system of claim 1, wherein a control panel having a plurality of icons is displayed on the display, and wherein an application related to a displayed icon of the plurality of icons is invoked when the processor determines that the detected gaze of the user is directed at the displayed icon. 12. The display system of claim 1, wherein a position or orientation of the surgical robotics camera is changed based upon the detected gaze of the user. 13. The display system of claim 1, wherein three-dimensional image data received from the surgical robotics camera is displayed on the display, and wherein when the processor determines that the detected gaze of the user is directed at an area or point on the three dimensional image data that is spaced apart from a center of the display, the processor generates and sends a signal to the surgical robotics camera to adjust the position or orientation of the surgical robotics camera such that the area or point on the three-dimensional image data is moved so as to be centered along the display. 14. A three-dimensional display system for use in a surgical robotics system, comprising: an autostereoscopic three-dimensional display configured to receive and display video from a surgical robotics camera; a plurality of sensor assemblies that include a first sensor assembly and a second sensor assembly including a plurality of strobes; a processor configured to detect and track a head position of a user relative to the display based on processing output data received from the first sensor assembly, and to detect and track a gaze of the user based on processing output data received from the second sensor assembly including illumination of an iris of the user with the plurality of strobes, the processor generating control signals to modify an operation of the display system based on the detected gaze of the user, and to automatically adjust a spatial relationship between the user and the display based on the detected head position of the user to affect the user's visualization of three-dimensional images on the display, wherein the processor communicates a signal to a controller of a surgical robotic system to pause an operation of the surgical robotic system when it is detected that the tracked gaze of the user is not directed towards the display. 15. The display system of claim 14, wherein the display includes at least one of polarizing filters, a pattern retarder, or dynamic shutters, and wherein the user uses glasses or other wearable components to view or visualize the three-dimensional images on the display. 16. The display system of claim 14, further comprising: a seat assembly having a seat in which the user is to sit, and a power adjustable seat support assembly connected to the seat and configured to change a position or an orientation of the seat based on receiving a signal from the processor, wherein the processor signals the power adjustable seat support assembly to automatically change the position or orientation of the seat based on the detected head position of the user. 17. The display system of claim 14, further comprising: a power adjustable display support assembly connected to and supporting the display, and configured to change a position or an orientation of the display based on receiving a signal from the processor, wherein the processor signals the power adjustable display support assembly to automatically change the position or orientation of the display based on the detected head position of the user. 18. The display system of claim 14, wherein three-dimensional image data received from the surgical robotics camera is displayed on the display, and wherein when the processor determines that the detected gaze of the user is directed at an area or point on the three dimensional image data that is spaced apart from a center of the display, the processor generates and sends a signal to the surgical robotics camera to adjust the position or orientation of the surgical robotics camera such that the area or point on the three-dimensional image data is moved so as to be centered along the display. 19. A method performed by a digital programmed processor executing instructions stored in a computer readable memory, the method comprising: receiving video signals from a surgical robotics camera and rendering images on a three-dimensional display based upon the received video signals; detecting and tracking a head position of a user relative to the display based on processing output data from a first sensor; detecting and tracking a gaze of the user based on processing output data from a second sensor, wherein tracking the gaze of the user includes illumination of an iris of the user with a plurality of strobes of the second sensor; automatically signaling an actuator subsystem to adjust a spatial relationship of the display in relation to the user based on the detected head position of the user to optimize the user's visualization of three-dimensional images on the display; automatically signaling the actuator subsystem to adjust a position or orientation of the display or a seat assembly based upon the head position of the user; and signaling to a controller of a surgical robotic system to pause an operation of the surgical robotic system when it is detected that the tracked gaze of the user is not directed towards the display. 20. The display system of claim 11, wherein the application is a timer application that is started or stopped based on the detected gaze of the user. 21. The display system of claim 11, wherein the application is an x-ray viewing tool that is initiated or controlled based on the detected gaze of the user.",
                    "FIELD_OF_INVENTION": "This disclosure relates generally to the field of surgical robotics and, more particularly, to display systems for use with surgical robotic systems for visualizing the surgical site.",
                    "STATE_OF_THE_ART": "Minimally-invasive surgery MIS, such as laparoscopic surgery, involves techniques intended to reduce tissue damage during a surgical procedure. For example, laparoscopic procedures typically involve creating a number of small incisions in the patient , in the abdomen, and introducing one or more tools and at least one endoscopic camera through the incisions into the patient. The surgical procedures are then performed by using the introduced tools, with the visualization aid provided by the camera. Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery. In some embodiments, MIS may be performed with surgical robotic systems that include one or more robotic arms for manipulating surgical instruments based on commands from an operator. For example, an operator may provide commands for manipulating surgical instruments, while viewing an image that is provided by a camera and displayed on a display to the user. However, conventional display systems fall short in enabling effective control of the display systems or of surgical robotic systems. Furthermore, conventional display systems generally provide two-dimensional 2-D surgical image data to the user, and current three-dimensional 3-D displays typically require the user to wear glasses or additional, similar wearable components , with polarizing filters or dynamic shutters for visualization of three-dimensional images. Such glasses and additional wearable components, however, may be problematic to use and handle in surgical or sterile environments. Thus, there is a need for improved 3-D display systems that enable a user to better visualize the surgical site during robotic surgery.",
                    "SUMMARY": [
                        "Generally, a three-dimensional display system for use with a surgical robotic system can include a three-dimensional display configured to receive and display video from a surgical robotics camera, such as an endoscopic camera. The display system can include a plurality of sensor assemblies having a first sensor assembly and a second sensor assembly. The first sensor assembly and the second sensor assembly can be coupled to or integrally formed with the display. The display system can include a processor or controller configured to detect and track an eye position or a head position of a user relative to the display based on processing output data of the first sensor assembly. The processor or controller also can be configured to detect and track a gaze of the user based on processing output data of the second sensor assembly. The processor or controller further is configured to modify or control an operation of the display system based on the detected and tracked gaze of the user, for example, to facilitate control of the display system with the user's eyes or eye motions. In addition, a spatial relationship of the display can be automatically adjusted in relation to the user based on the detected eye or head position of the user. For example, a distance or orientation between the detected eye or head position and the display can be automatically , without requiring deliberate user input updated to adjust the user's visualization of three-dimensional image data from the surgical robotics camera on the display. In some variations, the display can include a panel display or monitor, such as an LCD, LED, plasma, or other suitable flat, curved, or otherwise shaped panel display or monitor, having a plurality of pixels for displaying two or three-dimensional images. The display further can include one or more layers at least partially positioned over the panel display and configured to facilitate a user's visualization of three-dimensional images on the panel display. The one or more layers can include a polarizing filter, a pattern retarder, or dynamic shutters that allow users to uses glasses or other wearable components to view or visualize the three-dimensional images on the panel display. Alternatively, the one or more layers can include layers of micro-lenses that can at least partially cover the plurality of pixels of the panel display. The layers of micro-lenses further can be positioned or disposed in relation to the plurality of pixels to facilitate or otherwise allow the user's visualization or perception of three-dimensional images on the panel display, without the use of three-dimensional glasses or other additional wearable or similar components worn by a user. The display further can include a protective layer at least partially disposed over or sealing off the layers of the panel display. The protective layer can be bonded to layers or the panel display using an adhesive, such as an optically clear adhesive or other suitable adhesive. An additional protective layer can be provided on the display panel, , between the one or more layers including micro-lenses and the display panel. In some variations, the first sensor assembly can include at least one camera, such as a stereo camera, an infrared camera, or other suitable camera that does not filter infrared light, , to allow for detection and tracking of a head or eye position of a user , an xyz position of the user's head or eye position in relation to an origin or original position or to the display. The second sensor assembly can include one or more cameras and a plurality of strobes or strobe lights, , to allow for illumination of and detection and tracking of an iris or irises of the user's eyes. In addition, a seat assembly can be provided with the display system. The seat assembly can have a seat in which a user is to sit or otherwise engage, while the user is viewing the display. The seat assembly also can include a movable or adjustable seat support assembly that is connected to and at least partially supports the seat. The processor or controller can automatically generate and send one or more signals or other output data to an actuator subsystem of the seat support assembly to adjust or update a position or orientation of the seat based upon received output data from the first or second sensor assemblies. For example, the position or orientation of the seat can be adjusted based on the detected and tracked eye or head position of the user to optimize the user's visualization of three-dimensional images on the display. The display system also can include a movable or adjustable display support assembly connected to and supporting the display. The processor or controller can automatically generate and send one or more signals or other output data to an actuator subsystem of the movable or adjustable display support assembly to adjust or update a position or an orientation of the display based upon received output data from the first or second sensor assemblies. For example, the position or orientation of the display can be adjusted based on the detected and tracked eye or head position of the user to optimize the user's visualization of three-dimensional images from the surgical robotics camera on the display. In one example, the position or orientation of the seat or the display can be automatically adjusted or changed such that the user's head or eyes are located at a predetermined distance from or orientation in relation to the display. In some variations, the processor or controller can be in communication with the surgical robotic system. The processor further can be operable to send a signal or other output data to the surgical robotic system, , to a controller thereof, for control of the surgical robotic system based on received output data from the first or second sensor assemblies. For example, when the gaze of the user is not directed towards the display, , for a predetermined time interval, control of one or more operations of the surgical robotic system , operations of robotic arms or surgical instruments may be paused or otherwise disabled. Additionally, an endoscopic image or other suitable image of a surgical site from the surgical robotics camera may be displayed on the display, , as part of a GUI or display window on the display. Control panels or side panels having a plurality of icons or images additionally or alternatively can be displayed on the display. For example, control or side panels can be positioned to the left and right of the primary display or window on the display. The plurality of icons or images can be related to applications for the display system or the surgical robotic system. The detected and tracked gaze of the user further can be used to initiate or control the applications in the control/side panels. For example, a user can focus their gaze on the images or icons shown the control or side panels to trigger application interactions , to start and stop a timer application, initiate or control an x-ray viewing tool, enlarge a view, or to initiate or control other suitable applications. In some variations, a position or orientation of the surgical robotics camera can be dynamically or continuously updated based on the detected and tracked gaze of the user. For example, the position of the surgical robotics camera can be automatically updated such that an area or point substantially focused on by the user's gaze, , an area or point within the primary display or window showing the endoscopic image, is substantially centered on the display. In one embodiment, when the processor or controller determines that the detected gaze of the user is directed at an area or point that is spaced apart from the center of the display, the processor or controller generates and sends a signal to the surgical robotics camera to adjust the position or orientation of the surgical robotics camera such that the area or point at which the user's gaze is directed or focused on is moved to the center of the display. Furthermore, a method for three-dimensional visualization during robotic surgery can be provided. The method can be performed by a digital programmed processor executing instructions stored in a computer readable memory. The method can include receiving and displaying video from a surgical robotics camera on a three-dimensional display. The method further can include detecting and tracking a head position or an eye position of a user relative to the display based on processing output data of a first sensor assembly, and detecting and tracking a gaze of the user based on processing output data of a second sensor assembly. The detected and tracked gaze of the user can be used to facilitate control or modify operations of a display system or a surgical robotic system. In addition, the method can include automatically , without requiring deliberate user input signaling an actuator subsystem to adjust or update a spatial relationship of the display in relation to the user based on the detected eye or head position of the user to optimize the user's visualization of three-dimensional images from the surgical robotics camera on the display. In some variations, a position or orientation of the display or a seat assembly, which is configured to be sat in or otherwise engaged by the user when viewing the display, can be automatically adjusted or modified based upon the detected and tracked head or eye position of the user. In further variations, an operations of the surgical robotic system or display system also can be modified or otherwise controlled based on the detected and tracked gaze of the user. For example, the processor or controller can automatically signal an actuator subsystem of the surgical robotics camera to update or alter a position of a lens of the surgical robotics camera based on the gaze of the user. More specifically, the position or orientation of the surgical camera can be automatically altered or updated such that the point or area focused on by the user's gaze is substantially centered on/along the display. Further, when the detected and tracked gaze of the user is directed at an image or icon that is related to an application, , an image or icon of a control or side panel displayed on the display, the application can be initiated or otherwise controlled. Still further, when the detected and tracked gaze of the user is not directed at the display, , for a predetermined time interval, an operation of the surgical robotic system can be disabled.",
                        "1 is a pictorial view of an example surgical robotic system in an operating arena. 2 shows a schematic view of an exemplary console for use with the surgical robotics system. 3 shows an exploded side view of an exemplary display or monitor. 4 shows an exemplary display system for use with the surgical robotics system."
                    ],
                    "DESCRIPTION": "Non-limiting examples of various aspects and variations of the invention are described herein and illustrated in the accompanying drawings. Referring to 1, this is a pictorial view of an example surgical robotic system 1 in an operating arena. The robotic system 1 includes a user console 2, a control tower 3, and one or more surgical robotic arms 4 at a surgical robotic platform 5, , a table, a bed, etc. The system 1 can incorporate any number of devices, tools, or accessories used to perform surgery on a patient 6. For example, the system 1 may include one or more surgical tools 7 used to perform surgery. A surgical tool 7 may be an end effector that is attached to a distal end of a surgical arm 4, for executing a surgical procedure. Each surgical tool 7 may be manipulated manually, robotically, or both, during the surgery. For example, the surgical tool 7 may be a tool used to enter, view, or manipulate an internal anatomy of the patient 6. In one embodiment, the surgical tool 7 is a grasper that can grasp tissue of the patient. The surgical tool 7 may be controlled manually, by a bedside operator 8; or it may be controlled robotically, via actuated movement of the surgical robotic arm 4 to which it is attached. The robotic arms 4 are shown as a table-mounted system, but in other configurations the arms 4 may be mounted in a cart, ceiling or sidewall, or in another suitable structural support. Generally, a remote operator 9, such as a surgeon or other operator, may use the user console 2 to remotely manipulate the arms 4 or the attached surgical tools 7, , teleoperation. The user console 2 may be located in the same operating room as the rest of the system 1, as shown in 1. In other environments, however, the user console 2 may be located in an adjacent or nearby room, or it may be at a remote location, , in a different building, city, or country. The user console 2 may comprise a seat 10, foot-operated controls 13, one or more handheld user input devices, UID 14, and at least one user display 15 that is configured to display, for example, a view of the surgical site inside the patient 6. In the example user console 2, the remote operator 9 is sitting in the seat 10 and viewing the user display 15 while manipulating a foot-operated control 13 and a handheld UID 14 in order to remotely control the arms 4 and the surgical tools 7 that are mounted on the distal ends of the arms 4. In some variations, the bedside operator 8 may also operate the system 1 in an over the bed mode, in which the beside operator 8 user is now at a side of the patient 6 and is simultaneously manipulating a robotically-driven tool end effector as attached to the arm 4, , with a handheld UID 14 held in one hand, and a manual laparoscopic tool. For example, the bedside operator's left hand may be manipulating the handheld UID to control a robotic component, while the bedside operator's right hand may be manipulating a manual laparoscopic tool. Thus, in these variations, the bedside operator 8 may perform both robotic-assisted minimally invasive surgery and manual laparoscopic surgery on the patient 6. During an example procedure surgery, the patient 6 is prepped and draped in a sterile fashion to achieve anesthesia. Initial access to the surgical site may be performed manually while the arms of the robotic system 1 are in a stowed configuration or withdrawn configuration to facilitate access to the surgical site. Once access is completed, initial positioning or preparation of the robotic system 1 including its arms 4 may be performed. Next, the surgery proceeds with the remote operator 9 at the user console 2 utilizing the foot-operated controls 13 and the UIDs 14 to manipulate the various end effectors and perhaps an imaging system, to perform the surgery. Manual assistance may also be provided at the procedure bed or table, by sterile-gowned bedside personnel, , the bedside operator 8 who may perform tasks such as retracting tissues, performing manual repositioning, and tool exchange upon one or more of the robotic arms 4. Non-sterile personnel may also be present to assist the remote operator 9 at the user console 2. When the procedure or surgery is completed, the system 1 and the user console 2 may be configured or set in a state to facilitate post-operative procedures such as cleaning or sterilization and healthcare record entry or printout via the user console 2. In one embodiment, the remote operator 9 holds and moves the UID 14 to provide an input command to move a robot arm actuator 17 in the robotic system 1. The UID 14 may be communicatively coupled to the rest of the robotic system 1, , via a console computer system 16. The UID 14 can generate spatial state signals corresponding to movement of the UID 14, position and orientation of the handheld housing of the UID, and the spatial state signals may be input signals to control a motion of the robot arm actuator 17. The robotic system 1 may use control signals derived from the spatial state signals, to control proportional motion of the actuator 17. In one embodiment, a console processor of the console computer system 16 receives the spatial state signals and generates the corresponding control signals. Based on these control signals, which control how the actuator 17 is energized to move a segment or link of the arm 4, the movement of a corresponding surgical tool that is attached to the arm may mimic the movement of the UID 14. Similarly, interaction between the remote operator 9 and the UID 14 can generate for example a grip control signal that causes a jaw of a grasper of the surgical tool 7 to close and grip the tissue of patient 6. The surgical robotic system 1 may include several UIDs 14, where respective control signals are generated for each UID that control the actuators and the surgical tool end effector of a respective arm 4. For example, the remote operator 9 may move a first UID 14 to control the motion of an actuator 17 that is in a left robotic arm, where the actuator responds by moving linkages, gears, etc. , in that arm 4. Similarly, movement of a second UID 14 by the remote operator 9 controls the motion of another actuator 17, which in turn moves other linkages, gears, etc. , of the robotic system 1. The robotic system 1 may include a right arm 4 that is secured to the bed or table to the right side of the patient, and a left arm 4 that is at the left side of the patient. An actuator 17 may include one or more motors that are controlled so that they drive the rotation of a joint of the arm 4, to for example change, relative to the patient, an orientation of an endoscope or a grasper of the surgical tool 7 that is attached to that arm. Motion of several actuators 17 in the same arm 4 can be controlled by the spatial state signals generated from a particular UID 14. The UIDs 14 can also control motion of respective surgical tool graspers. For example, each UID 14 can generate a respective grip signal to control motion of an actuator, , a linear actuator, that opens or closes jaws of the grasper at a distal end of surgical tool 7 to grip tissue within patient 6. In some aspects, the communication between the platform 5 and the user console 2 may be through a control tower 3, which may translate user commands that are received from the user console 2 and more particularly from the console computer system 16 into robotic control commands that transmitted to the arms 4 on the robotic platform 5. The control tower 3 may also transmit status and feedback from the platform 5 back to the user console 2. The communication connections between the robotic platform 5, the user console 2, and the control tower 3 may be via wired or wireless links, using any suitable ones of a variety of data communication protocols. Any wired connections may be optionally built into the floor or walls or ceiling of the operating room. The robotic system 1 may provide video output to one or more displays, including displays within the operating room as well as remote displays that are accessible via the Internet or other networks , the robotic system 1 can include one or more endoscopic cameras that provide video output or other suitable image data to the displays. The video output or feed may also be encrypted to ensure privacy and all or portions of the video output may be saved to a server or electronic healthcare record system. 2 shows a schematic view of an exemplary user console 2. As shown in 2, a display system 140 can be provided for use with the user console 2 and the surgical robotic system 1. The display system 140 includes an autostereoscopic, three-dimensional monitor or display 142 configured display three-dimensional 3D or two-dimensional 2D information to a user. The monitor 142 may display various information associated with the surgical procedure , an endoscopic camera view of the surgical site, static images, GUIs, or surgical robotic system , status, system settings, or other suitable information in the form of 2D and 3D video, image data, text, graphical interfaces, warnings, controls, indicator lights, etc. The monitor 142 as described herein further may enable the user to interact with displayed content using eye movements or other suitable gestures of the user for control of the display system and operation of other instruments such as those in the surgical robotic system. 2 additionally shows that the display system 140 includes a plurality of sensor assemblies 144 and 146 including a first or head or eye tracking sensor assembly 144, and a second or gaze tracking sensor assembly 146. The first and second sensor assemblies 144 and 146 can be attached to, or in some variations integrally formed with, the monitor 142. For example, the first sensor assembly 144 can be connected to an upper or top portion of the monitor 142 and the second sensor assembly 146 can be connected to a lower or bottom portion of the monitor 142, as generally shown in 2. However, in the alternative, the second sensor assembly 146 can be attached to the top portion of the monitor 142 and the first sensor assembly 144 can be attached to the bottom portion of the monitor 142, or both the first and second sensor assemblies 144 and 146 can be attached to the top or bottom portions of the monitor 142, or the first or second sensor assemblies 144 and 146 can be attached to side portions of the monitor 142. The first or second sensor assemblies 144 and 146 also can be coupled to or incorporated with other suitable components or parts of or near the console 2, without departing from the scope of the present disclosure. As further shown in 2, the monitor 142 may be supported by a power adjustable monitor support assembly 150. The monitor 142 may be positioned proximate or near the seat 10 to enable a user to view the monitor while the user is seated in or otherwise engaged by the seat 10. For example, the support assembly 150 may have a support or column 152 positioned in front or forward of the seat 10, which support or column 152 at least partially supports the monitor 142. In one variation, the monitor 142 is connected to the support 152 by an adjustable mount assembly 154 including an actuator subsystem with one or more actuators 158, 162, 166 that enable automatic adjustment of a position or orientation of the monitor 152 , based upon output data received from the first or second sensor assemblies 144 or 146. The monitor 142 further can include one or more sensors , position sensors, motion sensors, accelerometers, attached to the monitor 142 that facilitate the detection and tracking of positions or orientations of the monitor. The mount assembly 154 can enable translation or rotational movement of the monitor 142 for up to six degrees of freedom including, , tilt, yaw, rotation, front-to-back movement, side-to-side movement, and up-and-down movement. For example, the mount assembly 154 can include a slidable support portion or member 156 coupled to the monitor 142. The slidable support portion 156 further can be driven by one or more actuators 158 , motors, hydraulic actuators, pneumatic actuators, for up-down and side-to-side translation of the monitor 142. The mounting assembly 154 further can include one or more telescoping portions or sections 160 or other suitable portions or components that are driven by one or more actuators 162 to enable forward and backward movement of the monitor 142 i. e. , movement of the monitor 142 towards and away from the seat 10, , to vary a distance between the seat 10 and the monitor 142. The telescoping portions 160 can connect the monitor 142 to the slidable support portion 156. The mounting assembly 154 also can include a pivotable connection 164 , a swivel fixture, ball joint, pivoting feature, connecting the monitor 142 to the telescoping portions 160. Movement of the monitor 142 about the pivotable connection 164 can be driven by an actuator 166 , motors, hydraulic actuators, pneumatic actuators, to enable tilt, yaw, and rotation of the monitor 142. The mounting assembly 154 further can allow for manual adjustment of the position or orientation of the monitor 142. 2 further shows that the seat 10 can be supported by a power adjustable seat support assembly 165. The power adjustable seat support assembly 165 can have an actuator subsystem including actuators 169/171 that drive movement of the seat 10. As shown in 2, the seat support assembly 165 includes seat support 167 having a single pillar at least partially supporting the seat 10, but in other examples, the seat support 167 may comprise two or more pillars. The seat support 167 can be angled posteriorly in relation to the monitor 142, but in other variations, may be angled vertically straight upward or tilted anteriorly. In some variations, the seat 10 can be moveably or adjustably mounted to the seat support 167. For example, the seat 10 can rotate, tilt, recline, etc. in relation to the support 167 to enable adjustment of a position or orientation of the seat 10 in relation to the monitor 142 , such that a position of the user's head or eyes can be automatically adjusted in relation to the monitor 142 to optimize visualization or perception of three-dimensional images thereon. The seat assembly 165 further can have one or more actuators 169 , motors, hydraulic actuators, pneumatic actuators, for automatically driving rotation, tilting, reclining, etc. of the seat 10 , in response to output data from the first or second sensor assemblies 144/146. In some variations, the seat 10 further is moveable along the support 167 , to move the seat 10 up and down and forward and backward in relation to the monitor 142. For example, an actuator 171 , a motor, a hydraulic actuator, a pneumatic actuator, can drive movement of the seat 10 along the support 167 , in response to output data from the first or second sensor assemblies 144/146. In addition, or in the alternative, the seat support 167 may be configured to change its angle or orientation, or to translate in the forward or rearward directions or in the lateral directions. In some further variations, the seat support 167 may be configured to telescope or otherwise extend or retract longitudinally or generally vertically. The seat support assembly 165 further may allow for manual adjustment the position or orientation of the seat 10. 3 shows a cross-sectional view of the display or monitor 142. The display or monitor 142 can include a flat, curved, or otherwise shaped panel display 170, such as an LCD, LED, plasma, or other suitable panel display, having a plurality of pixels for displaying two or three-dimensional images. The display 142 further can include one or more layers 172/174 at least partially overlaying, or otherwise disposed/positioned over, the display 142. The layers 172/174 are configured to facilitate a user's visualization of three-dimensional images on the display 142. In one embodiment, the layers 172/174 can include micro-lenses that can be at least partially positioned over the plurality of pixels of the panel display 170 to facilitate or otherwise allow for the user's visualization or perception of three-dimensional images on the panel display 170. In one embodiment, the pixels of the panel display 170 can display a left eye image and a right eye image that are continuously and/or dynamically interleaved, and the layers 172 or 174 can enable the user to visualize or perceive the left eye image and the right eye image as a single three-dimensional image, without the use of three-dimensional glasses or other additional wearable or similar components on the user. In the alternative, the one or more layers 172/174 can include polarizing filters, a patterned retarder, or dynamic shutters, and a user may use three-dimensional glasses or other similar wearable components to view or visualize three-dimensional images on the display. The display 142 further can include a protective layer 176 at least partially covering or sealing off the layers 172/174 or the panel display 170. The protective layer 176 may seal off and protect the layers 172/174 and panel display 170 such that the monitor 142 is suitable for use in a surgical environment. For example, the protective layer 176 may allow for sterilization or cleaning of the display , with cleaning chemicals, such as alcohol-based or chlorine-based cleaners without damage to the micro-lenses 172/174. In one embodiment, the protective layer 176 can include surgical-grade glass or other surgical-grade materials , surgical-grade plastics or other suitable composite materials. The protective layer 176 further can have a thickness in the range of approximately 1 mm to approximately 3. 0 mm, such as approximately 2. 0 mm or other suitable integer and non-integer numbers therebetween. Thicknesses of less than 1. 5 mm or greater than 3. 0 mm can be employed, however, without departing from the scope of the present disclosure. Additionally, or in the alternative, at least one additional protective layer 177 can be provided on the panel display 170 , between the panel display 170 and the layers 170. The additional protective layer 177 can have a thickness of up to 1. 0 mm, such as approximately 0. 3 mm, and can be formed from plastic, glass, or other suitable material. The protective layer 176 can be bonded to one or both of the layers 172/174 or the panel display 170 using an adhesive 178 , an optically clear adhesive or other suitable adhesive or glue. One or more spacers 179 further may be provided between the protective layer 176 and the layers 172/174 or the panel display 170. The spacers 179 can be positioned along a boundary of the protective layer 176 at equally spaced intervals, though in some variations the spacers 179 can be disposed intermittently or sporadically about the protective layer 176. The spacers 179 can prevent damage to the layers 174/176 during formation of the monitor, , during application and bonding of the protective layer 176. As shown in 4, in some variations, the first sensor assembly 144 can include one or more sensors 200. The sensors 200 can include a stereo cameras, an infrared cameras, or other suitable cameras 202 that does not filter infrared light. In one embodiment, the cameras 202 can include one Intel Real Sense Camera as provided by Intel Corp. of Santa Clara, Calif. The sensors 200 additionally or alternatively can include other types of cameras, , color cameras, or other suitable sensing devices, without departing from the scope of the present disclosure. Signals or output information from the first sensor assembly 144 can be received and processed, , by a controller or processor in communication with the first sensor assembly 144, to facilitate or otherwise allow for detection and tracking of a head or eye position of a user. For example, the first sensor assembly 144 can be used for detecting and tracking an xyz position of a user's head, eye, or eyes, , in relation to an origin or an original position, such that a position, , a distance, of the user's head or eyes can be continuously determined in relation to the monitor 142. In addition, in some variations, the second sensor assembly 146 includes one or more sensors 210, such as one or more cameras 212, and one or more strobes or strobe lights 214, , that flash light to facilitate detection and tracking of a gaze of a user by the cameras 212. The gaze of the user is detected based on a position or a movement of at least one iris of the user's eyes and includes an area or point at which the user is looking or substantially focused , an area or point on the monitor or an area or point off/away from the monitor. In one embodiment, the strobes 214 can be configured to provide multiple flashes of light per second, , flashes of light at a frequency in the range of approximately 80 Hz to approximately 100 Hz, such as approximately 90 Hz or other suitable frequency. The camera 212 includes a high-speed camera that is configured to capture the illuminated , by the strobes 214 and unilluminated irises of the user , such that the processor receiving and processing output data from the camera 212 can detect and track a user's irises to determine a point or area at which the user is looking or substantially focused on. The light flashes from the strobes 214 further may assist in perception of the user's eye or head position with the first sensor assembly 144, , during low light conditions. It should be understood that although some specific examples of sensor types, sensor locations, and sensor functions in the display system have been discussed above, a wide variety of other sensors and sensor types may additionally or alternatively be located throughout the various components of the display system in order to capture information about the user or for receiving user input as interactive user controls. An example of a graphical user interface GUI to be displayed on the monitor 142 is shown in 4. For example, the GUI may display a display portion or display window 180 showing endoscopic image or other suitable surgical image data , from an endoscopic camera or other suitable surgical robotics camera placed inside the patient. The GUI may further include control panels or side panels 182 including one or more images or icons 184 related to one or more applications related to the surgical robotic system 1 , a timer application, an x-ray imaging tool, . The control panels 182 also can include other suitable information, such as one or more medical images , pre-operative images of patient tissue, patient data , name, medical record number, date of birth, various suitable notes, , tool information , a left or right tool number, a left or right tool name, a left or right tool function, . Other suitable GUIs or other display content may appear on the monitor without departing from the present disclosure. Various user interactions , the user's gaze or eye or head movements also may cause changes to the displayed content type, as well as interaction with the applications as further described below. The display system 140 generally includes or is in communication with a processor or controller configured to detect and track a head position or an eye position of a user relative to the monitor 142 based on processing output data of the first sensor assembly 144. In some variations, a spatial relationship between the monitor 142 and a user , a user sitting in seat 10 can be adjusted based on the detected eye or head position of the user, , to optimize the user's visualization or perception of three-dimensional image data from the endoscopic or surgical robotics camera on the monitor 142. A user's perception of three-dimensional images on the monitor 142 may be optimal when the user's eyes are substantially centered with respect to the monitor 142 and spaced at a prescribed distance therefrom , approximately 70 cm to approximately 90 cm, such as approximately 80 cm from the monitor. Thus, the position or orientation of the seat 10 or the monitor 142 can be automatically , without requiring a deliberate user input adjusted or changed to ensure that the user's head or eyes are located and positioned at an optimal orientation or viewing distance in relation to the monitor. In one embodiment, the processor or controller can be in communication with the seat actuators 169 and 171 or the monitor actuators 158, 162, 166 and can automatically provide signals or information to seat actuators 169/171 or monitor 158, 162, 166 to adjust a position or orientation of the seat 10 or monitor 142 based upon processing output signals from the first or second sensor assemblies 144/166. For example, the processor or controller can determine a position of the user's head or eyes in relation to the monitor 142, and the processor can automatically generate and send a signals to the seat actuators 169 or 171 or the monitor actuators 158, 162, or 166 to adjust or change the position or orientation of the seat or monitor , the seat can be reclined, tilted, rotated, moved up or down, moved side to side, etc. or the monitor can be tilted, yawed, rotated, moved front-to-back, moved side-to-side movement, moved up-and-down, based on the determined position of the user's head or eyes, , to optimize the user's visualization of three-dimensional images from the surgical robotics camera on the monitor. For example, the position or orientation of the seat or monitor can be adjusted such that the user's head or eyes is substantially centered with respect to the monitor and is at a prescribed distance from the monitor for optimal viewing of three-dimensional images. The processor or controller additionally, or alternatively, can generate and send signals to the monitor 142 to display instructions thereon for manual adjustment of the monitor 142 or the seat 10 to optimize the user's perception or visualization of three dimensional image data on the display. Furthermore, the controller or processor is configured to detect the track the gaze of the user based on processing output data of the second sensor assembly 146, and in some variations, operations of the display system 140 or the surgical robotic system 1 can be modified or controlled based on the detected gaze of the user , to facilitate control of the display system with the user's eyes or eye gestures or to stop or pause operations of the display system or surgical robotic system when the detected gaze of the user is directed away from the monitor. In some variations, the processor or controller can be in communication with the surgical robotic system 1, and when the processor or controller determines that the gaze of a user is not directed at the monitor 142 , for a predetermined time period, such as approximately 3 seconds or up to approximately 5 seconds or more, the processor or controller is operable to automatically send a signals or other output data to the surgical robotic system 1 or the display system 140 to activate or disable one or more operations thereof , to disable or freeze operation of one or more subsystems of the surgical system, such as the robotic arms 4 or the surgical tools 7, or to generate an alarm with the display system. In one embodiment, when the processor or controller determined that the user's gaze is not directed at the monitor 142, , for a prescribed time period, such as when the user is distracted, falls asleep, etc. , the processor or controller automatically generates and sends one or more signals to the surgical system 1 to freeze or pause operation of the robotic arms 4 or the surgical tools 7, , to prevent injury to a patient being operated on. Further, when the processor or controller determines that the user's gaze has returned to the monitor 142, the processor or controller may automatically generate and send one or more signals to the surgical system to resume operation of the robotic arms 4 or the surgical tools 7. However, the processor or controller may require a specific user input , selection of an icon, a gesture, prior to sending the signals for resuming operation of the robotic arms or surgical tools 7. Additionally, or in the alternative, when the processor or controller determines that the user's gaze is not directed at the monitor 142, the processor or controller may generate and send a signals to the display system 140 to activate one or more alarms or notifications to get the attention of the user or other suitable entity , a speaker of the display system may play one or more audio sounds, the monitor may display one or more images indicating that the user's gaze is not directed at the monitor, or one or more vibrations or haptics of the seat or UIDs may be activated. In some variations, the detected and tracked gaze of the user also can be used to initiate or control the applications on the control/side panels 182. For example, a user can look at or focus on the one or more images 184 on the control or side panels 182 to trigger application interactions. The user can initiate or close the applications, open the applications in one or more new windows or pop-up windows, control features or operations of the applications, etc. by focusing on or looking at one or more areas or points on the GUI or using other suitable eye motions. In one example, the user can focus on or look at an image associated with a timer application shown on the control/side panels, , to start and stop the timer. In another example, the user can focus on or look at an image associated with an x-ray imaging tool to initiate the x-ray imaging tool , to open the x-ray imaging tool on one or more secondary or popup windows on the display. The user's gaze further can be used to close the x-ray image tool , when the user looks away or focuses on a close icon or image or other suitable feature. Additionally, a position or orientation of the surgical robotics camera also can be updated or adjusted based on the detected and tracked gaze of the user. In some variations, the position or orientation of the surgical robotics camera can be continuously or dynamically updated , the controller or processor can automatically generate and send signals to an actuator subsystem of the surgical robotics camera to tilt, rotate, or otherwise translate a lens of the surgical robotics camera such that an area or point on the monitor 142 that is being focused on by the user is substantially centered along the monitor 142 , centered in relation to the horizontal axis and the vertical axis of the monitor where perception or visualization of three-dimensional image data is optimal. That is, each time a user focuses on an area or point of the three-dimensional image data displayed in the display window 180 that is not substantially centered along the display window 180 , based on the user's detected gaze, the position or orientation of the surgical robotics camera can be updated or changed such that the area or point of the three-dimensional image data on which the user is focused is moved or otherwise adjusted along the display window 180 so as to be substantially centered therealong. For example, the user may initially focus on a point or area of the three-dimensional image data that is substantially centered within the display window 180, and when the user changes their focus or otherwise redirects their gaze to a new area or point on the image data shown in the display window 180 , the user looks at or focuses on an area or point that is proximate to or near an edge or corner of the display window 180 or the user looks at or focuses on an area or point that is otherwise spaced apart from the original point or area in the center of the display window, the processor or controller may generate and send one or more signals to the surgical robotics camera or a controller thereof to automatically adjust the position or orientation of the surgical robotics camera such that the new area or point of the three-dimensional image data that is focused on by the user is moved or adjusted so as to be substantially centered within the display window 180. In this way, the position or orientation of the surgical robotics camera can be continuously or dynamically adjusted or otherwise updated based upon the determined gaze of the user such that the user's focus is directed to be and remains generally centered along the display window to facilitate optimal three-dimensional perception or visualization of the three-dimensional image data displayed therein. The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the invention. However, it will be apparent to one skilled in the art that specific details are not required in order to practice the invention. Thus, the foregoing descriptions of specific embodiments of the invention are presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed; obviously, many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications, and they thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/57/957/108/0.pdf",
                    "CONTRADICTION_SCORE": 0.9853561520576477,
                    "F_SPEC_PARAMS": [
                        "reduced patient scarring, less patient pain, shorter patient recovery periods,",
                        "lower medical treatment costs"
                    ],
                    "S_SPEC_PARAMS": [
                        "problematic to use and handle in surgical or sterile environments"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "Generally, MIS provides multiple benefits, such as reduced patient scarring, less patient pain, shorter patient recovery periods, and lower medical treatment costs associated with patient recovery."
                    ],
                    "S_SENTS": [
                        "Such glasses and additional wearable components, however, may be problematic to use and handle in surgical or sterile environments."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Side Effects"
                    ],
                    "F_SIM_SCORE": 0.4541338384151459,
                    "S_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object"
                    ],
                    "S_SIM_SCORE": 0.5087361335754395,
                    "GLOBAL_SCORE": 1.6667911380529403
                },
                "sort": [
                    1.6667911
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11433551-20220906",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11433551-20220906",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2020-02-07",
                    "PUBLICATION_DATE": "2022-09-06",
                    "INVENTORS": [
                        "Yung-I Huang"
                    ],
                    "APPLICANTS": [
                        "INSTITUTE FOR INFORMATION INDUSTRY    ( Taipei , TW )"
                    ],
                    "INVENTION_TITLE": "Measurement system and method for positioning accuracy of a robotic arm",
                    "DOMAIN": "B25J 13087",
                    "ABSTRACT": "A measurement system for positioning accuracy of a robotic arm includes the robotic arm, a computing device, a robotic arm controlling device, a first magnetic element and a second magnetic element. The robotic arm controlling device is electrically connected to the robotic arm and the computing device. The first magnetic element is disposed on a robotic arm. The second magnetic element is disposed on a fixed platform. One of the first magnetic element and the second magnetic element is electrically connected to the computing device. The robotic arm controlling device controls the robotic arm to move the first magnetic element above the second magnetic element to generate a magnetic field. The computing device is configured to calculate a plurality of movement error information of the first magnetic element in the magnetic field, and count the plurality of movement error information to obtain a positioning accuracy of the robot arm.",
                    "CLAIMS": "1. A measurement system for positioning accuracy of a robotic arm, comprising: the robotic arm; a first magnetic element, disposed on the robotic arm; a second magnetic element, disposed on a fixed platform; a robotic arm controlling device, electrically connected to the robotic arm; and a computing device, electrically connected to the robotic arm controlling device and one of the first magnetic element and the second magnetic element; wherein the robotic arm controlling device controls the robotic arm to move the first magnetic element above the second magnetic element to generate a magnetic field, so that the first magnetic element moves in the magnetic field of the second magnetic element; wherein the computing device calculates a plurality of movement error information of the first magnetic element in the magnetic field, and counts the plurality of movement error information to obtain a positioning accuracy of the robotic arm; wherein the first magnetic element is a magnetic probe, and the second magnetic element is an array Hall sensor and electrically connected to the computing device; wherein the plurality of movement error information are obtained by calculating differences between distances that the first magnetic element moves and the distances moved by the first magnetic element that the computing device calculates. 2. The measurement system for positioning accuracy of a robotic arm as claimed in claim 1, wherein the robotic arm comprises an operation end on which an end-effector is disposed, and the first magnetic element is fixed on the end-effector. 3. The measurement system for positioning accuracy of a robotic arm as claimed in claim 1, wherein the robotic arm includes a plurality of shaft bodies rotatably connected to one another, and the first magnetic element is disposed on the shaft body at an extreme end of the plurality of shaft bodies. 4. The measurement system for positioning accuracy of a robotic arm as claimed in claim 1, further comprising a display interface connected to the computing device to display a position of the first magnetic element in the magnetic field. 5. A measurement method for positioning accuracy of a robotic arm, comprising steps of: providing a first magnetic element, wherein the first magnetic element is disposed on the robotic arm; providing a second magnetic element, wherein the second magnetic element is disposed on a fixed platform; providing a computing device, wherein the computing device is electrically connected to one of the first magnetic element and the second magnetic element; providing a robotic arm controlling device, wherein the robotic arm controlling device is electrically connected to the robotic arm and the computing device; controlling the robotic arm by the robotic arm controlling device to move the first magnetic element above the second magnetic element to generate a magnetic field; controlling the robotic arm by the robotic arm controlling device, so that the first magnetic element moves in the magnetic field of the second magnetic element; calculating, by the computing device, a plurality of movement error information of the first magnetic element in the magnetic field according to a movement of the first magnetic element, and counting the plurality of movement error information to obtain a positioning accuracy of the robotic arm; wherein the first magnetic element is a magnetic probe, and the second magnetic element is an array Hall sensor and electrically connected to the computing device; wherein the plurality of movement error information are obtained by calculating differences between distances that the first magnetic element moves and the distances moved by the first magnetic element that the computing device calculates. 6. The measurement method for positioning accuracy of a robotic arm as claimed in claim 5, wherein the step of calculating the plurality of movement error information of the first magnetic element comprises steps of: moving the first magnetic element to a positioning point above the second magnetic element; and repetitively moving the first magnetic element away from and then back to the positioning point. 7. The measurement method for positioning accuracy of a robotic arm as claimed in claim 5, further comprising a step of: providing a display interface, wherein the display interface is connected to the computing device to display a position of the first magnetic element in the magnetic field.",
                    "FIELD_OF_INVENTION": "The present invention relates to a measurement technology of positioning accuracy, and more particularly to a measurement system and method for positioning accuracy of a robotic arm.",
                    "STATE_OF_THE_ART": "Robotic arms have been used in many fields, such as part assembly or semiconductor wafer clamping. However, the terminal of the robotic arm must have a certain degree of accuracy to avoid damage to the product or reduction of the yield of the product due to excessive error during the processing. There are several positioning correction techniques for robotic arm terminals, such as laser interferometers, laser trackers, image sensors, and laser cross positioning technology. The laser interferometer can only measure the positioning accuracy of one axis in the three-degree space at a time, and has the problem of time-consuming operation. The laser tracker has a fast and accurate measurement of high portability and positioning accuracy, but has a problem of excessive hardware cost. The image sensor has two kinds of cameras: a single camera and a three-dimensional camera. The single camera can only capture information of two axes in a three-dimensional space, and the software and hardware required for the three-dimensional camera are very expensive. Laser cross positioning technology has high accuracy, but it has the problem of measuring time too long and the cost of software and hardware too high. On the other hand, if the robotic arm can accurately measure the positioning accuracy during use, the robot arm can be corrected immediately without stopping. Among the above positioning correction technologies, only the image sensor can detect the positioning accuracy during the use of the robotic arm, but the image sensor generates oil stain of the lubricating oil during use.",
                    "SUMMARY": [
                        "Embodiments of the present invention provide a measurement system and method for positioning accuracy of a robotic arm, which can accurately and quickly measure the positioning accuracy of the robotic arm in a three-dimensional space, and can be used in the operation of the robotic arm without generating pollution. The measurement system for positioning accuracy of a robotic arm provided by the present invention includes the robotic arm, a computing device, a robotic arm controlling device, a first magnetic element and a second magnetic element. The robotic arm controlling device is electrically connected to the robotic arm and the computing device. The first magnetic element is disposed on a robotic arm. The second magnetic element is disposed on a fixed platform. One of the first magnetic element and the second magnetic element is electrically connected to the computing device. The robotic arm controlling device is configured to control the robotic arm to move the first magnetic element above the second magnetic element to generate a magnetic field, so that the first magnetic element moves above the second magnetic element. The computing device is configured to calculate a plurality of movement error information of the first magnetic element in the magnetic field, and count the plurality of movement error information to obtain a positioning accuracy of the robot arm. The measurement method for positioning accuracy of a robotic arm provided by the present invention includes steps of: a providing a first magnetic element, wherein the first magnetic element is disposed on the robotic arm; b providing a second magnetic element, wherein the second magnetic element is disposed on a fixed platform; c providing a computing device and a robotic arm controlling device, wherein the computing device is electrically connected to one of the first magnetic element and the second magnetic element, and the robotic arm controlling device being electrically connected to the robotic arm and the computing device; d controlling the robotic arm by the robotic arm controlling device to move the first magnetic element above the second magnetic element to generate a magnetic field; e controlling the robotic arm by the robotic arm controlling device, so that the first magnetic element moves above the second magnetic element; f calculating, by the computing device, a plurality of movement error information of the first magnetic element according to a movement of the first magnetic element, and counting the plurality of movement error information to obtain a positioning accuracy of the robotic arm. In an embodiment of the present invention, the robotic arm comprises an operation end on which an end-effector is disposed, and the first magnetic element is fixed on the end-effector. In an embodiment of the present invention, the first magnetic element is a magnetic probe, and the second magnetic element is an array Hall sensor and electrically connected to the computing device. In an embodiment of the present invention, the second magnetic element is a magnetic probe, and the first magnetic element is an array Hall sensor and electrically connected to the computing device. In an embodiment of the present invention, the robotic arm includes a plurality of shaft bodies rotatably connected to one another, and the first magnetic element is disposed on a shaft body at an extreme end of the plurality of shaft bodies. In an embodiment of the present invention, the measurement system for positioning accuracy of a robotic arm further includes a display interface applied to display a position of the first magnetic element or the second magnetic element in the magnetic field. In an embodiment of the present invention, the step of calculating the plurality of movement error information of the first magnetic element includes: moving the first magnetic element to an initial point above the second magnetic element; and repetitively moving the first magnetic element away from the initial point and then back to the initial point. The measurement system and method for positioning accuracy of a robotic arm of the embodiment of the present invention configure the first magnetic element and the second magnetic element between the robotic arm and the fixed platform. The magnetic field generated by the first magnetic element and the second magnetic element can quickly and accurately measure the three-axis movement information of the operation end of the robotic arm in the three-dimensional space, thereby providing compensation information. In addition, the measurement system and method for positioning accuracy of a robotic arm of the embodiment of the present invention can be used during the operation of the robotic arm without causing pollution such as oil stains. In order to make the above and other objects, features, and advantages of the present invention more comprehensible, embodiments are described below in detail with reference to the accompanying drawings, as follows.",
                        "1A is a schematic diagram of a measurement system for positioning accuracy of a robotic arm provided by an embodiment of the present invention; 1B is a schematic diagram of a transformation matrix for obtaining a coordinate system between a robotic arm and a fixed platform provided by an embodiment of the present invention; 2 is a schematic diagram of a measurement system for positioning accuracy of a robotic arm provided by another embodiment of the present invention; 3 is a schematic diagram of a measurement system for positioning accuracy of a robotic arm provided by another embodiment of the present invention; and 4 is a schematic flowchart of a measurement method for positioning accuracy of a robotic arm provided by an embodiment of the present invention."
                    ],
                    "DESCRIPTION": "1A is a schematic diagram of a measurement system for positioning accuracy of a robotic arm provided by an embodiment of the present invention. 1B is a schematic diagram of a transformation matrix for obtaining a coordinate system between a robotic arm and a fixed platform provided by an embodiment of the present invention. Please refer to 1A and 1B, a measurement system 100 for positioning accuracy of a robotic arm in the embodiment includes a robotic arm 2, a computing device 111, a robotic arm controlling device 112, a first magnetic element 120, and a second magnetic element 130 to measure the positioning accuracy of an operation end 21 of the robotic arm 2. The robotic arm controlling device 112 is electrically connected to the robotic arm 2 and the computing device 111. The first magnetic element 120 is disposed on the robotic arm 2. The second magnetic element 130 is disposed on a fixed platform 3. One of the first magnetic element 120 and the second magnetic element 130 can be electrically connected to the computing device 111. In the embodiment, the robotic arm 2 is disposed an end-effector 22 on an operation end 21 thereof, and the first magnetic element 120 is a magnetic probe and disposed on the end-effector 22. On the other hand, the robotic arm 2 can include a plurality of shaft bodies 20 rotatably connected to one another, and the end-effector 22 is disposed on a shaft body at an extreme end of the plurality of shaft bodies 20. The second magnetic element 130 disposed on the fixed platform 3 is an array Hall sensor and electrically connected to the computing device 111. In the embodiment, the end-effector 22 shown in 1A is a gripper jaw, but is not limited thereto. In the embodiment, the computing device 111 is electrically connected to the second magnetic element 130 to receive a signal of a magnetic field generated by the first magnetic element 120 and the second magnetic element 130 and obtain a position of the first magnetic element 120 in the magnetic field through a program having an algorithm. The robotic arm controlling device 112 is electrically connected to the robotic arm 2 and the computing device 111 to receive instructions of the computing device 111 for controlling the movement of the robotic arm 2. The measurement system 100 for positioning accuracy of a robotic arm of the embodiment can further include a display interface 113. The display interface 113 is electrically connected to the computing device 111 to display the position of the first magnetic element 120 in the magnetic field. The computing device 111 can be a computer or a microcomputer, the robotic arm controlling device 112 can drive and control the robotic arm 2 and usually include an electric control box not shown and a guiding device not shown electrically connected to the electric control box, and the display interface 113 can be a screen, but are not limited thereto. The robotic arm controlling device 112 is configured to control the robotic arm 2 to move the first magnetic element 120 above the second magnetic element 130 to generate the magnetic field, so that the first magnetic element 120 moves above the second magnetic element 130. The computing device 111 is configured to calculate a plurality of movement error information of the first magnetic element 120 in the magnetic field and count the plurality of movement error information to obtain a positioning accuracy of the robotic arm 2. In the embodiment, a coordinate system of the end-effector 22 of the first magnetic element 120 includes three axes of XR, YR, and ZR, and a coordinate system of the fixed platform 3 includes three axes of XM, YM, and ZM. The robotic arm controlling device 112 can control the robotic arm 2 to reciprocate in the range of the magnetic field. For example, in 1A, the robotic arm controlling device 112 first controls the robotic arm 2 to move the first magnetic element 120 the magnetic probe to an initial position R0 of the coordinate system of the end-effector 22 above a center of the second magnetic element 130 the array Hall sensor, and then the robotic arm controlling device 112 continuously controls the robotic arm 2 to repeatedly move the first magnetic element 120 away from and then back to the initial position R0. However, due to the movement error of the robotic arm 2, the first magnetic element 120 may not accurately return to the initial position R0 and move to a position near the initial position R0. Assuming that it has been moved n times and each time it moves to the position Mi of the coordinate system relative to the fixed platform 3 where i=1 to n and a single-time movement error information is AMi-R0, where A is a transformation matrix used to transfer the coordinate system of the end-effector 22 to the coordinate system of the fixed platform 3. Thus, the positioning accuracy of the operation end 21 of the robotic arm 2 can be obtained by calculating the plurality of movement error information of the first magnetic element 120 in the magnetic field by the computing device 111. As shown in 1B, the obtain of the transformation matrix A may be realized by that the robotic arm controlling device 112 controls the robotic arm 2 so that the first magnetic element 120 moves a distance of DXR XRU, 0, 0 along the XR axis, moves a distance of DYR 0, YRV, 0 along the YR axis and moves a distance of DZR 0, 0, ZRW along the ZR axis. For the coordinate system of the fixed platform 3, the first magnetic element 120 moves a distance of U XMU, YMU, ZMU along the XM axis, moves a distance of V XMV, YMV, ZMV along the YM axis, and moves a distance of W XMW, YMW, ZMW along the ZM axis. Based on the distances DXR, DYR, and DZR are known and the distances U, V, and W can be measured, the transformation matrix A of the end-effector 22 relative to the fixed platform 3 can be obtained. The measurement system 100 for positioning accuracy of the robotic arm of the embodiment can detect the positioning accuracy by the above detection method before the end-effector 22 of the robotic arm 2 performs a task. When it is found that the positioning accuracy of the robotic arm 2 is insufficient, the user can compensate the movement of the robotic arm 2 by the robotic arm controlling device 112. Further, when it is found that the positioning accuracy is too low to correct the movement of the robotic arm 2 by compensation, maintenance or replacement of the robot arm 2 can be considered. The measurement system 100 for positioning accuracy of the robotic arm of the embodiment is provided with the first magnetic element 120 and the second magnetic element 130 between the robotic arm 2 and the fixed platform 3, and the magnetic field generated by the first magnetic element 120 and the second magnetic element 130 can quickly and accurately measure the movement error information of the operation end 21 of the robotic arm 2 in the three-dimensional space, thereby providing compensation information. In addition, the measurement system and method for positioning accuracy of the robotic arm provided by the embodiments of the present invention can be used during the operation of the robotic arm 2 without causing pollution such as oil pollution. 2 is a schematic diagram of a measurement system for positioning accuracy of a robotic arm provided by another embodiment of the present invention. Please refer to 2, the measurement system 100a of the positioning accuracy of the embodiment is substantially the same as the embodiment of 1A, except that the first magnetic element 120a of the embodiment is an array Hall sensor disposed on the robotic arm 2 and electrically connected to the computing device 111, and the second magnetic element 130a is a magnetic probe disposed on the fixed platform 3. In addition, in the embodiment, the first magnetic element 120a being as an array Hall sensor is disposed on the shaft body 20, which is different from that the first magnetic element 120 being as a magnetic probe is coupled to the end-effector 22 in the embodiment of 1A. In the embodiment, the computing device 111 is electrically connected to the first magnetic element 120a and is configured to receive the signal of the magnetic field generated by the first magnetic element 120a and the second magnetic element 130a, and obtain a position of the second magnetic element 130a in the magnetic field through a program having an algorithm. The employed algorithm may include various mathematical formulas related variables for a specific solution, such as homogeneous transformation matrices or regression algorithms, or may include data training model methods such as neural network, back propagation, ANN, CNN, or RNN, but it is not limited thereto. 3 is a schematic diagram of a measurement system for positioning accuracy of a robotic arm provided by another embodiment of the present invention. Please refer to 3, the measurement system 100b of the embodiment is substantially the same as the embodiment of 1A, except that the first magnetic element 120b of the embodiment is an array Hall sensor disposed on the robotic arm 2 and electrically connected to the computing device 111, and the second magnetic element 130a is a magnetic probe disposed on the fixed platform 3. In addition, in the embodiment, the first magnetic element 120b being as an array Hall sensor is coupled to the end-effector 22 as the same as the first magnetic element 120 being as a magnetic probe in the embodiment of 1A. Preferably, the end-effector 22 of the embodiment is a gripper jaw. 4 is a schematic flowchart of a measurement method for positioning accuracy of a robotic arm provided by an embodiment of the present invention. Please refer to 4, the measurement method for positioning accuracy of a robotic arm includes steps as follow. In step S401, a first magnetic element is provided, and the first magnetic element is disposed on the robotic arm. The robotic arm can be the robotic arm 2 in 1A. The first magnetic element can be the first magnetic element 120 in 1A, the first magnetic element 120a in 2, or the first magnetic element 120b in 3. In step S402, a second magnetic element is provided, and the second magnetic element is disposed on a fixed platform. The fixed platform can be the fixed platform 3 in 1A. The second magnetic element can be the second magnetic element 130 in 1A, the second magnetic element 130a in 2, or the second magnetic element 130b in 3. In step S403, a computing device and a robotic arm controlling device are provided, the computing device is electrically connected to one of the first magnetic element and the second magnetic element, and the robotic arm controlling device is electrically connected to the robotic arm and the computing device. In 1A, the computing device 111 is electrically connected to the second magnetic element 130. In 2 and 3, the computing device 111 is electrically connected to the first magnetic elements 120a and 120b, respectively. In step S404, the robotic arm is controlled by the robotic arm controlling device to move the first magnetic element above the second magnetic element to generate a magnetic field. In 1A, the first magnetic element 120 moved by the robotic arm is a magnetic probe. In 2, the first magnetic element 120a moved by the robotic arm is an array Hall sensor. In 3, the first magnetic element 120b moved by the robotic arm is an array Hall sensor. In step S405, the robotic arm is controlled by the robotic arm controlling device, so that the first magnetic element moves above the second magnetic element. For example, in 1A, the robotic arm controlling device 112 controls the robotic arm 2, so that the first magnetic element 120 is moved to an initial location R0 above the second magnetic element 130, and the first magnetic element 120 is repetitively moved away from and then back to the initial location R0. In step S406, the computing device calculates a plurality of movement error information of the first magnetic element in the magnetic field according to the movement of the first magnetic element and counts the plurality of movement error information to obtain a positioning accuracy of the robotic arm. For example, in 1A, the computing device 111 calculates a plurality of movement error information of AMi-R0, counts the plurality of movement error information through the algorithm, and obtains a positioning accuracy of the robotic arm 2. The measurement system and method for positioning accuracy of the robotic arm of the embodiment is provided with the first magnetic element and the second magnetic element between the robotic arm and the fixed platform, and the magnetic field generated by the first magnetic element and the second magnetic element can quickly and accurately measure the movement error information of the operation end of the robotic arm in the three-dimensional space, thereby providing compensation information. In addition, the measurement system and method for positioning accuracy of the robotic arm according provided by the embodiments of the present invention can be used during the operation of the robotic arm without causing pollution such as oil pollution. Although the present invention has been disclosed as above with the embodiments, it is not intended to limit the present invention. Those ordinarily skilled in the art may make some modifications and retouching without departing from the spirit and scope of the present invention. Therefore, the protection scope of the present invention shall be determined by the scope of the attached claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/51/335/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9915215969085693,
                    "F_SPEC_PARAMS": [
                        "accuracy",
                        "avoid damage to the product"
                    ],
                    "S_SPEC_PARAMS": [
                        "measure the positioning accuracy",
                        "time-consuming operation",
                        "fast and accurate measurement of high portability",
                        "positioning accuracy,",
                        "excessive hardware cost",
                        "accuracy,",
                        "measuring time too long",
                        "cost of software and hardware"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "However, the terminal of the robotic arm must have a certain degree of accuracy to avoid damage to the product or reduction of the yield of the product due to excessive error during the processing."
                    ],
                    "S_SENTS": [
                        "The laser interferometer can only measure the positioning accuracy of one axis in the three-degree space at a time, and has the problem of time-consuming operation.",
                        "The laser tracker has a fast and accurate measurement of high portability and positioning accuracy, but has a problem of excessive hardware cost.",
                        "Laser cross positioning technology has high accuracy, but it has the problem of measuring time too long and the cost of software and hardware too high."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Harmful Factors Acting on Object"
                    ],
                    "F_SIM_SCORE": 0.6924925148487091,
                    "S_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Productivity",
                        "Convenience of Use",
                        "Complexity of Device",
                        "Speed"
                    ],
                    "S_SIM_SCORE": 0.5419842064380646,
                    "GLOBAL_SCORE": 1.6659028146948134
                },
                "sort": [
                    1.6659029
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10899018-20210126",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10899018-20210126",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2017-09-05",
                    "PUBLICATION_DATE": "2021-01-26",
                    "INVENTORS": [
                        "Yasuhiro Naitou"
                    ],
                    "APPLICANTS": [
                        "FANUC CORPORATION    ( Yamanashi , JP )"
                    ],
                    "INVENTION_TITLE": "Human-collaborative robot",
                    "DOMAIN": "B25J 13085",
                    "ABSTRACT": "A human-collaborative robot, by which the robot is unlikely to be accidentally operated despite the intention of the human, even if a relatively simple action pattern is determined as a command to the robot. When an external force by a human is applied to the robot, the force is detected by a force sensor. The robot is configured to be stopped when the detected external force exceeds a predetermined threshold. In order to restart the stopped motion of the robot, the human purposely applies the external force to the robot. When a judging section judges that the external force is applied to a specified portion the robot based on a predetermined action pattern, a commanding section commands the robot so that the robot performs a motion which is previously associated with the pattern.",
                    "CLAIMS": "1. A robot, comprising: a sensor configured to detect an external force applied to the robot; and a controller configured to calculate, by using the external force, a force in a first direction and a moment about a second direction intersecting the first direction, specify, by using the force in the first direction and the moment about the second direction, a portion of the robot to which the external force is applied, judge as to whether or not the external force is applied to the robot based on a predetermined pattern, and as to whether or not the portion of the robot to which the external force is applied corresponds to a predetermined specified portion of the robot, command the robot, when the external force is applied to the robot based on the predetermined pattern and when the portion to which the external force is applied corresponds to the predetermined specified portion, so that the robot performs a motion which is previously associated with the predetermined pattern, and command the robot to stop when the external force applied to the robot exceeds a predetermined threshold, wherein the predetermined pattern includes restarting the robot after the robot has been stopped or shutdown. 2. The robot as set forth in claim 1, wherein the sensor comprises a six-axis force sensor. 3. The robot as set forth in claim 1, wherein the robot has a rotation axis, the first direction is an axial direction of the rotation axis, and the second direction is a direction intersecting the axial direction. 4. The robot as set forth in claim 1, wherein the robot is a human-collaborative robot configured to be operated while sharing a working space with a human. 5. A robot, comprising: a sensor configured to detect an external force applied to the robot; and a controller configured to: calculate, by using the external force, a force in a predetermined direction, judge, by using the force in the predetermined direction, as to whether or not a portion of the robot to which the external force is applied corresponds to a predetermined specified portion, judge as to whether or not the external force is applied to the robot based on a predetermined pattern, command the robot, when the external force is applied to the robot based on the predetermined pattern and when the portion to which the external force is applied corresponds to the predetermined specified portion, so that the robot performs a motion which is previously associated with the predetermined pattern, and command the robot to stop when the external force applied to the robot exceeds a predetermined threshold, wherein the predetermined pattern includes restarting the robot after the robot has been stopped or shutdown. 6. The robot as set forth in claim 5, wherein the sensor comprises a six-axis force sensor. 7. The robot as set forth in claim 5, wherein the robot is a human-collaborative robot configured to be operated while sharing a working space with a human. 8. The robot as set forth in claim 1, wherein after the robot is stopped, the controller is further configured to command the robot to restart when the external force is applied to the robot based on the predetermined pattern and when the portion to which the external force is applied corresponds to the predetermined specified portion. 9. A robot, comprising: at least two sensors respectively provided to a plurality of axes of the robot and configured to detect an external force about the respective axes; and a controller configured to calculate, by using the external force detected by the sensors, a force in a first direction and a moment about a second direction intersecting the first direction, specify, by using the force in the first direction and the moment about the second direction, a portion of the robot to which the external force is applied, judge as to whether or not the external force is applied to the robot based on a predetermined pattern, and as to whether or not the portion of the robot to which the external force is applied corresponds to a predetermined specified portion of the robot, and command the robot, when the external force is applied to the robot based on the predetermined pattern, and when the portion to which the external force is applied corresponds to the predetermined specified portion, so that the robot performs a motion which is previously associated with the predetermined pattern, wherein the predetermined pattern includes restarting the robot after the robot has been stopped or shutdown.",
                    "FIELD_OF_INVENTION": "The present invention relates to a human-collaborative robot, which shares a working area with a human.",
                    "STATE_OF_THE_ART": "Recently, a human-collaborative robot, which is operated while sharing a working area with a human, has become popular. In many cases, such a human-collaborative robot has a contact sensor or a force sensor for detecting contact between a human and the robot, and is configured to stop the motion of the robot when the contact is detected so that the robot does not injure the human. As a relevant prior art document, JP 2008-200764 A discloses a working manipulator including: a movable body such as a robot having a movable part and a control part; and a manipulator part attached to the movable body, the manipulator part having a contact sensor for detecting a contact state between the sensor and an object to be worked, and a force sensor for detecting a contact force at the time of contact. JP 2008-200764 A further recites that the control part has a means for grasping a working state based on the contact state detected by the contact sensor and the contact force detected by the force sensor, and a means for interpreting pattern information as a command, in which the pattern command has an artificial regularity detected by the contact sensor or the force sensor. Further, there is a well-known technique in which a human purposely applies an external force to a robot so that the robot performs an evacuation motion. For example, JP 2016-153156 A discloses a human-collaborative robot system including: a detecting part configured to detect an external force applied to a robot; an evacuation motion commanding part configured to command an evacuation motion for moving the robot so as to reduce the external force when the external force detected by the detecting part is larger than a first threshold; and a monitoring part configured to stop the evacuation motion when a range of variation of the detected external force in a predetermined period of time after the evacuation motion is commanded is smaller than a second threshold. In case that the human-collaborative robot is stopped or suspended upon when the robot comes into contact the human, it is necessary to use a means for restarting the robot, in order to restart the operation by the robot. For example, the robot may be restarted by pushing a restart button of an operation panel, whereas the operation panel is not always positioned near the robot. When the restart button is positioned away from the robot, the human operator must move to a place where the restart button is positioned, and thus the productivity of the robot may be reduced. On the other hand, in case that the restart button is positioned near the robot, the robot can be rapidly restarted when the robot is stopped, and thus the productivity can be prevented from being reduced. In this case, however, it is necessary that the restart button be arranged separately from the operation panel, and thus it is more costly than the case in which the restart button is positioned on the operation panel. In the technique of JP 2008-200764 A, the pattern information having the artificial regularity is interpreted as the command, and thus the manipulatorrobot can perform the predetermined motion even if the restart button, etc. , is not used. However, in many cases, the sensor used to receive the contact in the human-collaborative robot is configured to detect the contact with respect to the entirety of the robot arm. Therefore, in case that the prior art is applied to the human-collaborative robot, even if the human erroneously applies the force to the robot in a regular pattern, the robot may be restarted despite the intention of the human. For example, in case that the robot is configured to be restarted when the human consecutively knocks the robot twice, the robot may be restarted despite the intention of the human, even if the human erroneously knocks the robot twice consecutively, whereby the human may be exposed to danger. As a method for solving the above problem, the action pattern to be judged or interpreted as the command of restarting, etc. , may be determined as a complicated pattern which is unlikely to be performed accidentally, whereas this method is inconvenient. Therefore, the action pattern is desired to be easily performed.",
                    "SUMMARY": [
                        "Therefore, an object of the present invention is to provide a human-collaborative robot, by which the robot is unlikely to be accidentally operated despite the intention of the human, even if a relatively simple action pattern is determined as a command to the robot. According to the present invention, there is provided a human-collaborative robot configured to be operated while sharing a working space with a human, the human-collaborative robot comprising: a sensor section configured to, when an external force is applied to the robot, specify a portion of the robot to which the external force is applied; a judging section configured to judge as to whether or not the external force is applied to the robot based on a predetermined pattern, and as to whether or not the portion to which the external force is applied corresponds to a predetermined specified portion of the robot; and a commanding section configured to, when the external force is applied to the robot based on the predetermined pattern and when the portion to which the external force is applied corresponds to the specified portion, command the robot so that the robot performs a motion which is previously associated with the pattern. In a preferred embodiment, the sensor section may have a six-axes force sensor configured to detect a magnitude and a moment of the external force applied to the robot. Alternatively, the sensor section may have a contact sensor configured to detect that the human contacts the robot and detect a portion of the robot where the human contacts. Alternatively, the sensor section may have a torque sensor configured to detect a torque of each axis of the robot. The pattern may be determined based on at least one of a magnitude and a direction of the external force applied to the robot, and a number of times that the external force is applied to the robot.",
                        "The above and other objects, features and advantages of the present invention will be made more apparent by the following description of the preferred embodiments thereof, with reference to the accompanying drawings, wherein: 1 shows a schematic configuration of a human-collaborative robot according to a first embodiment of the present invention; 2 shows the robot of 1 viewed from another angle; 3 shows a schematic configuration of a human-collaborative robot according to a second embodiment of the present invention; and 4 shows a schematic configuration of a human-collaborative robot according to a third embodiment of the present invention."
                    ],
                    "DESCRIPTION": "1 shows a schematic configuration of a mechanical unit of a human-collaborative cooperative robot 10 according to a first embodiment of the present invention. For example, robot 10 is a vertical multi-joint robot configured to be operated while sharing a working space with a human, and has a base J1 base 12, a rotating body J2 base 16 arranged on base 12 and rotatable about a first axis J1 axis 14, an upper arm J2 arm 20 arranged on rotating body 16 and rotatable about a second axis J2 axis 18, and a forearm J3 arm 24 arranged on a front end of upper arm 20 and rotatable about a third axis J3 axis 22. In robot 10, the front end position of robot 10 can be moved and controlled by rotating the J2 base about the J1 and by rotating the J2 and J3 axes, so as to change the posture of each axis. In the illustrated embodiment, the J2 and J3 axes are rotation axes parallel to each other. Further, a wrist axis not shown may be attached to a front end of the J3 arm, and the front end position of the robot may be controlled by the wrist axis. The motion of robot 10 can be controlled by a robot controller 26 connected to robot 10. As shown in 1, controller 26 may include a judging section 28 and a commanding section 30, and the functions of judging section 28 and commanding section 30 may be realized by a central processing unit CPU, etc. , provided to controller 26. Alternatively, the functions of judging section 28 and commanding section 30 may be realized by another device such as a personal computer not shown, which is arranged separately from controller 26. Robot 10 has a sensor section 32 attached to a lower part of J1 base 12, and sensor section 32 has a six-axes force sensor configured to detect a force and a moment. Hereinafter, the function and motion of robot 10 in the first embodiment will be explained. When an external force by a human operator, etc. , is applied to robot 10, the force is transmitted to and detected by force sensor 32. Robot 10 is configured to be stopped in many cases, immediately for safety purposes, when the detected external force exceeds a predetermined threshold. As such, when the external force larger than the specified value is applied to robot 10 due to the contact between the robot and the human, the human can be prevented from being injured by stopping the robot. In order to restart the stopped motion of robot 10, the human operator purposely applies the external force to the robot. In this regard, judging section 28 judges as to whether the external force is applied to robot 10 based on a predetermined action pattern and as to whether the external force is applied to a specified portion of robot 10. Then, when it is judged that the external force is applied to the specified portion of robot 10 based on the predetermined pattern, commanding section 30 commands robot 10 so that the robot performs a motion in this case, a restart motion which is previously associated with the pattern. For example, it is assumed that a portion of a lateral side of the robot arm J2 arm or J3 arm or the rotating body which is not separated from an upper end of the force sensor by 20 cm in the upper direction is determined as the specified portion of robot 10, and knocking twice consecutively , within 0. 5 second or one second is determined as the predetermined pattern. In this case, when an action that the portion of the lateral side of the robot arm or the rotating body, which is not separated from of the force sensor by 20 cm in the upper direction, is consecutively knocked twice is performed by the human, the action of the human may be judged as a command for restarting the motion of the robot and then may be executed. By virtue of this, in case that robot 10 is stopped due to the accidental contact between the human and the robot, the motion of robot 10 can be restarted by the simple action by the human, i. e. , that the human knocks the lateral side twice which is not separated by 20 cm in the upper direction from force sensor 32. When the six-axes force sensor is used as the sensor section as in the first embodiment, it is difficult to directly detect or specify a position of robot 10 where the external force is applied, whereas the position can be calculated as follows. First, as shown in 2 in which robot 10 is viewed from the back side or the left side in 1, forces FX, FY, FZ of three-axes X, Y, Z are projected onto the rotation axis of J2, so as to obtain force F2 along J2 axis. Next, moment M2 about a cross product of J1-axis vector and J2-axis vector is obtained from moments MX, MY, MZ of the three axes. At this point, a value obtained by dividing M2 by F2 M2/F2 corresponds to the height of a working point of the force, and thus the working position of the external force can be specified or calculated. In 2, the direction of force F2 corresponds to a lateral direction parallel to the sheet, and M2 is a moment about the direction perpendicular to the sheet. In the first embodiment, by virtue of the above process, when the force applied to a portion of robot 10 other than the specified portion , the front end of the arm cannot be interpreted as the command. Therefore, only when the external force is applied to the specified portion with the predetermined pattern, such action may be interpreted as the predetermined command , the motion restarting command and the robot may be operated based on the command. 3 shows a schematic configuration of a mechanical unit of a human-collaborative cooperative robot 40 according to a second embodiment of the present invention. In the second embodiment, only a subject matter different from the first embodiment will be explained, and therefore, the same reference numerals are added to the components of the second embodiment corresponding to the components of the first embodiment, and detailed explanations thereof will be omitted. In the second embodiment, the sensor section has a contact sensor 42 configured to detect a contact position, instead of the six-axis force sensor, and robot 40 is covered by contact sensor 42. By using contact sensor 42, it can be judged that the human comes into contact with the robot, and it can be judged which portion of the robot the human comes into contact with. Therefore, it can be directly judged as to whether or not the portion where the human contacts corresponds to the specified portion as described above. In the second embodiment, for example, when the portion of the rotation axis of J3 axis 22 of robot 40 is consecutively knocked twice, this action may be interpreted as the command for restarting the motion of the robot. In such a case, the human can restart the motion of robot 40 by consecutively knocking the portion of the rotation axis of the J3 axis of the robot twice. Therefore, also in the second embodiment, only when the external force is applied to the specified portion with the predetermined pattern, such action may be interpreted as the predetermined command , the motion restarting command and the robot may be operated based on the command. 4 shows a schematic configuration of a mechanical unit of a human-collaborative cooperative robot 50 according to a third embodiment of the present invention. In the third embodiment, only a subject matter different from the first embodiment will be explained, and therefore, the same reference numerals are added to the components of the third embodiment corresponding to the components of the first embodiment, and detailed explanations thereof will be omitted. In the third embodiment, the sensor section has torque sensors 52, 54 and 56, instead of the six-axes force sensor. Torque sensors 52, 54 and 56 are provided to the respective axes in the illustrated embodiment, J1 to J3 axes of the robot and configured to detect the torque of the corresponding axis. When the human comes into contact with robot 50, at least one of torque sensors 52, 54 and 56 detects the torque due to the external force, and then the motion of robot 50 is stopped or suspended. In the third embodiment, for example, when J2 arm upper arm 20 of robot 50 is consecutively knocked twice, this action may be interpreted as the command for restarting the motion of the robot. In this case, when the human knocks upper arm 20 twice, torque sensor 54 of the J2 axis detects the external force, whereas torque sensor 56 of the J3 axis does not detect the external force. On the other hand, when the human knocks J3 arm forearm 24 twice, both torque sensors 54 and 56 detect the external force. As such, the portion to which the external force is applied can be identified based on the detection result of the plural sensors. For example, even when the external force is applied to forearm 24, this action cannot be interpreted as the command. Therefore, also in the third embodiment, only when the external force is applied to the specified portion with the predetermined pattern, such action may be interpreted as the predetermined command , the motion restarting command and the robot may be operated based on the command. The specified portion is not limited to the example as described above. For example, the specified portion may be a lateral side of the robot arm, the height of which from the force sensor or a reference surface such as an installation surface of the robot is between 50 cm and 70 cm may be determined as the specified portion. In other words, an arbitrary portion of the robot may be determined as the specified portion. In particular, by determining a portion of the robot, to which the external force is hardly applied in normal operation, as the specified portion, the possibility that the human may accidentally or mistakenly activate or operate the robot can be significantly reduced. In addition, the predetermined pattern is not limited to consecutively knocking the robot twice, etc. , and thus the pattern may be determined based on at least one of a magnitude and a direction of the external force applied to the robot, and a number of times and a time interval if the number is plural that the external force is applied to the robot. Further, it is preferable that the predetermined pattern be hardly carried out in the normal operation in other words, the possibility that the pattern is accidentally carried out is considerably low. In the above embodiment, the command to be executed when the external force is applied to the specified portion of the robot with the predetermined pattern is explained as restarting the suspended motion of the robot. However, the present invention is not limited as such. For example, the robot may be moved to a predetermined waiting position or initial position, or may be operated so as to perform the other evacuating motion. Further, a plurality of patterns may be prepared, so that the robot can perform different motions depending on the patterns. For example, the motion of the robot may be restarted when the robot is consecutively knocked twice, and the robot may be moved to the initial position when the robot is consecutively knocked thrice. According to the present invention, the action pattern performed against the predetermined specified portion of the robot may be judged as the command so that the robot executes the predetermined motion such as restarting, and the action pattern performed against a portion of the robot other than the specified portion may be ignored. Therefore, the possibility that the robot performs the predetermined motion despite the intention of the human can be significantly reduced. While the invention has been described with reference to specific embodiments chosen for the purpose of illustration, it should be apparent that numerous modifications could be made thereto, by one skilled in the art, without departing from the basic concept and scope of the invention.",
                    "IMAGE": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAEBAMgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKxP7P1NNQ1KVZlkhuJFeBTOymMeWqMBwcchmGO5/GqraRrjeFp9La+Q3eQIbsSMrEb8/NgcHaMZHB9BVeTSfFsLtFZavCbaEKLc3B3SPiNgTIdnJ3FT+H4VbsbHxKmrpLfanBLZI7HykUKWUhgAfl9dp6+tdFWNfL4i+2yNYPp/2YgBVuN2QR1PA7/wBKlK6yNVWQNbmwIAeIt84OOSp2+ueD19ulVLe28TLKhnvbVo9ykgAZxlcg/LycbuRj6em/WJdReJBqLvZ3Fg1oT8sc6tuA47jr0P5+1T26a55cbXEtn5o3l1jDbT8o2jnnhs59qLKLWRJA17PbsNzmVYhhcYG0DIzwcnOfz7alFY32TWvtgcXsfki6LlSM7oTj5enBHPOf/rUrqw8VNJdfZtTt1SRXWHcvMRMhZWPy84TC49+vGajOl+LY4pgmuQyvIpVC8QHlHcDu4HPAI/4F7VP/AGf4lzeZ1SEh7aRIBt+5KXJVvu9ApAxz0q/otvqlvFKNUuUnkO3aytkcKATjaMZOTjmtSiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiisy90+S61ezudw8mCKTKn+JyV2/Toef8aztF03VLHUIXuFDQ/ZEib98W2sFQYx7FXOf9r61s6dby21vJHMxZjPK6ksW+VnYqMn2Iq3RRRRRRRRRRRRRRRRRRRRUF3eQWMQluJNiE4BwTk4Jxx9KljkWWNZEOUcBlPqDTqZLIsMTyOcKilj9BWXHHe6gWlaYwoCQqgt26/dI6dMk844AHW3YW91Ar/aZhITjaASQOOcZ5/PP1q5RRRRRRRRRRRRRRRRRRUF3axXlu0MqqwOcblBwcYzg/WofMurWEA2iSRxr/ywbnA9FI/TNWLa4ju7WK5i3eXKgddykHBGeQeQfakuoTcWksIOC6EAn1qnBcSWmlO8tu4ZGKxxkjLkn5R+JOM/j0psl/qMKxNNp0KK5Ck/aSQjHgA/J68ZFTiXUz1tLUf9vLf/ABun77//AJ97b/v+3/xFN+03Mc8Ec0EIWVygZJSxB2luhUelXKKKKKKKKKKKKKKKKKKqajetYWwlSHzSXC7ckcHqeAazZfEE0aRf8S9zI6qzLliEBC9wv+0fyqTU/tF3aLIIniWN0ZUdtpkbcBgkZIGCffn25ryWvkxSxW1jNa/bLhDcSyTBuflXKjLc4A9B3rVNnck8ancj6JF/8RSGyucHGqXOccZSL/4iorC1aSC0nlu7iUxjcFk2fe2lTnCgnqe9aVFFFFFFFFFFFFFFFFFFIzKuNzAZ9TWfq11FFbeWxYyF4zsRSxxvHJA6U/UHDW9uy5wbiLqMfxir1VtQmnt7KSW2i82UY2ptJzkgdBzUWkeeNLiFzAYZgWDISCR8x5yPXr+NXqKKKKKKKKKKKKKKzdau76ztkewgSaVm27X6dDjnPrj/ADzWhGWaNS42sQCR6GnUVnaro1vq/lC4dwIs42Y749QfT9fpTr2CKCxfy1xulQsc5JO9epNP1LPkw4/5+Iv/AEMU+/nnt7VpLeHzpAR8nrWQ2tan5UBTTH3uqs+UYquVUkZHflvyFbyEsisy7WIBK+lOoooooooooooorFn8R29u7CUJGAxGZJlXIBIz7cj+VNXxNavIsa+XvY7QpmUEnOP58UXGriaWK1SJBcM6MimZee/8ga0oLt5bh4JbdonVA4ywIIJI7fSo9SbUlRTpyRO2DuEhxzkY/Td+lVll1k3nMCLbggckEsd3JHPA2/jxV/zbn5V+zAMepMg2j+p/KqV7cMunztdPEoSdFBXPTcp796jv9Vtn8m3hLPcNLGyxFShIDjn5sce9TXMrF0hvpRGkgJ8mBWcsBjOWA6cjsPrWhE8ckSvEyshHBU8U+iiiiiiiiiiikZgilmICgZJPaora7t7xC9tPHKoOCUbODjP8iK5xtKh1KV45be3fHmFzKHbcPNcYwGA7D9PSsBryNrvy10RNqPhZQsqnG/Kt16dGHoWH1rS0mV7nXI4zpqQyIu+O4cSsPu/wkt6EiujsZnn1B3k27xEUO3odsjrn9K06zXnvodXUTGFbBtwViQGzhcfrvqydQslbabuAH08wVl6dLHd65duAGhQ7oGPRmwA7D6YUA+59adLFO2l31zfQRw3KytJDtffjYcREHAwTgcerEd607s2giBvfJCZwDLjGT2Ge9U9IGJLkJu8rIxu65yevvt2/pnnNalFFFFFFFFFFFMlCmJw7bUKncc4wPr2qjaaTYW8ZFsrKC2SyTNknAHUH2FYhuodHLvG20ymRWDI79JXwev1/Kubs73TYpfl1OCXyWDlXgPBGD6/7ParNlc2lhqEWqC/ydgjUGBtrAgjpnk8HH4dq6nTLpFuA58yQyRMx2QtwfNfIxzjk4rTOoR/88bo/9u7/AOFNa+hkUq9tcsp6hrdiP5VQka7vbqaC1Ux2qRqGjcmEtnOf4CcYGMgimw2lyVkihs7aDyZP3bR3LAp8q9Pk6Yxx09qfYlpblo9SuHkmim2xoRiPIAIIIUBm/l6VFqlxJJfeSRgxHchjHODjIJDjr6Y9KtaXqPmTiy+zGEBCy/uynAI7H6+vrWtRRRRRRRRRRRUVxAtzbSwOSFkUqSMZGfrVKy0S3sLfyLea5VM5/wBZj+X069aws6ZFNKl6WmXLgKz7irCR/fI4INc9ZWNmJc3FpbLukBZljTkZXPbqfm9OverNjFALq3hvYbR9OSMB4wo3BgOCp9if8jFdHpcjteYsTAVWJ/lZvuqZWKjjpwa2P+Jln/l0A/4EaP8AiZetp+TVmW/9nLPc/wBo/ZvthlO/Ocle3XnFOjfQg8+77MB5nHH+yKm02Czmj1CO3VfsskwxsGBnYuSPfI6+tT21pBcqJ54le4GY5D2LKcE46c46+mKuRQRQAiKJIweoRQKkooooooooooooooooooqC4mkRkihUNLJnBb7qgdSfzHHv+NLBb+TuZnaSV8b3bv7Adh7VIqbS5yTuOee3AH9KdVYwSxXJlgK7JCPMRvXpuB9cdu+O1R6dPeTib7XCIirAKAuM/KCe5zySPwq7RRRRRRRRRRRRRRRRRRWbJpJk1pNR+0EBQP3W3rgMOuf9r07VpUUUUUUUUUUUUUUUUUUUUUUU1nVCAzAZOBk9aUOrdGB4zwe1AIPQg9uKWiiiiiiiiiiiiiiiiiiiiis+8037XqtjeMwCWqyfLgHJbbjqP9k1maRoF3pl/BOZLd0W2WFwAQQQFGFOOnyk/VjWjo+nvp8dyJVhV5rhpSIiduDgDggYOAM9cnJ71pUUUUUUUUUUUUUUUUUUUUUUhYKpZiAB1JrLl1pRFHNDA7QOQPPkO2MAkAHPPBJ64pl7pTaxLFJcsYFiHyeS4JY5BznHT5Rx3zWxRRRRRRRRRRRRRRRRRRRRRWXrCzNDGoMJRriIBXQt/EOvPPNYCNqI8MWtqwsNgMIyJCSVDqfu+vtW9pbToRA3kCIGUgIpByJGGAM9Bx+YrVoooooooooooooooooooooqnqsavpVyTnKRmRSCQVZRkEEehFLPa2620kqwRCRVLBggyDjOazWvoLFtMtvLcuwV9wIJ+bg5yc5JOfwNTweIbS5vo7WMMWcfeBUgHA4ODnPI/X0rWooooooooooooooooooooqrqf/IKvP8Arg//AKCakuP+PKX/AK5n+VNtY0a0t2ZFLCNcEjkcVJGsLYaMRnHQrjjj/CpKKKKKKKKKKKKKKKKKKKKKq6nxpN5/1wf/ANBNLeRrLp0yOMqYzkZ68VBaWKCyg8uWaNDGuUVsr09DnH4U3StItdHSSO3d2MhBO8jsPQAev61pUUUUUUUUUUUUUUUUUUUU1nVMbmC54GTisHX9QkWy1BIp4FSOEbgRlm3ZBGc8ce1as9xAbGTE8ZGwjIcelZOnajdyWlnGJbVjPbl1J+Xy8bR6nd19ulXrfSbVdQW/jmkklQMpJcEEkKDn3+UVp0UUUUUUUUUUUUUUUUUUU0FHGRtYfnWJrEJ82WI24ljvFVAFznKksQcDIBHcVVaOYoV+w3hB4wbubH/oNW9GibMYa3ESWcRgO7OSx2NwCOgHGT19KtWmo2IhlKTxCNXZsq2Rg4bP/jw/MVdt7mG7hE0EiyRnowqWiiiiiiiiiiiiiiiiikZdylfUYrLsdDTTYXW1uJFckMGIGOABggAAjj6+9RTP5uqBb2c2nlRBk2zABiSwJGeTwB1qYGzJwNYcn/r4Wq9tbj7VfS2k7TSqyNuMmd/yjKnt0H4daYLTQl2W/wBnkjMjqdmJFO4/KM+n3cenHvWzbWsNnCIYVKoPViT+ZqaiiiiiiiiiiiiiiiiimySCKJ5GztRSxx7Vn2+sxXULukMysGVAjgAszKGA68cHvUgkuHQfaNO3yDPKshX8MnNICxPOlMPxj/xqa1mDu8X2ZrdlAYqdvIOf7pPoakktoJZUleJGkQ5ViOR16H8T+dS0UUUUUUUUUUUUUUUUUUdRg1G8EUiOjRqVf7wx1qsmk2axhXhEpH8cvzMfqaX+yrD/AJ9Iv++anhtobcsYolQtjJA61LRRRRRRRRRRRRRRRRRRRQSACScAU1ZEcEo6sB1IOcVmSStdspwzrJzFArFAV/vueuPb9D2Hs2iXebWLC97YlHUe3976fzq1Z3DOfKdxISoeOUDHmJ6/Ud/qPXFW6KKKKKKKKKKKKKKKKKKKZNEs8EkL52yKVOPQjFY9l4attM0qSws5plRiCCzc8ADBK4JBxz9TV61tZBctdzYSRkEYijclFUHI+p5POKu1SispY7pX8xPKR3ZVCnd83UE56Z56dhV2iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiv/Z",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/18/990/108/0.pdf",
                    "CONTRADICTION_SCORE": 0.977262556552887,
                    "F_SPEC_PARAMS": [
                        "complicated pattern"
                    ],
                    "S_SPEC_PARAMS": [
                        "costly"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "As a method for solving the above problem, the action pattern to be judged or interpreted as the command of restarting, etc.",
                        ", may be determined as a complicated pattern which is unlikely to be performed accidentally, whereas this method is inconvenient.",
                        "For example, in case that the robot is configured to be restarted when the human consecutively knocks the robot twice, the robot may be restarted despite the intention of the human, even if the human erroneously knocks the robot twice consecutively, whereby the human may be exposed to danger."
                    ],
                    "S_SENTS": [
                        "In this case, however, it is necessary that the restart button be arranged separately from the operation panel, and thus it is more costly than the case in which the restart button is positioned on the operation panel.",
                        "However, in many cases, the sensor used to receive the contact in the human-collaborative robot is configured to detect the contact with respect to the entirety of the robot arm."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Shape"
                    ],
                    "F_SIM_SCORE": 0.4880993366241455,
                    "S_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.4865226149559021,
                    "GLOBAL_SCORE": 1.6645735323429107
                },
                "sort": [
                    1.6645736
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11253329-20220222",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11253329-20220222",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-06-05",
                    "PUBLICATION_DATE": "2022-02-22",
                    "INVENTORS": [
                        "David Gene Bowling"
                    ],
                    "APPLICANTS": [
                        "MAKO Surgical Corp.    ( Ft. Lauderdale , US )"
                    ],
                    "INVENTION_TITLE": "Robotic surgical system and method for producing reactive forces to implement virtual boundaries",
                    "DOMAIN": "A61B 3470",
                    "ABSTRACT": "A robotic system and methods are provided. The robotic system includes a tool and a manipulator with links for moving the tool. A controller implements a virtual simulation wherein the tool is represented as a virtual volume interacting with a virtual boundary defined by a mesh of polygonal elements. The controller computes a reactive force responsive to penetration of polygonal elements by the virtual volume. The reactive force is computed based on a penetration factor being a function of a geometry of the virtual volume bound relative to a geometry of the polygonal element. The controller applies the reactive force to the virtual volume to reduce penetration of the polygonal element by the virtual volume. The controller commands the manipulator to move the tool in accordance with application of the reactive force to the virtual volume to constrain movement of the tool relative to the virtual boundary.",
                    "CLAIMS": "1. A robotic system comprising: a tool; a manipulator comprising a plurality of links and configured to move the tool; and a controller coupled to the manipulator and configured to implement a virtual simulation wherein the tool is represented as a virtual volume being adapted to interact relative to a virtual boundary defined by a mesh of polygonal elements, each of the polygonal elements comprising a plane and a perimeter, and wherein the controller is configured to: compute a reactive force in response to penetration of one of the polygonal elements by the virtual volume in the virtual simulation, wherein the reactive force is computed based on a penetration factor being a function of a planar intersection between the virtual volume and the plane of the polygonal element, wherein the planar intersection is bound by the perimeter of the polygonal element; apply the reactive force to the virtual volume in the virtual simulation to reduce penetration of the polygonal element by the virtual volume; and command the manipulator to move the tool in accordance with application of the reactive force to the virtual volume in the virtual simulation to constrain movement of the tool relative to the virtual boundary. 2. The robotic system of claim 1, wherein the penetration factor is functionally independent of a linear depth of penetration of the polygonal element by the virtual volume. 3. The robotic system of claim 1, further comprising a navigation system configured to track states of the tool and being coupled to the controller and wherein the virtual volume is positioned based on tracked states of the tool, wherein the tool is configured to interact with a target site and wherein the navigation system is configured to track states of the target site and wherein the virtual boundary is associated with the target site. 4. The robotic system of claim 1, further comprising a navigation system configured to track states of the tool and being coupled to the controller and wherein the virtual volume is positioned based on tracked states of the tool, wherein the navigation system is configured to track states of an object to be avoided by the tool and wherein the virtual boundary is associated with the object to be avoided. 5. The robotic system of claim 1, wherein the controller is further configured to compute multiple reactive forces in response to simultaneous penetration of multiple polygonal elements by the virtual volume in the virtual simulation, wherein each reactive force is computed based on the penetration factor being a function of the planar intersection between the virtual volume and the plane of each polygonal element, wherein each planar intersection is bound by the perimeter of each polygonal element. 6. The robotic system of claim 5, wherein the controller is further configured to: simultaneously apply the multiple reactive forces to the virtual volume in the virtual simulation; and command positioning of the manipulator in accordance with application of the multiple reactive forces to the virtual volume in the virtual simulation. 7. The robotic system of claim 5, wherein the controller is further configured to: combine the multiple reactive forces to generate a combined reactive force; apply the combined reactive force to the virtual volume in the virtual simulation; and command positioning of the manipulator in accordance with application of the combined reactive force to the virtual volume in the virtual simulation. 8. The robotic system of claim 7, wherein the controller is further configured to apply a weighting factor to each of the reactive forces such that the combined reactive force is constant for any given simultaneous penetration of multiple polygonal elements by the virtual volume. 9. The robotic system of claim 1, further comprising a force-torque sensor being configured to sense an input force applied to the tool and wherein the input force is applied to the virtual volume in the virtual simulation to cause penetration of one of the polygonal elements by the virtual volume. 10. The robotic system of claim 1, wherein the penetration factor is further defined as a function of an area defined by the planar intersection between the virtual volume and the plane of the polygonal element, wherein the area is bound by the perimeter of the polygonal element. 11. The robotic system of claim 1, wherein the penetration factor is further defined as a function of one or more arcs of a shape defined by the planar intersection between the virtual volume and the plane of the polygonal element, wherein the one or more arcs are bound by the perimeter of the polygonal element. 12. A method of controlling a robotic system comprising a tool, a manipulator comprising a plurality of links and configured to move the tool, and a controller coupled to the manipulator and configured to implement a virtual simulation wherein the tool is represented as a virtual volume being configured to interact relative to a virtual boundary defined by a mesh of polygonal elements, each of the polygonal elements comprising a plane and a perimeter, the method comprising the controller: computing a reactive force in response to penetration of one of the polygonal elements by the virtual volume in the virtual simulation, wherein the reactive force is computed based on a penetration factor being a function of a planar intersection between the virtual volume and the plane of the polygonal element, wherein the planar intersection is bound by the perimeter of the polygonal element; applying the reactive force to the virtual volume in the virtual simulation for reducing penetration of the polygonal element by the virtual volume; and commanding the manipulator for moving the tool in accordance with application of the reactive force to the virtual volume in the virtual simulation for constraining movement of the tool relative to the virtual boundary. 13. The method of claim 12, wherein the penetration factor is functionally independent of a linear depth of penetration of the polygonal element by the virtual volume. 14. The method of claim 12 further comprising: receiving, from a navigation system, tracked states of the tool; positioning the virtual volume based on tracked states of the tool; commanding the manipulator for moving the tool in relation to a target site; receiving, from the navigation system, tracked states of the target site; and associating the virtual boundary with the target site. 15. The method of claim 12 further comprising: receiving, from a navigation system, tracked states of the tool; positioning the virtual volume based on tracked states of the tool; receiving, from the navigation system, tracked states of an object to be avoided by the tool; and associating the virtual boundary with the object to be avoided. 16. The method of claim 12, further comprising computing multiple reactive forces in response to simultaneous penetration of multiple polygonal elements by the virtual volume in the virtual simulation, wherein each reactive force is computed based on the penetration factor being a function of the planar intersection between the virtual volume and the plane of each polygonal element, wherein each planar intersection is bound by the perimeter of each polygonal element. 17. The method of claim 16, further comprising: simultaneously applying the multiple reactive forces to the virtual volume in the virtual simulation; and commanding positioning of the manipulator in accordance with application of the multiple reactive forces to the virtual volume in the virtual simulation. 18. The method of claim 16, further comprising: combining the multiple reactive forces for generating a combined reactive force; applying the combined reactive force to the virtual volume in the virtual simulation; and commanding positioning of the manipulator in accordance with application of the combined reactive force to the virtual volume in the virtual simulation. 19. The method of claim 18, further comprising applying a weighting factor to each of the reactive forces such that the combined reactive force is constant for any given simultaneous penetration of multiple polygonal elements by the virtual volume. 20. The method of claim 12, further comprising: determining an input force applied to the tool based on measurements from a force-torque sensor, and applying the input force to the virtual volume in the virtual simulation for causing penetration of one of the polygonal elements by the virtual volume. 21. The method of claim 12, wherein computing the reactive force is based on the penetration factor further defined as being a function of an area defined by the planar intersection between the virtual volume and the plane of the polygonal element, wherein the area is bound by the perimeter of the polygonal element. 22. The method of claim 12, wherein computing the reactive force is based on the penetration factor further defined as being a function of one or more arcs of a shape defined by the planar intersection between the virtual volume and the plane of the polygonal element, wherein the one or more arcs are bound by the perimeter of the polygonal element. 23. A controller-implemented method of simulating dynamics of a tool of a robotic system, the method comprising: simulating the tool as a virtual volume; simulating a virtual boundary comprising a mesh of polygonal elements; simulating movement of the virtual volume to penetrate multiple polygonal elements simultaneously; computing multiple reactive forces in response to simultaneous penetration of the multiple polygonal elements, wherein each reactive force is computed based on a penetration factor being a function of a geometry of the virtual volume bound relative to a geometry of each polygonal element; combining the multiple reactive forces for generating a combined reactive force; applying a weighting factor to each of the reactive forces such that the combined reactive force is constant for any given simultaneous penetration of multiple polygonal elements by the virtual volume; and simulating application of the combined reactive force to the virtual volume.",
                    "FIELD_OF_INVENTION": "The present disclosure relates to techniques for producing reactive forces to implement virtual boundaries for a robotic system.",
                    "STATE_OF_THE_ART": "Robotic systems are commonly used to perform surgical procedures and include a robot comprising a robotic arm and a tool coupled to an end of the robotic arm for engaging a surgical site. To prevent the tool from reaching undesired areas, a virtual surface is often implemented for constraining movement of the tool. For instance, the virtual surface may be registered to the surgical site to delineate areas where the tool should and should not manipulate the anatomy. The virtual surface is often defined as a mesh of polygonal elements, such as triangles. If a force is applied to the tool in attempt to penetrate the virtual surface, a counter force is computed for each triangle of the mesh that experiences this attempted penetration. In other words, when the tool pushes on the virtual surface, the virtual surface pushes back due to the compression of the virtual surface or the tool. Conventionally, this counter force is modeled as a spring and the magnitude of this counter force is proportional to a linear depth of the penetration of the triangle i. e. , a distance by which the tool protrudes into the virtual surface. In turn, the robotic arm moves the tool according to the computed counter forces to constrain the tool relative to the virtual surface. Modeling and computing these counter forces is anything but trivial. The virtual surfaces often are complex in shape and define geometric features for which surface interaction by the tool is difficult to model. The issue is exacerbated because of the modeled shape of the tool and/or the pose of the tool during penetration. In turn, when the tool attempts to penetrate the virtual surface, and in particular, when the tool simultaneously penetrates multiple triangles, it has been observed that the robotic arm may provide inconsistent or unexpected movement of the tool responsive to the computed counter forces. For example, as the tool is moved around a flat virtual surface, the tool conventionally experiences a kick-back, interpreted as two counter forces, when simultaneously engaging more than one triangle. The more planar the virtual surface, the worse the kick-back will be. Worse still is when the tool engages a vertex shared among multiple triangles. For example if the vertex is shared among five triangles, the momentary increase of counter force at the vertex will be five times the counter force of one triangle. Additionally, as the tool rolls over an outside corner defined by the virtual surface, many triangles of the mesh along the edges of the outside corner simultaneously experience the attempted penetration. The counter forces for these triangles, when combined, provide a cumulative force spike causing unexpected kick back of the tool while rolling over the outside corner. Further complications arise in conventional robotic systems based on modeling surface interactions simply based on the linear depth of penetration, regardless of whether one or multiple triangles are penetrated. For example, there may be situations where the linear depth of penetration is the same, yet the cross sectional area or displaced volume of the virtual surface is different , based on the modeled shape of the tool or the pose of the tool during penetration, . In such situations, conventional surface modeling applies the same counter force based simply on the linear depth of penetration without taking into account the cross sectional area or displaced volume of the virtual surface. Similar situations arise where only a portion of the tool penetrates the virtual surface, such as at an outer edge of the virtual surface. For example, assuming the modeled shape and pose of the tool are the same during penetration, the entire tool may be over the virtual surface in one situation, and the tool may overhang the outer edge of the virtual surface in another situation. In such situations, conventional surface modeling again applies the same counter force based simply on the linear depth of penetration without taking into account how much of the tool is engaging the virtual surface. As such, there is a need in the art for systems and methods for addressing at least the aforementioned problems.",
                    "SUMMARY": [
                        "One example of a robotic system is provided. The robotic system comprises a tool and a manipulator comprising a plurality of links and configured to move the tool. A controller is coupled to the manipulator and configured to implement a virtual simulation wherein the tool is represented as a virtual volume. The virtual volume is configured to interact relative to a virtual boundary defined by a mesh of polygonal elements. The controller is configured to compute a reactive force in response to penetration of one of the polygonal elements by the virtual volume in the virtual simulation. The reactive force is computed based on a penetration factor being a function of a geometry of the virtual volume bound relative to a geometry of the polygonal element. The controller is configured to apply the reactive force to the virtual volume in the virtual simulation to reduce penetration of the polygonal element by the virtual volume. The controller commands the manipulator to move the tool in accordance with application of the reactive force to the virtual volume in the virtual simulation to constrain movement of the tool relative to the virtual boundary. One example of a method of controlling a robotic system is provided. The robotic system comprises a tool, a manipulator comprising a plurality of links and configured to move the tool, and a controller coupled to the manipulator and configured to implement a virtual simulation. The tool is represented as a virtual volume being configured to interact relative to a virtual boundary. The virtual boundary is defined by a mesh of polygonal elements. The method comprises the controller computing a reactive force in response to penetration of one of the polygonal elements by the virtual volume in the virtual simulation. The reactive force is computed based on a penetration factor being a function of a geometry of the virtual volume bound relative to a geometry of the polygonal element. The method comprises the controller applying the reactive force to the virtual volume in the virtual simulation for reducing penetration of the polygonal element by the virtual volume. The controller commands the manipulator for moving the tool in accordance with application of the reactive force to the virtual volume in the virtual simulation for constraining movement of the tool relative to the virtual boundary. One example of a controller-implemented method of simulating dynamics of a tool of a robotic system is provided. The method comprises simulating the tool as a virtual volume and simulating a virtual boundary comprising a mesh of polygonal elements. Movement of the virtual volume is simulated to penetrate multiple polygonal elements simultaneously. The method comprises computing multiple reactive forces in response to simultaneous penetration of the multiple polygonal elements. Each reactive force is computed based on a penetration factor being a function of a geometry of the virtual volume bound relative to a geometry of each polygonal element. The method comprises combining the multiple reactive forces for generating a combined reactive force. A weighting factor is applied to each of the reactive forces such that the combined reactive force is constant for any given simultaneous penetration of multiple polygonal elements by the virtual volume. Application of the combined reactive force is simulated to the virtual volume. The robotic system and methods advantageously compute the reactive force related based as a function of a geometry of the virtual volume bound relative to a geometry of the polygonal element. In so doing, the reactive force provides a natural reactive force to movement of the tool for any given situation accounting for complexities of the virtual boundary, number of polygonal elements penetrated, modeled shapes of the tool as the virtual volume, and/or poses of the tool during penetration. In turn, when the tool attempts to penetrate the virtual boundary, and in particular, when the tool simultaneously penetrates multiple polygonal elements, the manipulator moves the tool in a manner to provide consistent and expected movement of the tool responsive to the reactive forces. The techniques described herein further account for situations where the linear depth of penetration i. e. , the distance by which the virtual volume protrudes into the polygonal element and/or virtual boundary is the same, yet the cross sectional area or displaced volume of the virtual volume and/or virtual boundary is different. The reactive forces are different when computed as function of a geometry of the virtual volume because the reaction forces account more accurately account for a magnitude of penetration by the geometry of virtual volume. The penetration factor does not change linearly with respect to linear depth of penetration because the penetrating body is volumetric and does not apply a linear impact force to the polygonal element and/or virtual boundary. Instead, the penetrating body applies an impact force as a higher order function related to the volumetric shape of the virtual volume. Accordingly, the penetration factor changes with respect to linear depth of penetration according to this higher order volumetric function. Said differently, the penetration factor takes into account the displaced volume or penetrating volume of the virtual boundary or virtual volume. Moreover, at an outer edge of the virtual boundary, for example, wherein in one situation the entire tool may be over the virtual boundary and in another situation a portion of the tool overhangs the outer edge of the virtual boundary assuming the same virtual volume and the same pose of the virtual volume during penetration, the techniques described herein are likely to generate different reactive forces because the penetration factor more accurately accounts for how much of the virtual volume is engaging the virtual boundary. The techniques further provide a smoother response to opposing the virtual boundary at non-planar polygonal elements of the virtual boundary, such as corners, peaks, valleys, etc. For example, the techniques provide gradual increase in reactive force to avoid discrete jumps in reactive force that may move the tool abruptly or unnaturally during encounters with such non-planar polygonal elements. For example, as the tool rolls over an outside corner defined by the virtual boundary, the penetration factors are more likely to offset, thereby providing combined reactive forces that are substantially consistent. This eliminates any force spikes applied to the tool while rolling over such corners. The reactive response is smooth even though penetration by the virtual volume may occur with respect to more than one polygonal element, and even at the same linear depth. In turn, unexpected kick back of the tool while rolling over the non-planar polygonal elements is mitigated. Thus, the techniques described herein solve surface modeling issues relating to non-planar polygonal elements. Of course, depending on various configurations and situations experienced, the robotic system and method may exhibit advantages and provide technical solutions other than those described herein.",
                        "Advantages of the present invention will be readily appreciated as the same becomes better understood by reference to the following detailed description when considered in connection with the accompanying drawings wherein: 1 is a perspective view of a robotic system, according to one example. 2 is a block diagram of one example of a controller of the robotic system. 3 is a block diagram of control loops implemented by the controller and a manipulator of the robotic system. 4 is an example of a simulated virtual volume representative of a tool of the manipulator and wherein the virtual volume is configured to interact relative to a virtual boundary that is associated with the anatomy and comprises a mesh of polygonal elements. 5 illustrates one example of using a penetration factor based on projected area to generate a reactive force responsive to interaction of the virtual volume with one polygonal element of the virtual boundary. 6 illustrates another example of using the penetration factor based on projected area to generate a reactive force responsive to interaction of the virtual volume with one polygonal element of the virtual boundary. 7 illustrates yet another example of using the penetration factor based on projected area to generate a reactive force responsive to interaction of the virtual volume with one polygonal element of the virtual boundary. 8 illustrates one example of computing projected area with respect to one polygonal element of the virtual boundary. 9 illustrates another example of computing projected area with respect to one polygonal element of the virtual boundary. 10 illustrates one example of using projected areas to generate multiple reactive forces responsive to interaction of the virtual volume with multiple polygonal elements of the virtual boundary. 11 illustrates one example of using projected areas to generate a combined reactive force responsive to interaction of the virtual volume with multiple polygonal elements of the virtual boundary. 12 illustrates another example of using projected areas to generate multiple reactive forces responsive to interaction of the virtual volume with multiple polygonal elements of the virtual boundary. 13 illustrates one example of using projected areas to generate reactive forces responsive to interaction of the virtual volume with polygonal elements forming an outside corner of the virtual boundary. 14 illustrates another example of using projected areas to generate reactive forces responsive to interaction of the virtual volume with polygonal elements forming the outside corner of the virtual boundary. 15 illustrates yet another example of using projected areas to generate reactive forces responsive to interaction of the virtual volume with polygonal elements forming the outside corner of the virtual boundary. 16 illustrates one example of computing the penetration factor based on projected arc with respect to one polygonal element of the virtual boundary. 17 illustrates another example of computing the penetration factor based on projected arc with respect to one polygonal element of the virtual boundary. 18 illustrates one example of computing the penetration factor based on displaced volume with respect to one polygonal element of the virtual boundary."
                    ],
                    "DESCRIPTION": "I. Overview of Robotic SystemReferring to the Figures, wherein like numerals indicate like or corresponding parts throughout the several views, a robotic system 10 hereinafter system and methods for operating the system 10 are shown throughout. As shown in 1, the system 10 is a robotic surgical system for treating an anatomy of a patient 12, such as bone or soft tissue. In 1, the patient 12 is undergoing a surgical procedure. The anatomy in 1 includes a femur F and a tibia T of the patient 12. The surgical procedure may involve tissue removal or treatment. Treatment may include cutting, coagulating, lesioning the tissue, treatment in place of tissue, or the like. In some examples, the surgical procedure involves partial or total knee or hip replacement surgery. In one example, the system 10 is designed to cut away material to be replaced by surgical implants, such as hip and knee implants including unicompartmental, bicompartmental, multicompartmental, total knee implants, or spinal related applications. Some of these types of implants are shown in Patent Application Publication 2012/0030429, entitled, Prosthetic Implant and Method of Implantation, the disclosure of which is hereby incorporated by reference. Those skilled in the art appreciate that the system 10 and method disclosed herein may be used to perform other procedures, surgical or non-surgical, or may be used in industrial applications or other applications where robotic systems are utilized. The system 10 includes a manipulator 14. In one example, the manipulator 14 has a base 16 and plurality of links 18. A manipulator cart 17 supports the manipulator 14 such that the manipulator 14 is fixed to the manipulator cart 17. The links 18 collectively form one or more arms of the manipulator 14. The manipulator 14 may have a serial arm configuration as shown in 1 or a parallel arm configuration. In other examples, more than one manipulator 14 may be utilized in a multiple arm configuration. The manipulator 14 comprises a plurality of joints J. Each pair of adjacent links 18 is connected by one of the joints J. The manipulator 14 according to one example has six joints J1-J6 implementing at least six-degrees of freedom DOF for the manipulator 14. However, the manipulator 14 may have any number of degrees of freedom and may have any suitable number of joints J and redundant joints J. A plurality of position sensors 19 are located at the joints J for determining position data of the joints J. For simplicity, only one position sensor 19 is illustrated in 1, although it is to be appreciated that the other position sensors 19 may be similarly illustrated for other joints J. One example of the position sensor 19 is an encoder that measures the joint angle of the respective joint J. At each joint J, there is an actuator, such as a joint motor 21 disposed between the adjacent links 18. For simplicity, only one joint motor 21 is shown in 1, although it is to be appreciated that the other joint motors 21 may be similarly illustrated. Each joint J is actively driven by one of the joint motors 21. The joint motors 21 are configured to rotate the links 18. As such, positions of the links 18 are set by joint motors 21. Each joint motor 21 may be attached to a structural frame internal to the manipulator 14. In one example, the joint motor 21 is a servo motor, such as a permanent magnet brushless motor. However, the joint motor 21 may have other configurations, such as synchronous motors, brush-type DC motors, stepper motors, induction motors, and the like. The joint motors 21 are positionable at one of a plurality of angular positions, hereinafter referred to as joint angles. The joint angle is the angle of the joint J between adjacent links 18. In one example, each joint motor 21 may be equipped with one of the position sensors 19. Alternatively, each link 18 being driven by that particular joint motor 21 may be equipped with the position sensor 19. In some examples, two position sensors 19, one for the joint motor 21 and one for the link 18 being moved can be used to determine the joint angle, such as by averaging the joint angle, and the displacement between motor 21 and joint J through compliant transmission. Each joint J is configured to undergo a joint torque. The joint torque is a turning or twisting force of the joint J and is a function of the force applied at a length from a pivot point of the joint J. A torque sensor 27 3 may be connected to one or more joint motors 21 for measuring the joint torque of the joint J. Alternatively, signals representative of currents applied to the joint motors 21 may be analyzed by a controller to measure the joint torques. The base 16 of the manipulator 14 is generally a portion of the manipulator 14 that is stationary during usage thereby providing a fixed reference coordinate system i. e. , a virtual zero pose for other components of the manipulator 14 or the system 10 in general. Generally, the origin of the manipulator coordinate system MNPL is defined at the fixed reference of the base 16. The base 16 may be defined with respect to any suitable portion of the manipulator 14, such as one or more of the links 18. Alternatively, or additionally, the base 16 may be defined with respect to the manipulator cart 17, such as where the manipulator 14 is physically attached to the cart 17. In one example, the base 16 is defined at an intersection of the axes of joints J1 and J2. Thus, although joints J1 and J2 are moving components in reality, the intersection of the axes of joints J1 and J2 is nevertheless a virtual fixed reference point, which does not move in the manipulator coordinate system MNPL. A tool 20 couples to the manipulator 14 and is movable by the manipulator 14. Specifically, the manipulator 14 moves one or more of the joints J1-J6 of the links 18 to move the tool 20 relative to the base 16. The tool 20 is or forms part of an end effector 22. The tool 20 interacts with the anatomy in certain modes and may be grasped by the operator in certain modes. One exemplary arrangement of the manipulator 14 and the tool 20 is described in 9,119,655, entitled, Surgical Manipulator Capable of Controlling a Surgical Instrument in Multiple Modes, the disclosure of which is hereby incorporated by reference. The tool 20 can be like that shown in Patent Application Publication 2014/0276949, filed on Mar. 15, 2014, entitled, End Effector of a Surgical Robotic Manipulator, hereby incorporated by reference. The tool 20 may be a surgical configured to manipulate the anatomy of the patient, or may be any other type of tool surgical or non-surgical employed by the manipulator 14. The manipulator 14 and the tool 20 may be arranged in configurations other than those specifically described herein. In one example, the tool 20 includes an energy applicator 24 designed to contact the tissue of the patient 12 at the surgical site. The energy applicator 24 may be a drill, a saw blade, a bur, an ultrasonic vibrating tip, or the like. The tool 20 may comprise a tool center point TCP, which in one example, is a predetermined reference point defined at the energy applicator 24. The TCP has known position in its own coordinate system. In one example, the TCP is assumed to be located at the center of a spherical feature of the tool 20 such that only one point is tracked. The TCP may relate to a bur having a specified diameter. In other examples, the tool 20 may be a probe, cutting guide, guide tube, or other type of guide member for guiding a hand-held tool with respect to the anatomy. Referring to 2, the system 10 includes a controller 30 coupled to the manipulator 14. The controller 30 includes software and/or hardware for controlling the manipulator 14 for moving the tool 20. The controller 30 directs the motion of the manipulator 14 and controls a state position and/or orientation of the tool 20 with respect to a coordinate system. In one example, the coordinate system is the manipulator coordinate system MNPL, as shown in 1. The manipulator coordinate system MNPL has an origin located at any suitable pose with respect to the manipulator 14. Axes of the manipulator coordinate system MNPL may be arbitrarily chosen as well. Generally, the origin of the manipulator coordinate system MNPL is defined at the fixed reference point of the base 16. One example of the manipulator coordinate system MNPL is described in 9,119,655, entitled, Surgical Manipulator Capable of Controlling a Surgical Instrument in Multiple Modes, the disclosure of which is hereby incorporated by reference. As shown in 1, the system 10 may further include a navigation system 32. The navigation system 32 is configured to track movement of various objects. Such objects include, for example, the tool 20 and the anatomy, , femur F and tibia T. The navigation system 32 tracks these objects to gather state information of each object with respect to a navigation localizer coordinate system LCLZ. Coordinates in the localizer coordinate system LCLZ may be transformed to the manipulator coordinate system MNPL, and/or vice-versa, using transformation techniques described herein. One example of the navigation system 32 is described in 9,008,757, filed on Sep. 24, 2013, entitled, Navigation System Including Optical and Non-Optical Sensors, hereby incorporated by reference. The navigation system 32 includes a cart assembly 34 that houses a navigation computer 36, and/or other types of control units. A navigation interface is in operative communication with the navigation computer 36. The navigation interface includes one or more displays 38. The navigation system 32 is capable of displaying a graphical representation of the relative states of the tracked objects to the operator using the one or more displays 38. An input device 40 may be used to input information into the navigation computer 36 or otherwise to select/control certain aspects of the navigation computer 36. As shown in 1, the input device 40 includes an interactive touchscreen display. However, the input device 40 may include any one or more of a keyboard, a mouse, a microphone voice-activation, gesture control devices, and the like. The manipulator 14 and/or manipulator cart 17 house a manipulator computer 26, or other type of control unit. The controller 30 may be implemented on any suitable device or devices in the system 10, including, but not limited to, the manipulator computer 26, the navigation computer 36, and any combination thereof. The navigation system 32 may also include a navigation localizer 44 hereinafter localizer that communicates with the navigation computer 36. In one example, the localizer 44 is an optical localizer and includes a camera unit 46. The camera unit 46 has an outer casing 48 that houses one or more optical sensors 50. In the illustrated example of 1, the navigation system 32 includes one or more trackers. In one example, the trackers include a pointer tracker PT, a tool tracker 52, a first patient tracker 54, and a second patient tracker 56. In 1, the tool tracker 52 is firmly attached to the tool 20, the first patient tracker 54 is firmly affixed to the femur F of the patient 12, and the second patient tracker 56 is firmly affixed to the tibia T of the patient 12. In this example, the patient trackers 54, 56 are attached to sections of bone. The pointer tracker PT is attached to a pointer P used for registering the anatomy to the localizer coordinate system LCLZ. Those skilled in the art appreciate that the trackers 52, 54, 56, PT may be fixed to their respective components in any suitable manner. Additionally, the navigation system 32 may include trackers for other components of the system, including, but not limited to, the base 16 tracker 52B, the cart 17, and any one or more links 18 of the manipulator 14. Any one or more of the trackers may include active markers 58. The active markers 58 may include light emitting diodes LEDs. Alternatively, the trackers 52, 54, 56 may have passive markers, such as reflectors, which reflect light emitted from the camera unit 46. Other suitable markers not specifically described herein may be utilized. The localizer 44 tracks the trackers 52, 54, 56 to determine a state of each of the trackers 52, 54, 56, which correspond respectively to the state of the tool 20, the femur F and the tibia T. The localizer 44 provides the state of the trackers 52, 54, 56 to the navigation computer 36. In one example, the navigation computer 36 determines and communicates the state the trackers 52, 54, 56 to the manipulator computer 26. As used herein, the state of an object includes, but is not limited to, data that defines the position and/or orientation of the tracked object or equivalents/derivatives of the position and/or orientation. For example, the state may be a pose of the object, and may include linear data, and/or angular velocity data, and the like. Although one example of the navigation system 32 is shown in the Figures, the navigation system 32 may have any other suitable configuration for tracking the tool 20 and the patient 12. In one example, the navigation system 32 and/or localizer 44 are ultrasound-based. For example, the navigation system 32 may comprise an ultrasound imaging device coupled to the navigation computer 36. The ultrasound imaging device images any of the aforementioned objects, , the tool 20 and the patient 12 and generates state signals to the controller 30 based on the ultrasound images. The ultrasound images may be 2-D, 3-D, or a combination of both. The navigation computer 36 may process the images in near real-time to determine states of the objects. The ultrasound imaging device may have any suitable configuration and may be different than the camera unit 46 as shown in 1. In another example, the navigation system 32 and/or localizer 44 are radio frequency RF based. For example, the navigation system 32 may comprise an RF transceiver in communication with the navigation computer 36. Any of the tool 20 and the patient 12 may comprise RF emitters or transponders attached thereto. The RF emitters or transponders may be passive or actively energized. The RF transceiver transmits an RF tracking signal and generates state signals to the controller 30 based on RF signals received from the RF emitters. The navigation computer 36 and/or the controller 30 may analyze the received RF signals to associate relative states thereto. The RF signals may be of any suitable frequency. The RF transceiver may be positioned at any suitable location to track the objects using RF signals effectively. Furthermore, the RF emitters or transponders may have any suitable structural configuration that may be much different than the trackers 52, 54, 56 as shown in 1. In yet another example, the navigation system 32 and/or localizer 44 are electromagnetically based. For example, the navigation system 32 may comprise an EM transceiver coupled to the navigation computer 36. The tool 20 and the patient 12 may comprise EM components attached thereto, such as any suitable magnetic tracker, electro-magnetic tracker, inductive tracker, or the like. The trackers may be passive or actively energized. The EM transceiver generates an EM field and generates state signals to the controller 30 based upon EM signals received from the trackers. The navigation computer 36 and/or the controller 30 may analyze the received EM signals to associate relative states thereto. Again, such navigation system 32 examples may have structural configurations that are different than the navigation system 32 configuration as shown throughout the Figures. Those skilled in the art appreciate that the navigation system 32 and/or localizer 44 may have any other suitable components or structure not specifically recited herein. Furthermore, any of the techniques, methods, and/or components described above with respect to the camera-based navigation system 32 shown throughout the Figures may be implemented or provided for any of the other examples of the navigation system 32 described herein. For example, the navigation system 32 may utilize solely inertial tracking or any combination of tracking techniques. Examples of software modules of the controller 30 are shown in 2. The software modules may be part of a computer program or programs that operate on the manipulator computer 26, navigation computer 36, or a combination thereof, to process data to assist with control of the system 10. The software modules include instructions stored in memory on the manipulator computer 26, navigation computer 36, or a combination thereof, to be executed by one or more processors of the computers 26, 36. Additionally, software modules for prompting and/or communicating with the operator may form part of the program or programs and may include instructions stored in memory on the manipulator computer 26, navigation computer 36, or a combination thereof. The operator interacts with the first and second input devices 40, 42 and the one or more displays 38 to communicate with the software modules. The user interface software may run on a separate device from the manipulator computer 26 and navigation computer 36. As shown in 1 and 2, the controller 30 includes a manipulator controller 60 configured to process data to direct motion of the manipulator 14. In one example, as shown in 1, the manipulator controller 60 is implemented on the manipulator computer 26. The manipulator controller 60 may receive and process data from a single source or multiple sources. The controller 30 may further include a navigation controller 62 for communicating the state data relating to the femur F, tibia T, and/or tool 20 to the manipulator controller 60. The manipulator controller 60 receives and processes the state data provided by the navigation controller 62 to direct movement of the manipulator 14. In one example, as shown in 1, the navigation controller 62 is implemented on the navigation computer 36. The manipulator controller 60 and/or navigation controller 62 may also communicate states of the patient 12 and/or tool 20 to the operator by displaying an image of the femur F and/or tibia T and the tool 20 on the one or more displays 38. The manipulator computer 26 or navigation computer 36 may also command display of instructions or request information using the display 38 to interact with the operator and for directing the manipulator 14. As shown in 2, the controller 30 includes a boundary generator 66. The boundary generator 66 is a software module that may be implemented on the manipulator controller 60. Alternatively, the boundary generator 66 may be implemented on other components, such as the navigation controller 62. The boundary generator 66 generates one or more virtual boundaries 55 for constraining the tool 20, as shown in 4. In situations where the tool 20 interacts with a target site of the anatomy, the virtual boundaries 55 may be associated with the target site, as shown in 4. The virtual boundaries 55 may be defined with respect to a 3-D bone model registered to actual anatomy such that the virtual boundaries 55 are fixed relative to the bone model. In this situation, the virtual boundaries 55 delineate tissue that should be removed from tissue that should not be removed. In some instances, the state of the tool 20 may be tracked relative to the virtual boundaries 55 using the navigation system 32 which tracks the states of the tool 20 , using tool tracker 52 and states of the anatomy , using patient trackers 54, 56. In one example, the state of the TCP of the tool 20 is measured relative to the virtual boundaries 55 for purposes of determining when and where reactive forces should be applied to the manipulator 14, or more specifically, the tool 20. Additional detail regarding the virtual boundaries 55 and such reactive forces are described below. One exemplary system and method for generating virtual boundaries 55 relative to the anatomy and controlling the manipulator 14 in relation to such virtual boundaries 55 is explained in Provisional Patent Application 62/435,254, filed on Dec. 16, 2016 and entitled, Techniques for Modifying Tool Operation in a Surgical Robotic System Based on Comparing Actual and Commanded states of the Tool Relative to a Surgical Site, the disclosure of which is hereby incorporated by reference. In another example, the navigation system 32 is configured to track states of an object to be avoided by the tool 20 and the virtual boundary 55 is associated with the object to be avoided. The object to be avoided may be any object in the sterile field with which the tool 20 may inadvertently interact. Such objects include, but are not limited to, parts of the patient other than the surgical site, surgical personnel, leg holders, suction/irrigation tools, patient trackers 54, 56, retractors, other manipulators 14, lighting equipment, or the like. One exemplary system and method for generating virtual boundaries 55 relative to objects to be avoided and controlling the manipulator 14 in relation to such virtual boundaries 55 is explained in Patent Application Publication 2014/0276943, filed on Mar. 12, 2014 and entitled, Systems and Methods for Establishing Virtual Constraint Boundaries, the disclosure of which is hereby incorporated by reference. The controller 30, and more specifically, the manipulator controller 60, may execute another software module providing a tool path generator 68, as shown in 2. The tool path generator 68 generates a path for the tool 20 to traverse, such as for removing sections of the anatomy to receive an implant. One exemplary system and method for generating the tool path is explained in 9,119,655, entitled, Surgical Manipulator Capable of Controlling a Surgical Instrument in Multiple Modes, the disclosure of which is hereby incorporated by reference. In some examples, the virtual boundaries 55 and/or tool paths may be generated offline rather than on the manipulator computer 26 or navigation computer 36. Thereafter, the virtual boundaries 55 and/or tool paths may be utilized at runtime by the manipulator controller 60. As shown in 1, a sensor 70, such as a force-torque sensor, may be coupled to the manipulator 14. Specifically, the force-torque sensor 70 may be mounted between the distal link 18 and the tool 20. The force-torque sensor 70 is configured to output variable signals as a function of forced and/or torques to which the tool 20 is exposed as the operator grasps the tool 20. By doing so, the force-torque sensor 70 allows sensing of input forces and/or torques applied to the tool 20. As will be described below, the input force and/or torque is utilized to control movement of the manipulator 14. In one example, the force-torque sensor 70 is a 6DOF sensor such that the force-torque sensor 70 is configured to output signals representative of three mutually orthogonal forces and three torques about the axes of the orthogonal forces that are applied to the tool 20. Additionally or alternatively, the input force and/or torque applied to the tool 20 may be determined using joint torques, as is described in detail below. As shown in 3, the controller 30 is in communication with the joint motors 21 for commanding movement and position of the links 18. The controller 30 is further coupled to the position sensors , encoders 19 and is configured to measure an actual joint angle of each respective joint J using signals received from the position sensors 19. The controller 30 commands the joint motors 21, such as through a joint motor sub-controller, to move to a commanded joint angle. The controller 30 may receive signals indicative of the measured joint torque of the joint J from the torque sensors 28 at the joint motors 21. The controller 30 further is coupled to the force-torque sensor 70 for receiving signals indicative of the input force and/or torque applied to the tool 20. II. Admittance Control and Virtual SimulationIn one example, the controller 30 is an admittance-type controller. In other words, the controller 30 determines control forces and/or torques and commands position of the manipulator 14. Examples of the control forces and/or torques are described below. In one example, the controller 30 includes solely a single admittance controller such that control forces and/or torques are determined and analyzed solely by the single controller 30 to determine the force. In other words, in this example, separate admittance controllers for different control forces and/or torques are not utilized. In other examples, additional controllers may be used. Using admittance control, the techniques described herein may, at times, give the impression that some of the joints J are passive, meaning that the joint J is moved directly by the force exerted by the user similar to a door joint. However, in the examples described the joints J are actively driven. The system 10 and method mimic passive behavior by actively driving the joints J and thereby commanding control of the manipulator 14 in response to determined control forces and/or torques. To execute force determinations with admittance control, the controller 30 is configured to implement a virtual simulation 72, as represented in 3. The controller 30 simulates dynamics of the tool 20 in the virtual simulation 72. In one example, the virtual simulation 72 is implemented using a physics engine, which is executable software stored in a non-transitory memory of any of the aforementioned computers 26, 36 and implemented by the controller 30. For the virtual simulation, the controller 30 models the tool 20 as a virtual rigid body as shown in 4, for example. The virtual rigid body is a dynamic object and a rigid body representation of the tool 20 for purposes of the virtual simulation 72. The virtual rigid body is free to move according to 6DOF in Cartesian task space according to the virtual simulation 72. Specific examples of the virtual rigid body are described in the subsequent section. The virtual simulation 72 and the virtual rigid body may be simulated and otherwise processed computationally without visual or graphical representations. Thus, it is not required that the virtual simulation 72 virtually display dynamics the virtual rigid body. In other words, the virtual rigid body need not be modeled within a graphics application executed on a processing unit. The virtual rigid body exists only for the virtual simulation 72. In some instances, simulated movement of a virtual tool, which is tracked to the actual tool 20, may be displayed at the surgical site to provide visual assistance during operation of the procedure. However, in such instances, the displayed tool is not directly a result of the virtual simulation 72. The tool 20 may be modeled as the virtual rigid body according to various methods. For example, the virtual rigid body may correspond to features, which may be on or within the tool 20. Additionally or alternatively, the virtual rigid body may be configured to extend, in part, beyond the tool 20. The virtual rigid body may take into account the end effector 22 as a whole including the tool 20 and the energy applicator 32 or may take into account the tool 20 without the energy applicator 32. Furthermore, the virtual rigid body may be based on the TCP. In yet another example, the virtual rigid body is based on a range of motion of the tool 20, rather than a static position of the tool 20. For example, the tool 20 may comprise sagittal saw blade that is configured to oscillate between two end points. The virtual rigid body may be statically defined to include the two end points and any appropriate space in between these two end points to account for the full range of motion of the tool 20 in relation to the virtual boundary 55. Similar modeling techniques for tools 20 that effect a range of motion may be utilized other than those described above for the sagittal saw. In one example, the virtual rigid body is generated about a center of mass of the tool 20. Here center of mass is understood to be the point around which the tool 20 would rotate if a force is applied to another point of the tool 20 and the tool 20 were otherwise unconstrained, i. e. , not constrained by the manipulator 14. The center of mass of the virtual rigid body may be close to, but need not be the same as, the actual center of mass of the tool 20. The center of mass of the virtual rigid body can be determined empirically. Once the tool 20 is attached to the manipulator 14, the position of the center of mass can be reset to accommodate the preferences of the individual practitioners. In other examples, the virtual rigid body may correspond to other features of the tool 20, such as the center of gravity, or the like. The controller 30 effectively simulates rigid body dynamics of the tool 20 by virtually applying control forces and/or torques to the virtual rigid body. As shown in 3, the control forces and/or torques applied to the virtual rigid body may be user applied as detected from the force/torque sensor 70 and/or based on other behavior and motion control forces and/or torques. These control forces and/or torques are applied, in part, to control joint J position and may be derived from various sources. One of the control forces and/or torques may be a reactive force Fr responsive to interaction of the tool 20 with the virtual boundaries produced by the boundary generator 68. Techniques for generating these reactive forces Fr are the primary subject of the subsequent section and are described in detail below. Additionally, control forces and/or torques may be applied to constrain movement of the tool 20 along the tool path provided from the path generator 68. These control forces and/or torques may be applied to constrain orientation of the tool 20 further within an acceptable range of orientations along the tool path. Backdrive control forces indicative of a disturbance along the tool path , based on external forces applied to the manipulator 14 also may be applied to the virtual rigid body. Control forces and/or torques may be applied to the virtual rigid body to overcome the force of gravity. Other control forces that may applied to the virtual rigid body include, but are not limited to forces to avoid joint limits, forces to avoid singularities between links 18 of the manipulator 14, forces to maintain the tool 20 within a workspace boundary of the manipulator 14, and the like. These various control forces and/or torques to apply to the virtual rigid body are detected and/or determined by the controller 30 and are inputted into a system of equations that the controller 30 solves in order to provide a kinematic solution satisfying the system of equations i. e. , satisfying the various control forces and/or torques and any applicable constraints. The controller 60 may be configured with any suitable algorithmic instructions , such as an iterative constraint solver to execute this computation. This operation is performed in the virtual simulation 72 in order to determine the next commanded position of the manipulator 14. The virtual simulation 72 simulates rigid body dynamics of the tool 20 before such dynamics of the tool 20 are physically performed during positioning of the manipulator 14. Understood differently, the virtual rigid body is in a first pose at commencement of each iteration of the virtual simulation 72. The controller 30 inputs the control forces and/or torques into the virtual simulation 72 and these control forces and/or torques are applied to the virtual rigid body in the virtual simulation 72 when the virtual rigid body is in the first pose. The virtual rigid body is moved to a subsequent pose having a different state i. e. , position and/or orientation within Cartesian space in response to the controller 30 satisfying the inputted control forces and/or torques. In one example, the virtual rigid body does not actually move during simulation. In other words, the control forces and/or torques are inputted into the system of equations and solved computationally. Each solved equation may indicate theoretical movement of the virtual rigid body according to the respective control forces for the equations. In other words, anticipated movement of the virtual rigid body in accordance with each applied control force and/or torque is taken into account. Additionally or alternatively, the virtual rigid body moves in the virtual simulation 72 during solving of the system of equations. In other words, the virtual rigid body is moved in accordance with the applied control forces and/or torques during the process of solving the system of equations. The virtual rigid body may move to the subsequent pose in the virtual simulation 72 after the system of equations are solved. However, even this subsequent pose may be represented strictly in a computational sense such that movement of the virtual rigid body from the first pose to the second pose does not occur. Knowing the subsequent pose of the virtual rigid body based on the virtual simulation 72, the controller 30 is configured to command action of the joints J in accordance with the virtual simulation 72. That is, the controller 30 converts the dynamics of the virtual rigid body in Cartesian space to direct motion of the manipulator 14 and to control the state of the tool 20 in joint space. For instance, forces resulting in the subsequent pose are applied to a Jacobian calculator, which calculates Jacobian matrices relating motion within Cartesian space to motion within joint space. In one example, the controller 30 is configured to determine the appropriate joint angles to command for the joints J based on the output of the virtual simulation 72. That is, the controller 30 computes the commanded joint angle for each of the joints J. From here, the controller 30 regulates the joint angle of each joint J and continually adjusts the torque that each joint motor 21 outputs to, as closely as possible, ensure that the joint motor 21 drives the associated joint J to the commanded joint angle. The controller 30 is configured to apply signals to each joint motor 21 so that each joint motor 21 drives the associated joint J to the commanded joint angle. The controller 30 may use any suitable position control algorithms for controlling positioning the joints J based on the commanded joint angles. The controller 30 may generate the commanded joint angle only for those joints J that are active, i. e. , expected to move based on the output of the virtual simulation 72. In some examples, as represented in 3, the controller 30 generates the commanded joint angle for each of the joints J separately and individually , per each active joint. For example, the joints J may be considered in succession such that the commanded joint angle for J1 is generated first, and the commanded joint angle for J6 is generated last, or vice-versa. III. Techniques for Computing Reactive Forces Based on Penetration Factors to Implement Virtual Boundaries. Referring now to 4-18, described herein are techniques for generating reactive forces Fr that are applied to the virtual rigid body in the virtual simulation 72 responsive to interaction of the tool 20 with the virtual boundaries 55, , in the manipulator coordinate system MNPL. In accordance with application of the reactive forces Fr to the virtual volume 74 in the virtual simulation 72, the controller 30 commands the manipulator 14 to move the tool 20 to constrain movement of the tool 20 relative to the virtual boundary 55. Details regarding configurations and functions of the virtual rigid body and the virtual boundaries 55, as well as techniques for computing the reactive forces Fr are also provided below. The methods for implementing these techniques are fully understood through any functional description of the elements described herein. To implement the techniques described herein for computing the reactive forces Fr, the virtual rigid body is defined as a virtual volume 74, as shown in 4. Thus, the virtual rigid body is a three-dimensionally modeled object, rather than a single point or 2D planar element. Features, functionality, and configurations of the virtual rigid body described in the previous section should be understood to apply to virtual volume 74 described in this section. As will be apparent based on the techniques described below, providing the virtual rigid body as the virtual volume 74 enables more precise dynamic interaction between the virtual volume 74 and the virtual boundaries 55, as compared with a two-dimensional or single dimensional rigid body. The virtual volume 74 may have various configurations. In one example, the virtual volume 74 comprises a single face, zero edges, and zero vertices. For instance, as shown in 4-18, the virtual volume 74 is a sphere. The virtual volume 74 may be other shapes having single face, zero edges, and zero vertices, such as a spheroid prolate or oblate, an ellipsoid, a toroid , a doughnut shape, or any combination thereof. By having the single face, zero edges, and zero vertices, the entire virtual volume 74 is provided with a smooth surface. As will be apparent based on the techniques described below, reactive forces Fr computed in response to interaction of the single face virtual volume 74 with the virtual boundary 55 are likely to provide responses that more accurately reflect interaction as compared with reactive forces Fr computed in response to interaction of the virtual boundary 55 by volumes having a greater number of faces. It is possible to implement the techniques described herein with the virtual volume 74 having more than one face. For instance, the virtual volume 74 may be a cone, a semi-sphere, or any of the aforementioned volumes i. e. , spheres, spheroids ellipsoids, toroids, wherein the virtual volume 74 has a high resolution of faces thereby mimicking the single-faced and smooth version of the respective volume. Other examples of the virtual volume 74 are contemplated in view of the teachings of the techniques provided herein. One example of the virtual boundary 55 is shown in 4. Of course, any number of virtual boundaries 55 may be utilized. The virtual boundaries 55 may be spaced apart and separated from one another or may be integrally connected to one another. The virtual boundaries 55 may be planar or may be defined by more complex shapes, such as polyhedrons, or the like. As shown in 4, the virtual boundaries 55 are defined by a mesh of polygonal elements 80. The mesh is formed of multiple polygonal elements 80 being disposed adjacent to one another and having adjacent vertices and edges aligned with one other. Each polygonal element 80 may be formed of any polygon having a plane figure with at least three straight sides and angles. Ideally, the polygon sides enable the mesh to be formed without any gaps between adjacent polygonal elements 80. In one example, as shown in 4, the polygonal elements 80 are triangles. The triangles may be any type, such as equilateral, scalene, isosceles, obtuse, oblique, and/or right. In other examples, the polygonal elements 80 may be quadrilaterals rectangles, squares, pentagons, hexagons, etc. Each virtual boundary 55 may comprise the mesh being formed of the same type of polygonal element 80. For instance, in 4, all the polygonal elements 80 shown are triangles. In other examples, one virtual boundary 55 may comprise the mesh being formed of one type of polygonal element 80 and another virtual boundary 55 may comprise the mesh being formed of another type of polygonal element 80. In yet another example, one virtual boundary 55 may comprise the same mesh being formed by more than one type of polygonal element 80. For instance, groups of each type of polygonal element 50 may be provided for different sections of the same mesh. It is to be appreciated that the virtual boundaries 55, meshes, and polygonal elements 80 may comprise configurations other than those described herein and shown in the Figures. As described, the virtual volume 74 may interact with, attempt to penetrate, or otherwise penetrate overshoot the virtual boundary 55 in accordance with the control forces and/or torques applied to the virtual volume 74 in the virtual simulation 72. When the virtual volume 74 pushes on the virtual boundary 55, the virtual boundary 55 pushes back due to applied compressional impact of the virtual volume 74 and/or the virtual boundary 55. For simplicity, the impact force applied against the virtual boundary 55 by the virtual volume 74 or vice-versa is shown in the Figures as Fa. To offset this impact force Fa, the controller 30 generates the reactive force Fr to apply to the virtual volume 74, which opposes compression. Thus, the reactive force Fr is a component of the system of equations that the controller 30 attempts to satisfy in the virtual simulation 72. Thus, it should be understood that the virtual volume 74 may undergo multiple other control forces and/or torques during the virtual simulation 72 other than the reactive force Fr. As described, interaction between the virtual volume 74 and the virtual boundary 55 may exist only in a computational sense rather than a graphical sense, by providing parameters and variables of this interaction in the system of equations for solution. The controller 30 is configured to compute the reactive force Fr specifically in response to penetration of one or more of the polygonal elements 80 by the virtual volume 74 in the virtual simulation 72. The reactive force Fr is computed based on a penetration factor being a function of a geometry of the virtual volume 74 bound relative to a geometry of the polygonal element 80. As will be apparent from the examples below, the geometry of the virtual volume 74 may be two-dimensional or three-dimensional. The geometry of the virtual volume 74 is bound by the polygonal element 80. For example, the geometry of the virtual volume 74 is bound by a perimeter of the polygonal element 80. In other words, the geometry of the virtual volume 74 for a single polygonal element 80 is considered in as much as the geometry of the virtual volume 74 exists within the perimeter of the polygonal element 80. The various examples of computing penetration factors are described in detail below. These examples may be utilized individually or in combination. A. Projected AreaIn accordance with one example, as shown in 5-15, the controller 30 is configured to compute the reactive force Fr based on the penetration factor being related to a projected area 90 defined by intersection of the virtual volume 74 with the polygonal element 80. In this example, the term projected is a mathematical expression indicating that the area 90 defined by intersection of the virtual volume 74 with the polygonal element 80 is mapped relative to the planar surface of the polygonal element 80. The projected area 90 is also labeled in the Figures as Aproj. Furthermore, throughout the Figures, the projected area 90 is shown by a shaded region. The projected area 90 is bound by the polygonal element 80. Specifically, the projected area 90 is bound by a perimeter of the polygonal element 80. In other words, the projected area 90 for a single polygonal element 80 is considered in as much as the projected area 90 exists within the perimeter of the polygonal element 80. Examples where multiple polygonal elements 80 are penetrated by the virtual volume 74 are described below. As will be apparent from the description and the Figures, the virtual volume 74 is defined such that the projected area 90 changes non-linearly relative to a linear depth of penetration i. e. , the distance by which the virtual volume 74 protrudes into the polygonal element 80 and/or virtual boundary 55 by the virtual volume 74. Although the reactive force Fr may change with changes in the linear depth of penetration, the reactive force Fr is computed based on the projected area 90 and without computationally accounting for the linear depth of penetration. As described, the reactive force Fr is related to a projected area 90. In one example, the reactive force Fr is directly correlated with the projected area 90. Additionally or alternatively, the reactive force Fr is proportional to the projected area 90. In one example, the reactive force Fr is modeled as a spring with a constant of k. The spring constant k is multiplied by Aproj such that FR=k Aproj. The spring constant k may have any suitable value depending on design configurations reflecting how strongly to oppose penetration of the virtual boundary 55 by the tool 20. In one example, the reactive force Fr is applied as a vector being normal to a plane of the polygonal element 80. The location of the vector with respect to the plane of the polygonal element 80 may vary depending on the location of projected area 90 mapped on to the polygonal element 80. The magnitude of the reactive force Fr may vary depending on the size of the projected area 90. The reactive force Fr vector may be at an angle that is not normal with respect to the plane of the polygonal element 80 depending on the projected area 90 and/or the pose of the virtual volume 74 during penetration. Techniques for computing the reactive force Fr from the projected area 90 are described below. The controller 30 is configured to apply the reactive force Fr to the virtual volume 74 in the virtual simulation 72 to reduce penetration of the polygonal element 80 by the virtual volume 74. Thus, the reactive force Fr is configured to offset the impact force Fa partially or completely. It should be appreciated that the reactive force Fr may be applied directly to the virtual volume 74 and/or may be applied to the virtual boundary 55 itself. In either instance, application of the reactive force Fr to the virtual volume 74 causes acceleration and a change in the velocity and hence the pose of the virtual volume 74 for as long as the virtual volume 74 acts upon the virtual boundary 55. Because the magnitude of impact between the virtual volume 74 and the virtual boundary 55 is likely to occur variably over time, the controller 30 may be configured to generate impulses for minimizing the impact force Fa. Impulses may be generated iteratively to compute the reactive forces Fr applied to the virtual volume 74. The impulse is the integral of the reactive force Fr over the time. The impulse may be perceived as the effect of the momentary increase of the reactive forces Fr. Referring now to 5-7, examples are shown illustrating the projected area 90 in a situation where the virtual volume 74 , a sphere penetrates one polygonal element 80 , a triangle in accordance with the impact force Fa. For simplicity in illustration, a convention is used in the Figures wherein a length of a force arrow is representative of a magnitude of the force. Thus, greater magnitudes of the force are represented by longer arrows and lesser the magnitudes of the force are represented by shorter arrows. It is to be appreciated that arrow lengths are illustrative and are not intended to represent direct mathematical correlation between the projected area 90 and the corresponding reactive force Fr. It should be understood that for simplicity, the figures illustrate three separate examples and do not represent gradual penetration of the virtual boundary 55 by the virtual volume 74 over time. Mainly, for each example, the respective reactive force Fr is shown to offset the respective impact force Fa fully, thereby eliminating penetration by the virtual boundary 55. Of course, gradual penetration by the virtual volume 74 is likely to occur and one skilled in the art should appreciate that the techniques are fully capable of iteratively applying the reactive force Fr to the virtual volume 74 for various iterations of impact force Fa. For subsequent changes of state of the tool 20 relative to the virtual boundary 55, the controller 30 is configured to iteratively compute the reactive force Fr, iteratively apply the reactive force Fr, and iteratively command the manipulator 14 to move the tool 20 in accordance with application of the reactive force Fr to the virtual volume 74 in the virtual simulation 72. For instance, the reactive force Fr may only partially displace the virtual volume 74 relative to the virtual boundary 55 such that the virtual volume 74 continues to intersect the virtual boundary 55. In such situations, the subsequent state of the tool 20 after such partial displacement is tracked and the virtual volume 74 pose is updated in the virtual simulation 72. The update pose of the virtual volume 74 may cause a different , lesser intersection with the virtual boundary 55, and hence, a lesser projected area 50 and ultimately, a subsequent reactive force Fr of lesser magnitude. In turn, the tool 20 may be partially displaced further from the virtual boundary 55. This process may be repeated iteratively until penetration by the virtual boundary 55 is completely rescinded or until a threshold is reached. For the specific examples, the polygonal element 80 is shown below the virtual volume 74 and hence terms of orientation such as upper or lower may be used to describe this orientation. Such terms of orientation are described relative to the subject examples and are not intended to limit the scope of the subject matter. It is to be appreciated that other orientations are possible, such as the virtual volume 74 approaching the polygonal element 80 from below or from the side. Additionally, the projected area 90 in 5-7 is based on a circle because the virtual volume 74 in these examples is a sphere. Of course, depending on the configuration, shape, and/or pose of the virtual volume 74, the intersection, and hence, the projected area 90 may be a size or shape other than shown in the Figures. Furthermore, the projected area 90 in 5-7 is shown in the center of the polygonal element 80 for simplicity, and based on the assumption that the virtual volume 74 has penetrated the geometrical center of the area of the polygonal element 80. However, depending on the configuration, shape, and/or pose of the virtual volume 74, the projected area 90 may be offset from the center of the polygonal element 80. Referring now to 5, the left-most illustration shows a side view of the virtual volume 74 and polygonal element 80. In accordance with the impact force Fa, the lower-most tip of the virtual volume 74 slightly penetrates the polygonal element 80. The projected area 90 shown in the middle figure of 5 represents the intersection of the virtual volume 74 and the polygonal element 80, and is a circle. A cross-section 100 of the virtual volume 74 at the plane of intersection coincides with the projected area 90 in this example. The projected area 90 in 5 is small relative to the area of the polygonal element 80 because only the lower-most tip of the virtual volume 74 is penetrating. In the right-most illustration in 5, the reactive force Fr computed based on the projected area 90 is shown. The reactive force Fr is shown with an arrow applied to the virtual volume 74 in a direction normal to the plane of the polygonal element 80 and opposing the impact force Fa. In 5, the arrow for the reactive force Fr is sized to reflect the relative projected area 90. In 6, the virtual volume 74 more deeply penetrates the polygonal element 80, and hence, a greater intersection between the virtual volume 74 and the polygonal element 80 exists. Thus, the projected area 90 in 6 middle illustration is greater than the projected area 90 in 5. The projected area 90 in 6 is based on a circle i. e. , from the intersection of the sphere, but is not circular. Instead, the projected area 90 is bound by the perimeter of the polygonal element 80 and therefore, is considered in as much as the projected area 90 is bound within the perimeter of the polygonal element 80. The cross-sectional area 100 of the virtual volume 74 at the plane of intersection extends beyond the bounds of the polygonal element 80, and as such, unbound regions exist at 92 in 6. These unbound regions 92 are not considered in the computation of the reactive force Fr for the impacted polygonal element 80. In the right-most illustration in 6, the computed reactive force Fr is shown having an arrow sized greater than the arrow for the reactive force Fr in 5 because the projected area 90 in 6 is greater than the projected area 90 in 5. In 7, an even greater penetration of the polygonal element 80 by the virtual volume 74 is shown. Specifically, one-half of the spherical virtual volume 74 penetrates the polygonal element 80. As expected, the projected area 90 in 7 middle illustration is greater than the projected area 90 in 6. Once again, unbound regions 92 are present and are disregarded in the computation of the reactive force Fr. As expected with deeper penetration, the area of the unbound regions 92 is also greater than the area of the unbound regions 92 in 6. In the right-most illustration in 7, the computed reactive force Fr is shown having an arrow sized greater than the arrow for the reactive force Fr in 6. Those skilled in the art appreciate that measures may be taken to account for situations where penetration is so deep that the projected area 90 actually decreases because of the shape of the virtual volume 74. For instance, this may occur in 7 when more than one-half of the spherical virtual volume 74 penetrates the polygonal element 80. Such measures may include computing and combing more than one projected area 90 for any given penetration for any single polygonal element 80 and/or taking into account the displaced volume of the virtual boundary 55. As should be apparent based on 5-7, the projected area 90 varies with respect to the linear depth of penetration. However, the projected area 90 does not change linearly with respect to linear depth of penetration because the penetrating body is volumetric and does not apply a linear impact force Fa to the polygonal element 80 and/or virtual boundary 55. Instead, the penetrating body applies a higher order impact force Fa as a function of the volumetric shape of the virtual volume 74. Accordingly, the projected area 90 changes with respect to linear depth of penetration according to this higher order volumetric function. Said differently, the projected area 90 accounts for the displaced volume or penetrating volume of the virtual volume 74. This variable response occurs in part because the virtual volume 74 in the examples shown has only one face , is spherical and does not have identical cross-sectional areas adjacent to one another. For instance, the projected area 90 would have been identical for 5-7 had the virtual volume 74 been a cube having a lower face penetrating the polygonal element 80 with flat sides coinciding. Hence, the reactive forces Fr for each example would have been the same despite the relative differences in linear depth of penetration warranting different reactive forces Fr according to the techniques described herein. Thus, reactive forces Fr computed in response to penetration by the virtual volume 74 are variably responsive to the linear depth of penetration. However, even though the reactive force Fr may change with changes in the linear depth of penetration, the reactive force Fr is indeed computed based on the projected area 90. The reactive force Fr is not computed simply based on the linear depth by which the virtual volume 74 protrudes into the polygonal element 80 and/or virtual boundary 55. Examples of techniques for computing the projected area 90 are shown in 8 and 9. As described, the projected area 90 for a given polygonal element 80 is illustrated by the shaded region within the polygonal element 80 and excludes any unbound regions 92 extending beyond the polygonal element 80. In 8, calculations of the projected area 90 specifically for a triangular polygonal element 80 are shown. In 8, the projected area 90 is derived from intersection with a spherical virtual volume 74 and thus is circular-based. A circle is shown in 8 and represents the cross-sectional area 100 of the virtual volume 74 at the plane of intersection with the triangular polygonal element 80. The circle is off-center from the geometrical center of the triangular polygonal element 80 and intersects a lower edge of the polygonal element 80 at intersection points 94 and 96. In turn, the cross-sectional area 100 is cut-off by the lower edge, resulting in the unbound region 92, as shown. Center point c is the center of the cross-sectional area 100 of the virtual volume 74 and radius r is the radius of the cross-sectional area 100. In this example, the projected area 90 is computed by determining an overlap between the circle and the polygonal element 80. Specifically, in this situation, wherein intersection of the circle occurs with respect to only one edge and no vertices of the triangular polygonal element 80, the projected area 90 is computed by breaking down the overlap into a triangular area Atri and a circular sector area Asector. The triangular area Atri is defined within three points, i. e. , center point c, and intersection points 94, 96. A base b of the triangular area Atri is defined between the intersection points 94, 96 and a height h of the triangular area Atri is defined between center point c and the base b. The triangular area Atri is computed using the equation Atri= bh. The circular sector area Asector is based on the sector angle , which is defined about the center point c and between two legs of the triangular area Atri defined respectively between the center point c and each intersection point 92, 94. The circular sector area Asector is computed using the equation Asector=r2*/360 degrees. Notably, in this example, the center point c of the circle is located within the polygonal element 80. Hence, the circular sector area Asector is also located within the polygonal element 80. With the triangular area Atri and the circular sector area Asector occupying the entire shaded region, as shown in 8, the projected area Aproj is computed by adding Atri and Asector. 9 is yet another example illustrating computation of the projected area Aproj based on triangular polygonal element 80 and the circular cross-section 100 of the virtual volume 74. In this situation, intersection of the circular cross-section 100 once again occurs with respect to only one edge and no vertices of the triangular polygonal element 80. However, in this example, the center point c of the circular cross-section 100 is located outside of the polygonal element 80. As a result, the entire triangular area Atri and a portion of the circular sector area Asector are also located outside the polygonal element 80. Thus, unlike the projected area 90 of 8, which was computed by adding together the triangular area Atri and the circular sector area Asector, the projected area 90 of 9 is computed by subtracting the triangular area Atri from the circular sector area Asector. The projected area 90 of 9 is lesser than the projected area 90 of 8. It should be reiterated that the calculations with respect to 8 and 9 are specific not only to a triangular polygonal element 80 and a circular cross-section 100 of the virtual volume 74, but further specific to intersection of the circle with respect to only one edge of the triangular polygonal element 80 and no vertices of the triangular polygonal element 80. Of course, the projected area 90 and hence the calculations for computing the projected area 90 may be different depending on the geometry of the polygonal element 80, the geometry of the virtual volume 74, and the relative locations of these geometries to each other. Furthermore, for the examples shown, computations of the projected area 90 will differ depending on how many edges from 0 up to 3 and vertices from 0 up to 3 of the triangular polygonal element 80 are intersected by the circular cross-section 100 of the virtual volume 74. Geometric computation of the projected area 90 is contemplated for any geometric configuration and situation other than those described herein. Referring to 10-12, examples are shown wherein multiple reactive forces , FrA-FrF are generated in response to simultaneous penetration of multiple polygonal elements , 80A-80F by the virtual volume 74 in the virtual simulation 72. For simplicity, the examples in 10-12 continue to reference triangular polygonal elements 80 and a circular cross-section 100 of the virtual volume 74. In 10, the spherical virtual volume 74 penetrates the virtual boundary 55 comprised of polygonal elements 80A-80F, which for this example are assumed to be on the same plane. Of course, the mesh of the virtual boundary 55 may comprise more polygonal elements other than polygonal elements 80A-80F and the various polygonal elements 80 may be adjacently disposed next to each other at angles such that they are non-planar. Unlike 5-9, wherein intersection with only one polygonal element 80 was shown and described, in 10, the circular cross-section 100 of the virtual volume 74 intersects each of the polygonal elements 80A-80F at a center vertex shared among polygonal elements 80A-80F. Thus, the projected areas 90A-90F mapped onto each respective polygonal element 80A-80F are identical. Identical projected areas 90A-90F are described for simplicity in this example, and it should be understood that a different shape or absence of the projected area 90 for each polygonal element 80A-80F is possible. Reactive forces FrA-FrF are computed for respective polygonal elements 80A-80F. Each reactive force FrA-FrF is related to the respective projected area 90A-90F defined by intersection of the virtual volume 74 with each polygonal element 80A-80F. Again, each projected area 90A-90F is bound by each polygonal element 80A-80F. Thus, in this example, the reactive forces FrA-FrF are also identical. In 10, the controller 30 is configured to apply the multiple reactive forces FrA-FrF simultaneously to the virtual volume 74 to offset penetration of the virtual boundary 55 by the virtual volume 74. In the specific example of 10, each of the multiple reactive forces FrA-FrF is applied individually to the virtual volume 74. For instance, each polygonal element 80 may individually react to penetration by the virtual volume 74. The reactive forces FrA-FrF are applied according to positions corresponding to the respective polygonal element 80A-80F, and hence, may differ slightly from the positions shown in 10, which are limited by the side-view. The situation in 11 is similar to the situation in 10 except that the controller 30 combines the multiple reactive forces FrA-FrF to generate a combined reactive force FrTotal. The controller 30 is configured to apply the single combined reactive force FrTotal, at one time, to the virtual volume 74 to offset penetration of the virtual boundary 55 by the virtual volume 74. A magnitude of the combined reactive force FrTotal may be computed by summing respective magnitudes of the multiple reactive forces FrA-FrF. In this case, the magnitude of the combined reactive force FrTotal is six times the respective magnitude of any one of the reactive forces FrA-FrF. Location of the combined reactive force FrTotal may be computed by averaging or finding a center of the respective locations of the multiple reactive forces FrA-FrF. In this case, the location of the combined reactive force FrTotal is at the center vertex shared among polygonal elements 80A-80F. Since the assumption in this example is that the polygonal elements 80A-80F are located in the same plane, the direction of the combined reactive force FrTotal is normal to the plane and opposing the impact force Fa. The magnitude, location, and direction of the combined reactive force FrTotal may be computed according to methods other than those described herein. In some examples, the controller 30 can apply a weighting factor to each of the reactive forces Fr such that the combined reactive force is constant for any given simultaneous penetration of multiple polygonal elements 80 by the virtual volume 74. In other words, the controller 30 can utilize affine combination algorithms to manipulate the weighting factors such that the sum of these weighting factors is constant. For example, weighting factors may be defined to sum to one when the virtual boundary 55 is a flat surface. The weighting factors may be defined to sum to a number greater than one when two virtual boundaries 55 are close to perpendicular or perpendicular. The weighting factors may be defined to sum to a number less than one at an edge of the virtual boundary. This technique helps to provide a predictable and smooth reactive response to penetration of the virtual volume 55 for these given scenarios such that the user is provided with a natural reactive response. In other words, the user does not experience unexpected increases or decreases in force. In 12, the spherical virtual volume 74 penetrates the virtual boundary 55 comprised of polygonal elements 80A-80D. Unlike 10 and 11, the circular cross-section 100 of the virtual volume 74 in 12 does not equally intersect each of the polygonal elements 80A-80D. Instead, the projected area 90A mapped onto polygonal element 80A is greater than the projected areas 90B-90D mapped onto polygonal elements 80B-80D. Projected areas 90B-90D are identical. Reactive forces FrA-FrF are computed for each polygonal element 80A-80F. The reactive force FrA related to projected area 90A is greater than each of the reactive forces FrB-FrD related to projected areas 90B-90D because projected area 90A is greater than projected areas 90B-90D. Again, the reactive forces FrA-FrD are applied according to positions corresponding to the respective polygonal element 80A-80D, and hence, may differ slightly from the positions shown in 12, which are limited by the side-view. 12 further exemplifies the dynamic response of the projected area techniques described herein because the linear depth of penetration is the same for each of the polygonal elements 80A-80D. However, since the projected area 90A for polygonal element 80A is greater than the respective projected areas 90B-90D for polygonal elements 80B-80D, the reactive force FrA related to projected area 90A is greater than each of the other reactive forces FrB-FrD and accurately accounts for the penetration impact provided by this specific virtual volume 74. Thus, this example reiterates that the projected area 90, and consequently, the reactive force Fr do not change linearly with respect to linear depth of penetration. The previous examples have described situations in which the polygonal elements 80 are located in the same plane. The techniques described herein are equally applicable to situations wherein the polygonal elements 80 are located in different planes. One example of this situation is where the virtual volume 74 encounters a corner defined by the polygonal elements 80. For example, 13-15 illustrate use of the projected area 90 in a situation where the virtual volume 74 , a sphere penetrates two polygonal elements 80A, 80B, , squares in accordance with the impact force Fa. Specifically, the two polygonal elements 80A, 80B form an outside corner of the virtual boundary 55. It should be understood that for simplicity 13-15, as shown, illustrate three separate examples and do not represent gradual penetration of the virtual boundary 55 by the virtual volume 74 over time. Mainly, for each example, the respective reactive force Fr is shown to offset the respective impact force Fa fully, thereby eliminating penetration by the virtual boundary 55. Of course, gradual penetration by the virtual volume 74 is likely to occur and one skilled in the art should appreciate that the techniques are fully capable of iteratively applying the reactive force Fr to the virtual volume 74 for various iterations of impact force Fa. Referring now to 13, a side view of the virtual volume 74 and polygonal elements 80A, 80B are shown. Further shown in 13 are a respective top view of polygonal element 80A and a respective front view of polygonal element 80B. The virtual volume 74 penetrates polygonal element 80A but not polygonal element 80B. The projected area 90A is a circle and represents the intersection of the virtual volume 74 and the polygonal element 80A. The reactive force FrA is computed based on the projected area 90A and is applied to the virtual volume 74 in a direction opposing the impact force Fa. In this example, the projected area 90A is mapped to a square center of polygonal element 80A because of the location of penetration by the virtual volume 74. Thus, the reactive force FrA is applied to a location that is central to the polygonal element 80A, and hence, central to the penetrating virtual volume 74. Furthermore, the projected area 90A is a full cross-sectional area 100 of the virtual volume 74. Accordingly, the reactive force FrA is applied with a relatively large magnitude. In the example of 14, the virtual volume 74 is shown to penetrate both polygonal elements 80A, 80B at the outside corner. Due to the location of penetration, the respective projected areas 90A, 90B mapped to polygonal elements 80A, 80B are each only a portion of the cross-sectional area 100 of the virtual volume 74. The reactive force FrA computed based on projected area 90A is applied to the virtual volume 74 in a direction opposing the impact to polygonal element 80A. The reactive force FrB computed based on projected area 90B is applied to the virtual volume 74 in a different direction, i. e. , opposing the impact to polygonal element 80B. Based on the location of impact, projected area 90A is mapped to a right edge of polygonal element 80A and projected area 90B is mapped to an upper edge of polygonal element 80B. Thus, reactive force FrA is applied to a location that is near the right edge of polygonal element 80A and reactive force FrB is applied to a location that is near the upper edge of polygonal element 80B. Furthermore, since the projected areas 90A, 90B are each only a portion of the cross-sectional area 100 of the virtual volume 74, the reactive forces FrA, FrB are applied with corresponding magnitudes, each of which are less than the magnitude of reactive force FrA applied in 13. This is so despite that fact that the linear depth of penetration is the same between 13 and 14. In 15, an even lesser portion of the virtual volume 74 penetrates both polygonal elements 80A, 80B at the outside corner, as compared with 14. The linear depth of penetration is also less than that of 13 and 14. As compared with 14, the projected areas 90A, 90B in 15 are lesser and the reactive force FrA is applied nearer the right edge of polygonal element 80A and reactive force FrB is applied nearer the upper edge of polygonal element 80B. Furthermore, the reactive forces FrA, FrB are applied with corresponding magnitudes that are lesser than the magnitude of reactive forces FrA, FrB applied in 14. As should be apparent from these examples, the techniques for using projected area 90 to compute the reactive forces FrA, FrB at the outside corner provide a natural response to opposing the virtual boundary 55. For example, even though the one polygonal element 80A is penetrated in 13, and two polygonal elements 80A, 80B are penetrated at the outside corner in 14, the techniques provide gradual increase in reactive force Fr. In other words, increasing projected area 90 translates into increasing reactive force Fr and decreasing projected area 90 translates into decreasing reactive force Fr. In so doing, the techniques avoid discrete jumps in reactive force Fr that move the tool 20 abruptly or unnaturally during encounters with the virtual boundary 55. The projected area 90A in 13 is roughly the same area as the combination of the projected areas 90A, 90B in 14, thereby providing a smooth reactive response even though penetration by the virtual volume 74 has effectively doubled at the given linear depth. In other words, by using the projected areas 90A, 90 to compute the reactive forces FrA, FrB, respectively, unexpected kick back of the tool 20 while rolling over the outside corner is mitigated. Thus, the techniques described herein solve surface modeling issues relating to corners. The techniques apply fully to any other examples wherein multiple non-planar polygonal elements 80 are simultaneously penetrated by the virtual volume 74. Examples include, but are not limited to, inside corners, peaks, valleys, etc. B. Projected ArcIn accordance with another example, as shown in 16 and 17, the controller 30 is configured to compute the reactive force Fr based on the penetration factor being related to a projected arc 200. The projected arc 200 is defined by a combination of any arcs 202 of the cross-sectional area 100 of the virtual volume 74 being bound by the geometry of the polygonal element 80 during intersection of the virtual volume 74 with the polygonal element 80. Here, the term projected is a mathematical expression indicating that the arcs 202 defined by intersection of the virtual volume 74 with the polygonal element 80 are mapped relative to the planar surface of the polygonal element 80. The projected arc 200 is bound by the polygonal element 80. Specifically, the projected arc 200 is bound by a perimeter of the polygonal element 80. In other words, the projected arc 200 for a single polygonal element 80 is considered in as much as the projected arc 200 exists within the perimeter of the polygonal element 80. The virtual volume 74 is defined such that the projected arc 200 changes non-linearly relative to a linear depth of penetration i. e. , the distance by which the virtual volume 74 protrudes into the polygonal element 80 and/or virtual boundary 55 by the virtual volume 74. Although the reactive force Fr may change with changes in the linear depth of penetration, the reactive force Fr is computed based on the projected arc 200 and without computationally accounting for the linear depth of penetration. In this example, the reactive force Fr is related to the projected arc 200. In one example, the reactive force Fr is directly correlated with the projected arc 200. Additionally or alternatively, the reactive force Fr is proportional to the projected arc 200. The reactive force Fr may be applied as a vector being normal to the plane of the polygonal element 80. The location of the vector with respect to the plane of the polygonal element 80 may vary depending on the location of projected arc 200 mapped on to the polygonal element 80. The magnitude of the reactive force Fr may vary depending on the size of the projected arc 200. The reactive force Fr vector may be at an angle that is not normal with respect to the plane of the polygonal element 80 depending on the projected arc 200 and/or the pose of the virtual volume 74 during penetration. Techniques for computing the reactive force Fr from the projected arc 200 are described below. For comparative purposes, the same geometrical examples of 8 and 9, used above to describe computation of the projected area 90 are used in 16 and 17 to describe computation of the projected arc 200. In 16, the cross-sectional area 100 of the virtual volume 74 at the plane of intersection with the triangular polygonal element 80 is shown. The circle is off-center from the geometrical center of the triangular polygonal element 80 and intersects a lower edge of the polygonal element 80 at intersection points 94 and 96. In turn, the cross-sectional area 100 is cut-off by the lower edge. Center point c is the center of the cross-sectional area 100 of the virtual volume 74 and radius r is the radius of the cross-sectional area 100. In this example, the projected arc 200 is defined by a combination of any arcs 202 of the cross-sectional area 100 of the virtual volume 74 being bound by the geometry of the polygonal element 80 during intersection of the virtual volume 74 with the polygonal element 80. In other words, the projected arc 200 is computed by determining any arcs 202 of perimeter of the cross-sectional area 100 that lie within the area of the polygonal element 80. Specifically, in this situation, wherein intersection of the circle occurs with respect to only one edge and no vertices of the triangular polygonal element 80, the projected arc 200 is computed by breaking down cross-sectional area 100 into arc segments 202a, 202b defined respectively by sector angles 1 and 2. Notably, in this example, the center point c of the cross-sectional area 100 is located within the polygonal element 80. The sector angles 1 and 2 are each defined about the center point c between intersection points 94, 96 and equal 360 degrees when combined. As shown, arc segment 202a based on sector angle 1 lies entirely within and is bound by the polygonal element 80, while arc segment 202b based on sector angle 2 lies entirely outside of and is unbound by the polygonal element 80. The projected arc can be computed using the following equation Arcproj=n/360, where the angle n is a sector angle creating an arc segment 202 that is bound by the polygonal element 80. In the example of 16, Arcproj=1/360 because 1 creates an arc segment 202a that is bound by the polygonal element 80. There may be more than one sector angle in the numerator of this equation, i. e. , Arcproj=n+m+ . . . /360, and these sector angles may be added together to establish the projected arc 200. The resultant value of this equation may be multiplied, scaled, or otherwise modified to standardize the projected arc 200 effect. For example, the projected arc 200 may alternatively be based on a length of the arc segment 202 rather than the degrees of its respective sector angle. 17 is yet another example illustrating computation of the projected arc 200 based on triangular polygonal element 80 and the circular cross-section 100 of the virtual volume 74. In this situation, intersection of the circular cross-section 100 once again occurs with respect to only one edge and no vertices of the triangular polygonal element 80. However, in this example, the center point c of the circular cross-section 100 is located outside of the polygonal element 80. The projected arc 200 is computed in a similar fashion. However, sector angle 1 has decreased as compared with 16 and sector angle 2 has increased in comparison to 16. Arc segment 202a, created from sector angle 1 has reduced in length in comparison to 16 and arc segment 202b, created from sector angle 2 has increased in length in comparison to 16. The projected arc 200 is based on arc segment 202a, which is bound within the polygonal element 80. Therefore, comparing 16 and 17, this example illustrates how a lesser penetrative impact by the virtual volume 74 results in a lesser projected arc 200, and generally, a lesser reactive force Fr. It should be reiterated that the calculations with respect to 16 and 17 are specific not only to a triangular polygonal element 80 and a circular cross-section 100 of the virtual volume 74, but further specific to intersection of the circle with respect to only one edge of the triangular polygonal element 80 and no vertices of the triangular polygonal element 80. Of course, the projected arc 200 and hence the calculations for computing the projected arc 200 may be different depending on the geometry of the polygonal element 80, the geometry of the virtual volume 74, and the relative locations of these geometries to each other. Furthermore, for the examples shown, computations of the projected arc 200 will differ depending on how many edges from 0 up to 3 and vertices from 0 up to 3 of the triangular polygonal element 80 are intersected by the circular cross-section 100 of the virtual volume 74. More complex arc segments 202 can be computed in instances where the cross-sectional area 100 of the virtual volume 74 is not circular, but rather elliptical or the like. Geometric computation of the projected arc 200 is contemplated for any geometric configuration and situation other than those described herein. As should be apparent based on 16 and 17, the projected arc 200 varies with respect to the linear depth of penetration. However, the projected arc 200 does not change linearly with respect to linear depth of penetration because the penetrating body is volumetric and does not apply a linear impact force Fa to the polygonal element 80 and/or virtual boundary 55. Instead, the penetrating body applies a higher order impact force Fa as a function of the volumetric shape of the virtual volume 74. Accordingly, the projected arc 200 changes with respect to linear depth of penetration according to this higher order volumetric function. Said differently, the projected arc 200 accounts for the displaced volume or penetrating volume of the virtual volume 74 by capturing the arc segments 202 that are within the plane of the polygonal element 202. Once again, the variable nature of the projected arc 200 occurs in part because the virtual volume 74 in the examples shown in 16 and 17 has only one face , is spherical and does not have identical cross-sectional areas adjacent to one another. Thus, reactive forces Fr computed in response to penetration by the virtual volume 74 are variably responsive to the linear depth of penetration. However, even though the reactive force Fr may change with changes in the linear depth of penetration, the reactive force Fr is indeed computed based on the projected arc 200 in these examples. The reactive force Fr is not computed simply using the linear depth by which the virtual volume 74 protrudes into the polygonal element 80 and/or virtual boundary 55. The examples and different possibilities described with respect to 5-7 shown for projected area 90 may be fully understood with respect to projected arc 200 and therefore are not repeated for simplicity. Of course, one skilled in the art would appreciate that reactive forces Fr computed based on projected arc 200 are likely to be different as compared with those reactive forces Fr shown computed based on projected area 90. Furthermore, iterative application of reactive force Fr computed based on recalculation of projected arc 200 is fully contemplated. Furthermore, the examples of 10-12 showing multiple reactive forces , FrA-FrF computed using projected area 90 and generated in response to simultaneous penetration of multiple polygonal elements , 80A-80F by the virtual volume 74 may be fully understood with respect to the techniques described herein using projected arc 200. Mainly, each reactive force FrA-FrF would be related to the respective projected arc 200 bound by each polygonal element 80A-80F. For projected arc 200, the reactive forces Fr may be applied individually for each polygonal element 80 or in combination as a combined reactive force FrTotal. Similarities with the techniques described in 13-15 for projected area 90 wherein the polygonal elements 80 are located in different planes also apply fully to the projected arc 200 method. For example, using projected arc 200 to compute the reactive forces FrA, FrB at the outside corner would similarly provide a natural response to opposing the virtual boundary 55. Increases in the projected arc 200 translate into increases in reactive force Fr and decreases in projected arc 200 translate into decreases in reactive force Fr. In so doing, the projected arc 200 technique avoids discrete jumps in reactive force Fr that move the tool 20 abruptly or unnaturally during encounters with the virtual boundary 55. Unexpected kick back of the tool 20 while rolling over the outside corner is mitigated. Thus, the projected arc 200 techniques described herein solve surface modeling issues relating to corners. The projected arc 200 techniques apply fully to any other examples wherein multiple non-planar polygonal elements 80 are simultaneously penetrated by the virtual volume 74. Examples include, but are not limited to, inside corners, peaks, valleys, etc. C. Displaced VolumeIn accordance with yet another example, as shown in 18, the controller 30 is configured to compute the reactive force Fr based on the penetration factor being related to a displaced volume 300 defined by a portion of the volume of the virtual volume 74 that penetrates the polygonal element 80 and wherein the displaced volume 300 is bound by the geometry of the polygonal element. The displaced volume 300 is defined by a combination of any volumetric portions of the virtual volume 74 being bound by the geometry of the polygonal element 80 during intersection of the virtual volume 74 with the polygonal element 80. The displaced volume 300 is bound by the polygonal element 80. Specifically, the displaced volume 300 is bound by a perimeter of the polygonal element 80. In other words, the displaced volume 300 for a single polygonal element 80 is considered in as much as the displaced volume 300 exists within the perimeter of the polygonal element 80. This is so, even if the displaced volume 300 exists above or below the plane of the polygonal element 80 in Cartesian space. The virtual volume 74 is defined such that the displaced volume 300 changes non-linearly relative to a linear depth of penetration i. e. , the distance by which the virtual volume 74 protrudes into the polygonal element 80 and/or virtual boundary 55 by the virtual volume 74. Although the reactive force Fr may change with changes in the linear depth of penetration, the reactive force Fr is computed based on the displaced volume 300 and without computationally accounting for the linear depth of penetration. In this example, the reactive force Fr is related to the displaced volume 300. In one example, the reactive force Fr is directly correlated with the displaced volume 300. Additionally or alternatively, the reactive force Fr is proportional to the displaced volume 300. The reactive force Fr may be applied as a vector being normal to the plane of the polygonal element 80. The location of the vector with respect to the plane of the polygonal element 80 may vary depending on the location of displaced volume 300 with respect to the polygonal element 80. The magnitude of the reactive force Fr may vary depending on the volumetric size of the displaced volume 300. The reactive force Fr vector may be at an angle that is not normal with respect to the plane of the polygonal element 80 depending on the displaced volume 300 and/or the pose of the virtual volume 74 during penetration. Techniques for computing the reactive force Fr from the displaced volume 300 are described below. For comparative purposes, the spherical virtual volume 74 and the triangular polygonal element 80 are shown for computation of the displaced volume 300. Of course, other examples are possible. In 18, the virtual volume 74 penetrates the plane of the polygonal element 80 and creates the displaced volume 300 below the plane. Here, the displaced volume 300 takes the shape of a spherical cap or dome cut off by a plane of the polygonal element 80. In this example, c is the spherical center, h is the height of the displaced volume 300, and r is the radius of the sphere. Here, the plane of the polygonal element 80 passes through a portion short of the spherical center c. Had, the penetration reached the spherical center c, the height h of the displaced volume 300 would equal the radius r of the sphere, and the displaced volume 300 would be a hemisphere. To compute the displaced volume 300 in this example, parameters of the virtual volume 74 and the displaced volume 300 are utilized. Such parameters include the radius r of the virtual volume 74, the height h of the displaced volume 300, and a radius a of the base 302 of the displaced volume 300 at the plane of intersection. For example, the displaced volume 300 for a sphere may be computed using the equation Vdisplaced=h2/33rh. Alternatively, the displaced volume 300 may be computed using calculus techniques such as using integration under a surface of rotation for the displaced volume 300, or the like. Of course, computation of the displaced volume 300 will be different given shapes other than a sphere. Again, since the displaced volume 300 is bound by the polygonal element 80, those portions of the virtual volume 74 that extend beyond the polygonal element 80 would not be taken into account to generate the reactive force Fr for the specific polygonal element 80 at hand. To bind the displaced volume 300 to the polygonal element, 80, in one example, adjacent polygonal elements 80 can be modeled as three-dimensional elements, such as adjacent triangular prisms for triangles, corresponding to the perimeter of each polygonal element 80. Thus, if the penetrating virtual volume 74 extends across a triangular prism wall, only the portion of the virtual volume 74 within the triangular prism for the corresponding polygonal element 80 is taken into account to generate the reactive force Fr for that specific polygonal element 80. The portion of the virtual volume 74 that extended across the triangular prism wall is taken into account to generate the reactive force Fr for the adjacent polygonal element 80. It should be reiterated that the calculations with respect to 18 are specific to a spherical virtual volume 74, and further specific to displaced volume 300 displaced completely within edges of the triangular polygonal element 80 and intersecting no vertices of the triangular polygonal element 80. Of course, the displaced volume 300 and hence the calculations for computing the displaced volume 300 may be different depending on the geometry of the polygonal element 80, the geometry of the virtual volume 74, and the relative locations of these geometries to each other. Furthermore, for the examples shown, computations of the displaced volume 300 will differ depending on how many edges from 0 up to 3 and vertices from 0 up to 3 of the triangular polygonal element 80 are implicated by the displaced volume 300. More complex displaced volumes 300 can be computed in instances where the virtual volume 74 is not spherical, but rather a spheroid, an ellipsoid, a toroid, or the like. Geometric computation of the displaced volume 300 is contemplated for any geometric configuration and situation other than those described herein. As should be apparent based on 18, the displaced volume 300 also varies with respect to the linear depth of penetration, which in the example of 18, is the height h of the displaced volume 300. However, the displaced volume 300 does not change linearly with respect to linear depth of penetration because the penetrating body is volumetric and does not apply a linear impact force Fa to the polygonal element 80 and/or virtual boundary 55. Instead, the penetrating body applies a higher order impact force Fa as a function of the volumetric shape of the virtual volume 74. Accordingly, the displaced volume 300 changes with respect to linear depth of penetration according to this higher order volumetric function. Once again, the variable nature of the displaced volume 300 occurs in part because the virtual volume 74 in the examples shown in 18 has only one face , is spherical and does not have identical cross-sectional areas adjacent to one another. Thus, reactive forces Fr computed in response to penetration by the virtual volume 74 are variably responsive to the linear depth of penetration. However, even though the reactive force Fr may change with changes in the linear depth of penetration, the reactive force Fr is indeed computed based on the displaced volume 300 in this example. The reactive force Fr is not computed based simply on the linear depth, h, by which the virtual volume 74 protrudes into the polygonal element 80 and/or virtual boundary 55. The examples and different possibilities described with respect to 5-7 shown for projected area 90 may be fully understood with respect to displaced volume 300 and therefore are not repeated for simplicity. Of course, one skilled in the art would appreciate that reactive forces Fr computed based on displaced volume 300 are likely to be different as compared with those reactive forces Fr shown computed based on projected area 90. Furthermore, iterative application of reactive force Fr computed based on recalculation of displaced volume 300 is fully contemplated. Furthermore, the examples of 10-12 showing multiple reactive forces , FrA-FrF computed using projected area 90 and generated in response to simultaneous penetration of multiple polygonal elements , 80A-80F by the virtual volume 74 may be fully understood with respect to the techniques described herein using displaced volume 300. Mainly, each reactive force FrA-FrF would be related to the respective displaced volume 300 bound by each polygonal element 80A-80F. For displaced volume 300, the reactive forces Fr may be applied individually for each polygonal element 80 or in combination as a combined reactive force FrTotal. Similarities with the techniques described in 13-15 for projected area 90 wherein the polygonal elements 80 are located in different planes also apply fully to the displaced volume 300 method. For example, using displaced volume 300 to compute the reactive forces FrA, FrB at the outside corner would similarly provide a natural response to opposing the virtual boundary 55. Increases in the displaced volume 300 translate into increases in reactive force Fr and decreases in displaced volume 300 translate into decreases in reactive force Fr. In so doing, the displaced volume 300 technique avoids discrete jumps in reactive force Fr that move the tool 20 abruptly or unnaturally during encounters with the virtual boundary 55. Unexpected kick back of the tool 20 while rolling over the outside corner is mitigated. Thus, the displaced volume 300 techniques described herein solve surface modeling issues relating to corners. The displaced volume 300 techniques apply fully to any other examples wherein multiple non-planar polygonal elements 80 are simultaneously penetrated by the virtual volume 74. Examples include, but are not limited to, inside corners, peaks, valleys, etc. D. Other ApplicationsThose skilled in the art appreciate that the above described examples of projected area, projected arc, and displaced volume each compute the reactive force Fr based on the penetration factor being a function of a geometry of the virtual volume 74 bound relative to a geometry 2D or 3D of the polygonal element 80. However, it is fully contemplated that there are techniques other than those described herein for computing the reactive force Fr based on the penetration factor being a function of a geometry of the virtual volume 74 bound relative to a geometry of the polygonal element 80. For example, instead of projected arc, a projected perimeter may be utilized if the cross-sectional area 100 has no arc segments 202, or the like. Any of the different surface modeling techniques described herein may be selectively turned off and on by the controller 30. For example, projected area 90 techniques may be reserved for traversing outside corners, while projected arc 200 techniques may be reserved for traversing a flat surface, etc. Such selection of these surface-modeling techniques may be based on, , user input. The user may select the surface-modeling mode on the displays 38 or the user input device 40. In another example, the controller 30 automatically identifies what is occurring between the virtual volume 74 and the virtual boundary 55 and selects the surface-modeling mode based on predetermined settings stored in memory. For example, the controller 30 may automatically determine the situation based on how many, where, and what polygonal elements 80 have been penetrated by the virtual volume 74. Furthermore, it is contemplated to blend any of the different surface modeling techniques described herein. This is possible because the techniques all utilize the penetration factor being a function of a geometry of the virtual volume 74 bound relative to a geometry of the polygonal element 80. Thus, any combination of projected area 90, projected arc 200 and/or displaced volume 300 modes may be utilized simultaneously to derive the reactive force Fr for any given polygonal element 80. The techniques described herein may be utilized for several practical applications or situations for the robotic surgical system 10. For example, the robotic surgical system 10 may be utilized in a manual mode of operation. During the manual mode, the operator manually directs, and the manipulator 14 controls, movement of the tool 20. The operator physically contacts the tool 20 to cause movement of the tool 20. The controller 30 monitors the forces and/or torques placed on the tool 20 using the force-torque sensor 70. The virtual boundary 55 may delineate areas of the anatomy to be treated from areas that should be avoided. Alternatively or additionally, the virtual boundary 55 may be provide a guide for directing the operator to move the tool 20 manually towards the target site. In yet another example, the virtual boundary 55 is defined relative to an object , equipment to be avoided. In any of these instances, if manual operation responsive to the forces and/or torques detected by the force-torque sensor 70 result in penetration of the virtual boundary 55, the controller 30 may control the manipulator 14 to move the tool 20 away from the virtual boundary 55. In turn, this provides the operator with a haptic sense of the location of the virtual boundary 55 in effort to avoid the same in the manual mode. In another application, the controller 30 may command the manipulator 14 to direct autonomous movement of the tool 20 in an autonomous mode of operation. Here, the manipulator 14 is capable of moving the tool 20 free of operator assistance. Free of operator assistance may mean that an operator does not physically contact the tool 20 to apply force to move the tool 20. Instead, the operator may use some form of control to manage starting and stopping of movement. For example, the operator may hold down a button of a remote control to start movement of the tool 20 and release the button to stop movement of the tool 20. In one instance, the positioning of the tool 20 is maintained on the tool path during autonomous mode but the operator may desire to re-orient the tool 20. Reorientation of the tool 20, while maintaining position, may implicate one or more virtual boundaries 55. By accounting for the updated orientation of the tool 20 and the virtual boundary 55 in the virtual simulation 72, the system 10 and method can react, , to undesired collisions between the re-oriented tool 20 and objects in the vicinity and/or objects interfering with the path of movement of the re-oriented tool 20. In another example, the virtual boundary 55 is provided in the autonomous mode as an added precaution in the event that autonomous movement may be inadvertently altered. Those skilled in the art will appreciate that various other applications or situations may utilize the projected area techniques described herein. The many features and advantages of the invention are apparent from the detailed specification, and thus, it is intended by the appended claims to cover all such features and advantages of the invention which fall within the true spirit and scope of the invention. Further, since numerous modifications and variations will readily occur to those skilled in the art, it is not desired to limit the invention to the exact construction and operation illustrated and described, and accordingly, all suitable modifications and equivalents may be resorted to, falling within the scope of the invention.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/29/533/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9624972343444824,
                    "F_SPEC_PARAMS": [
                        "complex in shape",
                        "difficult to model"
                    ],
                    "S_SPEC_PARAMS": [
                        "kick-back",
                        "increase of counter force"
                    ],
                    "A_PARAMS": [
                        "Modeling and computing these counter forces",
                        "modeled shape and pose of the tool",
                        "conventional surface modeling",
                        "counter force based simply on the linear depth of penetration"
                    ],
                    "F_SENTS": [
                        "In turn, the robotic arm moves the tool according to the computed counter forces to constrain the tool relative to the virtual surface.",
                        "Modeling and computing these counter forces is anything but trivial.",
                        "The virtual surfaces often are complex in shape and define geometric features for which surface interaction by the tool is difficult to model.",
                        "The issue is exacerbated because of the modeled shape of the tool and/or the pose of the tool during penetration."
                    ],
                    "S_SENTS": [
                        "For example, assuming the modeled shape and pose of the tool are the same during penetration, the entire tool may be over the virtual surface in one situation, and the tool may overhang the outer edge of the virtual surface in another situation.",
                        "In such situations, conventional surface modeling again applies the same counter force based simply on the linear depth of penetration without taking into account how much of the tool is engaging the virtual surface.",
                        "The more planar the virtual surface, the worse the kick-back will be.",
                        "Similar situations arise where only a portion of the tool penetrates the virtual surface, such as at an outer edge of the virtual surface.",
                        "For example if the vertex is shared among five triangles, the momentary increase of counter force at the vertex will be five times the counter force of one triangle."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Shape",
                        "Complexity of Control"
                    ],
                    "F_SIM_SCORE": 0.5955155342817307,
                    "S_TRIZ_PARAMS": [
                        "Force Torque"
                    ],
                    "S_SIM_SCORE": 0.532646119594574,
                    "GLOBAL_SCORE": 1.659911394615968
                },
                "sort": [
                    1.6599114
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11413753-20220816",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11413753-20220816",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2020-12-02",
                    "PUBLICATION_DATE": "2022-08-16",
                    "INVENTORS": [
                        "Rosen Nikolaev Diankov",
                        "Yoshiki Kanemoto",
                        "Denys Kanunikov"
                    ],
                    "APPLICANTS": [
                        "MUJIN, Inc.    ( Tokyo , JP )"
                    ],
                    "INVENTION_TITLE": "Robotic system control method and controller",
                    "DOMAIN": "B25J 91664",
                    "ABSTRACT": "A control method includes: deriving an approach location at which the end effector grips an operation object; deriving a scan location for scanning an identifier of the operation object; and based on the approach location and the scan location, creating or deriving a control sequence to instruct the robot to execute the control sequence. The control sequence includes 1 gripping the operation object from a start location; 2 scanning an identifier of the operation object with a scanner located between the start location and a task location; 3 temporarily releasing the operation object from the end effector and regripping the operation object by the end effector to be shifted, at a shift location, when a predetermined condition is satisfied; and 4 moving the operation object to the task location.",
                    "CLAIMS": "1. A control method of a robotic system that includes a robot having a robotic arm and an end effector, the method comprising: deriving an approach location at which the end effector grips an operation object; deriving a scan location for scanning an identifier of the operation object; and based on the approach location and the scan location, creating or deriving a control sequence to instruct the robot to execute the control sequence, wherein the control sequence includes: scanning identification information of the operation object with a scanner located between the start location and a task location; temporarily releasing the operation object from the end effector and regripping the operation object by the end effector to be shifted, at a shift location, when a predetermined condition is satisfied; and moving the operation object to the task location. 2. The control method of claim 1, wherein the predetermined condition is based on a storage efficiency of the operation object at the task location. 3. The control method of claim 2, wherein the storage efficiency is based on the height of the operation object. 4. The control method of claim 3, further comprising calculating the height of the operation object from a height location of a top surface of the operation object and a height location of a bottom surface of the operation object measured in a state of being gripped by the end effector. 5. The control method of claim 1, further comprising: obtaining imaging data representative of a pick-up area including the operation object; determining an initial pose of the operation object based on the imaging data; calculating a confidence measure in relation to an accuracy of the initial pose of the operation object; and deriving the approach location and the scan location based on the confidence measure. 6. The control method of claim 5, wherein the control sequence further includes selectively calculating the approach location and the scan location according to a performance metric and/or a scan metric, based on a result of comparing the confidence measure to a sufficiency threshold. 7. The control method of claim 6, wherein for a situation where the confidence measure does not satisfy the sufficiency threshold, the approach location and the scan location are derived based on the scan metric or are derived based on the scan metric with prioritizing the scan metric over the performance metric. 8. The control method of claim 6, wherein for a situation where the confidence measure satisfies the sufficiency threshold, the approach location and the scan location are derived based on the performance metric. 9. The control method of claim 1, wherein the control sequence further includes: deriving a first scan location for providing identification information of the operation object to the scanner; deriving a second scan location for providing alternative identification information of the operation object to the scanner; and moving the operation object 1 to the task location and ignoring the second scan location in a situation where a scan result indicates a successful scan after the operation object is moved to the first scan location, or 2 to the second scan location when a scan result indicates a failed scan after the operation object is moved to the first scan location. 10. A non-transitory computer-readable medium storing processor instructions for performing a control method of a robotic system that includes a robot having a robotic arm and an end effector, the processor instructions including: an instruction for deriving an approach location at which the end effector grips an operation object; an instruction for deriving a scan location for scanning identification information of the operation object; and an instruction for creating or deriving a control sequence to instruct the robot to execute the control sequence, based on the approach location and the scan location, wherein the control sequence includes: scanning an identifier of the operation object with a scanner located between the start location and a task location; temporarily releasing the operation object from the end effector and regripping the operation object by the end effector to be shifted, at a shift location, when a predetermined condition is satisfied; and moving the operation object to the task location. 11. The non-transitory computer-readable medium of claim 10, wherein the predetermined condition is based on a storage efficiency of the operation object at the task location. 12. The non-transitory computer-readable medium of claim 11, wherein the storage efficiency is based on the height of the operation object. 13. The non-transitory computer-readable medium of claim 11, wherein the control sequence further includes calculating a height of the operation object from a height location of a top surface of the operation object and a height location of a bottom surface of the operation object measured in a state of being gripped by the end effector. 14. A controller of a robotic system that includes a robot having a robotic arm and an end effector, the controller comprising: a communication device configured to communicate with a remote device; and a processor coupled to the communication device, wherein the processor is configured to execute a control method, the control method comprising: deriving an approach location at which the end effector grips an operation object; deriving a scan location for scanning an identifier of the operation object; and based on the approach location and the scan location, deriving a control sequence to instruct the robot to execute the control sequence, wherein the control sequence includes: scanning identification information of the operation object with a scanner located between the start location and a task location; temporarily releasing the operation object from the end effector and regripping the operation object by the end effector to be shifted, at a shift location, when a predetermined condition is satisfied; and moving the operation object to the task location. 15. The controller of claim 14, wherein the predetermined condition is based on a storage efficiency of the operation object at the task location. 16. The controller of claim 14, wherein the storage efficiency is based on the height of the operation object. 17. The controller of claim 14, wherein the control method further includes: obtaining imaging data representative of a pick-up area including the operation object; determining an initial pose of the operation object based on the imaging data; calculating a confidence measure in relation to an accuracy of the initial pose of the operation object; and deriving the approach location and the scan location based on the confidence measure. 18. The controller of claim 14, wherein the control sequence further includes: deriving a first scan location for providing identification information of the operation object to the scanner; deriving a second scan location for providing alternative identification information of the operation object to the scanner; and generating commands and/or settings to move the operation object to the task location while bypassing the second scan location when a scan result indicates a successful scan after the operation object is moved to the first scan location. 19. The controller of claim 14, wherein the control sequence further includes: deriving a first scan location for providing identification information of the operation object to the scanner; deriving a second scan location for providing alternative identification information of the operation object to the scanner; and generating commands and/or settings to move the operation object 1 to the task location and ignoring the second scan location in a situation where a scan result indicates a successful scan after the operation object is moved to the first scan location, or 2 to the second scan location when a scan result indicates a failed scan after the operation object is moved to the first scan location. 20. The non-transitory computer readable medium of claim 11, wherein the control sequence further includes: deriving a first scan location for providing identification information of the operation object to the scanner; deriving a second scan location for providing alternative identification information of the operation object to the scanner; and generating commands and/or settings to move the operation object 1 to the task location and ignoring the second scan location in a situation where a scan result indicates a successful scan after the operation object is moved to the first scan location, or 2 to the second scan location when a scan result indicates a failed scan after the operation object is moved to the first scan location.",
                    "FIELD_OF_INVENTION": "The present disclosure relates generally to a robotic system, and more particularly to a controller and a control method of a robotic system that manipulates an operation object such as an article, a distribution system, a program, and a medium.",
                    "STATE_OF_THE_ART": "With their ever-increasing performance and lowering cost, many robots , machines configured to automatically/autonomously execute physical actions are now extensively used in many fields. Robots, for example, can be used to execute various tasks such as manipulating or moving of an operation object in manufacturing, assembly, packing, transfer, transport, and the like. In executing the tasks, the robots can replicate human actions, thereby replacing or reducing dangerous or repetitive human tasks. As a system robotic system using such a robot, for example, Japanese Patent Application Laid-Open 2018-167950 discloses an automatic distribution system. In order to automate and save labor from warehousing to delivering of an article, the automatic distribution system includes a carrying container storage mechanism that temporarily stores a carrying container; and an automatic article delivery mechanism in which articles in the carrying container are automatically collected in a shipment container based on delivery information. However, despite technological advancement, robots often lack the sophistication required for replicating human involvement task in order to execute larger and/or more complex tasks. For this reason, automation and advanced functionality in robotic systems are not yet sufficient, and there are many tasks that are difficult to replace human involvement, and robotic systems lack the granularity of control and flexibility in actions to be executed. Therefore, there is still need for technical improvements to manage various actions and/or interactions between robots and further promote automation and advanced functionality of robotic systems. Therefore, an object of the present disclosure is to provide a controller and a control method of a robotic system, and the like that can realize a high degree of cooperation between units including a robot and can sufficiently increase a storage efficiency of an operation object, for example.",
                    "SUMMARY": [
                        "The present invention employs the following configuration to solve the above-described problems. [1] According to the present disclosure, a control method of a robotic system that includes a robot having a robotic arm and an end effector comprises: deriving an approach location at which the end effector grips an operation object; deriving a scan location for scanning an identifier of the operation object; and based on the approach location and the scan location, creating or deriving a control sequence to instruct the robot to execute the control sequence. The control sequence includes the following 1 to 4:1 gripping the operation object at a start location;2 scanning identification information of the operation object , a computer-readable identifier such as a barcode or a Quick Response QR code registered trademark with a scanner located between the start location and a task location;3 temporarily releasing the operation object from the end effector and regripping the operation object by the end effector to be shifted, at a shift location, when a predetermined condition is satisfied; and4 moving the operation object to the task location. Here, the operation object indicates an object to be manipulated by the robot provided in the robotic system, and includes, for example, one or more articles items, a bin, a container, and/or a box in which the articles are placed or stored. The containers may be packed or unpacked, and a part of the containers , an upper surface thereof may be opened. In addition, in some embodiments and examples, the operation object may be placed on a shelf, a pallet, a conveyor, and other temporary placing places. The control sequence indicates an ordered set controls , commands and/or settings for causing each corresponding robotic unit in the robotic system to execute an individual task. [2] In the above described configuration, the control sequence may further include the following 5 and 6:5 setting a condition that a storage efficiency of the operation object at the task location is increased in a case where the operation object is shifted and a direction of gripping the operation object by the end effector is changed, as the predetermined condition; and6 calculating a storage efficiency at the task location before shifting the operation object and a storage efficiency at the task location after shifting the operation object. [3] In the above described configuration, the control sequence may further include the following 7 and 8:7 deriving a height of the operation object; and8 calculating the storage efficiency based on the height of the operation object. [4] In the above described configuration, the height of the operation object may be calculated from a height location level of a top surface of the operation object and a height location level of a bottom surface of the operation object measured in a state of being gripped by the end effector. [5] In the above described configuration, the height of the operation object may be measured when the operation object is scanned with the scanner. [6] In the above described configuration, the control sequence may further include 9 temporarily releasing the operation object from the end effector by placing the operation object on a temporary placing table, at the shift location, when the predetermined condition is satisfied. [7] In the above described configuration, the control method may further include: deriving imaging data indicating a pick-up area including the operation object; determining an initial pose of the operation object based on the imaging data; calculating a confidence measure indicating a likelihood that the initial pose of the operation object is accurate; and deriving the approach location and the scan location based on the confidence measure. Here, the pose indicates a location and/or an orientation of the operation object , a posture including an orientation in a stopped state, and includes a translational component and/or a rotational component in a grid system utilized by the robotic system. In addition, the pose can be represented by a vector, a set of angles , Euler angles and/or roll-pitch-yaw angles, a homogeneous transformation, or a combination thereof. In the pose of the operation object, a coordinate transformation thereof and the like may include a translational component, a rotational component, changes thereof, or a combination thereof. In addition, the confidence measure indicates a quantified measure representing a degree of certainty a degree of a certainty or a likelihood that a determined pose of the operation object matches an actual pose of the operation object in a real-world. In other words, the confidence measure may be a measure indicating an accuracy of a determined pose of the operation object. The confidence measure may be referred to as an index indicating a likelihood that a determined pose matches an actual pose of the operation object. For example, the confidence measure can be a measure to be derived based on a result of matching between one or more visible characteristics of the operation object , a shape, a color, an image, a design, a logo, a text, and the like in image data of a pick-up area including the operation object and information regarding the visible characteristics of the operation object stored in master data. [8] In the above described configuration, the control sequence may further include 10 selectively calculating the approach location and the scan location according to a performance metric and/or a scan metric, based on a result of comparing the confidence measure to a sufficiency threshold, and the scan metric may be related to a likelihood that the identifier of the operation object is not covered by the end effector, regardless of whether the initial pose of the operation object is accurate or not. [9] In the above described configuration, when the confidence measure does not satisfy the sufficiency threshold, the approach location and the scan location may be derived based on the scan metric or may be derived based on the scan metric with prioritizing the scan metric over the performance metric. [10] Possibly, in the above described configuration, when the confidence measure satisfies the sufficiency threshold, the approach location and the scan location may be derived based on the performance metric. [11] In the above described configuration, the control sequence may further include the following 11 and 12:11 deriving a first scan location for providing identification information of the operation object to the scanner, and a second scan location for providing alternative identification information of the operation object to the scanner; and12 moving the operation object to the task location and ignoring the second scan location in a case where a scan result indicates a successful scan, or moving the operation object to the second scan location in a case where the scan result indicates a failed scan, after the operation object is moved to the first scan location. [12] In addition, according to the present disclosure, there is provided a non-transitory computer-readable medium storing processor instructions for performing a control method of a robotic system that includes a robot having a robotic arm and an end effector, in which the processor instructions include an instruction for deriving an approach location at which the end effector grips an operation object; an instruction for deriving a scan location for scanning an identifier of the operation object; and an instruction for creating or deriving a control sequence to instruct the robot to execute the control sequence, based on the approach location and the scan location. The control sequence includes the following 1 to 4:1 gripping the operation object at a start location;2 scanning identification information of the operation object with a scanner located between the start location and a task location;3 temporarily releasing the operation object from the end effector and regripping the operation object by the end effector to be shifted, at a shift location, when a predetermined condition is satisfied; and4 moving the operation object to the task location. [13] In the above described configuration, the control sequence may further include the following 5 and 6:5 setting a condition that a storage efficiency of the operation object at the task location is increased in a case where the operation object is shifted and a direction of gripping the operation object by the end effector is changed, as the predetermined condition; and6 calculating a storage efficiency at the task location before shifting the operation object and a storage efficiency at the task location after shifting the operation object. [14] In the above described configuration, the control sequence may further include the following 7 and 8:7 deriving a height of the operation object; and8 calculating the storage efficiency based on the height of the operation object. [15] In the above described configuration, the height of the operation object may be calculated from a height location level of a top surface of the operation object and a height location level of a bottom surface of the operation object measured in a state of being gripped by the end effector. [16] In addition, according to the present disclosure, there is provided a controller of a robotic system that includes a robot having a robotic arm and an end effector, the controller executing the control method according to any one of [1] to [11].",
                        "1 is an illustration of an exemplary environment in which a robotic system in accordance with an embodiment of the present disclosure may operate. 2 is a block diagram illustrating an example of a hardware configuration of the robotic system in accordance with the embodiment of the present disclosure. 3A is a perspective view schematically illustrating a first pose of an operation object. 3B is a perspective view schematically illustrating a second pose of the operation object. 3C is a perspective view schematically illustrating a third pose of the operation object. 4A is a top view illustrating an example task executed by the robotic system in accordance with the embodiment of the present disclosure. 4B is a front view illustrating an example task executed by the robotic system in accordance with the embodiment of the present disclosure. 5A is a flow diagram illustrating an example process flow of the robotic system in accordance with the embodiment of the present disclosure. 5B is a flow diagram illustrating an example process flow of the robotic system in accordance with the embodiment of the present disclosure."
                    ],
                    "DESCRIPTION": "According to the present disclosure, a robotic system in which multiple units , various robots, various devices, a controller provided integrally therewith or separately therefrom, and the like are highly integrated, the controller thereof, a distribution system provided with these, a method therefor, and the like are provided. For example, a robotic system in accordance with an embodiment of the present disclosure is an integrated system that can autonomously execute one or more tasks. The robotic system in accordance with the embodiment of the present disclosure can perform advanced handling of objects that can and significantly increase a storage efficiency of a storage container, based on a shape or dimension of an operation object and a space volume of the storage container, when the operation object is stored in the storage container and the like. In addition, an advanced scan can be performed on the operation object by creating or deriving a control sequence based on a confidence measure related to an initial pose of the operation object and executing the control sequence. The robotic system in accordance with the embodiment of the present disclosure can be configured to execute a task based on operating , physical movement and/or orientation on the operation object. More specifically, for example, the robotic system can sort or transfer various operation objects by picking up the operation object from a pick-up area including a start location , a large box, a bin, a container, a pallet, a storage container, a bucket, a cage, a belt conveyor, and the like as a supply source of the operation object and moving the operation object to a placement area including an objective task location , a large box, a bin, a container, a pallet, a storage container, a bucket, a cage, a belt conveyor, and the like as a transfer destination of the operation object. A control sequence which is executed by the robotic system can include scanning one or more identifiers , a barcode, a Quick Response QR code registered trademark, and the like located on one or more specific locations and/or surfaces of the operation object, during transfer. Therefore, the robotic system can execute various tasks such as gripping and picking up the operation object, scanning the identifier at an appropriate location/orientation, adjusting the pose, changing the pose and shifting releasing grip, and regripping and picking up the operation object, transferring the operation object to the task location and releasing grip, and disposing the operation object at the task location. The robotic system can further include an imaging device , a camera, an infrared sensor/camera, a radar, a lidar, and the like used to identify a location and a pose of the operation object and an environment around the operation object. Further, the robotic system can calculate a confidence measure associated with the pose of the operation object. In addition, the robotic system can derive an image indicating a location and an orientation of the operation object at a time of being transferred to a pick-up area including a start location, a placement area including a task location, an area including a shift location in the middle of a movement path of the operation object , a task table such as a temporary placement table, other robots, and the like, and the like. The robotic system can further perform image processing in order to identify or select an operation object according to a predetermined order , from top to bottom, outside to inside, inside to outside, and the like. Furthermore, for example, the robotic system can determine the initial pose of the operation object in a pick-up area from the image by identifying outlines of the operation object and grouping the outlines based on a color, a brightness, and a depth/location of a pixel, and/or a combination thereof in a pattern image of imaging data, and changes in their values, for example. In determining the initial pose, the robotic system can calculate the confidence measure according to a predetermined process and/or equation. The robotic system can further perform shifting of the operation object changing of the grip location of the operation object as necessary at a shift location provided in the middle of a route from the pick-up area including the start location and the like to the placement area including the task location and the like. Then, while the operation object is moved from the pick-up area including the start location and the like to the placement area including the task location and the like, the robotic system can derive a height of the operation object as necessary by an imaging device having a distance measuring function, for example. The robotic system can further execute a control sequence for executing each task according to a location, a pose, a height, and a confidence measure of the operation object, or a combination thereof, and/or a location and a pose of the robot, or a combination thereof. Such a control sequence can be created or derived by machine learning such as motion planning and deep learning. The control sequence corresponds to gripping of the operation object, manipulating of the operation object, placing the operation object at an objective task location, and the like, at the start location and/or an any location during movement, in order to sort, shift, and replace the operation object, for example. Here, traditional robotic systems execute a control sequence in which an operation object is gripped in a pick-up area including a start location and the like, and the operation object is moved to a placement area, including a task location and the like, in the originally gripped state. Therefore, in the traditional systems, the gripped operation object is merely moved in a gripped state and released from the gripped state, and thus it could not be said that a space in which the operation object is stacked or stored can be used in a sufficiently effective manner. For this reason, from a viewpoint of stacking or a storage efficiency of operation objects, human intervention adjustment, re-execution, complementation, system stop, and the like and an operation input therefor may be required. Unlike the traditional systems, the robotic system according to the present disclosure can create or derive a control sequence based on shape information of the operation object and stacking or storage information of the operation object and execute the control sequence. In other words, the robotic system according to the present disclosure can further optimize a stacking or storage efficiency of the operation object based on shape information of the operation object and stacking or storage information of the operation object. In addition, the robotic system according to the present disclosure can change a grip location of the operation object to a grip location suitable for optimization of a stacking or storage efficiency of the operation object, at a shift location that is in the middle of a route between the task location and the start location. In addition, unlike the traditional systems, the robotic system according to the present disclosure can create or derive a control sequence suitable for optimization of a stacking or storage efficiency of the operation object according to an actual height of the operation object, as necessary and execute the control sequence. For example, even though one or more identifiers located on one or more specific locations and/or a surface of the scanned operation object are the same, the operation object may have different shape dimensions. Therefore, on an upstream side previous stage of the shift location, in the control sequence, a height of the operation object is measured based on distance information from the imaging device camera or distance measuring device located and oriented in along a vertical direction to the operation object for which a supported location is known, for example. Then, based on the measured height of the operation object, a stacking or storage efficiency of the operation object at the task location can be calculated. Based on the result, the control sequence can be further optimized. Further, unlike the traditional systems, the robotic system according to the present disclosure can create, derive, and implement a control sequence according to a confidence measure as necessary. For example, according to the confidence measure, approach to the operation object can be changed, the grip location on the operation object can be changed, the pose/location of the operation object can be changed, and/or a part of the movement path can be changed. In the pose of the operation object gripped in the pick-up area and the like, generally, a top surface of the operation object can be exposed facing horizontally upward and a side surface of the operation object can be exposed facing vertically laterally. Therefore, the robotic system according to the present disclosure can include master data where the operation object has one identifier on a bottom surface of the operation object i. e. , a side opposite to the top surface of the operation object, and has another identifier on one of the side surfaces of the operation object. The robotic system according to the present disclosure can further calculate a confidence measure as necessary when processing an image of the pick-up area in identifying an operation object. In a case where the confidence measure exceeds a sufficiency threshold and there is sufficient certainty that the top surface of the operation object is exposed, the robotic system can dispose an end effector on the exposed top surface, grip the top surface, and rotate the operating object so as to present the bottom surface of the operation object at a predetermined location in front of the scanner. On the other hand, in a case where the confidence measure is less than a sufficiency threshold and it is uncertain whether the top surface or the bottom surface of the operation object is exposed, the robotic system can dispose the end effector along one of the side surfaces of the operation object, grip the side surface of the operation object, and rotate the operation object so as to pass between a set of opposed scanners, for example. In this case, a task efficiency and a task speed are improved by scanning the operation object in the movement path of the operation object, for example, between the pick-up area including the start location and the placement area including the task location. At this time, the robotic system according to the present disclosure can effectively combine a movement task of the operation object and the scan task of the operation object by creating or deriving a control sequence that also coordinates with or operates the scanner when the operation object is at the scan location. Further, by creating or deriving a control sequence based on the confidence measure of the initial pose of the operation object, an efficiency, a speed, and an accuracy regarding the scan task can be further improved. The robotic system according to the present disclosure can further create or derive a control sequence corresponding to a case where the initial pose of the operation object is not accurate. As a result, even when there is an error in determining the pose of the operation object , an error in determining the result of a calibration error, an unexpected pose, an unexpected light condition, and the like, a likelihood of accurately and reliably scanning the operation object can be increased. As a result, overall throughput for the robotic system can be increased and operator efforts/interventions can be further reduced. Further, in this specification, numerous specific details are set forth to provide a thorough understanding of the present disclosure, but the present disclosure is not limited thereto. In addition, in the embodiment of the present disclosure, the techniques described herein can be implemented without these specific details. Further, well-known specific functions, routines, or the like are not described in detail in order to avoid unnecessarily obscuring the present disclosure. References in this specification to an embodiment, one embodiment, or the like mean that a particular feature, structure, material, or characteristic being described is included in at least one embodiment of the present disclosure. Thus, the appearances of such phrases in this specification do not necessarily all refer to the same embodiment. On the other hand, such references are not necessarily mutually exclusive either. Further, the particular features, structures, materials, or characteristics can be combined in any suitable manner in one or more embodiments. In addition, it is to be understood that the various embodiments shown in the figures are merely illustrative representations and are not necessarily drawn to scale. In addition, for structures or processes that are well-known and often associated with robotic systems and subsystems, but that can unnecessarily obscure some significant aspects of the present disclosure, the description is omitted for purposes of clarity of the present disclosure. Moreover, in this specification, although various embodiments of the present disclosure are set forth, the present disclosure includes configurations or components different from description in this section, as other embodiments. Accordingly, the present disclosure can include other embodiments with additional elements or without some of elements described below. In addition, each embodiment of the present disclosure can take a form of computer- or controller-executable instructions, including routines executed by a programmable computer or controller. It should be noted that one of ordinary skill in the art to which the present disclosure belongs can understand that the techniques of the present disclosure can be implemented in systems including various computers or controllers. The techniques of the present disclosure can be implemented in a special purpose computer or data processor that is programmed, configured, or constructed to execute one or more instructions on various computers. Accordingly, the terms computer and controller used herein may be any data processor and can include Internet-based devices and handheld devices including palm-top computers, wearable computers, cellular or mobile phones, multi-processor systems, processor-based or programmable consumer electronics, network computers, mini computers, and the like. Information handled by these computers and controllers can be provided at any suitable display medium such as a liquid crystal display LCD. Instructions for executing computer- or controller-executable tasks can be stored in or on any suitable computer-readable medium, including hardware, firmware, or a combination of hardware and firmware. In addition, these instructions can be recorded in any suitable memory device including a flash drive and/or other suitable media, for example. In addition, in this specification, the terms coupled and connected, along with their derivatives, can be used to describe structural relationships between components. It should be understood that these terms are not intended as synonyms for each other. Rather, in particular embodiments, connected can be used to indicate that two or more elements are in direct contact with each other. Unless otherwise made apparent in the context, the term coupled can be used to indicate that two or more elements are in direct contact with each other or indirect contact with each other with other intervening elements therebetween, or that the two or more elements cooperate or interact with each other, for example, as in a cause-and-effect relationship, such as for signal transmission/reception or for function calls, or both. Suitable Environments 1 is a view illustrating an example environment in which a robotic system 100 in accordance with an embodiment of the present disclosure can operate. The robotic system 100 includes units such as one or more robots configured to execute one or more tasks. For the example illustrated in 1, the robotic system 100 can include an unloading unit 102, a transfer unit 104, a transport unit 106, a loading unit 108, or a combination thereof in a warehouse or a distribution/transport hub. The various units can be examples of the robots that manipulates an operation object. The robotic units can include a robot for operating the operation object by a robotic arm and an end effector, such as a devanning robot, a piece picking robot, and a fetching robot. In addition, each unit in the robotic system 100 can execute a control sequence in which multiple actions are combined so as to perform one or more tasks, for example, the multiple actions such as unloading the operation object from a truck, a van, and the like for storage in a warehouse, unloading the operation object from a storage location, for example, moving the operation object between containers, or loading the operation object into a truck or a van for transport. In other words, the task can include various movements and actions intended to transfer the operation object from one location to another location. The task can include operating transferring an operation object 112 from a start location 114 of the operation object 112 to a task location 116 , movement, orientation, pose change, and the like, shifting the operation object 112 at a shift location 118 provided in the middle of the movement path of the operation object 112 from the start location 114 to the task location 116, scanning the operation object 112 for deriving identification information of the operation object 112, and the like. For example, the unloading unit 102 can be configured to transfer the operation object 112 from a location in a carrier , a truck to a location on a belt conveyor. Further, the transfer unit 104 can be configured to transfer the operation object 112 from a location , a pick-up area including a start location to another location , a placement area including a task location on the transport unit 106, and to shift the operation object 112 in the middle of the movement path thereof. Further, the transport unit 106 can transfer the operation object 112 from an area associated with the transfer unit 104 to an area associated with the loading unit 108. Furthermore, the loading unit 108 can transfer the operation object 112 from the transfer unit 104 to a storage location , a predetermined location on a shelf such as a rack in a warehouse by moving, for example, a pallet on which the operation object 112 is placed. Further, in the description herein, the robotic system 100 is described as an example applied in a transport center; however, it is understood that the robotic system 100 can be configured to execute tasks in other environments/for other purposes, such as for manufacturing, assembly, packaging, healthcare, and/or other types of automation. It is also understood that the robotic system 100 can include other units, such as a manipulator, a service robot, and a modular robot, which are not shown in 1. For example, the robotic system 100 can include, an unloading unit from a pallet to transfer the operation object 112 from a cage cart or a pallet to a conveyor or other pallet, a container-switching unit for transferring the operation object 112 between containers, a packaging unit for wrapping the operation object 112, a sorting unit for performing grouping according to characteristics of the operation object 112, a picking unit for various operation , sorting, grouping and/or transferring the operation object 112 according to the characteristics of the operation object 112, a self-propelled carriage unit , automated guided vehicle, unmanned guided vehicle, and the like for moving a pallet or rack for storing the operation object 112, or any combination thereof. Suitable System 2 is a block diagram illustrating an example of a hardware configuration of the robotic system 100 in accordance with the embodiment of the present disclosure. For example, the robotic system 100 can include an electronic or electrical device, such as one or more processors 202, one or more storage devices 204, one or more communication devices 206, one or more input-output devices 208, one or more actuation devices 212, one or more transport motors 214, one or more sensors 216, or a combination thereof. These various electronic or electrical devices can be coupled to each other via a wire connection and/or a wireless connection. The robotic system 100 can include, for example, a bus, such as a system bus, a Peripheral Component Interconnect PCI bus or PCI-Express bus, a HyperTransport or industry standard architecture ISA bus, a small computer system interface SCSI bus, a universal serial bus USB, an IIC I2C bus, or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus also referred to as Firewire. Further, the robotic system 100 can include, for example, a bridge, an adapter, an amplifier, or other signal-related devices for providing the wire connection between the electronic or electrical devices. In addition, the wireless connection can be based on, for example, a cellular communication protocol , 3G, 4G, LTE, 5G, and the like, a wireless local area network LAN protocol , wireless fidelity WIFI, a peer-to-peer or device-to-device communication protocol , Bluetooth registered trademark, Near-Field communication NFC, and the like, an Internet of Things loT protocol , NB-loT, LTE-M, and the like, and/or other wireless communication protocols. The processor 202 can include a data processor , a central processing unit CPU, a special-purpose computer, and/or an onboard server configured to execute instructions , software instructions stored on the storage device 204 , a computer memory. The processor 202 can implement the program instructions to control/interact with other devices, thereby causing the robotic system 100 to execute a control sequence including various actions, tasks, and/or operations. The storage device 204 can include a non-transitory computer-readable medium having stored thereon program instructions , software. Examples of the storage device 204 can include, for example, volatile memory , cache and/or random-access memory RAM and/or non-volatile memory , a flash memory and/or a magnetic disk drive, a portable memory drive, and/or a cloud storage device, and the like. In addition, the storage device 204 can be used to further store and provide access to a processing result and/or predetermined data/threshold, and can store, for example, master data 252 that includes information related to the operation object 112. The master data 252 includes information related to the operation object 112, such as a dimension, a shape outline, a mass or a weight, a location of the center of mass, a template related to a pose and an outline, model data for recognizing different poses, a stock keeping unit SKU, a color scheme, an image, identification information, a logo, an expected location of the operation object, an expected measurement value by a sensor , physical quantity related to a force, a torque, a pressure, a contact measure value, a combination thereof, or the like. The storage device 204 can further store, for example, tracking data 254 of the operation object 112. The tracking data 254 can include a log of an operation object to be scanned or manipulated, imaging data , a photograph, a point cloud, a live video, and the like of the operation object 112 at one or more locations , an appropriate start location, a task location, a shift location, and the like, and a location and/or a pose of the operation object 112 at one or more locations thereof. The communication device 206 can include, for example, a circuit, a receiver, a transmitter, a modulator/demodulator modem, a signal detector, a signal encoder/decoder, a connector port, a network card, and the like, configured to communicate with an external or remote device via a network. In addition, the communication device 206 can be configured to send, receive, and/or process an electrical signal according to one or more communication protocols , the Internet Protocol IP, a wireless communication protocol, and the like. The robotic system 100 can use the communication device 206 to exchange information between respective units or exchange information with an external system or an external device for the appropriate purposes of, for example, reporting, data gathering, analyzing, troubleshooting, and the like. The input-output device 208 is a user interface device configured to input information and instructions from the operator and to communicate and present information to the operator, and can include, for example, an input device such as a keyboard, a mouse, a touchscreen, a microphone, a user interface UI sensor , a camera for receiving motion commands, and a wearable input device, and an output device such as a display 210, a speaker, a tactile circuit, and a tactile feedback device. In addition, the robotic system 100 can use the input-output device 208 to communicate with the operator in executing an action, a task, an operation, or a combination thereof. The robotic system 100 can include, for example, a physical or structural member , a robotic manipulator, a robotic arm, and the like, and hereinafter, referred to as structural member connected by a link or a joint in order to execute a control sequence including displacement such as movement or rotation of the operation object 112. Such a physical or structural member, link, or joint, can be configured to manipulate an end effector , a gripper, a hand, and the like configured to execute one or more tasks , gripping, rotation, welding, assembly, and the like in the robotic system 100. In addition, the robotic system 100 can include the actuation device 212 , a motor, an actuator, a wire, an artificial muscle, an electroactive polymer, and the like configured to drive or manipulate , displace and/or reorient the structural member about a joint or at a joint, and the transport motor 214 configured to transfer the units from a location to another location. The robotic system 100 can further include the sensor 216 configured to derive information used to implement the task, such as for manipulating the structural member and/or for transferring the unit. The sensor 216 can include a device configured to detect or measure one or more physical characteristics of the robotic system 100 , a state, a condition, a location, and the like of one or more structural members, links, or joints and/or characteristics of a surrounding environment, for example, an accelerometer, a gyroscope, a force sensor, a strain gauge, a tactile sensor, a torque sensor, a location encoder, and the like. Further, the sensor 216 can include one or more imaging devices 222 , a visual and/or infrared camera, a 2-dimensional and/or 3-dimensional imaging camera, a distance measuring device such as a lidar or a radar, and the like configured to detect the surrounding environment. The imaging device 222 can generate a representation of the detected environment, such as a digital image and/or a point cloud, in order to obtain visual information for automatic inspection, robot guidance, or other robot applications, for example. The robotic system 100 can further process the digital image, the point cloud, distance measurement data, and the like via, , the processors 202 to identify the operation object 112 of 1, the start location 114 of 1, the task location 116 of 1, the shift location 118 between the start location 114 and the task location 116, a pose of the operation object 112, a confidence measure regarding the pose of the operation object at the start location 114 and the like, a confidence measure regarding a height of the operation object 112, or a combination thereof. Further, in order to manipulate the operation object 112, the robotic system 100 can identify the operation object 112, the start location 114 thereof, the task location 116 thereof, the shift location 118, and the like by obtaining and analyzing an image of a designated area , a pick-up area such as in a truck or on a belt conveyor, a placement area for disposing the operation object 112 on the belt conveyor, area for shifting the operation object 112, an area for disposing the operation object in the container, an area on the pallet for stacking the operation object 112, and the like through various units. In addition, the imaging device 222 can include, for example, one or more cameras configured to generate an image of a pick-up area, a placement area, an area for shifting the operation object 112 set therebetween, and the like. The imaging device 222 can further include, for example, one or more distance measuring devices such as lidars or radars configured to measure a distance to the operation object 112 supported at a predetermined location, before or upstream from the shift location 118. Based on the processed image and/or distance measurement data, the robotic system 100 can determine a start location 114, a task location 116, a shift location 118, a related pose, an actual height of the operation object 112, a confidence measure, and the like. In addition, for scanning the operation object 112, the imaging device 222 can include one or more scanners 412 and 416 , a barcode scanner, a QR code scanner registered trademark, and the like: see 4A and 4B described below configured to scan identification information , an identifier 332 of 3A and/or 3C described below of the operation object 112 during the transport or movement of the operation object, for example, between the start location 114 and the task location 116 preferably, before the shift location 118. Therefore, the robotic system 100 can create or derive a control sequence for providing one or more portions of the operation object 112 to one or more scanners 412. The sensor 216 can further include, for example, a location sensor 224 , a location encoder, a potentiometer, and the like configured to detect a location of a structural member, a link, or a joint. This location sensor 224 can be used to track the location and/or orientation of the structural member, the link, or the joint during execution of the task. Further, the sensor 216 can include, for example, a contact sensor 226 , a pressure sensor, a force sensor, a strain gauge, a piezoresistive/piezoelectric sensor, a capacitive sensor, an elastoresistive sensor, other tactile sensors, and the like configured to measure a characteristic associated with a direct contact between physical structures or surfaces. The contact sensor 226 can measure the characteristic of the operation object 112 corresponding to a grip of the end effector. Accordingly, the contact sensor 226 can output a contact measure that represents a quantified and measured value , a measured force, torque, location, and the like corresponding to a degree of contact between the end effector and the operation object 112. Here, the contact measure can include, for example, one or more force or torque readings associated with forces applied to the operation object 112 by the end effector. Determination of Confidence Measure for Initial Pose 3A, 3B, and 3C are perspective views schematically illustrating a first pose 312, a second pose 314, and a third pose 316, respectively, as an example of an operation object 302 in various poses locations and orientations. In order to identify the pose of the operation object 302, the robotic system 100 can process, for example, a 2-dimensional image, a 3-dimensional image, a point cloud, and/or other imaging data from the imaging device 222. In addition, in order to identify an initial pose of the operation object 302, the robotic system 100 can analyze, for example, imaging data by one or more imaging devices 222 directed to the pick-up area. In order to identify the pose of the operation object 302, the robotic system 100 can first analyze and identify the operation object 302 depicted in the imaging data based on a predetermined recognition mechanism, a recognition rule, and/or a template related to a pose or an outline. The robotic system 100 can identify an outline , a perimeter edge or surface of the operation object 302, or group the outlines in the image data. The robotic system 100 can identify, for example, the groupings of the outlines that correspond to a set of pixels in the image data that match or correspond to values the color, the brightness, the depth/location, and/or a combination thereof of a corresponding aspect of an object registered in the master data 252. When the outlines of the operation object 302 are grouped, the robotic system 100 can identify, for example, one or more surfaces, edges, and/or points, and poses of the operation object 302 in a grid or coordinate system used in the robotic system 100. In addition, the robotic system 100 can identify one or more exposed surfaces , a first exposed surface 304, a second exposed surface 306, and the like of the operation object 302 in the imaging data. Further, the robotic system 100 can identify the operation object 302, for example, by determining an outline shape and one or more dimensions , a length, a width, and/or a height of the operation object 302 from the imaging data according to the outline and the calibration of the operation object 302 or mapping data for the imaging device 222, and comparing the determined dimensions with corresponding data in the master data 252. Further, the robotic system 100 can identify whether an exposed surface corresponds to any of a top surface 322, a bottom surface 324, and an outer peripheral surface 326, based on a length, a width, and a height of the operation object 302 in which dimensions of the exposed surface is identified. In addition, the robotic system 100 can identify the operation object 302, for example, by comparing one or more markings , a letter, a number, a shape, a visual image, a logo, or a combination thereof displayed on one or more exposed surfaces with one or more predetermined images in the master data 252. In this case, the master data 252 can include, for example, one or more images of a product name, a logo, a design/image on a package surface of the operation object 302, or a combination thereof. In addition, the robotic system 100 can identify the operation object 302 by comparing a portion of the imaging data , a portion within an outline of the operation object 302 with the master data 252, and similarly, can identify a pose particularly, an orientation of the operation object 302 based on a unique and predetermined image pattern on a surface. 3A illustrates a first pose 312 where the first exposed surface 304 , an upward-facing exposed surface is the top surface 322 of the operation object 302 and the second exposed surface 306 , an exposed surface generally facing a source of the imaging data is one of the peripheral surfaces 326 of the operation object 302. In identifying the exposed surfaces, the robotic system 100 can process the imaging data of 3A to map measurement values of the dimensions , the number of pixels of the first exposed surface 304 and/or the second exposed surface 306 into real-world dimensions using a predetermined calibration or mapping function. The robotic system 100 can compare the mapped dimensions with dimensions of the known/expected operation object 302 in the master data 252 and identify the operation object 302 based on the result. Further, since a pair of intersecting edges that define the first exposed surface 304 matches the length and the width of the identified operation object 302, the robotic system 100 can identify that the first exposed surface 304 is either the top surface 322 or the bottom surface 324. Similarly, because one of the edges defining the second exposed surface 306 matches the height of the identified operation object 302, the robotic system 100 can identify the second exposed surface 306 as the peripheral surface 326. In addition, the robotic system 100 can process the imaging data of 3A to identify one or more markings unique to a surface of the operation object 302. In this case, the master data 252 can include one or more images and/or other visual characteristics , a color, a dimension, a size, and the like of surfaces and/or unique markings of the operation object 302 as described above. As illustrated in 3A, since the operation object 302 has A on the top surface 322, the robotic system 100 can identify the operation object 302 as a registered object stored in the master data 252, and identify the first exposed surface 304 as the top surface 322 of the operation object 302. In addition, the master data 252 can include an identifier 332 as identification information of the operation object 302. More specifically, the master data 252 can include an image and/or coded message of the identifier 332 of the operation object 302, a location 334 of the identifier 332 relative to a surface and/or a set of edges, one or more visual characteristics thereof, or a combination thereof. As illustrated in 3A, the robotic system 100 can identify the second exposed surface 306 as the peripheral surface 326 based on the presence of the identifier 332 and/or the location thereof matching the location 334 of the identifier 332. 3B illustrates a second pose 314 obtained by rotating the operation object 302 by 90 degrees about a vertical axis along a direction B in 3A. For example, a reference point a of the operation object 302 can be in a lower left front corner in 3A and in an upper right back corner in 3B. Accordingly, in comparison with the first pose 312, the top surface 322 of the operation object 302 can be recognized as a different orientation in the imaging data and/or the peripheral surface 326 of the operation object 302 having the identifier 332 may not be visually recognized. The robotic system 100 can identify various poses of the operation object 302 based on a specific orientation of the identifier 332 having one or more visual features. For example, it is possible to determine the first pose 312 and/or the third pose 316 in a case where a dimension matching a known length of the operation object 302 extends horizontally in the imaging data, a dimension matching a known height of the operation object 302 extends vertically in the imaging data, and/or a dimension matching a known width of the operation object 302 extends along a depth axis in the imaging data. Similarly, the robotic system 100 can determine the second pose 314 in a case where a dimension matching a width extends horizontally in the imaging data, a dimension matching a height extends vertically in the imaging data, and/or a dimension matching a length extends along a depth axis in the imaging data. In addition, the robotic system 100 can determine that the operation object 302 is in the first pose 312 or the second pose 314 based on an orientation of a visible marking such as A illustrated in 3A and 3B, for example. In addition, for example, in a case where the identifier 332 of the operation object 302 is visually recognized with a marking A i. e. , on a different surface, the robotic system 100 can determine that the operation object 302 is in the first pose 312 based on the visible marking to be visually recognized in a combination of respective surfaces. 3C illustrates the third pose 316 obtained by rotating the operation object 302 by 180 degrees about a horizontal axis along a direction C in 3A. For example, a reference point a of the operation object 302 can be in a lower left front corner in 3A and in an upper left back corner in 3C. Accordingly, in comparison with the first pose 312, the first exposed surface 304 is the bottom surface 324 of the operation object, and both the top surface 322 and the peripheral surface 326 having the identifier 332 of the operation object 302 are not visually recognized. As described above, the robotic system 100 can identify that the operation object 302 is in either the first pose 312 or the third pose 316 based on the dimensions determined from the image data. The robotic system 100 can further identify that the operation object 302 is in the first pose 312 in a case where the marker , A of the top surface 322 is visible. In addition, the robotic system 100 can identify that the operation object 302 is in the third pose 316 in a case where a bottom-surface marker , an instance of the identifier 332 of the operation object is visually recognized. When determining the pose of the operation object 302, real-world conditions may affect an accuracy of the determination. For example, lighting conditions may reduce visibility of a surface marking due to a reflection and/or a shadow. In addition, according to an actual orientation of the operation object 302, an exposure or viewing angle of one or more surfaces may be reduced, and therefore any marking on the surface may be unidentifiable. Accordingly, the robotic system 100 can calculate a confidence measure associated with the determined pose of the operation object 302. The robotic system 100 can further calculate the confidence measure based on a certainty interval associated with the dimension measurement within the image in the imaging data. In this case, the certainty interval can increase as a distance between the operation object 302 and an imaging source , the imaging device 222 decreases and/or in a case where a measured edge of the operation object 302 is closer to the imaging source in a direction orthogonal to a direction radiating from the imaging source and farther away from the imaging source in a direction parallel to the radiating direction. Also, the robotic system 100 can calculate, for example, the confidence measure based on a degree of match between a marker or a design in the imaging data and a known marker/design in the master data 252. Furthermore, the robotic system 100 can measure a degree of an overlap or a deviation between at least a portion of the imaging data and a predetermined marker/image. In this case, the robotic system 100 can identify the operation object 302 and/or an orientation thereof according to a greatest overlap and/or a lowest deviation measurement for a minimum mean square error MMSE mechanism, and furthermore can calculate a confidence measure based on a degree of the obtained overlap/deviation. The robotic system 100 can calculate the movement path of the operation object 302 in the control sequence based on the obtained confidence measure. In other words, the robotic system 100 can transfer the operation object 302 differently according to the obtained confidence measure. System Operation 4A is a top view illustrating an example task 402 which is executed by the robotic system 100 in accordance with the embodiment of the present disclosure. As described above, the example task 402 can correspond to a control sequence which is executed by the robotic system 100 , executed by one of the units illustrated in 1. As illustrated in 4A, for example, the task 402 can include moving the operation object 112 from the pick-up area including the start location 114 to the placement area including the task location 116 via the shift location 118. Also, the task 402 can include scanning the operation object 112 while moving the operation object from the start location 114 to the task location 116, and shifting the operation object 112 at the shift location 118 by changing the grip location. Accordingly, the robotic system 100 can update the tracking data 254 of the operation object 112 by adding the scanned operation object 112 to the tracking data 254, removing the operation object 112 from the tracking data 254, and/or evaluating the operation object 112, and the like. In addition, in order to identify and/or specify the start location 114, the robotic system 100 can include a scanner 412 an instance of the imaging device 222 such as a 3D vision device directed at a pick-up area so as to image the pick-up area , an area designated for a part procurement pallet or a large box and/or a region on a receiving side of a belt conveyor, and the like, and thereby can derive imaging data of the designated area. Therefore, the robotic system 100 can implement a computer aided image process vision process for the imaging data, in order to identify the various operation objects 112 located in the designated area via the processor 202, for example. In addition, the robotic system 100 can select an operation object 112 for which the task 402 is to be executed from among the recognized operation objects 112. The robotic system 100 can select based on, for example, a predetermined selection measure, and/or a selection rule, and/or a template related to a pose or an outline. The robotic system 100 can further process the imaging data in order to determine the start location 114 and/or the initial pose for the selected operation object 112. In order to identify and/or specify the task location 116 and the shift location 118, the robotic system 100 can include other scanners 416 an instance of the imaging device 222 facing the following areas so as to image the placement area and other predetermined areas , an area designated for a sorted pallet or a large box and/or a region on a receiving side of a belt conveyor, and the like. Accordingly, the robotic system 100 can derive imaging data of the designated area. Therefore, the robotic system 100 can implement a computer aided image process vision process for the imaging data, in order to identify the task location 116 for disposing the operation object 112, the shift location 118, and/or the pose of the operation object 112 via the processor 202, for example. In addition, the robotic system 100 can identify and select the task location 116 and the shift location 118 based on a predetermined criterion or rule for stacking and/or disposing multiple operation objects 112 based on the imaging result or not based on the imaging result. The scanner 416 can be disposed to face in a horizontal direction so as to scan a mark that is adjacent thereto , at a height corresponding to a height of the corresponding scanners and on a vertically oriented surface of the operation object 112. Further, the scanner 416 can be disposed to face in a vertical direction so as to scan a mark that is above/below thereof and on a horizontally oriented surface of the operation object 112. Furthermore, the scanners 416 can be disposed to oppose each other so as to scan opposite sides of the operation object 112 that is placed between the scanners 416. In addition, the robotic system 100 can operate the operation object 112 so as to place the operation object 112 at a presentation location and/or so as to scan one or more surfaces/portions of the operation object 112 with the scanners 416, according to the location and/or scanning direction of the scanner 416. Further, the robotic system 100 can include the imaging device 222 configured to measure a height location of the bottom surface 324 of the operation object 112 which has been scanned by the scanner 416 and a support location of which is known, for example see 4B. In order to execute the task 402 using such an identified start location 114, the shift location 118, and/or the task location 116, the robotic system 100 can operate one or more structural members , a robotic arm 414 and/or the end effector of each unit. Accordingly, the robotic system 100 can create or derive a control sequence that corresponds to one or more actions that will be implemented by the corresponding unit to execute the task 402, via the processor 202, for example. For example, the control sequence for the transfer unit 104 can include placing the end effector at an approach location , a location/position for gripping the operation object 112, gripping the operation object 112, lifting the operation object 112, moving the operation object 112 from above the start location 114 to the presentation location/pose for the scanning operation, shifting the operation object 112 at the shift location 118 changing the grip location, moving the operation object 112 from the start location 114 to above the task location 116, as necessary, via a shift location 118, lowering the operation object 112, and releasing the operation object 112. In addition, the robotic system 100 can create or derive the control sequence by determining a sequence of commands and/or settings for one or more actuation devices 212 that operate the robotic arm 414 and/or the end effector. In this case, the robotic system 100 can use, for example, the processor 202 to calculate the commands and/or settings of the actuation device 212 for manipulating the end effector and the robotic arm 414 to place the end effector at the approach location about the start location 114, grip the operation object 112 with the end effector, place the end effector at the approach location around the scan location or shift location 118, place the end effector at the approach location around the task location 116, and release the operation object 112 from the end effector. Accordingly, the robotic system 100 can execute an operation for completing the task 402 by operating the actuation device 212 according to the determined control sequence of commands and/or settings. In addition, the robotic system 100 can create or derive a control sequence based on the confidence measure for the pose of the operation object 112. In this case, the robotic system 100 can consider placement of the end effector at various locations for pickup in order to grip or cover a different surface, calculate various presentation locations/poses for the operation object 112, or a combination thereof according to the confidence measure for the pose, for example. As an illustrative example, in a case where the operation object 112 is the operation object 302 placed in the first pose 312 of 3A in this case, the top surface 322 of the operation object 302 generally faces upward and is exposed and the confidence measure for the pose is high i. e. , a degree of a certainty exceeds the sufficiency threshold and the determined pose is more likely accurate, the robotic system 100 can create or derive a first control sequence 422 that includes a first approach location 432 and a first presentation location 442. At this time, for example, since there is a sufficient certainty that the top surface 322 of the operation object 302 faces upward i. e. , the bottom surface 324 with the identifier 332 of the operation object 302 of 3C faces downward, the robotic system 100 can calculate the first control sequence 422 that includes the first approach location 432 for placing the end effector directly over the top surface 322 of the operation object 302. As a result, the robotic system 100 can grip the operation object 112 with the end effector contacting/covering the top surface 322 of the operation object 302 such that the bottom surface 324 of the operation object 302 is exposed. In addition, the robotic system 100 can calculate the first control sequence 422 that includes the first presentation location 442 for causing the operation object 112 to be directly over an upward-facing scanner 416 for scanning the identifier 332 located on the bottom surface 324. In contrast, in a case where the confidence measure for the pose is low i. e. , a degree of a certainty is less than a sufficiency threshold and a likelihood that the determined pose is accurate is low, the robotic system 100 can create or derive a second control sequence 424 i. e. , different from the first control sequence 422 that includes a second approach location 434 and one or more second presentation locations 444. At this time, for example, the robotic system 100 can measure the dimensions of the operation object 112, compare the dimensions with the master data 252, and determine that the operation object 302 is in either the first pose 312 of 3A or the third pose 316 of 3C , in a case where a certainty level of the measurement exceeds a predetermined threshold. However, the robotic system 100 may have a difficulty in imaging/processing a mark printed on the surface of the operation object 112, and as a result, the confidence measure associated with the determined pose can be less than a sufficiency threshold. In other words, the robotic system 100 may not be sufficiently certain whether the upward-facing exposed surface of the operation object 302 is the top surface 322 thereof corresponding to, , the first pose 312 or the bottom surface 324 thereof corresponding to, , the third pose 316. In this case, due to the low degree of the confidence measure the low degree of a certainty, the robotic system 100 can calculate the second control sequence 424 that includes the second approach location 434 for placing the end effector , aligned with and/or facing in a direction parallel to the top surface 322 and/or the bottom surface 324 of the operation object 302 to be adjacent to one of the peripheral surfaces 326 of the operation object 302 of 3A. As a result, the robotic system 100 can grip the operation object 112 with the end effector contacting/covering one of the peripheral surfaces 326 of the operation object 302 and causing both the top surface 322 and the bottom surface 324 of the operation object 302 to be exposed. In addition, the robotic system 100 can simultaneously or sequentially present or place the top surface 322 and the bottom surface 324 of the operation object 302 in front of the scanners 416 , in a scanning field and/or in a state of facing the scanning field. In a case where the operation object 112 is in the scan location, the robotic system 100 can operate the scanners 416 , at least the scanners 416 facing the top surface 322 and the bottom surface 324 of the operation object 302 to simultaneously and/or sequentially scan the presented surfaces and derive the identifiers 332 of the operation object 302 above the scanner. In addition, the second control sequence 424 includes the second presentation locations 444 for disposing the surface that faces downward initially the bottom surface 324 of the operation object 302 horizontally and directly over the upward-facing scanner 416 and/or for placing the surface that faces upward initially the top surface 322 of the operation object vertically and directly in front of a horizontally-facing scanner 416. The second control sequence 424 can include a reorienting/rotating action , an action as represented by a dotted-unfilled circle for providing two presentation locations/poses, and thereby both the top surface 322 and the bottom surface 324 are scanned using orthogonally facing scanners 416. Further, for example, the robotic system 100 can sequentially present the top surface 322 of the operation object 302 to the upward-facing scanner and scan the top surface, and then rotate the operation object 302 by 90 degrees to present the bottom surface 324 thereof to the horizontally-facing scanner 416 for scanning. At that time, the reorienting/rotating action can be conditional such that the robotic system 100 implements the corresponding commands in a case where reading the identifier 332 of the operation object 302 fails. Also, as an example, the robotic system 100 can create or derive a control sequence not shown for gripping/covering one of the peripheral surfaces 326 along a width of the operation object 302 in a case where the confidence measure is low. The robotic system 100 can move the operation object 302 between a horizontally opposing pair of the scanners 416 to present the peripheral surfaces 326 of the operation object 302 along the length thereof and scan the identifier 332 on one of the peripheral surfaces 326 for example, as illustrated in 3A, for example. Further, details regarding the control sequence based on the confidence measure will be described later with reference to 5A and 5B described later. In addition, the robotic system 100 can derive the control sequence based on a 2-dimensional or 3-dimensional shape of the operation object 112 gripped by the end effector hereinafter, referred to as the operation object 112 instead of the operation object 302 and the information regarding the operation object 112 in a storage container 450 placed at the task location 116 , a box, a bin, and the like. As an example, the robotic system 100 determine dimensions of the operation object 112 in both cases of the first control sequence and the second control sequence described above, for example. The robotic system 100 can determine/track placement locations, orientations, and/or dimensions of other objects , previously stored objects in the storage container 450 placed at the task location 116. Accordingly, the robotic system 100 can obtain information regarding open/available space in the storage container 450. The robotic system 100 can calculate space shape parameters of the operation object 112 according to various poses of the operation object 112 and according to various different grip locations. Therefore, by comparing these space shape parameters with the available space in the storage container 450, the robotic system 100 can optimize and select a pattern or plan with which the operation object 112 can be stored at a higher filling density in the storage container 450. In this case, when the end effector accesses the storage container 450, the robotic system 100 can consider the presence/absence of interference between the end effector and the storage container 450 or the operation object 112 already stored. Therefore, the robotic system 100 can determine an increase in a filling rate in the storage container 450 for a pose change or a grip location change. Accordingly, the robotic system 100 can create or derive a control sequence including an operation of shifting the operation object 112 to a pose associated with the increased/higher storage rate. 4B is a front view illustrating an example task 404 which is executed by the robotic system 100 in accordance with the embodiment of the present disclosure. In this example, multiple operation objects 112 are placed on a pallet 464 that is carried to the pick-up area including the start location 114 in a state of being mounted on a self-propelled carriage unit 462 such as an automated guided vehicle AGV. For illustrative purposes, 4B shows multiple operation objects 112 having the same shape and stacked according to a pattern However, it is understood that, in many cases, multiple operation objects 112 having different dimensions may be randomly stacked on the pallet 464. The pick-up area in which the pallet 464 is carried is imaged by the scanner 412, and the operation object 112 is selected in the same manner as described with reference to 4A. For the selected operation object 112, in this example, the top surface 322 of the operation object 112 is gripped by the end effector installed at the tip of the robotic arm 414 of the transfer unit 104, the identifier 332 is scanned with the scanner 416, and the information of the identifier 332 is derived. For example, the robotic system 100 can obtain information including the dimensions of the operation object 112 by comparing the information of the identifier 332 of the operation object 112 with the master data 252. In some instances, objects having the same identifier 332 may actually have different dimensions particularly height. Therefore, for example, when scanning the operation object 112, the robotic system 100 measures the distance to the bottom surface 324 of the operation object 112 by using a distance measuring device 466 an example of the imaging device 222 installed on a floor of the task space or near the floor surface. At this time, in a case where the movement of the operation object 112 is temporarily stopped during scanning, the distance to the bottom surface 324 of the operation object 112 can be measured during the temporary stop. For illustrative purposes, 4B show that the measurement by the distance measuring device 466 is performed immediately after the operation object 112 is unloaded depalletized from the pallet 464. However, it is understood that a timing of the measurement is not particularly limited The distance measuring device 466 can be configured to obtain the measurements at an upstream location of before the shift location 118 in the control sequence. In this example, the robotic system 100 can obtain a height location gripping level of the top surface 322 of the operation object 112 at the time of measurement according to a control sequence or an appropriate location measurement. Therefore, a height 112h of the operation object 112 can be derived using the height level and the measured value of the distance to the bottom surface 324 of the operation object 112. That is, the robotic system 100 receives the measurement data of the bottom surface 324 of the operation object 112 by the distance measuring device 466, and the height 112h can be calculated from the received measurement data and the height location gripping level of the top surface 322 of the operation object 112. In a case where the height 112h is different from a value stored as the master data 252 of the operation object 112, the robotic system 100 can replace the master data 252 or update the master data 252 by adding the different value thereto. After actual dimensions of the operation object 112 are determined in this way, the robotic system 100 can calculate the space shape parameters of the pose when the operation object 112 is gripped in various directions. Therefore, by comparing these space shape parameters with the information on a space in the storage container 450 placed at the task location 116, the robotic system 100 can optimize and select a plan or pattern with which the operation object 112 is stored at a higher filling density in the storage container 450. At this time, when the end effector accesses the storage container 450, the robotic system 100 calculates the presence/absence of interference between the end effector and the storage container 450 or the already-stored objects. When the interference may occur, the pattern can be eliminated. Therefore, when a new pose or grip location different than the current grip location/orientation increases a filling rate of the storage container 450, the robotic system 100 can change remaining portions of the control sequence 472 corresponding to the first control sequence 422 or the second control sequence 424 in 4A and 4B and create an updated control sequence including an operation of shifting the operation object 112 so as to be a pose optimized for storage. On the other hand, in a case where the pose of the operation object 112 gripped at a current time point is optimal from a viewpoint of a storage efficiency, the robotic system 100 stores the gripped operation object 112 in the storage container 450 without changing the control sequence 472. In addition, in a case where the operation object 112 is to be shifted, the robotic system 100 operates the operation object 112 according to the updated control sequence 474. For example, the operation object 112 can be moved to a peripheral area of the shift location 118 after the scan. The robotic system 100 can orient the end effector according to a predetermined orientation for temporary placement, place the operation object 112 on a temporary placing table 468 accordingly, and release the grip. The temporary placing table is not particularly limited and can include, for example, a pedestal and the like which can place the operation object 112 so that at least two surfaces thereof are exposed. In some embodiments, the temporary placing table may be configured to hold the operation object 112 in a tilted state while supporting the operation object 112, thereby improving access to grip the object and increase stability during the gripping operation. The robotic system 100 can shift the operation object 112 by changing the orientation of the end effector and gripping surfaces of the operation object 112 different from the previous grip locations before temporarily placing the operation object 112. The robotic system 100 stores the shifted operation object 112 in the storage container 450. At this time, the end effector may be rotated or adjusted with respect to a target location without directly positioning the end effector at a time. In addition, multiple end effectors or multiple units may be provided, and control may be performed so that each end effector is properly used in relation to the size of the operation object 112. Further, in the above description, in order to execute the actions for the task 402, the robotic system 100 can track a current location , a set of coordinates corresponding to a coordinate system used by the robotic system 100 and/or a current pose of the operation object 112. For example, the robotic system 100 can track the current location/pose according to data from the location sensor 224 of 2 via the processor 202, for example. The robotic system 100 can place one or more portions of the robotic arm 414 , the link or the joint according to data from the location sensor 224. The robotic system 100 can further calculate the location/pose of the end effector, and thereby calculate the current location of the operation object 112 held by the end effector, based on the location and orientation of the robotic arm 414. Also, the robotic system 100 can track the current location based on processing other sensor readings , force readings or accelerometer readings, the executed actuation commands/settings, and/or the associated timings, or a combination thereof according to a dead-reckoning mechanism. Operational Flow Control Sequence Based on Confidence Measure 5A is a flow diagram of a method 500 illustrating an example process flow of the robotic system 100 in accordance with the embodiment of the present disclosure. The method 500 includes a procedure of deriving/calculating and implementing a control sequence based on a confidence measure to execute the task 402 of 4A. The confidence measure can be associated with determining the initial pose of the operation object 112. In addition, the method 500 can be implemented based on executing the instructions stored on one or more storage devices 204 with one or more processors 202. At block 501, the robotic system 100 can identify scanning fields of one or more imaging devices 222 of 2. For example, the robotic system 100 via, , one or more processors 202 can identify spaces that can be scanned by one or more imaging devices 222, such as the scanners 412 and 416 of 4A and 4B. The robotic system 100 can identify the scanning fields that are oriented in opposite directions and/or orthogonal directions according to orientations of the scanners 416. As illustrated in 4A and 4B, the scanners 416 can be arranged opposite to each other and/or facing each other, such as across a horizontal direction or across a vertical direction. Also, the scanners 416 can be arranged perpendicular to each other, such as one facing up or down and another facing a horizontal direction. For example, the robotic system 100 can identify the scanning fields according to the master data 252. The master data 252 can include grid locations, coordinates, and/or other markers representing the imaging devices 222 and/or the corresponding scanning fields. The master data 252 can be predetermined according to a layout and/or a physical dislocation of the imaging devices 222, the capabilities of the imaging device 222, environmental factors , lighting conditions and/or obstacles/structures, or a combination thereof. In addition, the robotic system 100 can implement a calibration process to identify the scanning fields. For example, the robotic system 100 can use the transfer unit 104 to place a known mark or code at a set of locations and determine whether the corresponding imaging device 222 accurately scans the known mark. The robotic system 100 can identify the scanning fields based on the locations of the known mark that resulted in accurate scanning results. At block 502, the robotic system 100 can scan designated areas. The robotic system 100 can generate via, , via a command/prompt sent by the processor 202 imaging data , the derived digital images and/or point clouds of one or more designated areas, such as the pick-up area and/or the placement area, using one or more imaging devices 222 , the scanners 412 of 4A and 4B and/or other area scanners. The imaging data can be communicated from the imaging devices 222 to the one or more processors 202. Accordingly, one or more processors 202 can receive the imaging data that represents the pick-up area including, , operation objects 112 before execution of the task, the shift area, and/or the placement area including, , operation objects 112 after execution of the task for further processing. At block 504, the robotic system 100 can identify the operation object 112, the associated locations , the start location 114 of 1 and/or the task location 116 of 1, and/or the initial poses of the operation objects 112. The robotic system 100 can analyze via, , the processor 202 the imaging data based on a pattern recognition mechanism and/or a recognition rule in order to identify outlines , perimeter edges and/or surfaces of the operation objects 112. The robotic system 100 can further identify the groupings of outlines and/or surfaces of the operation objects 112 based on, for example, a predetermined recognition mechanism, a recognition rule, and/or templates related to poses or outlines as corresponding to the various operation objects 112. For example, the robotic system 100 can identify the groupings of the outlines of the operation objects 112 that correspond to a pattern having, , the same values or values that vary at a known rate/pattern in the color, the brightness, the depth/location, and/or a combination thereof over the outlines of the operation objects 112. In addition, for example, the robotic system 100 can identify the groupings of the outlines and/or surfaces of the operation objects 112 according to predetermined shape/pose templates, images, or a combination thereof defined in the master data 252. From the operation objects 112 recognized in the pick-up area, the robotic system 100 can select one as the operation object 112 , according to a predetermined sequence or set of rules and/or templates of outlines of operation objects. For example, the robotic system 100 can select the operation object 112 according to the point cloud representing the distances/locations relative to a known location of the scanner 412. In addition, for example, the robotic system 100 can select the operation object 112 that is located at a corner/edge and has two or more surfaces that are exposed/shown in the imaging results. Further, the robotic system 100 can select the operation object 112 according to a predetermined pattern or sequence , left to right, nearest to farthest, and the like, relative to a reference location. For the selected operation object 112, the robotic system 100 can further process the imaging data in order to determine the start location 114 and/or an initial pose. For example, the robotic system 100 can determine the start location 114 by mapping a location , a predetermined reference point for the determined pose of the operation object 112 in the imaging data to a location in the grid used by the robotic system 100. The robotic system 100 can map the locations according to a predetermined calibration map. The robotic system 100 can process the imaging data of the placement areas to determine available/open spaces between already-placed objects. The robotic system 100 can map the outlines of the operation object 112 according to a predetermined calibration map for mapping image locations to real-world locations and/or coordinates used by the system. Based on the mapping, the robotic system 100 can determine the open spaces. The robotic system 100 can determine the open spaces as the space between the outlines furthermore, surfaces of the operation object 112 belonging to different groupings that each correspond to an already-placed object. The robotic system 100 can determine the open spaces suitable for the operation object 112 by measuring one or more dimensions of the open spaces and comparing the measured dimensions with one or more dimensions of the operation objects 112 , as stored in the master data 252. In addition, the robotic system 100 can select one of the suitable/open spaces as the task location 116 according to a predetermined pattern , left to right, nearest to farthest, bottom to top, and the like, relative to a reference location. The robotic system 100 can determine the task location 116 without processing the imaging data or in addition to processing the imaging data. For example, the robotic system 100 can place a set of objects at the placement area according to a predetermined control sequence and locations without re-imaging the area after each placement. Also, for example, the robotic system 100 can process the imaging data for performing multiple tasks , moving multiple operation objects 112, such as tasks for operation objects 112 located on a common layer/column of a stack. At block 522, for example, the robotic system 100 can determine an initial pose , an estimate of a stopped pose of the operation object 112 at the pick-up area based on processing the imaging data , the imaging data from the scanner 412. The robotic system 100 can determine the initial pose of the operation object 112 based on comparing the outline of the operation object 112 with outlines in predetermined pose templates of the master data 252 , comparing pixel values. For example, the templates of the predetermined pose can include a different potential arrangement of the outlines of the operation objects 112 according to corresponding orientations of expected operation objects 112. The robotic system 100 can identify the sets of outlines of the operation objects 112 , edges of an exposed surface, such as the first exposed surface 304 of 3A and/or 3C and/or the second exposed surface 306 of 3A that were previously associated with the selected operation object 112. The robotic system 100 can determine the initial pose by selecting one of the pose templates that corresponds to a lowest difference measurement between the compared outlines of the operation objects 112. For further example, the robotic system 100 can determine the initial pose of the operation object 112 based on physical dimensions of the operation object 112. The robotic system 100 can estimate physical dimensions of the operation object 112 based on the dimensions of the exposed surfaces captured in the imaging data. The robotic system 100 can measure a length and/or an angle for each outline of the operation object 112 in the imaging data and then map or transform the measured length to a real-world length or a standard length using a calibration map, a transformation table or process, a predetermined equation, or a combination thereof. The robotic system 100 can use the measured dimensions to identify the operation object 112 and/or the exposed surfaces corresponding to the physical dimensions. The robotic system 100 can identify the operation object 112 and/or the exposed surfaces by comparing the estimated physical dimensions with a set of known dimensions , a height, a length, and/or a width of the operation objects 112 and their surfaces in the master data 252. The robotic system 100 can identify the exposed surfaces and the corresponding pose using the matched set of dimensions. For example, the robotic system 100 can identify the exposed surface as either the top surface 322 of the operation object 302 of 3A or the bottom surface 324 of the operation object 302 of 3B , a pair of opposing surfaces in a case where the dimensions of the exposed surface match a length and a width for an expected operation object 112. Based on the orientation of the exposed surface, the robotic system 100 can determine the initial pose of the operation object 112 , either the first pose 312 or the third pose 316 of the operation object 302 in a case where the exposed surface faces upward. For example, the robotic system 100 can determine the initial pose of the operation object 112 based on a visual image of one or more surfaces of the operation object 112 and/or one or more markings thereof. The robotic system 100 can compare the pixel values within a set of connected outlines with predetermined marking-based pose templates of the master data 252. For example, the marking-based pose templates can include one or more unique markings of expected operation objects 112 in various different orientations. The robotic system 100 can determine the initial pose of the operation object 112 by selecting one of the surfaces, the surface orientations, and/or the corresponding poses that result in a lowest difference measurement for the compared images. At block 524, the robotic system 100 can calculate a confidence measure associated with the initial pose of the operation object 112. The robotic system 100 can calculate the confidence measure as a part of determining the initial pose. For example, the confidence measure can correspond to a measure of a difference between the outline of the operation object 112 and the outline in the selected template described above. In addition, for example, the confidence measure can correspond to a tolerance level associated with the estimated physical dimensions and/or the angles described above. Also, for example, the confidence measure can correspond to the difference measure between a visual marking in the imaging data and the template images described above. At block 506, the robotic system 100 can calculate a control sequence for executing the task 402 related to the operation object 112 , the first control sequence 422 of 4A, the second control sequence 424 of 4A, the control sequence 472 in 4B, and the like, and the control sequence 474 including a shift operation of the operation object 112 illustrated in 4B. For example, the robotic system 100 can create or derive the control sequence based on calculating a sequence of commands or settings, or a combination thereof, for the actuation devices 212 to operate the robotic arm 414 of 4A and 4B and/or the end effector. For some tasks, the robotic system 100 can calculate control sequences and setting values for manipulating the robotic arm 414 and/or the end effector and for moving the operation object 112 from the start location 114 to the task location 116, as necessary via the shift location 118. The robotic system 100 can implement a control sequence mechanism , a process, a function, an equation, an algorithm, a computer-generated/readable model, or a combination thereof configured to calculate a movement path in space. For example, the robotic system 100 can use A* algorithm, D* algorithm, and/or other grid-based searches so as to calculate the movement path through a space for moving the operation object 112 from the start location 114 to the task location 116 through one or more presentation poses/locations , one or more corresponding scan locations for the end effector, as necessary via the shift location 118. The control sequence mechanism can transform the movement path into the sequence of commands or settings, or a combination thereof, for the actuation devices 212 using a further process, function, or equation, and/or a mapping table. In using the control sequence mechanism, the robotic system 100 can calculate the control sequence that will manipulate the robotic arm 414 and/or the end effector and cause the operation object 112 to follow the calculated movement path. The robotic system 100 can selectively create or derive a control sequence based on the confidence measure. The robotic system 100 can calculate the control sequence that includes an approach location , the first approach location 432 of 4A and/or the second approach location 434 of 4A, one or more scan locations , the first presentation location 442 of 4A and/or the second presentation location 444 of 4A, or a combination thereof according to the confidence measure. For example, the robotic system 100 can calculate the approach location and/or the scan location according to a metric , a performance metric and/or a scan metric based on an outcome of comparing the confidence measure to a sufficiency threshold. The scan location can be for placing the end effector so as to present one or more surfaces of the operation object 112 before one or more corresponding scanners 416 for scanning the one or more identifiers 332 of the operation object 112 by, , placing/orienting the object in the scanning field. At block 532, the robotic system 100 can calculate via, , the processors 202 a set of available approach locations. The available approach locations can correspond to open or non-occupied spaces about the start location 114 sufficient for placing the end effector. In addition, the robotic system 100 can place the end effector at a selected approach location for contacting and gripping the operation object 112 without disturbing other operation objects 112. For example, the robotic system 100 can calculate the set of available approach locations by calculating separation distances between the outline of the operation object 112 and the outlines of adjacent operation objects 112. The robotic system 100 can compare the separation distances with a predetermined set of distances that correspond to a physical size/shape of the end effector and/or various orientations thereof. The robotic system can identify each of the available approach locations in a case where the corresponding separation distances exceed the predetermined set of distances corresponding to the size of the end effector. In decision block 534, the robotic system 100 can compare the confidence measure to one or more sufficiency thresholds to determine whether or not the confidence measure is satisfied. In a case where the confidence measure satisfies the sufficiency threshold , when the confidence measure exceeds the required sufficiency threshold, as illustrated at block 536, the robotic system 100 can calculate the control sequence , the first control sequence 422 based on a performance metric. When the confidence measure satisfies the sufficiency threshold, the robotic system 100 can determine that the initial pose is suitable and calculate the control sequence without considering a scan metric that corresponds to a likelihood for scanning at least one identifier 332 of the operation object 112 and/or without considering a likelihood that the initial pose may be inaccurate. As an example, the robotic system 100 can calculate candidate plans at block 542. Each of the candidate plans can be an instance of a control sequence that corresponds to a unique combination of an available approach location and a scan location , corresponding presentation location/orientation for the operation object 112. The robotic system 100 can calculate the location 334 of the identifier 332 according to the initial pose by rotating the locations 334 of the identifier 332 or a corresponding model/pose in the master data 252. The robotic system 100 can eliminate available approach locations that causes the end effector to cover the location 334 of the identifier 332 , to be placed directly over, in front of, and/or within a threshold distance from the location of the identifier. The robotic system 100 can calculate a candidate plan for each remaining available approach location in the set , a calculation result of block 532. For each of the candidate plans, the robotic system 100 can further calculate a unique scan location according to the available approach location. The robotic system 100 can calculate the scan location based on rotating and/or moving a model of the operation object 112, and thereby the surface corresponding to the location 334 of the identifier 332 is in the scanning field and faces the corresponding scanner 416. The robotic system 100 can rotate and/or move the model according to a predetermined process, equation, function, and the like. At block 544, the robotic system 100 can calculate a performance metric for each candidate plan. The robotic system 100 can calculate the performance metric that corresponds to a throughput rate for completing the task 402. For example, the performance metric can be associated with a movement distance of the operation object 112, an estimated movement duration, the number of commands and/or setting changes for the actuation devices 212, a completion rate , a rate complementary to a piece-loss amount, or a combination thereof for the candidate plan. The robotic system 100 can calculate the corresponding values for the candidate control sequence using one or more measured or known data , an acceleration/speed associated with settings/commands and/or piece-loss rate associated with a grip surface and/or a maneuver and a predetermined calculation process, equation, function, and the like. At block 546, the robotic system 100 can select the candidate plan with the maximum performance metric i. e. , along with the corresponding approach location as the control sequence. For example, the robotic system 100 can select, as the control sequence, the candidate plan that corresponds to the highest completion rate, the shortest movement distance, the lowest number of commands and/or setting changes, the fastest movement duration, or a combination thereof among the set of candidate plans. Accordingly, the robotic system 100 can select the available approach location in the set that corresponds to the highest performance metric as the approach location. In comparison, the robotic system 100 can calculate the candidate plan according to a different measure in a case where the confidence measure does not satisfy the sufficiency threshold , the confidence measure is less than the required sufficiency threshold. As illustrated at block 538, the robotic system 100 can calculate the control sequence , the second control sequence 424 based on a scan metric. The scan metric is a value , a binary value or a non-binary score/percentage that corresponds to a likelihood that at least one of the identifiers 332 of the operation object 112 remains uncovered by the end effector and is to be scannable, regardless of whether or not the initial pose is accurate. For example, the robotic system 100 can prioritize the scan metric , satisfy first and/or give it a heavier weight over the performance metric in a case where the confidence measure does not satisfy the sufficiency threshold. Accordingly, the robotic system 100 can calculate the control sequence that includes one or more scan locations for providing i. e. , in the scanning field and/or facing the corresponding scanner at least one uncovered identifier 332 of the operation object 112 in front of one or more scanners 416. 5B is a flow diagram illustrating an example process flow of the robotic system in accordance with the embodiment of the present disclosure, and illustrates a flow diagram 538 for selectively calculating a control sequence , one or more locations for the end effector based on a scan metric. In this example, calculating the control sequence based on a scan metric can include calculating a set of locations of the exposed identifiers 332 as illustrated in block 552. The robotic system 100 can calculate the set of locations of the exposed identifiers 332 , the locations 334 of the identifier 332 that remain uncovered or scannable with the end effector in the grip location relative to the initial pose of the operation object 112. The robotic system 100 can calculate the location 334 of the exposed identifier 332 for each of the available approach locations. The locations 334 of the exposed identifiers 332 can correspond to locations 334 of the identifiers 332 of the operation object 112 that remain uncovered with the end effector at the corresponding approach location according to a hypothesis that the initial pose is accurate. As described above for block 542, the master data 252 can include a computer model or a template , offset measurements relative to one or more edges and/or images of the operation object 112 in which the location 334 of the identifier 332 for each of the expected operation objects 112 is described. The robotic system 100 can calculate the set of locations of the exposed identifiers 332 based on rotating and/or moving the predetermined model/template in the master data 252 to match the initial pose. The robotic system 100 can eliminate the approach locations that cause the end effector to cover the location 334 of the identifier 332 , with the end effector placed directly over, in front of, and/or within a threshold distance from the location of the identifier. In other words, the robotic system 100 can eliminate the available approach locations that are directly over, in front of, and/or within a threshold distance from the locations 334 of the identifiers 332. At block 554, the robotic system 100 can calculate a set of locations 334 of alternative identifiers 332. The robotic system 100 can calculate the set of locations 334 of the alternative identifiers 332 for poses alternative to the initial pose. For each of the available approach locations, the robotic system 100 can calculate alternative poses, and for each of the alternative poses, the robotic system 100 can calculate locations of the alternative identifiers 332. Accordingly, the locations of the alternative identifiers 332 can correspond to the locations 334 of the identifiers 332 of the operation objects 112 that remain uncovered with the end effector at the corresponding approach location according to a hypothesis that the initial pose is not accurate. As described above for the locations 334 of the exposed identifiers 332, the robotic system 100 can calculate the locations 334 of the alternative identifiers 332 based on rotating and/or moving the predetermined model/template in the master data 252 according to the alternative pose. At block 556, the robotic system 100 can calculate an exposure likelihood for each of the approach locations, each of the alternative poses, each of the identifiers 332 of the operation object 112, or a combination thereof. The exposure likelihood represents a likelihood that one or more identifiers of the operation object 112 remain exposed and scannable with the end effector gripping the operation object 112 from the corresponding approach location. The exposure likelihood can represent both the scenario that the initial pose is accurate and the scenario that the initial pose is not accurate. In other words, the exposure likelihood can represent the likelihood that one or more identifiers of the operation object 112 remain exposed and scannable even if the initial pose is inaccurate. For example, the robotic system 100 can calculate the exposure likelihood as a conditional certainty, such as a probabilistic value corresponding to a particular condition , a unique instance of the approach location, the alternative pose, the identifier of the operation object 112, or a combination thereof. The robotic system 100 can calculate the exposure likelihood based on combining via, , adding and/or multiplying the conditional certainty with a certainty/likelihood that the particular condition is true , a value close to the confidence measure. The robotic system 100 can calculate the exposure likelihood based on adding the certainty for each of the identifiers considered to be exposed when multiple identifiers are exposed for the considered approach location and/or the considered pose. The robotic system 100 can calculate the exposure likelihood based on combining the certainty values based on locations of the exposed identifiers and locations of the alternative identifiers, for each of potential poses for a considered approach location. For example, the robotic system 100 can calculate the exposure likelihood using the certainties for locations of the exposed identifiers and locations of the alternative identifiers with opposing signs , positive and negative. The robotic system 100 can calculate the exposure likelihood based on adding the magnitudes of the two certainties and/or adding the certainties with the signs. The overall magnitude can represent an overall likelihood that one or more identifiers 332 of the operation object 112 remain to be scannable, and the signed/vectored likelihood can represent a likelihood that one or more identifiers of the operation object 112 remain to be scannable even if the initial pose was inaccurate. Accordingly, an approach location would be ideal when the overall magnitude is higher, and the signed/vectored likelihood is closer to zero, such as representing similar chances that the identifier 332 of the operation object 112 would be scannable regardless of the accuracy for the initial pose. At block 558, the robotic system 100 can select an approach location. The robotic system 100 can select, as the approach location, the available approach location that includes the location 334 of the uncovered identifier 332 in both a set of the exposed identifiers 332 , a set of estimated locations of the identifiers 332 of the operation object 112 according to a hypothesis that the initial pose is accurate and a set of the alternative identifiers 332 , one or more sets of estimated locations of the identifiers 332 of the operation object 112 according to a hypothesis that the initial pose is not accurate. In other words, the robotic system 100 can select the approach location that would leave at least one identifier 332 exposed and scannable regardless of the accuracy of the initial pose. The robotic system 100 can select, as the approach location, the available approach location that corresponds to the exposure likelihood matching and/or closest to a target condition, such as the largest overall magnitude and/or the signed/vectored likelihood that is closer to zero. The robotic system 100 can calculate a scan likelihood , a likelihood that an exposed identifier 332 of the operation object 112 is successfully scanned based on the exposure likelihood. For example, the robotic system 100 can combine the exposure likelihood with an evaluation value , a tracked rate of successful scans, a physical size, and/or a type of the identifier 332 associated with the corresponding exposed identifier 332 of the operation object 112. The robotic system 100 can select, as the approach location, the available approach location that corresponds to the highest scan likelihood. The robotic system 100 can compare the set of the exposed identifier 332 to the set of the alternative identifier 332 to determine whether the set of the exposed identifier 332 and the set of the alternative identifier 332 include locations on opposing surfaces of the operation object 112 , between the first pose 312 and the third pose 316. Accordingly, the robotic system 100 can select an available approach location that corresponds to a third surface , one of the peripheral surfaces 326 of the operation object 302 that is orthogonal to the two opposing surfaces. At block 560, in a case where the confidence measure does not satisfy the sufficiency threshold, the robotic system 100 can create or derive candidate control sequences based on the selected approach location. The robotic system 100 can calculate the candidate control sequences that include one or more scan locations for the end effector that correspond to one or more presentation locations/orientations for placing the identifiers 332 of the operation object 112 in both the set of the exposed identifier 332 and the set of the alternative identifier 332. In other words, the robotic system 100 can calculate the candidate control sequences that can scan the operation object 112 regardless of the accuracy of the initial pose. The robotic system 100 can create or derive the candidate control sequences that account for the locations 334 of the identifiers 332 in both the set of the exposed identifier 332 and the set of the alternative identifiers 332. For example, the robotic system 100 can calculate the candidate control sequences that account for the locations of the identifiers 332 having a likelihood, on opposing and/or orthogonal surfaces. Accordingly, the robotic system 100 can account for an opposing pose , a pose oriented in an opposite direction where the outline of the operation object 112 is placed so as to be the same from a visual recognition location/angle and/or other rotated poses in addition to the initial pose. Referring back to 3A and 3C as an illustrative example, the robotic system 100 can calculate the candidate control sequences that account for both the first pose 312 and the third pose 316 in a case where the grip location corresponds to one of the peripheral surfaces 326 of the operation object 302. In order to account for multiple possible poses , erroneous estimation of the initial pose, the robotic system 100 can calculate a scanning pose for placing the identifiers 332 of the operation object 112 in both the set of the exposed identifiers 332 and the set of the alternative identifiers 332. As illustrated at block 562, the robotic system 100 can calculate a set of candidate poses for the operation object 112 in the scanning fields or through the scanning fields. When the approach location is selected, the robotic system 100 can calculate candidate scan locations as described above for block 542, such as by rotating and/or moving a model of the location 334 of the identifier 332 so as to place the location 334 of the identifier 332 in the scanning field. At block 564, the robotic system 100 can map the set of the exposed identifier 332 and the set of the alternative identifier 332 to each of the candidate scan locations. The robotic system 100 can map the set of the exposed identifier 332 based on rotating the model of the location 334 of the identifier 332 starting from the initial pose. The robotic system 100 can map the set of the alternative identifier 332 based on rotating the model of the location 334 of the identifier 332 starting from one of the alternative poses , the opposing pose. When the locations 334 of the identifiers 332 are mapped, at block 568, the robotic system 100 can compare the locations 334 and/or orientations of the identifiers 332 of the operation object 112 in both the set of the exposed identifier 332 and the set of the alternative identifier 332 with the scanning fields. At decision block 570, the robotic system 100 can determine whether in the candidate pose, the identifiers 332 of the operation object 112 in both the set of the exposed identifier 332 and the set of the alternative identifier 332 simultaneously are presented to the scanners. At block 572, the robotic system 100 can identify, as the scanning pose, the candidate poses that simultaneously present the identifiers 332 of the operation object 112 in both the set of the exposed identifiers 332 and the set of the alternative identifiers 332 to different scanners/scanning fields. For example, in a case where the grip location corresponds to one of the peripheral surfaces 326 of the operation object 112 with the locations of the operation object 112 in the set of the exposed identifier 332 and the set of the alternative identifier 332 of the operation object 112 being on opposing surfaces, the robotic system 100 can identify the scanning pose for placing the operation object 112 between a pair of opposing/facing scanners with each of the opposing surfaces of the operation object 112 facing one of the scanners. At block 574, in a case where none of the candidate poses simultaneously presents the identifiers 332 of the operation object 112 in both the set of the exposed identifier 332 and the set of the alternative identifier 332 of the operation object 112, the robotic system 100 can calculate multiple scan locations , a first scan location and a second scan location that each present at least one identifier 332 of the operation object 112 from the set of the exposed identifier 332 and the set of the alternative identifier 332 of the operation object 112. For example, the first scan location can present the locations 334 of one or more identifiers 332 in the set of the identifiers 332 of the exposed operation object 112 to one of the scanners, and the second scan location can present the locations 334 of one or more identifiers 332 in the set of the identifiers 332 of the alternative operation object 112 to one of the scanners. The second scan location can be associated with rotating the end effector about an axis, translating the end effector, or a combination thereof from the first scan location. Referring back to the example illustrated in 4A and 4B, the second control sequence 424 can correspond to the second approach location 434 that corresponds to the third surface , one of the peripheral surfaces 326 of the operation object 112 that is orthogonal to the two opposing surfaces , for the first pose 312 and the third pose 316 as described above. Accordingly, the first scan location can correspond to one first location of the second presentation locations 444 that places a surface , estimated to be the bottom surface 324 of the operation object 112 corresponding to the initial pose , the first pose 312 above an upward-facing scanner 416 and facing the scanner. The second scan location can correspond to one second location of the second presentation locations 444 that rotates the operation object 112 by 90 degrees in a counter-clockwise direction relative to an overall movement direction , generally from the start location 114 to the task location 116. Accordingly, the second scan location can correspond to the second presentation location 444 that places a surface , determined to be the bottom surface 324 of the operation object 112 corresponding to the alternative pose , the third pose 316 in front of a horizontally facing scanner 416 and in a vertical orientation facing this scanner 416. According to the resulting scanning pose and/or the set of scan locations, the robotic system 100 can create or derive the candidate control sequence. The robotic system 100 can calculate the candidate plans to place the end effector at the selected approach location, thereby to contact and grip the operation object 112, and lift and move the operation object 112 to the identified scanning pose and/or the set of scan locations, using one or more mechanisms described above , the A* mechanism. For example, when the scanning pose is identified, the robotic system 100 can calculate the candidate plans to establish the scanning pose for the operation object 112 in the scanning fields or through the scanning fields. In a case where the robotic system 100 does not identify the scanning pose, the robotic system 100 can calculate the candidate plans to move/orient the end effector sequentially through the set of multiple scan locations, thereby sequentially moving/rotating the operation object 112 according to multiple presentation locations/orientations. At block 576, the robotic system 100 can create again or update the scanning likelihood for each of the candidate control sequences. The robotic system 100 can update the scanning likelihood based on combining the various likelihoods and/or preferences as described above for block 544 , probabilities and/or scores for the approach location, the scan location, the utilized scanner 416, the identifier 332 considered to be exposed, an associated error and/or a loss rate, or a combination thereof, but with respect to the scan metric instead of the performance metric. At block 578, the robotic system 100 can create or derive the control sequence based on selecting the candidate plan according to the scanning likelihood. The robotic system 100 can select, as the control sequence, the candidate plan that has maximum scanning likelihood among the candidate plans. For example, the robotic system 100 can select the candidate plan that has the highest likelihood of placing at least one of the locations 334 of the exposed identifiers 332 and at least one of the locations 334 of the alternative identifiers 332 in one or more scanning fields , in front of one or more scanners 416 during the movement of the operation object 112 for scanning in the space between the start location 114 and the task location 116, for example. In a case where two or more candidate plans correspond to scanning likelihoods within a relatively small difference value , a predetermined threshold, the robotic system 100 can calculate and evaluate the performance metric corresponding to the corresponding candidate plan , as described above for blocks 544 and 546. The robotic system 100 can select, as the control sequence, the candidate plan that is closest to the target condition. The robotic system 100 can deviate from the illustrated example flow. For example, the robotic system 100 can select the approach location as described above. Based on the selected approach location, the robotic system 100 can grip the operation object 112 and implement a predetermined set of maneuvers, such as to lift, reorient, horizontally move, place back down and release, or a combination thereof. During or after the predetermined set of maneuvers, the robotic system 100 can re-image or scan the pick-up area via, , looping back to block 502 and redetermine the initial pose and the confidence measure via, , block 522 and block 524. Returning back to 5A, at block 508, the robotic system 100 can begin implementing the resulting control sequence. The robotic system 100 can implement the control sequence based on operating the one or more processors 202 to send the commands and/or settings of the control sequence to other devices , the corresponding actuation devices 212 and/or other processors to execute the tasks 402 and 404. Accordingly, the robotic system 100 can execute the control sequence by operating the actuation devices 212 according to the sequence of commands or settings or combination thereof. For example, the robotic system 100 can operate the actuation devices 212 to dispose the end effector at the approach location around the start location 114, contact and grip the operation object 112, or perform a combination thereof. At block 582, the robotic system 100 can move the end effector to the scan location, thereby moving the operation object 112 to the presentation location/orientation. For example, after or along with lifting the operation object 112 from the start location 114, the robotic system 100 can move the end effector to establish the scanning pose for the operation object 112. In addition, the robotic system 100 can move the end effector to the first scan location. At block 584, the robotic system 100 can operate the scanners 416 to scan the operation object 112. For example, one or more processors 202 can send a command to the scanners 416 to implement a scan and/or send a query to the scanners 416 to receive a scan status and/or a scanned value. At block 585 and the like, in a case where the control sequence includes the scanning pose, the robotic system 100 can implement the control sequence to move the operation object 112 in the scanning pose across the scanning fields in a direction orthogonal to orientations of the scanning fields. While the operation object 112 is moved, the scanners 416 can simultaneously and/or sequentially scan multiple surfaces for multiple possible locations 334 of the identifier 332 of the operation object 112. In decision block 586, the robotic system 100 can evaluate the scan result , status and/or the scanned value to determine whether the operation object 112 is scanned. For example, the robotic system 100 can verify the scan result after implementing the control sequence up to the first scan location. At block 588 and the like, in a case where the scan result indicates a successful scan of the operation object 112 , the status corresponds to detection of a valid code/identifier and/or the scanned value matches the identified/expected operation object 112, the robotic system 100 can move the operation object 112 to the task location 116. Based on the successful scan, the robotic system 100 can ignore any subsequent scan location , the second scan location and directly move the operation object 112 to the task location 116. In a case where the scan result indicates an unsuccessful scan of the operation object 112, the robotic system 100 can determine at decision block 590 whether the current scan location is the last one in the control sequence. In a case where it is not the last control sequence, the robotic system 100 can move the operation object 112 to the next presentation location/orientation as represented by a loop back to block 582. In a case where the current scan location is the last one in the control sequence, the robotic system 100 can implement one or more remedial actions as illustrated at block 592. The robotic system 100 can stop and/or cancel the control sequence in a case where the scan results for all of the scan locations in the control sequence indicate failed scans. The robotic system 100 can generate an error status/message for notifying an operator. The robotic system 100 can place the operation object 112 inside of an area i. e. , at a location different from the start location 114 and the task location 116 designated for the operation object 112 that failed to be scanned. Based on either successfully completing the tasks 402 and 404 i. e. , successfully scanning the operation object 112 and placing the operation object at the task location 116 or implementing the remedial actions, the robotic system 100 can move on to the next task/operation object 112. The robotic system 100 can scan again the designated area as illustrated by a loop back to block 502 and select the next operation object 112 using the existing imaging data as illustrated by a loop back to block 504. Scanning the operation object 112 in the air , at a location between the start location 114 and the task location 116 provides improved efficiency and speed for performing the tasks 402 and 404. By calculating the control sequence for cooperating with the scanner 416 as the control sequence that includes the scan locations, the robotic system 100 can effectively combine the task for moving the operation object 112 with the task for scanning the operation object 112. Moreover, creating or deriving a control sequence according to the confidence measure of the initial pose further improves efficiency, speed, and accuracy for the scan task. As described above, the robotic system 100 can create or derive the control sequence for accounting for alternative orientations that correspond to the scenario of the initial pose being inaccurate. Accordingly, the robotic system 100 can increase the likelihood of accurately/successfully scanning the operation object 112 even with pose determination errors, such as due to calibration errors, unexpected poses, unexpected lighting conditions, and the like The increased likelihood in accurate scans can lead to increased overall throughput for the robotic system 100 and further reduce operator efforts/interventions.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/53/137/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9822537302970886,
                    "F_SPEC_PARAMS": [
                        "save labor"
                    ],
                    "S_SPEC_PARAMS": [
                        "sophistication",
                        "larger and/or more complex tasks",
                        "difficult to replace human involvement,",
                        "granularity of control and flexibility"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "In order to automate and save labor from warehousing to delivering of an article, the automatic distribution system includes a carrying container storage mechanism that temporarily stores a carrying container; and an automatic article delivery mechanism in which articles in the carrying container are automatically collected in a shipment container based on delivery information."
                    ],
                    "S_SENTS": [
                        "However, despite technological advancement, robots often lack the sophistication required for replicating human involvement task in order to execute larger and/or more complex tasks.",
                        "For this reason, automation and advanced functionality in robotic systems are not yet sufficient, and there are many tasks that are difficult to replace human involvement, and robotic systems lack the granularity of control and flexibility in actions to be executed."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "F_SIM_SCORE": 0.6039926409721375,
                    "S_TRIZ_PARAMS": [
                        "Adaptability",
                        "Productivity",
                        "Complexity of Control"
                    ],
                    "S_SIM_SCORE": 0.5433403849601746,
                    "GLOBAL_SCORE": 1.6559202432632447
                },
                "sort": [
                    1.6559203
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11345041-20220531",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11345041-20220531",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-04-23",
                    "PUBLICATION_DATE": "2022-05-31",
                    "INVENTORS": [
                        "Satoru Shimizu",
                        "Kenta Kawamoto",
                        "Yoshiaki Iwai"
                    ],
                    "APPLICANTS": [
                        "Sony Corporation    ( Tokyo , JP )"
                    ],
                    "INVENTION_TITLE": "Robot device, method of controlling the same, computer program, and robot system",
                    "DOMAIN": "B25J 91697",
                    "ABSTRACT": "Provided is a robot device including an image input unit for inputting an image of surroundings, a target object detection unit for detecting an object from the input image, an object position detection unit for detecting a position of the object, an environment information acquisition unit for acquiring surrounding environment information of the position of the object, an optimum posture acquisition unit for acquiring an optimum posture corresponding to the surrounding environment information for the object, an object posture detection unit for detecting a current posture of the object from the input image, an object posture comparison unit for comparing the current posture of the object to the optimum posture of the object, and an object posture correction unit for correcting the posture of the object when the object posture comparison unit determines that there is a predetermined difference or more between the current posture and the optimum posture.",
                    "CLAIMS": "1. A mobile apparatus, comprising: circuitry configured to: input environment information regarding an environment of the mobile apparatus including an object and posture data of the object, including a set of coordinates of the object in the environment, to a learning model that has learned to output target posture data regarding a target posture of the object based on the inputted environment information and the posture data of the object, the learning model repeating observations of the object to obtain a plurality of sets of the coordinates of the object using the environment information and the posture data until a total number of the observations reaches a predetermined number; in response to the total number of the observations reaching the predetermined number, acquire the target posture data of the object from the learning model, by setting, as the target posture data, the set of the coordinates of the object having been observed a most number of times through the repeating of the observations of the object; acquire an image of the environment including the object; obtain current posture data regarding a current posture of the object based on the acquired image of the environment, the current posture data including the set of the coordinates of the object; and perform a process of correcting the posture of the object based on the target posture data and the current posture data by comparing the set of the coordinates of the object for the target posture data and the current posture data, such that the current posture of the object becomes closer to the target posture. 2. The mobile apparatus according to claim 1, wherein the circuitry is further configured to: acquire the environment information including the position of the object; and input the environment information on to the learning model. 3. The mobile apparatus according to claim 2, wherein the circuitry is further configured to: detect the position of the object from the image; acquire the environment information including the position of the object; and input the environment information to the learning model. 4. The mobile apparatus according to claim 1, wherein the learning model is updated to output the target posture data based on a frequency of combination of the environment information and the posture data of the object. 5. The mobile apparatus according to claim 1, wherein the learning model is updated to output the target posture data based on user input. 6. The mobile apparatus according to claim 1, wherein the circuitry is further configured to: obtain posture change data indicating an amount of a change of the posture of the object to be corrected in a case that a predetermined difference exists between the target posture data and the current posture data; and perform the process of correcting the posture of the object based on the posture change data. 7. The mobile apparatus according to claim 6, wherein the circuitry is further configured to: cause an end effector to move as the process of correcting the posture of the object based on the posture change data. 8. A method comprising: inputting, by circuitry of a mobile apparatus, environment information regarding an environment of the mobile apparatus including an object and posture data of the object, including a set of coordinates of the object in the environment, to a learning model that has learned to output target posture data regarding a target posture of the object based on the inputted environment information and the posture data of the object, the learning model repeating observations of the object to obtain a plurality of sets of the coordinates of the object using the environment information and the posture data until a total number of the observations reaches a predetermined number; in response to the total number of the observations reaching the predetermined number, acquiring, by the circuitry, the target posture data of the object from the learning model, by setting, as the target posture data, the set of the coordinates of the object having been observed a most number of times through the repeating of the observations of the object; acquiring an image of the environment including the object; obtaining, by the circuitry, current posture data regarding a current posture of the object based on the acquired image of the environment, the current posture data including the set of the coordinates of the object; and performing, by the circuitry, a process of correcting the posture of the object based on the target posture data and the current posture data by comparing the set of the coordinates of the object fax the target posture data and the current posture data, such that the current posture of the object becomes closer to the target posture. 9. The method of claim 8, further comprising: acquiring the environment information including the position of the object; and inputting the environment information to the learning model. 10. The method of claim 9, further comprising: detecting the position of the object from the image; acquiring the environment information including the position of the object; and inputting the environment information to the learning model. 11. The method of claim 8, wherein the learning model is updated to output the target posture data based on a frequency of combination of the environment information and the posture data of the object. 12. The method of claim 8, wherein the learning model is updated to output the target posture data based on user input. 13. The method of claim 8, further comprising: obtaining posture change data indicating an amount of a change of the posture of the object to be corrected in a case that a predetermined difference exists between the target posture data and the current posture data; and performing the process of correcting the posture of the object based on the posture change data. 14. The method of claim 13, further comprising: causing an end effector to move as the process of correcting the posture of the object based on the posture change data. 15. The mobile apparatus according to claim 1, wherein the circuitry is configured to se a posture learned flag indicating an optimum posture can be calculated by sufficiently observing the object in the environment. 16. The mobile apparatus according to claim 15, wherein the circuitry is configured to set the posture learned flag in response to determining that a total number of observation of the environment has reached the predetermined number. 17. The mobile apparatus according to claim 1, wherein the circuitry is configured to: determine whether a predetermined difference or more exists between the target posture data and the current posture data; and perform the process of correcting the posture of the object in a case where the predetermined difference or more is determined to exist between the target posture data and the current posture data. 18. The method of claim 8, further comprising: determining, using the circuitry, whether a predetermined difference or more exists between the target posture data d the current posture data; and performing, using the circuitry, the process of correcting the posture of the object in a case where the predetermined difference or more is determined to exist between the target posture data and the current posture data.",
                    "STATE_OF_THE_ART": "The present technology disclosed in this specification relates to a robot device, which works to autonomously arrange a comfortable environment for a user, a method of controlling the same, a computer program, and a robot system, and more particularly, to a robot device that autonomously arranges placing positions and postures of objects to be frequently used so that a user can use the objects without difficulty, a method of controlling the same, a computer program, and a robot system. Recently, robot devices for performing various tasks have been implemented. However, it is difficult for many robot devices to operate an object serving as an operation target in an optimum posture according to content of its operation or a surrounding environment. Objects to be frequently used are normally placed in personally determined positions so that persons with disabilities, persons with physical disabilities, or disabled elders can use the objects without difficulty. Because placing positions and postures of these objects are very important for these users, robot devices capable of autonomously arranging comfortable environments for the users are desired. In addition, when an object held by a robot is handed over to a user, the robot capable of handing over the object to the user in a posture in which the user can easily hold or use the object is desired. For example, a cleanup robot for keeping daily goods and putting the daily goods into an empty space according to an instruction from a user or autonomously has been proposed for example, see Japanese Patent Application Laid-Open 2007-152443. However, the cleanup robot does not consider an optimum position or posture of the object when putting the object into the empty space. In addition, an information processing apparatus for generating an environment map on the basis of an image captured by a camera has been proposed for example, see Japanese Patent Application Laid-Open 2008-304268. This information processing apparatus uses an ontology map to limit a search region of a target object to be recognized for example, a user's face is likely to be above a sofa or in front of a television TV. This information processing apparatus can be applied to an agent movable body such as a robot provided with a camera. However, a posture of the target object is not considered. In addition, a goods management system, which has a shape database of a target object, determines a posture at the time of the installation or the like after grasping/transporting the target object on the basis of information of the database, and enables a robot to move or transport the target object, has been proposed for example, Japanese Patent 373854. However, the goods management system only grasps, transports, and installs the object. A process of autonomously arranging a comfortable environment for a user and exchanging the object with the user is not considered. In addition, a correct posture of the object in a complex environment is not considered.",
                    "SUMMARY": [
                        "It is desirable to provide an excellent robot device, a method of controlling the same, a computer program, and a robot system that can enable placing positions and postures of objects that are frequently used to be autonomously arranged so that a user can use the objects without difficulty. Also, it is desirable to provide an excellent robot device, a method of controlling the same, a computer program, and a robot system that can enable an object held by a robot to be easily held by a user or handed over to the user in a posture in which the user can easily use the object. According to a first embodiment of the present technology, there is provided a robot device including:an environment map including surrounding environment information;an image input unit for inputting an image of surroundings;a target object detection unit for detecting an object from the input image of the image input unit;an object position detection unit for detecting a position of the object detected by the target object detection unit;an environment information acquisition unit for acquiring surrounding environment information of the position of the object detected by the object position detection unit from the environment map;an optimum posture acquisition unit for acquiring an optimum posture corresponding to the surrounding environment information acquired by the environment information acquisition unit for the object detected by the target object detection unit;an object posture database for retaining an optimum posture of each object for each environment;an object posture detection unit for detecting a current posture of the object from the input image of the image input unit;an object posture comparison unit for comparing the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in the object posture database; and an object posture correction unit for correcting the posture of the object when the object posture comparison unit determines that there is a predetermined difference or more between the current posture of the object and the optimum posture. According to a second embodiment of the present technology based on the robot device of the first embodiment, the robot device further includes: a task execution unit for executing a task based on an instruction of a user. When the task execution unit is not executing the task, the detection of the current posture of the object by the object posture detection unit, the comparison with the optimum posture by the object posture comparison unit, and the correction of the posture of the object by the object posture correction unit are configured to be autonomously performed. According to a third embodiment of the present technology based on the robot device of the first embodiment, the robot device further includes: a moving unit for moving a position of the robot device. The object posture detection unit is configured to detect the current posture of the object while the moving unit moves a current position of the robot device. According to a fourth embodiment of the present technology based on the robot device of the first embodiment, the optimum posture acquisition unit is configured to acquire an optimum posture of each object by learning and retain the acquired optimum posture in the object posture database. According to a fifth embodiment of the present technology based on the robot device of the fourth embodiment, the object posture database is configured to further retain the optimum posture of each object taught from a user. According to a sixth embodiment of the present technology based on the robot device of the first embodiment, the robot device further includes: an environment map update unit for updating the environment map on a basis of the input image of the image input unit. According to a seventh embodiment of the present technology based on the robot device of the first embodiment, the optimum posture acquisition unit is configured to acquire an optimum posture for placing the object when arranging a comfortable environment for a user and an optimum posture of the object when handing the object over to the user. According to an eighth embodiment of the present technology based on the robot device of the first embodiment, the environment information acquisition unit is configured to acquire the surrounding environment information of the position of the object detected by the object position detection unit from the environment map including the surrounding environment information. According to a ninth embodiment of the present technology based on the robot device of the first embodiment, the object posture comparison unit is configured to compare the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in the object posture database retaining the optimum posture of each object for each environment. According to a tenth embodiment of the present technology, there is provided a method of controlling a robot device, including:inputting an image of surroundings;detecting an object from the image input in the step of inputting;detecting a position of the object detected in the step of detecting an object;acquiring surrounding environment information of the position of the object detected in the step of detecting from an environment map including the surrounding environment information;acquiring an optimum posture corresponding to the surrounding environment information acquired in the step of acquiring surrounding environment information for the object detected in the step of detecting an object;retaining an optimum posture of each object for each environment in an object posture database;detecting a current posture of the object from the image input in the step of inputting;comparing the current posture of the object detected in the step of detecting a current posture to the optimum posture of the object retained in the object posture database; andcorrecting the posture of the object when it is determined in the step of comparing that there is a predetermined difference or more between the current posture of the object and the optimum posture. According to an eleventh embodiment of the present technology, there is provided a computer program, written in a computer-readable format, for controlling a robot device and causing a computer to function as:an environment map including surrounding environment information;an image input unit for inputting an image of surroundings;a target object detection unit for detecting an object from the input image of the image input unit;an object position detection unit for detecting a position of the object detected by the target object detection unit;an environment information acquisition unit for acquiring surrounding environment information of the position of the object detected by the object position detection unit from the environment map;an optimum posture acquisition unit for acquiring an optimum posture corresponding to the surrounding environment information acquired by the environment information acquisition unit for the object detected by the target object detection unit;an object posture database for retaining an optimum posture of each object for each environment;an object posture detection unit for detecting a current posture of the object from the input image of the image input unit;an object posture comparison unit for comparing the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in the object posture database; andan object posture correction unit for correcting the posture of the object when the object posture comparison unit determines that there is a predetermined difference or more between the current posture of the object and the optimum posture. The computer program according to the eleventh embodiment of the present technology is defined as a computer program, written in a computer-readable format, for implementing a predetermined process on a computer. In other words, it is possible to exhibit a cooperative function on the computer and obtain the same functional effect as in the robot device according to the first embodiment of the present technology by installing the computer program according to the eleventh embodiment in the computer. According to a twelfth embodiment of the present technology, there is provided a robot system including:an environment map constructed on an external device and including surrounding environment information;an object posture database, constructed on an external device, for retaining an optimum posture of each object for each environment; anda robot device including an image input unit for inputting an image of surroundings, a target object detection unit for detecting an object from the input image of the image input unit, an object position detection unit for detecting a position of the object detected by the target object detection unit, an environment information acquisition unit for acquiring surrounding environment information of the position of the object detected by the object position detection unit from the environment map, an optimum posture acquisition unit for acquiring an optimum posture corresponding to the surrounding environment information acquired by the environment information acquisition unit for the object detected by the target object detection unit, an object posture detection unit for detecting a current posture of the object from the input image of the image input unit, an object posture comparison unit for comparing the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in the object posture database, and an object posture correction unit for correcting the posture of the object when the object posture comparison unit determines that there is a predetermined difference or more between the current posture of the object and the optimum posture. The system used herein refers to an assembly obtained by logically assembling a plurality of devices or functional modules realizing specific functions, regardless of whether or not devices or functional modules are in a single housing. According to the embodiments of the present disclosure described above, it is possible to provide an excellent robot device, a method of controlling the same, a computer program, and a robot system that can enable placing positions and postures of objects that are frequently used to be autonomously arranged so that a user can use the objects without difficulty. In addition, according to the embodiments of the present technology disclosed in this specification, it is possible to provide an excellent robot device, a method of controlling the same, a computer program, and a robot system that can enable an object held by a robot to be easily held by a user or handed over to the user in a posture in which the user can easily use the object. Other objects, features and advantages of the present technology disclosed in this specification will become more apparent from the following further detailed description of embodiments of the present technology taken in conjunction with the accompanying drawings.",
                        "1 is a diagram illustrating the external appearance of a robot device 100 to which the present technology disclosed in this specification is applicable; 2 is a diagram schematically illustrating a joint degree-of-freedom configuration of the robot device 100 to which the present technology disclosed in this specification is applicable; 3 is a diagram schematically illustrating a functional configuration for controlling the robot device 100 illustrated in 1; 4 is a diagram schematically illustrating a functional configuration for enabling the robot device 100 to implement an operation corresponding to an environment; 5 is a diagram illustrating a state in which a comfortable environment for a user is arranged as an operation corresponding to an environment of the robot device 100; 6 is a diagram illustrating a state in which a comfortable environment for a user is arranged as an operation corresponding to an environment of the robot device 100; 7 is a diagram illustrating a state in which a target object is handed over in a correct posture in which the user easily holds the target object as an operation corresponding to an environment of the robot device 100; 8A is a diagram illustrating a state in which a unique coordinate system is set in relation to a cylindrical object or an approximately cylindrical object; 8B is a diagram illustrating a state in which object coordinates x, y and z are three-dimensionally rotated with respect to world coordinates x, y, and z; 9 is a diagram illustrating a configuration example of an object posture database 409; 10 is a flowchart illustrating a processing procedure of generating and updating the object posture database 409 online; 11 is a flowchart illustrating a processing procedure of generating and updating the object posture database 409; 12 is a diagram illustrating a modified example of a database 903 of posture data of each object/environment; 13 is a state transition diagram illustrating an operation executed by the robot device 100; and 14 is a flowchart illustrating a processing procedure executed by the robot device 100 under an object posture correction mode."
                    ],
                    "DESCRIPTION": "Hereinafter, preferred embodiments of the present disclosure will be described in detail with reference to the appended drawings. Note that, in this specification and the appended drawings, structural elements that have substantially the same function and structure are denoted with the same reference numerals, and repeated explanation of these structural elements is omitted. In 1, the external appearance of a robot device 100 to which the present technology disclosed in this specification is applicable is illustrated. The robot device 100 is a link tectonic belt in which a plurality of links are connected by joints, each of which is operated by an actuator. In 2, a joint degree-of-freedom configuration of the robot device 100 is schematically illustrated. Although the illustrated robot device 100 mainly installed in a home environment performs household chores or care, the robot device 100 is available for various purposes such as industrial uses. The illustrated robot device 100 is provided with two drive wheels 101R and 101L facing a base unit as moving means. The drive wheels 101R and 101L are driven by drive-wheel actuators 102R and 102L, each of which rotates about a pitch axis. In 2, reference numerals 151, 152, and 153 denote non-existent underactuated joints. The joints 151, 152, and 153 correspond to a degree of translational freedom of an X direction front-back direction to a floor surface of the robot device 100, a degree of translational freedom of a Y direction left-right direction, and a degree of rotational freedom about a yaw axis, respectively. The joints 151, 152, and 153 are expressions of moving of the robot device 100 around the virtual world. The moving means is connected to an upper body via a hip joint. The hip joint is driven by a hip joint pitch-axis actuator 103 that rotates about the pitch axis. The upper body is constituted by two limbs including left and right arm portions and a head portion connected via a neck joint. The left and right arm portions each have a total of seven degrees of freedom including three degrees of freedom of a shoulder joint, two degrees of freedom of an elbow joint, and two degrees of freedom of a wrist joint. The three degrees of freedom of the shoulder joint are driven by a shoulder-joint pitch-axis actuator 104R/L, a shoulder-joint roll-axis actuator 105R/L, and a shoulder-joint yaw-axis actuator 106R/L. The two degrees of freedom of the elbow joint are driven by an elbow-joint pitch-axis actuator 107R/L and an elbow-joint yaw-axis actuator 108R/L. The two degrees of freedom of the wrist joint are driven by a wrist-joint roll-axis actuator 109R/L and a wrist-joint pitch-axis actuator 110R/L. In addition, two degrees of freedom of the neck joint are driven by a neck-joint pitch-axis actuator 111 and a neck-joint yaw-axis actuator 112. In addition, one degree of freedom of a hand joint is driven by a hand-joint roll-axis actuator 113R/L. Although the illustrated robot device 100 has moving means of two facing wheels, the present technology disclosed in this specification is not limited to the moving means of the two facing wheels. For example, the present technology disclosed in this specification can also be applied to the robot device 100 having leg type moving means. Each axis actuator is attached to a speed reducer for obtaining sufficient generative force as well as an encoder for measuring a joint angle, a motor for generating torque, and a current control motor driver for driving the motor. In addition, a microcomputer, which controls driving of the actuator, is attached to the actuator although not illustrated in 2. A dynamic arithmetic operation of the robot device 100 is executed on a host computer, and generates a control target value of a torque or joint angle of each joint actuator. The control target value is transmitted to the control microcomputer attached to the actuator, and used for control of the actuator to be executed on the control microcomputer. In addition, each joint actuator is controlled by a force control scheme or a position control scheme. 3 schematically illustrates a functional configuration which controls the robot device 100 shown in 1. The robot device 100 includes a control unit 310, which performs integrated control of the entire operation or processes data, an input/output I/O unit 320, and a drive unit 330. Hereinafter, these units will be described. The I/O unit 320 includes a camera 321 corresponding to the eye of the robot device 100, a distance sensor 322, which detects a distance to a human or obstacle, a microphone 323 corresponding to the ear of the robot device 100, and the like as input units. In addition, the I/O unit 320 includes a speaker 324 corresponding to the mouth of the robot device 100 and the like as output units. Here, the distance sensor 322 is constituted by a space sensor, for example, such as a laser range finder or the like. The robot device 100 can receive a task instruction by an audio input of a user, for example, from the microphone 323. However, the robot device 100 may have another task-instruction input means not illustrated via a wired link, a wireless link, recording media, or the like. The drive unit 330 is a functional module for implementing a degree of freedom in each joint of the robot device 100, and is constituted by a plurality of drive units provided for each roll, pitch, or yaw axis in each joint. Each drive unit is constituted by a combination of a motor 331, which performs a rotation operation about a predetermined axis, an encoder 332, which detects a rotation position of the motor 331, and a driver 333, which adaptively controls a rotation position or a rotation speed of the motor 331 on the basis of the output of the encoder 332. The control unit 310 includes a recognition unit 311, a drive control unit 312, and an environment map 313. The recognition unit 311 recognizes a surrounding environment on the basis of information obtained from the input unit such as the camera 321 or the distance sensor 322 within the I/O unit 320. For example, the recognition unit 311 pre-constructs or updates the environment map 313 on the basis of the input information. The drive control unit 312 controls the output unit of the I/O unit 320 or the drive of the drive unit 330. For example, the drive control unit 312 controls the drive unit 330 to enable the robot device 100 to implement tasks. Here, the tasks to be implemented by the robot device 100 include household chores or care corresponding to an instruction, an operation corresponding to an environment, and the like. In addition, the operation corresponding to the environment can be an operation of enabling placing positions and postures of objects that are frequently used to be autonomously arranged so that a user can use the objects without difficulty, or an operation of enabling an object held by a robot to be easily held by the user or handed over to the user in a posture in which the user can easily use the object. In 4, a functional configuration for enabling the robot device 100 to implement an operation corresponding to an environment is schematically illustrated. The environment map 313 is a database including shape, position, and posture data of objects such as a table, a sofa, a bookshelf, and a TV in a work area in which the robot device 100 operates such as a room in which the user is located. As a difference between an object to be managed by the database in the environment map 313 and an object of which a position and a posture are autonomously arranged by the robot device 100, the former can be a large object incapable of being operated or moved by the arm of the robot device 100 or the like, and the latter can be a small object capable of being grasped and moved by the robot device 100. While the objects such as the table, the sofa, the bookshelf, and the TV are managed in the environment map, a PET bottle, a remote controller, and the like are target objects of which positions and postures are autonomously arranged by the robot device 100 or which are handed over to the user. A self-position detection unit 401 detects or estimates a self-position of the robot device 100 based on a measurement result of the camera 321 or the distance sensor 322. In addition, an environment map update unit 403 constructs and updates the environment map 313. A target object detection unit 402 detects a target object from an image captured by the camera 321. The target object detection unit 402 can detect the target object, for example, by image processing of a feature quantity such as a contour, an edge, a corner, a luminance change, or a color, a three-dimensional 3D shape of the object, or an artificial geometric pattern such as a barcode attached to the object. Alternatively, the target object may be detected by reading information such as an identifier ID from a wireless tag attached to the target object. The robot device 100 autonomously learns an optimum posture of the detected target object. During the learning, an object position detection unit 404 calculates a 3D position of the target object from the self-position estimated by the self-position detection unit 401 and the position of the target object detected by the target object detection unit 402 from the image captured by the camera 321. In addition, an environment information acquisition unit 406 acquires surrounding environment information of the position of the detected object from the environment map updated by the environment map update unit 403. The surrounding environment information of the target object can be acquired from the 3D position and the environment map of the target object. For example, the information may indicate a state in which the PET bottle should be on the table and the remote controller should currently be on the sofa and the TV should be opposite to the remote controller. When acquiring a correct or optimum posture, corresponding to the surrounding environment information, for placing a target object at the time of arranging a comfortable environment for the user or a correct or optimum posture, corresponding to the surrounding environment information, of the target object at the time of handing the target object over to the user as dictionary data, an optimum posture acquisition unit 407 registers the acquired posture in an object posture database 409. For example, it is possible to acquire the posture of the target object a plurality of times and determine a correct posture on the basis of vote results of the postures as will be described later. Table 1 shows an example of dictionary data of the correct posture for placing the target object at the time of arranging the comfortable environment for the user. In addition, Table 2 shows an example of dictionary data of the correct posture in which the target object is handed over to the user. TABLE 1 Environment Target object information Correct posture PET bottle On table or floor State in which cap is put on top Mug On table or floor Stand-up state Remote controller On table, sofa, Buttons facing upward or floor Near TV Transmitter toward TV Slippers On floor Tops of slippers facing upward Near bed Ankle side closer than bed TABLE 2 Environment Target object information Correct posture PET bottle Cap put on top Mug Handle portion toward user Scissors Handle portion toward user Cane Handle portion toward user Pen Side opposite to pen tip toward userHowever, in Table 1, designations of surrounding environments can be combined. For example, the slippers are on the floor and near the bed. The robot device 100 learns the optimum posture of each target object and registers the optimum posture in the object posture database 409 as the dictionary data. Alternatively, the user may cause the dictionary data of the robot device 100 to be remembered or teach the robot device 100 via a user interface UI. When normally walking around, the robot device 100 observes and learns a usual posture in which an object is placed. For example, the fact that the bottle cap is normally put on the top is learned by observing the PET bottle. As shown in the above-described Table 1, a usual posture may be learned for each environment in which the target object is placed. For example, the PET bottle on the table seems to normally stand upright and the PET bottle on the floor seems to normally lie down because the bottle is empty. In addition, the robot device 100 arranges a comfortable environment for the user on the basis of the learned result. For example, even when the user places an object in an unusual posture, the robot device 100 appropriately corrects its posture. The object posture detection unit 405 detects a current 3D posture of a target object detected by the target object detection unit 402 from an image captured by the camera 321. The optimum posture acquisition unit 407 estimates a correct posture of the target object by retrieving the object posture database 409 on the basis of the target object detected by the target object detection unit 402 from the image captured by the camera 321 and a latest environment map acquired by the environment information acquisition unit 406. In addition, the optimum posture acquisition unit 407 checks the object posture database 409 using information a feature quantity, a wireless tag, or the like used when the target object is detected from the environment, and obtains an optimum posture of the target object. An object posture comparison unit 408 compares the 3D posture detected by the object posture detection unit 405 to the optimum posture estimated on the basis of the dictionary data registered in the object posture database 409. If there is a difference between the two postures, the object posture determination unit 410 determines a control amount for operating or correcting a position/posture of the target object so that the target object is in a correct posture. As described above, the robot device 100 operates or corrects the target object to the correct posture on the basis of the dictionary data registered in the object posture database 409. Before the position posture of the target object is operated or corrected, a query may be sent to the user. In addition, when the position posture of the target object operated or corrected by the robot device 100 is wrong, the user may perform teaching. The user can teach the robot the correct posture of the object by showing the correct posture. For example, the user indicates Place like this by audio while showing the PET bottle in the standing-up posture. Alternatively, the user can teach the correct posture of the object by audio. For example, an indication indicating that the PET bottle stands upright is made. In the robot device 100 according to this embodiment, the optimum posture acquisition unit 407 can generate the object posture database 409 online. In the construction of the object posture database 409, a process of acquiring the position and posture of the target object detected by the target object detection unit 402 is indispensable. A shape of the object can be acquired as prior knowledge such as computer aided design CAD data or by polygonization after 3D measurement is performed by a stereo camera. When the shape of the object is acquired as the dictionary data, an object-specific coordinate system is set for the polygonized shape. For example, in terms of a cylindrical object or approximately cylindrical object as illustrated in 8A, a center is the origin, a main axis is set to an axis z and axes x and y are appropriately set to be symmetrical with a rotation of the axis z. In addition, when the cylindrical object or approximately cylindrical object as illustrated in 8A is placed in the real world in terms of the posture of the object, the posture of the object can be implemented according to a 3D rotation of object coordinates x, y, and z with respect to real-world coordinates x, y, and z see 8B. Arbitrary 3D rotation can be implemented by three parameters such as Euler angles or roll-pitch-yaw angles each rotation angle when a rotation is continuously performed about a certain axis among the three axes. In the following description, the three parameters of the posture of the object are referred to as , , and . Except for the moment when a task is performed according to an instruction from the user, the robot device 100 according to this embodiment acquires a posture of each target object by autonomously observing a surrounding environment on the basis of an image captured by the camera 321. The posture of the target object is continuously acquired. For each target object, a usual posture is calculated by a technique such as voting. Specifically, a 3D array representing a parameter space , , is provided as a voting space, and also parameters are appropriately discretized and handled. Because these parameters are angles, that is, continuous amounts, the parameter is represented by a median value of its step width within its step width, for example, as 0 to 5 degrees, 5 to 10 degrees, 10 to 15 degrees, . . . , every 5 degrees. Because 3605=72, if 360 degrees are divided by 5 degrees, the above-described parameter space , , becomes a 3D array of 727272. Every time the posture , , of the object is observed, a value of a corresponding element of the array is incremented by 1. If the total number of observations of the object the total number of votes reaches the prescribed number of times arbitrarily set, the optimum posture acquisition unit 407 finds parameters , , and in which the number of acquired votes becomes a maximum value, and sets the found parameters , , and to an optimum posture of the object. Although a configuration example in which voting is directly conducted for the number of observations by incrementing an element corresponding to the 3D array by 1 for one observation has been described above, a configuration may be made to conduct voting by weighting. For example, a configuration example in which voting is conducted after weighting to one observation by estimating/considering a period of time in which the object is in the posture may be included. In addition, when a surrounding environment is different even for the same object, learning as described above is performed for each environment. For example, when the same PET bottle is on the table or floor as shown in the above-described Table 1, an optimum posture is obtained for each placing position. Surrounding environment information of the object indicating that the object is placed on the table or the like is obtained on the basis of a current position of the object estimated by the object position detection unit 404 and the updated environment map. As described above, the optimum posture acquisition unit 407 registers a correct posture in which the target object is placed when the comfortable environment for the user is autonomously arranged or a correct posture of the target object when the target object is handed over to the user in the object posture database 409. A configuration example of the object posture database 409 is illustrated in 9. In the illustrated example, the object posture database 409 is constituted by three types of databases including a database 901 storing object data, a database 902 storing posture data for each object, and a database 903 storing posture data for each object/environment. The database 901 storing the object data has IDs for identifying various objects and a pointer for an object-related posture database. The database 902 storing the posture data for each object has information regarding a surrounding environment in which the object is placed and a pointer for the posture database in the object and the environment. The database 902 storing the posture data for each object may be configured to have object-specific information such as a shape, mass, and material as well as a posture, although this is not illustrated because it is not directly related to the technology disclosed in this specification, for generating the database online. The database 903 storing the posture data for each object environment has a posture learned flag indicating whether or not an optimum posture can be calculated by sufficiently observing the object in the environment, and a posture , , if learning is completed. In addition, middle results in a learning process, that is, a 3D array of a parameter space , , storing each number of votes and the total number of observations of the object, are also included in the database 903, and a configuration is made so that data can be referred to during learning. However, a configuration may be made so that these middle results are not referred to by arranging the middle results outside the database 903. Of course, the above-described three databases 901 to 903 may be manually created. For example, for an object of which a posture is known in advance, the posture learned flag of the database 903 for each object environment may be set to learned and its posture data may be set to the optimum posture. Next, a process of generating the object posture database 409 online will be described. In 10, a processing procedure of generating and updating the object posture database 409 online is illustrated in the form of a flowchart. The optimum posture acquisition unit 407 is assumed to periodically iterate this process. The target object detection unit 402 recognizes various surrounding objects on the basis of an image captured by the camera 321 step S1001. In addition, simultaneously, a shape and posture recognized by the object posture detection unit 405 are calculated by 3D measurement by the stereo camera step S1002, and a position of the object recognized by the object position detection unit 404 is calculated step S1003. Then, the optimum posture acquisition unit 407 acquires surrounding environment information of the object for example, on the table or the like according to the calculated 3D position of the object and the environment map acquired by the environment information acquisition unit 406 step S1004. The optimum posture acquisition unit 407 generates and further updates the object posture database 409 based on an object identification ID, the posture of the object, and the environment information obtained as described above step S1005. In 11, a processing procedure of generating and updating the object posture database 409, which is executed in step S1005 described above, is illustrated in the form of a flowchart. The optimum posture acquisition unit 407 first checks whether or not the ID of the recognized object is already registered in the database 901 retaining the object data step S1101. Here, when the ID of the recognized object is not registered in the database 901 No in step S1101, the optimum posture acquisition unit 407 registers the ID of the object in the database 901, and also creates posture data corresponding to the object ID in the database 902 step S1104. When the ID of the recognized object is already registered in the database 901 and after the posture data corresponding to the object ID is created in the database 902, the optimum posture acquisition unit 407 checks whether or not surrounding environment information of the object is already registered in the database 902 retaining posture data of each object step S1102. Here, when the surrounding environment information of the object is not registered in the database 902 No in step S1102, the optimum posture acquisition unit 407 registers the surrounding environment information of the object in the posture database 902 for each object and registers posture data corresponding to its environment in the database 903 step S1105. When the surrounding environment information of the object is already registered in the database 902 and after the posture data corresponding to the environment is registered in the database 903, the optimum posture acquisition unit 407 checks whether or not an optimum posture of the object is learned step S1103. Here, when the optimum posture of the object is not learned, the optimum posture acquisition unit 407 votes for a posture , , of the object calculated by the object posture detection unit 405 step S1106. When the total number of observations of the posture of the object does not reach the prescribed number of times No in step S1107, this processing routine ends. In addition, when the total number of observations of the posture of the object reaches the prescribed number of times Yes in step S1107, the optimum posture acquisition unit 407 determines a posture , , corresponding to a maximum value of a voting result in the database 903 as the optimum posture of the object, and sets the learned flag step S1108, so that this processing routine ends. When the optimum posture of the object is learned Yes in step S1103, the object posture comparison unit 408 compares a current posture of the object to the learned optimum posture of the object and checks whether or not there is a difference between the two postures step S1109. If there is no difference between the current posture of the object and the learned optimum posture of the object No in step S1109, this processing routine ends. On the other hand, when there is a difference between the current posture of the object and the learned optimum posture of the object Yes in step S1109, the object posture determination unit 410 starts up a process of correcting the posture of the object. Details of the process of correcting the posture of the object will be described later. Although an example in which the optimum posture is determined at a point in time when the total number of observations, that is, the total number of votes, is greater than or equal to the prescribed number of times in the processing procedure illustrated in 11 has been described, other implementation methods may be included. For example, as a result of voting as described above, it is possible to obtain a posture frequency distribution in the voting space , , . According to the frequency distribution, when the current posture is rare that is, a frequency is low as compared to other postures, the posture is changed to a posture of which a frequency is higher than those of others in vicinity of the posture in an object posture correction mode as will be described. It is possible to handle a plurality of optimum postures using a posture frequency distribution obtained by voting. For example, when a current posture of the object is obtained under a search mode as will be described later, it is preferable that the posture be changed to a posture of which a frequency is higher than those of others in vicinity of the posture in the object posture correction mode. In addition, it is possible to use the posture frequency distribution even when the number of votes is small. However, when the number of votes is small, it is rare to change an actual posture because a specific posture is not likely to be determined to be rare as compared to others. Still, it is not necessary to determine the prescribed number of votes instead, it is necessary to define a frequency in which the posture is determined to be rare. This method has another advantage in that it is not necessary to appropriately define the prescribed number of votes, which is a parameter for determining whether or not to make a posture change. Even in an initial state in which only the small number of votes is obtained, it is possible to determine how rare the posture is relatively as compared to a neighborhood of a certain posture pixel. Further, a process of dynamically changing a box size of the voting space according to a vote distribution may be implemented. It is possible to use various methods of determining a histogram width in histogram density estimation to determine the box size. In addition, kernel density estimation can be performed directly using sample points instead of the histogram density estimation. If a probability density function pposture|object, position of a target object can be appropriately estimated using sample data, it is possible to appropriately correct a posture of the object by changing the posture of the object to a posture x_opt in which the probability density function pposture|object, position becomes maximum at the periphery when a probability density around certain posture data x is less than or equal to a threshold value. Here, a process of generating and updating the object posture database 409 illustrated in 11 will be described as an example in which a PET bottle is detected as an object. However, hereinafter, a database name is distinguished by &lt; &gt;, a key within the database is distinguished by , and a value corresponding to the key within the database is distinguished by . First, the optimum posture acquisition unit 407 checks whether or not the PET bottle is registered in the database 901 retaining the object data step S1101. If the PET bottle is not registered in the database 901 No in step S1101, its ID PET Bottle 001 is registered in the database 901, posture data &lt;PetBottle001&gt; of each object corresponding to the ID is created in the database 902, and simultaneously its pointer is registered in the database 901 step S1105. At this time, a surrounding environment of the posture data &lt;PetBottle001&gt; of each object of the database 902 and the posture data are in an empty state. Then, the optimum posture acquisition unit 407 checks whether or not environment information indicating On Table as the case in which the PET bottle is placed on the table is registered in the posture data &lt;PetBottle001&gt; of the database 902 step S1102. In addition, if the environment information is not registered No in step S1102, the optimum posture acquisition unit 407 registers On Table as Environment Information in the database 902, creates posture data &lt;PetBottle001_OnTable&gt; of each object environment corresponding thereto in the database 903, and simultaneously registers its pointer in the database 902 step S1105. At this time, Posture Learned Flag of posture data &lt;PetBottle001_OnTable&gt; of each object environment registered in the database 903 is In Learning. Then, the optimum posture acquisition unit 407 checks whether or not learning is already completed in relation to the posture of the PET bottle on the table by referring to Posture Learned Flag of &lt;PetBottle001_OnTable&gt; of the database 903 step S1103. If the learning is not completed No in step S1103, the optimum posture acquisition unit 407 votes for the posture of the object calculated by the object posture detection unit 405 in a 3D array of a parameter space , , step S1106. If the total number of observations the total number of votes has reached the prescribed number of times in relation to the object Yes in step S1107, the optimum posture acquisition unit 407 finds parameters , , and in which the acquired number of votes becomes maximum, sets the found parameters , , and to Optimum Posture of the object, and sets Posture Learned Flag to Learned step S1108. In addition, if the total number of observations has not reached the prescribed number of times No in step S1107, Number of Posture Observations is incremented by 1 and the above-described process is continued until the total number of observations reaches the prescribed number of times. Although the learning is stopped after the optimum posture of the object is obtained once the learning ends in the above-described example, the optimum posture acquisition unit 407 may be configured to update Optimum Posture while continuing observation. In addition, although the robot device 100 iteratively observes an object and generates posture data of each object environment without &lt;PetBottle001_OnTable&gt; in the database 903 in the above-described example, any data related to &lt;PetBottle001_OnTable&gt; may be manually created as prior information and the posture data may be updated by observation/learning. In 12, a modified example of the database 903 of posture data of each object environment illustrated in 9 is illustrated. As a difference from 9, a key referred to as User Teaching Flag is added to posture data &lt;PetBottle001_OnTable&gt; of each object environment. User Teaching Flag is information indicating whether or not the user has taught the posture when placing the PET bottle on the table. For example, when the user causes the robot device 100 to view the PET bottle on the table by placing the PET bottle on the table and makes an instruction indicating a desire that the PET bottle be placed in the posture as described above, the robot device 100 changes User Teaching Flag from Non-Taught to Taught and changes Optimum Posture to the posture of the object observed at the time. This can be easily implemented by adding a process in which a taught posture is preferentially used when there is teaching from the user and the posture is not learned if User Teaching Flag is Taught to the process of step S1103 of 11. This is an effective technique when there is a desire for an object especially important for the user to be placed in a fixed posture or when the user corrects the posture due to wrong learning of the robot. In 13, the operation to be executed by the robot device 100 is illustrated in the form of a state transition diagram. In the illustrated example, the robot device 100 has three operation modes of task execution, search, and object posture correction. Under the task execution mode, the robot device 100 executes a task of an instruction from the user. In addition to an explicit instruction through an audio input or the like, an instruction of the task from the user includes a task to be periodically executed by a timer scheduled in advance a cleaning task when a certain prescribed time is reached and a task to be executed according to generation of a predetermined event a high-priority task according to sudden interruption such as an operation of viewing and picking up a ball rolling down from the table. When there is no instruction of execution of a task from the user, the robot device 100 moves to the search mode. In addition, if there is the instruction of execution of the task from the user under the search mode, the robot device 100 returns to the task execution mode by stopping the search. Under the search mode, the robot device 100 performs a process of autonomously observing a room while moving around in the room and generating/updating the object posture database 409 according to processing procedures illustrated in 10 and 11. In addition, if a currently observed posture of the object is found not to be a learned optimum posture of the object in the search mode, the process moves to the object posture correction mode. Under the search mode, the robot device 100 updates the object posture database 409 and simultaneously compares the current posture of the object to the optimum posture registered in the object posture database 409 step S1109 of 11. Specifically, the object posture detection unit 405 obtains the current posture , , of the object. When there is the current posture , , of the object in the already learned object posture database 409, the object posture comparison unit 408 can determine that the current posture of the object is substantially consistent with the optimum posture if the following Expression 1 is satisfied. In the following Expression 1, th, th, and th are preset threshold values. ||&lt;th,||&lt;th, and ||&lt;th1On the other hand, if Expression 1 is not satisfied, the process proceeds to the branch Yes of step S1109, so that the robot device 100 moves to the object posture correction mode. In 14, a processing procedure to be executed by the robot device 100 under the object posture correction mode is illustrated in the form of a flowchart. First, the object posture determination unit 410 calculates a trajectory of an arm/hand for grasping the object from a current posture , , of the object step S1401. Then, the object posture determination unit 410 grasps the object by controlling the arm/hand along the calculated trajectory through the drive control unit 312 step S1402. Then, the object posture determination unit 410 calculates the trajectory of the arm/hand for operating the object so that the grasped object becomes the optimum posture , , acquired from the object posture database 409 step S1403. The object posture determination unit 410 places the object of which the posture becomes the optimum posture , , by controlling the arm/hand along the calculated trajectory through the drive control unit 312 step S1404. A process in which the robot device 100 generates/updates the object posture database 409 by moving to the search mode when there is no instruction of execution of the task from the user has been described above. However, as a modified example, in a range in which the task in execution is not affected even when the task is in execution, line-of-sight control of the camera 321 for observing the environment is performed, and the object posture database 409 can be configured to be generated/updated. 5 and 6 illustrate a state in which a comfortable environment for the user is autonomously arranged as an operation corresponding to an environment of the robot device 100. The robot device 100 normally observes a surrounding target object while autonomously moving around. In the examples illustrated in 5 and 6, an environment map of a living room in which a TV, a table, and a sofa are arranged in that order is constructed and the robot device 100 autonomously moves around normally on the basis of the environment map. 5 is a perspective view of the living room, and 6 is a top view of the living room. The robot device 100 detects a target object of a remote controller for the TV placed on the table, and observes the remote controller every time the robot device 100 autonomously moves around. A correct posture of the detected target object is collated with dictionary data registered in the object posture database 409. Here, when an actually observed posture of the target object is different from a collation result of the object posture database 409, the robot device 100 operates the target object or corrects the posture. Referring to the above-described Table 1, as the collation result of the object posture database 409, a state in which buttons face upward when the remote controller is on the table, sofa, or floor and a transmitter is directed to the TV when the remote controller is close to the TV can be acquired as the correct posture of the remote controller. Referring to 6, the remote controller detected as the target object is placed on the table with the buttons facing upward. In addition, the remote controller is placed on the table close to the TV, but the transmitter is placed to be tilted from the direction of the TV. If the observed posture of the remote controller is detected to be inconsistent with the collation result indicating that the transmitter is directed to the TV when the remote controller is close to the TV, the robot device 100 corrects the posture of the remote controller as indicated by an arrow of 6. When a correction result of the posture of the target object by the robot device 100 is wrong, the user may correct the posture or teach the robot device 100. In addition, if an instruction to operate the target object is received from the user, the robot device 100 places the target object in the correct stable posture. In this case, the correct posture when the target object is placed is collated with the dictionary data within the object posture database 409. The target object is operated and placed in the posture. For example, if an instruction to place the remote controller on the table close to the TV is received from the user, the robot device 100 places the remote controller on the table by operating the remote controller so that the buttons face upward and the transmitter is directed to the TV as in the examples illustrated in 5 and 6. Alternatively, when an instruction to place the mug on the table is received from the user, the robot device 100 places the mug on the table by operating the mug in a stable state in which the mug stands upright. When an operation result of the target object by the robot device 100 according to the instruction from the user is wrong, the user may correct the posture or teach the robot device 100. In 7, a state in which a target object is handed over in a correct posture in which the user easily holds the target object as an operation corresponding to an environment of the robot device 100 is illustrated. If an instruction to bring the target object is received from the user, the robot device 100 collates the correct posture when the target object is handed over with the dictionary data within the object posture database 409. The robot device 100 hands over the target object to the user after operating the target object for the correct posture described in the dictionary data. In 7, an example of an instruction to bring a cane is illustrated. Referring to the above-described Table 2, because a state in which a handle portion is toward the user can be acquired as the correct posture of the cane as a collation result of the object posture database 409, the robot device 100 hands over the cane to the user by operating the cane so that the handle portion is toward the user. When an operation result of the target object by the robot device 100 according to the instruction from the user is wrong, the user may correct the posture or teach the robot device 100. As seen from 5 and 6, the robot device 100 according to this embodiment estimates the correct posture of the target object corresponding to the environment and operates and corrects the target object so that the target object is in the correct posture, thereby autonomously arranging the comfortable environment for the user. In addition, as seen from 7, when the target object is handed over from the robot device 100 according to this embodiment to the user, it is possible to implement natural interaction with the user by estimating an optimum posture of the target object for handing over and operating the target object so that the target object is in the optimum posture. As described above, the robot device 100 learns a posture in which a certain object is frequently placed as an optimum posture for the object. Because the robot device 100 autonomously learns an optimum posture in terms of a surrounding object normally placed for use by the user, the robot device 100 can autonomously arrange a comfortable environment for the user by appropriately correcting the posture of the object even when the user places the object in an unusual posture. These object posture correction functions are particularly effective for users such as persons with disabilities, persons with physical disabilities, or disabled elders who normally place objects to be frequently used in personally determined positions so that the objects can be used without difficulty as well as users capable of accurately arranging goods. Additionally, the technology disclosed in this specification may also be configured as below. 1 A robot device including:an environment map including surrounding environment information;an image input unit for inputting an image of surroundings;a target object detection unit for detecting an object from the input image of the image input unit;an object position detection unit for detecting a position of the object detected by the target object detection unit;an environment information acquisition unit for acquiring surrounding environment information of the position of the object detected by the object position detection unit from the environment map;an optimum posture acquisition unit for acquiring an optimum posture corresponding to the surrounding environment information acquired by the environment information acquisition unit for the object detected by the target object detection unit;an object posture database for retaining an optimum posture of each object for each environment;an object posture detection unit for detecting a current posture of the object from the input image of the image input unit;an object posture comparison unit for comparing the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in the object posture database; andan object posture correction unit for correcting the posture of the object when the object posture comparison unit determines that there is a predetermined difference or more between the current posture of the object and the optimum posture. 2 The robot device according to 1, further including:a task execution unit for executing a task based on an instruction of a user,wherein, when the task execution unit is not executing the task, the detection of the current posture of the object by the object posture detection unit, the comparison with the optimum posture by the object posture comparison unit, and the correction of the posture of the object by the object posture correction unit are autonomously performed. 3 The robot device according to 1 or 2, further including:a moving unit for moving a position of the robot device,wherein the object posture detection unit detects the current posture of the object while the moving unit moves a current position of the robot device. 4 The robot device according to any of 1 to 3, wherein the optimum posture acquisition unit acquires an optimum posture of each object by learning and retains the acquired optimum posture in an object posture database. 5 The robot device according to 4, wherein the object posture database further retains the optimum posture of each object taught from a user. 6 The robot device according to any of 1 to 5, further including:an environment map update unit for updating an environment map on a basis of the input image of the image input unit. 7 The robot device according to any of 1 to 6, wherein the optimum posture acquisition unit acquires an optimum posture for placing the object when arranging a comfortable environment for a user and an optimum posture of the object when handing the object over to the user. 8 The robot device according to any of 1 to 7, wherein the environment information acquisition unit acquires the surrounding environment information of the position of the object detected by the object position detection unit from an environment map including the surrounding environment information. 9 The robot device according to any of 1 to 8, wherein the object posture comparison unit compares the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in an object posture database retaining an optimum posture of each object for each environment. 10 A method of controlling a robot device, including:inputting an image of surroundings;detecting an object from the image input in the step of inputting;detecting a position of the object detected in the step of detecting an object;acquiring surrounding environment information of the position of the object detected in the step of detecting from an environment map including the surrounding environment information;acquiring an optimum posture corresponding to the surrounding environment information acquired in the step of acquiring surrounding environment information for the object detected in the step of detecting an object;retaining an optimum posture of each object for each environment in an object posture database;detecting a current posture of the object from the image input in the step of inputting;comparing the current posture of the object detected in the step of detecting a current posture to the optimum posture of the object retained in the object posture database; andcorrecting the posture of the object when it is determined in the step of comparing that there is a predetermined difference or more between the current posture of the object and the optimum posture. 11 A computer program, written in a computer-readable format, for controlling a robot device and causing a computer to function as:an environment map including surrounding environment information;an image input unit for inputting an image of surroundings;a target object detection unit for detecting an object from the input image of the image input unit;an object position detection unit for detecting a position of the object detected by the target object detection unit;an environment information acquisition unit for acquiring surrounding environment information of the position of the object detected by the object position detection unit from the environment map;an optimum posture acquisition unit for acquiring an optimum posture corresponding to the surrounding environment information acquired by the environment information acquisition unit for the object detected by the target object detection unit;an object posture database for retaining an optimum posture of each object for each environment;an object posture detection unit for detecting a current posture of the object from the input image of the image input unit;an object posture comparison unit for comparing the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in the object posture database; andan object posture correction unit for correcting the posture of the object when the object posture comparison unit determines that there is a predetermined difference or more between the current posture of the object and the optimum posture. 12 A robot system including:an environment map constructed on an external device and including surrounding environment information;an object posture database, constructed on an external device, for retaining an optimum posture of each object for each environment; anda robot device including an image input unit for inputting an image of surroundings, a target object detection unit for detecting an object from the input image of the image input unit, an object position detection unit for detecting a position of the object detected by the target object detection unit, an environment information acquisition unit for acquiring surrounding environment information of the position of the object detected by the object position detection unit from the environment map, an optimum posture acquisition unit for acquiring an optimum posture corresponding to the surrounding environment information acquired by the environment information acquisition unit for the object detected by the target object detection unit, an object posture detection unit for detecting a current posture of the object from the input image of the image input unit, an object posture comparison unit for comparing the current posture of the object detected by the object posture detection unit to the optimum posture of the object retained in the object posture database, and an object posture correction unit for correcting the posture of the object when the object posture comparison unit determines that there is a predetermined difference or more between the current posture of the object and the optimum posture. It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and alterations may occur depending on design requirements and other factors insofar as they are within the scope of the appended claims or the equivalents thereof. Although the embodiments applied to the human type robot device with two legs have been mainly described in this specification, the subject matter of the technology disclosed in this specification is not limited thereto. The present technology can also be equally applied to a robot device with other moving means or with no moving means. In addition, the embodiments related to the household robot have mainly been described. Of course, the present technology can also be equally applied to robot devices for various purposes including industrial robots. Namely, the present disclosure is merely an example, and should by no means be interpreted as limited to the content disclosed in this specification. The subject matter of the present disclosure is to be discerned while taking the claims into consideration. The present disclosure contains subject matter related to that disclosed in Japanese Priority Patent Application JP 2011-161550 filed in the Japan Patent Office on Jul. 25, 2011, the entire content of which is hereby incorporated by reference.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/41/450/113/0.pdf",
                    "CONTRADICTION_SCORE": 0.9686195850372314,
                    "F_SPEC_PARAMS": [
                        "without difficulty"
                    ],
                    "S_SPEC_PARAMS": [
                        "optimum position or posture"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "However, it is difficult for many robot devices to operate an object serving as an operation target in an optimum posture according to content of its operation or a surrounding environment.",
                        "Objects to be frequently used are normally placed in personally determined positions so that persons with disabilities, persons with physical disabilities, or disabled elders can use the objects without difficulty.",
                        "Recently, robot devices for performing various tasks have been implemented."
                    ],
                    "S_SENTS": [
                        "However, the cleanup robot does not consider an optimum position or posture of the object when putting the object into the empty space.",
                        "In addition, a goods management system, which has a shape database of a target object, determines a posture at the time of the installation or the like after grasping/transporting the target object on the basis of information of the database, and enables a robot to move or transport the target object, has been proposed for example, Japanese Patent 373854.",
                        "However, the goods management system only grasps, transports, and installs the object."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Waste of Time"
                    ],
                    "F_SIM_SCORE": 0.4933760464191437,
                    "S_TRIZ_PARAMS": [
                        "Shape"
                    ],
                    "S_SIM_SCORE": 0.4750173091888428,
                    "GLOBAL_SCORE": 1.6528162628412246
                },
                "sort": [
                    1.6528163
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11433541-20220906",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11433541-20220906",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-12-27",
                    "PUBLICATION_DATE": "2022-09-06",
                    "INVENTORS": [
                        "Chwen-Yi Yang",
                        "Cheng-Kai Huang",
                        "Jan-Hao Chen",
                        "Yi-Ying Lin",
                        "Bing-Cheng Hsu"
                    ],
                    "APPLICANTS": [
                        "Industrial Technology Research Institute    ( Hsin-Chu , TW )"
                    ],
                    "INVENTION_TITLE": "Automated calibration system and method for a workpiece coordinate frame of a robot",
                    "DOMAIN": "B25J 91692",
                    "ABSTRACT": "An automated calibration system for a workpiece coordinate frame of a robot includes a physical image sensor having a first image central axis, and a controller for controlling the physical image sensor adapted on a robot to rotate by an angle to set up a virtual image sensor having a second image central axis. The first and the second image central axes are intersected at an intersection point. The controller controls the robot to repeatedly move back and forth a characteristic point on the workpiece between these two axes until the characteristic point overlaps the intersection point. The controller records a calibration point including coordinates of joints of the robot, then the controller moves another characteristic point and repeats the foregoing movement to generate several other calibration points. According to the calibration points, the controller calculates relative coordinates of a virtual tool center point and the workpiece to the robot.",
                    "CLAIMS": "1. A automated calibration system for a workpiece coordinate frame of a robot, comprising: a physical image sensor, having a first image central axis, disposed on a flange at an end of the robot; and a controller, connected with the robot and the physical image sensor, used for controlling the physical image sensor and the robot to construct a virtual image sensor having a second image central axis, the second image central axis intersecting the first image central axis to form an intersection point; wherein the controller controls the robot to move back and forth between the first image central axis and the second image central axis until a characteristic point on the workpiece overlaps the intersection point, the characteristic point as a calibration point, out of a plurality of calibration points, including coordinates of a plurality of joints of the robot is recorded, then a next characteristic point is introduced to repeat the aforesaid movement for overlapping the intersection point again until a number of the plurality of calibration points is accepted, and each of the plurality of calibration points is evaluated to calculate coordinates of a virtual tool center point and the workpiece with respect to the robot; and wherein, when the characteristic point overlaps the intersection point of the first image central axis and the second image central axis, a distance between the characteristic point and the first image central axis and another distance between the characteristic point and the second image central axis are less than a threshold valve. 2. The automated calibration system of claim 1, wherein the controller controls the robot to move according to a coordinate system of the robot with respect to a transformation relationship of another coordinate system of the physical image sensor and the virtual image sensor, and a plurality of images of the physical image sensor and the virtual image sensor. 3. The automated calibration system of claim 1, wherein each of the plurality of calibration points includes rotational angles of the plurality of joints with respect to a preset point. 4. The automated calibration system of claim 3, wherein the controller calculates coordinates of the virtual tool center point according to the plurality of calibration points and a Denavit-Hartenberg parameter of the robot. 5. The automated calibration system of claim 1, wherein a quantity of the plurality of calibration points is larger than or equal to a preset value. 6. The automated calibration system of claim 5, wherein, upon when the quantity of the plurality of calibration points is less than the preset value, the controller applies a random-number generator to generate an Euler-angle increment for correcting an Euler angle of the robot. 7. The automated calibration system of claim 6, wherein, after the controller corrects the Euler angle of the robot, the controller controls the robot to move repeatedly another characteristic point back and forth between the first image central axis and the second image central axis so as to obtain another calibration point, until the quantity of the calibration point is larger than or equal to the preset value. 8. The automated calibration system of claim 1, wherein coordinates of the virtual tool center point are coordinates with respect to a base or the flange of the robot. 9. An automated calibration method for a workpiece coordinate frame of a robot, the robot connecting a controller, comprising the steps of: i providing a physical image sensor, forming an image coordinate system and having a first image central axis, disposed on a flange at an end of the robot; ii applying the controller to rotate the physical image sensor and the robot to construct a virtual image sensor having a second image central axis, the second image central axis intersecting the first image central axis to form an intersection point; iii controlling the robot, by the controller, to move back and forth between the first image central axis and the second image central axis, until a characteristic point on the workpiece and the intersection point are overlapped, and recording as a calibration point including coordinates of a plurality of joints of the robot wherein a first distance between the characteristic point and the first image central axis is less than a threshold value, and a second distance between the characteristic point and the second image central axis is less than the threshold valve; iv controlling the robot, by the controller, to move according to the aforesaid movement to overlap a next characteristic point with the intersection point for generating a plurality of other calibration points; and v calculating coordinates of a virtual tool center point and the workpiece with respect to the robot based on each of the plurality of calibration points. 10. The automated calibration method of claim 9, further including a method for constructing the virtual image sensor, the method for constructing the virtual image sensor including the steps of: a moving the robot to fall the characteristic point into a visual field of the physical image sensor, and obtaining coordinates of an arbitrary point within the visual field with respect to the flange; b obtaining a first point in the coordinate system of the physical image sensor, rotating by two different angles about the first image central axis to generate a second point and a third point, and calculating a center position according to the first point, the second point and the third point; c calculating a vector from the center position to a tool-image center, and transforming the vector in the image coordinate system into another vector with respect to the flange, the tool-image center being an intersection point of a coordinate axis of a tool and the first image central axis; d correcting coordinates of the first point, going back to perform Step a until the center position overlaps the tool-image center, and then obtaining a tool center coordinate; and e rotating the physical image sensor about an arbitrary coordinate axis of the flange by an angle so as to form the virtual image sensor. 11. The automated calibration method of claim 9, wherein the Step iii further includes the steps of: providing a transformation relationship between a coordinate system of the robot and another coordinate system of the physical image sensor and the virtual image sensor; and based on the transformation relationship, the physical image sensor and a plurality of images of the virtual image sensor, controlling the robot to move. 12. The automated calibration method of claim 11, wherein the step of providing the transformation relationship further includes the steps of: controlling the robot to move the characteristic point a distance along a horizontal axis of the coordinate system of the robot from an arbitrary position within an image-overlapped region of the physical image sensor and the virtual image sensor, and obtaining a first projection coordinate from the physical image sensor and the virtual image sensor; controlling the robot to move the characteristic point the distance along a vertical axis of the coordinate system of the robot from the arbitrary position within the image-overlapped region of the physical image sensor and the virtual image sensor, and obtaining a second projection coordinate from the physical image sensor and the virtual image sensor; and controlling the robot to move the characteristic point the distance along another vertical axis of the coordinate system of the robot from the arbitrary position within the image-overlapped region of the physical image sensor and the virtual image sensor, and obtaining a third projection coordinate from the physical image sensor and the virtual image sensor. 13. The automated calibration method of claim 12, wherein the step of providing the transformation relationship further includes the steps of: providing a first spatial vector, a second spatial vector and a third spatial vector corresponding to the first projection coordinate, the second projection coordinate and the third projection coordinate, respectively; based on an orthogonal relationship among the first spatial vector, the second spatial vector and the third spatial vector, calculating the first spatial vector, the second spatial vector and the third spatial vector; and based on the first spatial vector, the second spatial vector and the third spatial vector, calculating the transformation relationship between the coordinate system of the robot and that of the physical image sensor and the virtual image sensor. 14. The automated calibration method of claim 9, wherein coordinates of each of the plurality of joints are rotational angles of the plurality of joints with respect to a preset point. 15. The automated calibration method of claim 9, wherein the Step v further includes a step of, based on the plurality of calibration points and a Denavit-Hartenberg parameter of the robot, calculating coordinates of the virtual tool center point and the workpiece. 16. The automated calibration method of claim 9, wherein a quantity of the plurality of calibration points is larger than or equal to a preset value. 17. The automated calibration method of claim 9, wherein the Step iv further includes a step of, upon when a quantity of the plurality of calibration points is less than a preset value, applying a random-number generator to generate an Euler-angle increment for correcting an Euler angle of the robot. 18. The automated calibration method of claim 17, further including a following step of controlling the robot to move repeatedly another characteristic point back and forth between the first image central axis and the second image central axis so as to obtain another calibration point, until a quantity of the calibration points is larger than or equal to the preset value.",
                    "FIELD_OF_INVENTION": "The present disclosure relates in general to an automated calibration system for a coordinate frame, and more particularly to an automated calibration system for a workpiece coordinate frame of a robot. In addition, this present disclosure also relates in general to an automated calibration method applied to the automated calibration system for the workpiece coordinate frame of the robot.",
                    "STATE_OF_THE_ART": "As technology progresses, various robots have been applied more and more widely to different fields. Generally, a typical robot is formed as an articulated robot arm having multiple joints, and usually furnished with a tool at an end of the robot arm, such as a welding tool, a drilling tool and so on. However, automation application of the robot is obtained anyway through human teaching in advance. Prior to the work of robot, a position of a tool center point TCP needs to be precisely calibrated, such that a controller of the robot can instruct a tooling having the TCP to move along an expected path. Nevertheless, as the working path of the robot goes more and more complicated, accuracy of the working path is highly affected by the precision of the robot. In addition, position accuracy of the robot with respect to the workpiece coordinate system can affect directly the performance of the robot. Thus, the accuracy of the workpiece coordinate system is one of important factors for precisely operating the robot. Currently, while the robot is to be automatically operated, the relative position relationship between the workpiece and the robot shall be confirmed in advance. However, position bias is always possible due to precision of positioning devices, manufacturing tolerances upon the workpiece and so on. Thus, prior to the robot work, precise coordinates for manufacturing shall be confirmed in advance with relevant calibrations in positioning the workpiece. Conventionally, a typical position calibration method for a workpiece is to overlap the TCP and each of the designated points at the workpiece through human teaching, and record coordinates of the workpiece at each individual coinciding. In the human teaching, the robot is taught to overlap the tool center with each of multiple designated points in a workpiece coordinate system, and then the calibration can be complete. However, operational performance of this calibration method is highly influenced by the user's experience. In addition, operation of the calibration method is also degraded by less accuracy of the robot and some not proper operational behaviors, from which the tool center might collide and thus damage the workpiece. On the other hand, an automated calibration method is also introduced in the art. However, this automated calibration method does have the following disadvantages. 1 In this conventional automated calibration method, external image sensors are required for monitoring the robot, and, through the designated points to overlap target points or to measure distances with the target points, coordinates of the workpiece can be thus corrected. While in performing this calibration method to different tooling, it is quite possible that images of some designated points might be blocked by the robot itself or some other objects. 2 According to this conventional calibration method, the image sensors are mounted to measure in advance so as to confirm relative positions between the sensors and the robot, or a control center is applied to directly touch the workpiece. However, both of the aforesaid operations may damage the workpiece, and human operation error might be inevitable. 3 In this conventional calibration method, a CAD file can be alternatively applied to obtain relative distances between the robot and the calibration device or the workpiece. However, the foregoing operation is time-consuming, and also the workpiece shall have obvious characteristic points so that practical measurements can be successfully made to reduce possible errors. This disclosure is an improvement of a previous patent application of utility, titled as System and method for calibrating tool center point of robot, with application Ser. 15/845,168 filed on Dec. 18, 2017, and hereinafter the prior application. In this disclosure, improvements over the prior application include at least the following. 1 Images of the designated points can be always obtained without obstacles such as the robot itself or the tooling to block the image sensors. 2 Cost for constructing the entire system is substantially reduced. Thus, the automated calibration system for a workpiece coordinate frame of a robot and the method thereof provided by this disclosure can perform calibration without direct contacting or colliding the workpiece, can avoid blocking by the robot itself or the tooling, needs not to calibrate the position relationship between the image sensors and the robot prior to practical operations, needs no additional image sensors above the workpiece, can complete the position calibration of the workpiece in one single step, and can improve various disadvantages of the prior art.",
                    "SUMMARY": [
                        "In one embodiment of this disclosure, an automated calibration system for a workpiece coordinate frame of a robot includes a physical image sensor having a first image central axis and disposed on a flange of the robot, and a controller for controlling the physical image sensor and the robot to rotate by an angle to construct a virtual image sensor having a second image central axis. The second image central axis and first image central axis are intersected at an intersection point. The controller controls the robot to move repeatedly a characteristic point on a workpiece back and forth between the first image central axis and the second image central axis until the characteristic point overlaps the intersection point, the characteristic point as a calibration point including coordinates of a plurality of joints of the robot is recorded, then a next characteristic point is introduced to repeat the aforesaid movement for overlapping the intersection point again and again until a number of the plurality of calibration points is accepted, and each of the plurality of calibration points is evaluated to calculate coordinates of a virtual tool center point and the workpiece with respect to the robot. In another embodiment of this disclosure, an automated calibration method for a workpiece coordinate frame of a robot, includes a step of providing a physical image sensor, forming an image coordinate system and having a first image central axis, disposed on a flange at an end of the robot; a step of applying the controller to rotate by an angle the physical image sensor and the robot to construct a virtual image sensor having a second image central axis, the second image central axis intersecting the first image central axis to form an intersection point; a step of the controller controlling the robot to move repeatedly a characteristic point on a workpiece back and forth between the first image central axis and the second image central axis, until the characteristic point and the intersection point are overlapped, and recording as a calibration point including coordinates of a plurality of joints of the robot; a step of the controller controlling the robot to move a next characteristic point according to the aforesaid movement to overlap the intersection point so as to generate a plurality of other calibration points; and, a step of based on each of the plurality of calibration points to calculate coordinates of a virtual tool center point and the workpiece with respect to the robot. Further scope of applicability of the present application will become more apparent from the detailed description given hereinafter. However, it should be understood that the detailed description and specific examples, while indicating exemplary embodiments of the disclosure, are given by way of illustration only, since various changes and modifications within the spirit and scope of the disclosure will become apparent to those skilled in the art from this detailed description.",
                        "The present disclosure will become more fully understood from the detailed description given herein below and the accompanying drawings which are given by way of illustration only, and thus are not limitative of the present disclosure and wherein: 1 is a schematic framework of a first embodiment of the automated calibration system for a workpiece coordinate frame of a robot in accordance with this disclosure; 2A-2B demonstrate schematically transformation relationships for the first embodiment of 1; 3A-3B demonstrate schematically constructing steps for the first embodiment of 1; 4 shows schematically a visual servo view for the first embodiment of 1; 5 shows schematically coinciding movement for the first embodiment of 1; 6 is a schematic flowchart of a first portion of a first embodiment of the automated calibration system for a workpiece coordinate frame of a robot in accordance with this disclosure; 7 is a schematic flowchart of a second portion of the first embodiment of the automated calibration system for a workpiece coordinate frame of a robot in accordance with this disclosure; 8 is a schematic flowchart of a third portion of a first embodiment of the automated calibration system for a workpiece coordinate frame of a robot in accordance with this disclosure; and 9 is a schematic flowchart of a fourth portion of a first embodiment of the automated calibration system for a workpiece coordinate frame of a robot in accordance with this disclosure."
                    ],
                    "DESCRIPTION": "In the following detailed description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the disclosed embodiments. It will be apparent, however, that one or more embodiments may be practiced without these specific details. In other instances, well-known structures and devices are schematically shown in order to simplify the drawing. Referring to the system framework of 1, the automated calibration system 1 for a workpiece coordinate frame of a robot, applied to a robot R, includes a physical image sensor 11, a virtual image sensor 12 and a controller 13. In this embodiment, the controller 13 can be a controller of the robot R, an independent computer device, or any the like. The physical image sensor 11 can be a physical camera or the like image-capturing device. The robot R, preferably a robot arm, includes a main body M having a plurality of joints J1J6. The virtual image sensor 12 can be simulated by the physical image sensor 11, i. e. , without a physical body. In addition, the automated calibration system 1 for a workpiece coordinate frame of a robot can be used for calculating coordinates of a virtual tool center point TCP and the workpiece W, in which the virtual tool center point TCP is referred to a physical tooling mounted lately at the end of the robot arm. It shall be emphasized that, in the system and method provided by this disclosure, the physical tooling is not needed during the calibration process, but the characteristic points or the designated points on the workpiece W are used for calibration. On the workpiece W, the only one requirement to be a qualified characteristic point is any point within an intersection region of the visual fields of the physical image sensor 11 and the virtual image sensor 12, such as a center or an intersection point of lines and planes. As shown in 1, the characteristic point WPi is located at the intersection point of three surfaces. However, in this disclosure, the intersection point can be at another position. The physical image sensor 11 has a visual field which has a first image central axis A, and is disposed on a flange F at an end of the robot R. The flange F is defined with a coordinate system xf-yf-zf. The visual field of the physical image sensor 11 intersects a Z axis zf originated at a center of the flange F, and the Z axis zf is perpendicular to a horizontal plane expanded by an X axis xf and a Y axis yf of the coordinate system xf-yf-zf. 2A demonstrates vector positions captured by the physical image sensor 11, and 2B shows relationships between the positions of 2A and the coordinate system x1C-y1C-z1C of the physical image sensor 11. As shown in 2A and 2B, prior to perform calibration upon the workpiece coordinates, a transformation relationship between the coordinate system xR-yR-zR of the robot R and the coordinate system x1C-y1C-z1C of the physical image sensor 11 shall be derived in advance. Firstly, after the robot R moves an arbitrary designated point on the workpiece W into an arbitrary position within the visual field of the physical image sensor 11, then the position of the designated point is defined as an origin O not shown in the figure for an image coordinate system. The image coordinate system is the coordinate system x1C-y1C-z1C formed by the images captured by the physical image sensor 11. After the origin O is moved to overlap a designated point, the robot R moves in an xR direction of the coordinate system of the robot R by an arbitrary distance LR so as to obtain a projected coordinate point Px1=x11, y11 of the physical image sensor 11, and a spatial vector for this point Px1=x11, y11 is defined as {right arrow over U1}=x11, y11, z11. Similarly, after the origin O is moved to overlap another designated point, the robot R moves in an yR direction of the coordinate system of the robot R by the arbitrary distance LR so as to obtain another projected coordinate point Py1=x21, y21 of the physical image sensor 11, and a spatial vector for this point Py1=x21, y21 is defined as {right arrow over V1}=x21, y21, z21. Similarly, after the origin O is moved to overlap a further designated point, the robot R moves in an zR direction of the coordinate system of the robot R by the arbitrary distance LR so as to obtain a further projected coordinate point Pz1=x31, y31 of the physical image sensor 11, and a spatial vector for this point Pz1=x31,y31 is defined as {right arrow over W1}=x31, y31, z31. According to orthogonality of the coordinate system, following simultaneous equations can be obtained: {right arrow over U1}{right arrow over V1}=01 {right arrow over V1}{right arrow over W1}=02 {right arrow over U1}{right arrow over W1}=03Thus, constant vectors {right arrow over U1}, {right arrow over V1}, {right arrow over W1} can be obtained. Theoretically, these simultaneous equations have two sets of solutions, z11, z21, z31 and z12, z22, z32, in which z11, z21, z31=z12, z22, z32. Thus, the distance variation between two arbitrary designated points on the workpiece in the image can be utilized to judge whether, while in moving in the coordinate system xR-yR-zR of the robot R, the designated point moves toward or away from the physical image sensor 11, such that the correct branch solution can be determined. Hence, coordinates of the physical image sensor 11 with respect to the coordinate system of the flange F is: T CCD flange = T flange base - 1 [ U 1 U 1 V 1 V 1 W 1 W 1 ] 4 in which flangeTCCD stands for the coordinates of the physical image sensor 11 with respect to the coordinate system of the flange F, and baseTflange stands for the coordinates of the flange F with respect to the base coordinate system xb-yb-zb of the robot R. Thereupon, the transformation relationship between the coordinate system of the robot R and that of the physical image sensor 11 can be obtained as follows: SR=baseTflangeflangeTCCDSc5in which Sc stands for the movement in the coordinate system x1C-y1C-z1C of the physical image sensor 11, and SR stands for the movement in the coordinate system xR-yR-zR of the robot R. After the transformation relationship of coordinate systems between the physical image sensor 11 and the robot R has been established, then any point within the visual field of the physical image sensor 11 is rotated, with respect to an arbitrary coordinate axis of the flange F, by an angle v to generate a second viewing angle to locate the virtual image sensor 12, as shown in 1. Namely, the virtual image sensor 12 is substantially a simulated object from the physical image sensor 11. Referring now to 3A and 3B, methods for deriving the coordinates of the virtual tool center point TCP and for constructing the virtual image sensor 12 are explained as follows. Step a: Move the robot R to an arbitrary characteristic point WPi within the visual field of the physical image sensor 11, and assign [0, 0, DZ] to be the coordinates of another arbitrary point D within the visual field with respect to the flange F, i. e. , D=[0, 0, DZ] as shown in 3A. Step b: Apply the physical image sensor 11 to obtain a coordinate C1 of the characteristic point WPi with respect to the image coordinate system, and then the physical image sensor 11 rotates twice about the Z axis z1c axis so as to generate two coordinates C2 and C3. Further, according to the arcs formed by coordinates C1, C2, C3, calculate a center position D1, as shown in 3B. Step c: Calculate a vector {right arrow over D1I1}c from the center position D1 to a tool-image center I1, and transform the vector {right arrow over D1I1}c into a vector with respect to the flange F, i. e. , vector {right arrow over D1I1}f=flangeTCCD{right arrow over D1I1}c. The tool-image center I1 is the intersection point of the Z axis ztool of the tooling coordinate system and the first image central axis A. Step d: Correct the coordinate of point D by D=D+LD1I1f, in which L stands for a distance function. Then, go back to perform Step a, until the center position D1 overlaps the image center point I1. Thus, coordinate I of the virtual tool center point can be obtained. Based on each change at the vector {right arrow over D1I1}c, adjust constants of the function L . Step e: Rotate the physical image sensor 11 an angle v about an arbitrary coordinate axis of the flange F so as to generate the second viewing angle to locate the virtual image sensor 12. Referring to 4, apply a visual servo means to move the physical image sensor 11 so as to have a designated or characteristic point on the workpiece to fall at the visual axis. From the captured image information, coordinates in the image-sensor coordinate system for the designated point can be obtained, and then a corresponding servo motion can be calculated accordingly for controlling the robot. The aforesaid process can be performed by the following steps. Step a1: Apply the physical image sensor 11 to capture the image information including coordinates xc1,yc1 of the designated point with respect to the physical image sensor 11. Step b1: Transform S c = x C 1 , y C 1 x C 1 2 + y C 1 2 into SR=baseTflangeflangeTCCDSc in the coordinate system of the robot R, as shown in 4. Step c1: Move the robot R along SR, until the designated point reaches the coordinate axis of the physical image sensor 11. Step d1: If the designated point does not overlap the center point of the physical image sensor 11, then go back to perform Step a1. Otherwise, if the designated point overlaps the intersection points of axial lines, then the visual servo control process is completed. Referring to 5, the physical image sensor 11 has a first image central axis A, and the virtual image sensor 12 has a second image central axis B, in which the first image central axis A is not parallel to the second image central axis B, and thus an intersection point I exists between the first image central axis A and the second image central axis B. In addition, the physical image sensor 11 and the virtual image sensor 12 have a common image-overlapped region IA or a common visual field, such that the physical image sensor 11 and the virtual image sensor 12 together can be integrated to form a 2. 5D machine vision. The virtual image sensor 12 is not a camera or the like device having a volume, but a simulated object from the physical image sensor 11. The controller 13 controls the robot R as well as the physical image sensor 11 or the virtual image sensor 12 to rotate synchronously by an angle, so as to have an arbitrary characteristic point WPi on the workpiece W to move repeatedly back and forth between the first image central axis A and the second image central axis B, as shown in 1. Preferably, the controller 13 can be a computer device. When the characteristic point WPi overlaps the intersection point I of the first image central axis A and the second image central axis B, the controller 13 records immediately a calibration point. Then, based on the calibration point, the robot R changes its posture and ready to find out and record a next calibration point. Thus, a plurality of these calibration points can be recorded at different postures of the robot R. Finally, the controller 13 evaluates the plurality of calibration points to calculate corresponding coordinates of the virtual tool center points TCP and the workpiece W with respect to the robot R, in which each of the calibration points can include coordinates of all the joints J1J6. In this embodiment, coordinates of individual joints can be respective rotational angles for individual joints with respect to a preset initial point. In other words, one calibration point stands for a set of joint values. For example, if the joint angle is used as the coordinate of the joint, thus coordinates of joints J1J6 can be expressed by J1, J2, J3, J4, J5 and J6, and hence a calibration point can be expressed by J1, J2, J3, J4, J5, J6. From 5, the controller 13 firstly controls the robot R to move the characteristic point WPi to an arbitrary position within the image-overlapped region IA between the first image central axis A and the second image central axis B. Then, orderly, the controller 13 controls the robot R to move the characteristic point WPi toward the first image central axis A to point T1, as shown by path PH1. Then, the controller 13 controls the robot R to move the characteristic point WPi toward the second image central axis B from point T1 to T2, as shown by path PH2. Similarly, the controller 13 controls the robot R to move the characteristic point WPi toward the first image central axis A from point T2 to point T3, as shown by path PH3. Then, the robot R is controlled to move the characteristic point WPi toward the second image central axis B from point T3 to point T4, as shown by path PH4. Finally, the controller 13 controls the robot R to move the characteristic point WPi toward the first image central axis A from point T4 to the intersection point I gradually. Upon when the characteristic point WPi and the intersection point I are overlapped, a calibration point CP1 is recorded. In this embodiment, if both a distance of the characteristic point WPi to the first image central axis A and that of the characteristic point WPi to the second image central axis B are individually less than a threshold valve, then the characteristic point WPi and the intersection point I can be treated to be overlapped. Generally speaking, the threshold valve can be set to be 50% pixels of the characteristic point WPi. Namely, if 50% pixels of the characteristic point WPi overlap each of the first image central axis A and the second image central axis B, then the characteristic point WPi and the intersection point I can be treated as being overlapped. As described above, the controller 13 controls the robot R to move the characteristic point WPi repeatedly back and forth between the first image central axis A and the second image central axis B, so as to obtain the first calibration point CP1. Then, the controller 13 goes further to determine whether or not a quantity of the calibration points is larger than or equal to a preset value. In this embodiment, the quantity of the calibration point needs to be larger than or equal to 3. If the quantity of the calibration points is less than 3, then the controller 13 can apply a random-number generator to produce Euler-angle increments Rx, Ry, Rz for correcting the Euler angles of the robot R so as to vary the posture of the robot R. At this time, the Euler angles of the robot R can be expressed by Rx+Rx, Ry+Ry, Rz+Rz, in which Rx, Ry, Rz stands for the original Euler angle of the robot R, Rx stands for the Yaw angle, Ry stands for the pitch angle, and Rz stands for the roll angle. If the corrected Euler angle exceeds a motion region of the robot R or the overlapped region IA, then the controller 13 would apply again the random-number generator to generate a new set of the Euler-angle increments. Then, after a new Euler angle and a next characteristic point WPi are obtained, the controller 13 controls the robot R to move the virtual tool center point TCP repeatedly back and forth between the first image central axis A and the second image central axis B. Upon when the virtual tool center point TCP overlaps the second image central axis B, a second calibration point CP2 is recorded. Then, the controller 13 determines if or not the quantity of the calibration points is larger than or equal to 3. In the case that the controller 13 judges that the quantity of the calibration points is less than 3, then the controller 13 repeats the aforesaid procedures for obtaining and recording a third calibration point CP3. The same process would be repeated until the controller 13 confirms that the quantity of the calibration points is larger than or equal to 3. As described above, in the calibration process of this disclosure, at least 3 designated points in the workpiece coordinate system are adopted as the aforesaid designated or characteristic points. For example, these three designated points can be the origin of the workpiece coordinate system, an arbitrary point at the X axis of the workpiece coordinate system, and an arbitrary point on the X-Y plane of the workpiece coordinate system. Firstly, the robot R is controlled to move an i-th designated point in the workpiece coordinate system i. e. , the i-th characteristic point WPi on the workpiece W into an overlapped visual region of the physical image sensor 11 and the virtual image sensor 12. The aforesaid moving operation is repeated until the number i is greater than a present number, so that information-collection process of the designated points for calibration can be completed. Based on the plurality of calibration points, coordinates of the virtual TCP and the workpiece can be calculated. As shown in 1, the controller 13 can evaluate the plurality of calibration points CP1-CP3 to calculate coordinates of the virtual tool center point TCP. In particular, coordinates of each calibration point CP1CP3 can be obtained through Denavit-Hartenberg parameters of the robot R, coordinates of the joints J1J6, and information of the virtual tool center point TCP in the coordinate system xf-yf-zf of the flange F. The Denavit-Hartenberg parameters can include a link offset d, a joint angle , a link length a, a link twist u and so on. The coordinates of the virtual tool center point TCP can be derived by the following equation 6: T1iT2=P6in which matrix T1i is a 44 transformation matrix for transforming coordinates of the i-th calibration point from the basic coordinate system xb-yb-zb to the coordinate system xf-yf-zf of the flange F, column T2 stands for the coordinates of the virtual tool center point TCP in the coordinate system of the flange F, and column P stands for the coordinates of the calibration point in the basic coordinate system xb-yb-zb. Since each of the calibration points can apply equation 6 to obtain three linear equations, thus 3n linear equations can be obtained for n calibration points. Then, a pseudo-inverse matrix can be applied to derive the coordinates of the virtual tool center point TCP. As shown below, equation 7 is derived from equation 6. [ e 11 i e 12 i e 13 i e 14 i e 21 i e 22 i e 23 i e 24 i e 31 i e 32 i e 33 i e 34 i 0 0 0 1 ] [ T x T y T z 1 ] = [ P x P y P z 1 ] 7 in which column e11i, e21i, e31i stands for a coordinate vector for the i-th calibration point at the xf axis of the basic coordinate system xb-yb-zb, column e12i, e22i, e32i stands for a coordinate vector for the i-th calibration point at the yf axis of the basic coordinate system xb-yb-zb, and column e13i, e23i, e33i stands for a coordinate vector for the i-th calibration point at the zf axis of the basic coordinate system xb-yb-zb. From equation 7, equations 8 and 9 can be derived as follows: [ e 1 1 1 e 1 2 1 e 1 3 1 - 1 0 0 e 2 1 1 e 2 2 1 e 2 3 1 0 - 1 0 e 3 1 1 e 3 2 1 e 3 3 1 0 0 - 1 e 1 1 2 e 1 2 2 e 1 3 2 - 1 0 0 e 2 1 2 e 2 2 2 e 2 3 2 0 - 1 0 e 3 1 2 e 3 2 2 e 3 3 2 0 0 - 1 e 1 1 3 e 1 2 3 e 1 3 3 - 1 0 0 e 2 1 3 e 2 2 3 e 2 3 3 0 - 1 0 e 3 1 3 e 3 2 3 e 3 3 3 0 0 - 1 ] [ T x T y T z P x P y P z ] = [ - e 1 4 1 - e 2 4 1 - e 3 4 1 - e 1 4 2 - e 2 4 2 - e 3 4 2 - e 1 4 3 - e 2 4 3 - e 3 4 3 ] 8 [ T x T y T z P x P y P z ] = T 3 t T 3 T 3 t - 1 [ - e 1 4 1 - e 2 4 1 - e 3 4 1 - e 1 4 2 - e 2 4 2 - e 3 4 2 - e 1 4 3 - e 2 4 3 - e 3 4 3 ] 9 in which T 3 = [ e 1 1 1 e 1 2 1 e 1 3 1 - 1 0 0 e 2 1 1 e 2 2 1 e 2 3 1 0 - 1 0 e 3 1 1 e 3 2 1 e 3 3 1 0 0 - 1 e 1 1 2 e 1 2 2 e 1 3 2 - 1 0 0 e 2 1 2 e 2 2 2 e 2 3 2 0 - 1 0 e 3 1 2 e 3 2 2 e 3 3 2 0 0 - 1 e 1 1 3 e 1 2 3 e 1 3 3 - 1 0 0 e 2 1 3 e 2 2 3 e 2 3 3 0 - 1 0 e 3 1 3 e 3 2 3 e 3 3 3 0 0 - 1 ] , T3t is a transpose matric of T3, and T3T3t1 is an inverse matrix of T3T3t. If the quantity of the calibration points is met, plug entries of matrix T1i with respect to the known i-th calibration point into equation 8, and, through a shift operation upon matrix T3, equation 9 can be obtained. Thus, coordinates Tx, Ty, Tz of the virtual tool center point TCP in the coordinate system of the flange F and coordinates Px, Py, Pz of the virtual tool center point TCP in the coordinate system of the robot R xR-yR-zR can be obtained, calibrations of coordinates Tx, Ty, Tz of the virtual tool center point TCP can be completed, and coordinates for the workpiece W can be calculated. As described above, in this embodiment, the automated calibration system for a workpiece coordinate frame of a robot 1 can apply the visual servo means to automatically calibrate the coordinate of the robot with respect to the workpiece to be machined, with an acceptable calibration precision. Thus, related labor and time cost can be effectively reduced. In addition, since the automated calibration system for a workpiece coordinate frame of a robot 1 can calibrate the coordinates of the robot with respect to the workpiece in a single calibration process, so the automated calibration system 1 provided by this disclosure can effectively improve existing shortcomings in the art. Referring now to 6, the method for obtaining a transformation relationship between the coordinate system xR-yR-zR of the robot R and the coordinate system x1C-y1C-z1C of the physical image sensor 11 can include the following steps. Step S51: Control the robot R to move an arbitrary designed point within a visual field of the physical image sensor 11 a distance LR along a horizontal axis xR of the coordinate system xR-yR-zR of the robot R from an arbitrary position in an image-overlapped region IA, and obtain a first projection coordinate Px1 through the physical image sensor 11. Step S52: Control the robot R to move the designed point the distance LR along a vertical axis yR of the coordinate system xR-yR-zR of the robot R from the aforesaid position in the image-overlapped region IA, and obtain a second projection coordinate Py1 through the physical image sensor 11. Step S53: Control the robot R to move the designed point the distance LR along another vertical axis zR of the coordinate system xR-yR-zR of the robot R from the aforesaid position in the image-overlapped region IA, and obtain a third projection coordinate Pz1 through the physical image sensor 11. Step S54: Provide a first spatial vector {right arrow over U1}, a second spatial vector {right arrow over V1} and a third spatial vector {right arrow over W1} corresponding to the first projection coordinate Px1, the second projection coordinate Py1 and the third projection coordinate Pz1, respectively. Step S55: Based on an orthogonal relationship formed by the first spatial vector {right arrow over U1}, the second spatial vector {right arrow over V1} and the third spatial vector {right arrow over W1}, calculate the first spatial vector {right arrow over U1}, the second spatial vector {right arrow over V1} and the third spatial vector {right arrow over W1}. Step S56: Based on an orthogonal relationship formed by the first spatial vector {right arrow over U1}, the second spatial vector {right arrow over V1} and the third spatial vector {right arrow over W1}, calculate the first spatial vector {right arrow over U1}, the second spatial vector {right arrow over V1} and the third spatial vector {right arrow over W1}, referring to equation 4. Referring to 7, the method for obtaining a transformation relationship between the coordinate system xR-yR-zR of the robot R and the coordinate system x2-y2c-2c of the virtual image sensor 12 can include the following steps. Step S61: Control the robot R to move an arbitrary designed point within a visual field of the virtual image sensor 12 a distance LR along a horizontal axis xR of the coordinate system xR-yR-zR of the robot R from an arbitrary position in an image-overlapped region IA, and obtain a first projection coordinate Px2 through the virtual image sensor 12. Step S62: Control the robot R to move the designed point the distance LR along a vertical axis yR of the coordinate system xR-yR-zR of the robot R from the aforesaid position in the image-overlapped region IA, and obtain a second projection coordinate Py2 through the virtual image sensor 12. Step S63: Control the robot R to move the designed point the distance LR along another vertical axis zR of the coordinate system xR-yR-zR of the robot R from the aforesaid position in the image-overlapped region IA, and obtain a third projection coordinate Pz2 through the virtual image sensor 12. Step S64: Provide a first spatial vector {right arrow over U2}, a second spatial vector {right arrow over V2} and a third spatial vector {right arrow over W2} corresponding to the first projection coordinate Px2, the second projection coordinate Py2 and the third projection coordinate Pz2 respectively. Step S65: Based on an orthogonal relationship formed by the first spatial vector {right arrow over U2}, the second spatial vector {right arrow over V2} and the third spatial vector {right arrow over W2}, calculate the first spatial vector {right arrow over U2}, the second spatial vector {right arrow over V2} and the third spatial vector {right arrow over W2}. Step S66: Based on the first spatial vector {right arrow over U2}, the second spatial vector {right arrow over V2} and the third spatial vector {right arrow over W2}, calculate a transformation relationship between the coordinate system xR-yR-zR of the robot R and a coordinate system x2C-y2C-z2C of the virtual image sensor 12, referring to equation 8. Referring now to 8, the method for the automated calibration system for a workpiece coordinate frame of a robot 1 includes the following steps. Step S71: Provide a physical image sensor 11 having a first image central axis A. Step S72: Provide a virtual image sensor 12 having a second image central axis B intersecting the first image central axis A at an intersection point I. Step S73: Control a robot R to move a characteristic point WPi of a workpiece repeatedly back and forth between the first image central axis A and the second image central axis B. Step S74: Upon when the characteristic point WPi and the intersection point I are overlapped, record a calibration point including coordinates of a plurality of joints J1J6 of the robot R. Step S75: Repeat the aforesaid steps to generate a plurality of another calibration points. Step S76: Based on all the plurality of calibration points, calculate coordinates of a virtual tool center point TCP and the workpiece. Referring now to 9, a detail flowchart of the automated calibration method for a workpiece coordinate frame of a robot in accordance with this disclosure includes the following steps. Step S81: Given i=1, define an i-th characteristic point WPi of a workpiece W. Step S82: A controller 13 controls a robot R to move the characteristic point WPi into a common visual field of a physical image sensor 11 and a virtual image sensor 12. Step S83: The controller 13 moves the characteristic point WPi repeatedly back and forth between a first image central axis A and a second image central axis B, until the characteristic point WPi hits an intersection point I of the first image central axis A and the second image central axis B. Step S84: The controller 13 determines whether or not a distance between the characteristic point WPi and the intersection point I of the first image central axis A and the second image central axis B is smaller than a threshold valve. If positive, go to perform Step S85. If negative, then go to perform Step S841. Step S841: The controller 13 applies a random-number generator to generate Euler-angle increments Rx,Ry,Rz to correct corresponding Euler angles of the robot R, and the method goes back to perform Step S83. Step S85: The controller 13 records a first set of joint values of the characteristic point WPi, i. e. , a first calibration point. Step S86: The controller 13 determines whether or not a quantity of the first set of joint values is larger than or equal to 4. If positive, go to perform Step S87. If negative, go to perform Step S841. In this disclosure, the aforesaid quantity can be any other integer. Step S87: The controller 13 determines whether or not a quantity of the characteristic points WPi is larger than or equal to a preset number. If positive, go to perform Step S88. If negative, go to perform Step S871. Step S871: Given i=i+1. Step S88: Derive coordinates of a virtual tool center point TCP and the workpiece W with respect to the robot R by a tool-center calibration method which is disclosed in the prior application. As described above, in the automated calibration system and method for a workpiece coordinate frame of a robot provided by this disclosure, the calibration process mainly includes four portions: 1 mounting a physical image sensor on a flange at an end of the robot; 2 obtaining a transformation relationship between a coordinate system of the flange of the robot and that of the physical image sensor so as to transform motion information captured as an image into information convenient for the robot; 3 applying a multi-vision means to construct the virtual image sensor and the position of the virtual tool center point so as to generate a 2. 5D machine vision; and, 4 applying a visual servo means to control the robot to overlap a designated point on the workpiece and an intersection point of two image axes. In addition, the aforesaid overlapping operation can be achieved by any of the two following method: 41 controlling the robot having four, for example, different postures to overlap the virtual tool center point and the origin of the workpiece coordinate system, then overlapping the virtual tool center point of any posture of the robot and each of an arbitrary point at an X axis of the workpiece coordinate system and another arbitrary point on the X-Y plane, and recording the coordinates; or 42 controlling the robot having four, for example, different postures to overlap the virtual tool center point and each of four known coordinate points on the workpiece of the workpiece coordinate system, and recording the coordinates. According to this disclosure, only one physical image sensor is needed to be furnished onto the flange at the end of the robot, and, through calibrating the characteristic points, the calibration process avoids direct contact between the workpiece and the robot, and can complete the position correction of the workpiece in a single calibration process. Thereupon, the calibration precision can be effectively enhanced. With respect to the above description then, it is to be realized that the optimum dimensional relationships for the parts of the disclosure, to include variations in size, materials, shape, form, function and manner of operation, assembly and use, are deemed readily apparent and obvious to one skilled in the art, and all equivalent relationships to those illustrated in the drawings and described in the specification are intended to be encompassed by the present disclosure.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/41/335/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9916033148765564,
                    "F_SPEC_PARAMS": [
                        "accuracy",
                        "damage the workpiece"
                    ],
                    "S_SPEC_PARAMS": [
                        "damage the workpiece,",
                        "human operation error might be inevitable",
                        "time-consuming,",
                        "reduce possible errors",
                        "operational performance",
                        "images of some designated points might be blocked by the robot itself or some other objects"
                    ],
                    "A_PARAMS": [
                        "position bias",
                        "precision of positioning devices,"
                    ],
                    "F_SENTS": [
                        "However, position bias is always possible due to precision of positioning devices, manufacturing tolerances upon the workpiece and so on.",
                        "In addition, operation of the calibration method is also degraded by less accuracy of the robot and some not proper operational behaviors, from which the tool center might collide and thus damage the workpiece."
                    ],
                    "S_SENTS": [
                        "However, both of the aforesaid operations may damage the workpiece, and human operation error might be inevitable.",
                        "However, the foregoing operation is time-consuming, and also the workpiece shall have obvious characteristic points so that practical measurements can be successfully made to reduce possible errors.",
                        "However, operational performance of this calibration method is highly influenced by the user's experience.",
                        "However, this automated calibration method does have the following disadvantages.",
                        "While in performing this calibration method to different tooling, it is quite possible that images of some designated points might be blocked by the robot itself or some other objects."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Accuracy of Measurement",
                        "Harmful Factors Acting on Object"
                    ],
                    "F_SIM_SCORE": 0.6541807651519775,
                    "S_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object",
                        "Reliability",
                        "Waste of Time",
                        "Productivity",
                        "Area of Moving Object"
                    ],
                    "S_SIM_SCORE": 0.5533697307109833,
                    "GLOBAL_SCORE": 1.652521419950894
                },
                "sort": [
                    1.6525214
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11279384-20220322",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11279384-20220322",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-05-10",
                    "PUBLICATION_DATE": "2022-03-22",
                    "INVENTORS": [
                        "Juan Vega",
                        "Frank Thissen",
                        "John Morgan",
                        "Arun Aruljothi"
                    ],
                    "APPLICANTS": [
                        "Reliabotics, LLC    ( New Brunswick , US )"
                    ],
                    "INVENTION_TITLE": "Robotic system for installing equipment on vertical surfaces of railway tunnels",
                    "DOMAIN": "B61D 1500",
                    "ABSTRACT": "An automated system and method of mounting wayside equipment on a surface that is adjacent to railway tracks. A robot is carried by a railway car with an included odometry system. The robot has an articulating arm that can reach between the railway car and an adjacent wall. The robot is provided with working head units. The robot can connect to, and disconnect from, the various working head units in order to perform different tasks. The tasks performed by the robot include scanning the wall for defects and obstructions that may prevent a proper mounting, drilling holes in the wall, mounting bolts in the holes, mounting brackets to the bolts, and connecting electronics units to the brackets. The robot can optionally clean the mounting site and test the mounting site for signal strength.",
                    "CLAIMS": "1. A method of mounting wayside equipment on a surface that is adjacent to railway tracks, said method comprising the steps of: providing a railway car capable of traveling on said railway tracks adjacent to said surface; providing a robot on said railway car; providing working head units for said robot on said railway car, wherein said robot is capable of selecting and extending one of said working head units from said railway car to said surface, and wherein one of said working head units is a scanning head with a surface profiler; providing a mounting bracket that is accessible by said robot on said railway car; scanning said surface with said scanning head for an installation site appropriate for receiving said mounting bracket; attaching said mounting bracket to said installation cite utilizing said robot; and attaching said wayside equipment to said mounting bracket utilizing said robot. 2. The method according to claim 1, wherein said robot moves said scanning head from said railway car toward said surface to scan said installation site with said scanning head. 3. The method according to claim 1, wherein attaching said mounting bracket to said surface includes drilling holes in said surface. 4. The method according to claim 3, wherein one of said working head units is a drill head for drilling said holes, wherein said robot moves said drill head from said railway car toward said surface to drill said holes with said drill head. 5. The method according to claim 3, wherein attaching said mounting bracket to said surface includes setting anchor bolts in said holes. 6. The method according to claim 5, wherein one of said working head units includes a powered hammer for driving said anchor bolts into said holes, wherein said robot moves said powered hammer from said railway car toward said surface to drive said anchor bolts into said holes. 7. The method according to claim 3, wherein attaching said mounting bracket to said surface includes affixing said mounting bracket to said anchor bolts with said robot. 8. The method according to claim 1, further including testing said installation site by positioning a test electronics unit onto said installation site with said robot and running tests using said test electronics unit. 9. The method according to claim 1, further including cleaning said installation site using said robot. 10. A method of mounting wayside equipment on a surface that is adjacent to railway tracks, said method comprising the steps of: providing a railway car capable of traveling on said railway tracks adjacent to said surface; providing a robot on said railway car; providing a mounting bracket that is accessible by said robot on said railway car; providing working head units for said robot on said railway car, wherein said robot is capable of selecting and extending one of said working head units from said railway car to said surface, and wherein said working head units include a first working head for drilling holes in said surface, a second working head for installing said mounting bracket and another working head that scans said surface for an installation site appropriate for receiving said mounting bracket; attaching said mounting bracket to said surface utilizing said working head units as manipulated by said robot; and attaching said wayside equipment to said mounting bracket. 11. The method according to claim 10, wherein said second working head sets anchor bolts in said holes and connects said mounting bracket to said anchor bolts. 12. The method according to claim 10, wherein said working head units include a third working head that is manipulated by said robot and attaches said wayside equipment to said mounting bracket. 13. The method according to claim 10, wherein said working head units include a fifth working head that cleans said installation site. 14. The method according to claim 10, wherein said installation site is on a beam and said working head units include a working head that attaches said mounting bracket to said beam.",
                    "FIELD_OF_INVENTION": "In general, the present invention relates robotic systems that are designed and programmed to install wayside equipment on vertical surfaces. More particularly, the present invention relates to robotic systems that are mounted to rail cars and are used to install equipment on the surfaces of railway tunnels.",
                    "STATE_OF_THE_ART": "Many railway systems have trains that pass through tunnels. In certain cities, the railway systems are subways, wherein most of the train routes are directed through underground tunnels. In many instances, some of the tunnels can be over a century old. In this long period of time, the walls of the tunnel have been exposed to many contaminants. As such, many of the tunnel walls between stations are coated in thick deposits of dirt and grime. Furthermore, walls of many tunnels are riddled with cracks, old equipment mounts, running cables, and the like. As railway systems modernize, so do the control systems utilized by those railway systems. Old mechanical switches that are used to detect the presence of a train are being replaced with more modern electronic sensors. The electronic sensors mount to the surfaces of a tunnel and detect the presence and/or absence of a train in a particular section of the tunnel. The sensor data is then communicated to a central control facility through a data network. There are many problems associated with modernizing transit system by adding electronic sensors and other modern wayside equipment to railway tunnels. Most of the problems are associated with positioning and installation of the needed electronic sensors. In an underground tunnel, the various electronic sensors must be positioned within the line of sight of the previously installed sensor. This means that if a tunnel dips, rises and/or turns, then dozens of sensors may have to be installed per running mile of track. Furthermore, those sensors must be installed at mounting positions on the surfaces of the tunnels that are appropriate. Such mounting positions must meet many criteria and are rare in older tunnels. Mounting positions must not be obstructed by poles or other equipment. Mounting positions cannot be compromised by joint seams, cracks or crumbling concrete. Mounting positions must be clear of the moving train and railway maintenance equipment. Lastly, mounting positions must be clear of cables, tunnel mounted equipment, and dripping water. When considered cumulatively, there are actually very few locations on a tunnel wall that are well suited for receiving an electronic sensor. The few locations that are appropriate are often difficult to reach and the sensor units must be mechanically mounted into the material of the wall. This further limits the number of locations that a sensor unit can be placed because many selected locations get damaged or otherwise are discovered to be inappropriate during the mechanical installation process. It will therefore be understood that mounting electronic sensors in a railway tunnel is a complex, labor intensive and time-consuming process. Appropriate locations for mounting sensors must be located that meet both the line-of-sight and mounting surface criteria. The selected locations must then be cleaned and prepared for mounting. The tunnel wall must then be worked with tools to mechanically install the mounting for the sensor. Lastly, the sensor unit must be mounted in place and aligned. If any step fails, then a new location must be found and the process repeated. For the above referenced reasons, the installation of sensors in a railway tunnel requires a large commitment of equipment and labor. Furthermore, the railway tunnel must be shut down to traffic while the installation takes place. If the work is only performed during overnight, low traffic time, it can take many months, possibly years to install electronic sensors and other wayside equipment along any one railway line. The cost in time, labor and line closures, therefore, makes the modernization of railway lines with electronic sensors unappealing to many railway operators. A need therefore exists for a system and method of installing wayside equipment in railway tunnels, that is labor efficient, cost effective and time efficient. This need is met by the present invention as described and claimed below.",
                    "SUMMARY": [
                        "The present invention is an automated system and method of mounting wayside equipment on a surface that is adjacent to railway tracks. The system is mounted to a railway car that is capable of traveling on the railway tracks. As the railway car travels, it passes the walls onto which electronics units are to be mounted. The railway car is equipped with odometry equipment that automatically measures the location of the railway car in relation to the tracks in order to stop the train at predetermined installation waypoints and to record actual installation locations. A robot is carried by the railway car. The robot has an articulating arm that can reach between the railway car and the surface adjacent to the railway tracks. The robot is provided with a rack of working head units. The robot can connect to, and disconnect from, the various working head units in order to perform different tasks. The tasks performed by the robot include scanning the surface for defects and obstructions that may prevent a proper mounting, drilling holes in the surface, mounting bolts in the holes, mounting brackets to the bolts, and connecting electronics units to the brackets. The robot can optionally clean the mounting site and test the mounting site for signal strength. The robot repeats its actions as the railway car moves along the railway tracks. In this manner, a series of electronics units can be mounted along a railway in a labor and cost-efficient manner.",
                        "For a better understanding of the present invention, reference is made to the following description of an exemplary embodiment thereof, considered in conjunction with the accompanying drawings, in which: 1 is a perspective view of an exemplary embodiment of a sensor assembly being installed using the present invention installation system and method; 2 shows an exemplary embodiment of the installation system installing the sensor assembly of 1 onto a surface of a railway tunnel; 3 is a schematic of the exemplary installation system shown in 2; 4 shows an exemplary embodiment for a surface scanning head, which is utilized by the installation system of 2 and 3; 5 shows an exemplary embodiment for a ranging head, which is utilized by the installation system of 2 and 3; 6 shows an exemplary embodiment for a drill head, which is utilized by the installation system of 2 and 3; 7 shows an exemplary embodiment for a bracket fastening head, which is utilized by the installation system of 2 and 3; 8 shows an exemplary embodiment for a sensor installation head, which is utilized by the installation system of 2 and 3; 9 is a block logic flow diagram outlining a methodology for operations; and 10 shows an exemplary embodiment for a column bracket head, which is utilized by the installation system of 2 and 3."
                    ],
                    "DESCRIPTION": "Although the present invention system and method can be embodied in many ways in order to install different wayside equipment, only one exemplary embodiment is illustrated for the purposes of description and discussion. The exemplary embodiment shows an electronic sensor being installed. The exemplary embodiment is selected in order to set forth one of the best modes contemplated for the invention. The illustrated embodiment, however, is merely exemplary and should not be considered a limitation when interpreting the scope of the appended claims. The present invention is a system and method of installing wayside equipment onto the walls 12 of railway tunnels 13. Referring to 1, an exemplary sensor assembly 10 is shown as one type of wayside equipment. The sensor assembly 10 includes an electronics unit 14 and a supporting mounting bracket 16. The mounting bracket 16 has a base plate 18 and a neck 20 that extends from the base plate 18. The base plate 18 connects to the tunnel wall 12. The neck 20 extends from the base plate 18 and interconnects with the electronics unit 14. The mounting bracket 16 is bolted to the wall 12 of the railway tunnel 13. The base plate 18 of the mounting bracket 16 is flat and can only be mounted to a generally flat wall surface that contains irregularities and a curvature below within acceptable ranges. The acceptable ranges for surface irregularities and curvature vary with the area of the base plate 18. Anchor bolts 22 and nuts 24 are used to fasten the mounting bracket 16 directly to the wall 12 of the tunnel 13. Holes 25 are formed through the base plate 18 to accommodate the anchor bolts 22. The anchor bolts 22 are driven directly into the wall 12 of a tunnel 13, wherein the anchor bolts 22 must be advanced into bolt holes 26 that have been drilled into the wall 12. Once engaged with the wall 12, the nuts 24 are used to engage the anchor bolts 22 and connect the base plate 18 to the wall 12. A separate nut and bolt 28 are also used to adjust the mounting neck 20. Once adjusted, the electronics unit 14 is connected to the mounting neck 20. It will be understood that the base plate 18, mounting neck 20 and electronics unit 14 can all vary in shape and size depending upon the type of sensor assembly 10 being installed and the dimensions available within the railway tunnel 13. Referring to 2 and 3, the installation system 30 is shown. The installation system 30 is mounted on a railway car 32 that rides on the rails 34 through the railway tunnel 13. The railway car 32 is selected to meet the gauge, length, width and height requirements of the railway system. In this manner, the railway car 32 can be moved along the tracks of the railway system during the installation process. The railway car 32 has odometry equipment 33 that can automatically measure the relative position of the railway car 32 as it travels along the rails 34. This positional information enables the railway car 32 to travel to desired installation locations and measure the final installation position of the electronics unit 14. The railway car 32 has a work platform 36. A robot 38 is positioned on the work platform 36. The robot 38 has an articulating arm 40 that is capable of reaching from the work platform 36 to the wall 12 of the railway tunnel 13. The robot 38 is programmable. As such, it is capable of repeatedly performing programmed movements. Additionally, the robot 38 can also be manually controlled by a trained operator. A computer controller 42 for the robot 38 and manual controls 44 of the robot 38 are positioned in an operator's station 46 on the work platform 36. Additionally, one or more display screens 48 are provided at the operator's station 46, wherein an operator can remotely view various camera feeds, robot control data and other feedback data needed to operate the robot 38 and oversee its work. The articulated arm 40 of the robot 38 terminates with a tool head coupler 50. The tool head coupler 50 enables the articulating arm 40 to selectively connect to, and disconnect from, a variety of working head units 52. Each working head unit 52 serves a different purpose, as will be later explained. The working head units 52 are held at indexed positions on a tool rack 54. In this manner, the positions of the various working head units 52 is programmed into the robot 38 and the articulating arm 40 can interconnect with, and disconnect from, any of the working head units 52 on the tool rack 54. If all of the working head units 52 on the tool rack 54 are within the reach of the articulating arm 40, then both the tool rack 54 and the robot 38 can be set into fixed positions. However due to the size and number of the working head units 52, either the tool rack 54 and/or the robot 38 can be mounted on tracks 56 that enable the robot 38 and the tool rack 54 to move relative to one another, therein providing access to all the working head units 52. The working head units 52 provided on the tool rack 54 depend upon the requirements of the installation project. In the shown embodiment, the working head units 54 include a surface scanning head 60, a ranging head 62, a drill head 64, a bracket fastening head 66, and a sensor installation head 68. Optional additional working head units 52 include a column bracket head 70 and a cleaning head 72. The different working head units 52 may require electrical power, pneumatic pressure, and or hydraulic pressure to operate. Such supplies are carried on the railway car 32. For instance, the railway car 32 may include a generator 74 and fuel 76 to operate the generator 74. The generator 74 can supply the electrical power needed to operate the robot 38 as well as the power needed to operate, for example, an air compressor 80, a filtered vacuum 82 and/or a hydraulic pump 84. In this manner, the overall installation system 30 is self-sufficient for operations and no hoses or wires need to be extended through the railway tunnel 13. The railway car 32 is also supplied with the various parts that are to be installed within the railway tunnel 13. Those parts include the electronics unit 14 and brackets 16 of the sensor assemblies 10, as well as the anchor bolts 22 and the nuts 24. Each of these parts are held in supply bins on the railway car 32. The parts may be fed to specific pickup locations that can be accessed by the robot 38. Alternatively, the parts can be loaded into the various working head units 52 prior to the working head units 52 being engaged by the robot 38. One of the working head units 52 operated by the robot 38 is the surface scanning head 60. Referring to 4 in conjunction with 3, it can be seen that the surface scanning head 60 has a coupler 86 that can be selectively engaged by the articulating arm 40 of the robot 38. The surface scanning head 60 includes an array of infrared distance sensors 88. The distance sensors 88 detect the distance between the surface scanning head 60 and the wall 12, so that the position of the robot 38 and the articulating arm 40 relative the tunnel wall 12 becomes known. At least one surface profiling device 90, such as a camera, is provided. The camera 90 is connected to a slide 92 and is scanned back and forth across an area of interest on the tunnel wall 12 by a linear actuator 94. The camera 90 creates a depth map of the scanned area. By analyzing the depth mapping data, it can be determined if the area of interest is flat, defect-free, not curved, free of foreign objects, lacks surface irregularities, and is otherwise appropriate for use in mounting. A ranging head 62 can also be operated by the robot 38. Referring to 5, it can be seen that the ranging head 62 has a coupler 96 that can be selectively engaged by the articulating arm 40 of the robot 38. The ranging head 62 contains a test electronics unit 98. The test electronics unit 98 can simulate the operations of the real electronics units 14 1 being installed. The ranging head 62 is moved by the robot 38 so that the test electronics unit 98 is positioned and oriented in the same place that the actual electronics unit 14 1 will occupy, should it be installed. The ranging head 62 tests if a sensor unit set into such a position and orientation would be unobstructed and can properly communicate with an adjacent sensor unit that has been earlier installed. Referring to 6 in conjunction with 2 and 3, an exemplary drill head 64 is explained. The drill head 64 has a coupler 99 that can be selectively engaged by the articulating arm 40 of the robot 38. The drill head 64 contains hammer drills 100 that can be electrically, pneumatically or hydraulically powered. The hammer drills 100 hold drill bits 102 at positions that correspond to mounting points needed to mount the sensor assembly 10 of 1. The drill bits 102 are sized to create the bolt holes 26 needed to receive the anchor bolts 22. The hammer drills 100 are advanced by the robot 38, wherein the robot 38 can detect the force being applied to advance the hammer drills 100 during operation. The drill head 64 may also contain blowing nozzles 108 for blowing air toward the tunnel wall 12 and removing dust created by the hammer drills 100. An evacuation port 110 can also be provided that is connected to the filtered vacuum 82. In this manner, the dust and debris created by the drill head 64 can mostly be recovered, thereby eliminating the need for any secondary cleaning of the railway tunnel 13. Referring to 7 in conjunction with 2 and 3, an exemplary bracket fastening head 66 is shown. The bracket fastening head 66 has a coupler 112 that can be selectively engaged by the articulating arm 40 of the robot 38. The bracket fastening head 66 has a receptacle area 114 and gripper 116 that can lift and retain a base plate 18 and neck 20 of a mounting bracket 16. The bracket fastening head 66 also contains chucks 118 for holding a set of anchor bolts 22 and drive hammers 120 that can be used to drive the anchor bolts 22 into pre-drilled bolt holes 26. The bracket fastening head 66 also contains powered nut runners 122 that are capable of holding nuts 24 and driving those nuts 24 onto the anchor bolts 22. Referring to 8, an exemplary sensor installation head 68 is shown. The sensor installation head 68 has a coupler 123 that can be selectively engaged by the articulating arm 40 of the robot 38. The sensor installation head 68 has a clamp 124 that can grip the electronics unit 14. The sensor installation head 68 also has a nut runner 127 that can engage the nut and bolt 28 on the neck 20 of the mounting bracket 16, therein adjusting the mounting bracket 16. Referring to 9 in conjunction with all previous figures, the methodology of using the installation system 30 is explained. The railway car 32 is loaded and taken into a tunnel 13 where the sensor assemblies 10 are to be mounted. The railway car is positioned using the odometry equipment 33. See Block 121. Once in the correct location, the operator visually scans the wall 12 of the tunnel 13 looking for some candidate area that is not obviously inappropriate. See Block 123. If the candidate area is particularly dirty to a point where the surface characteristics of the tunnel wall 12 cannot be readily ascertained, then the operator can optionally clean the candidate area. See Block 125 and Block 126. To clean the candidate area, the operator can instruct the robot 38 to connect to the cleaning head 72. The cleaning head 72 can contain wheel brushes and/or blowers that can remove some of the contamination from the tunnel wall 12. The type of cleaner head 72 can be customized to the contamination type common within a particular railway tunnel. After the candidate area is cleaned, or if the candidate area does not require cleaning, then a scanning subroutine is executed. In executing the scanning subroutine, the robot connects to the surface scanning head 60. See Block 128. The articulating arm 40 of the robot 38 moves the surface scanning head 60 to the candidate area. See Block 130. The infrared distance sensors 88 provide feedback and cause the robot 38 to hold the surface scanning head 60 at a predetermined distance from the candidate area. The camera 90 is then used to create a depth map of the tunnel wall 12 within the candidate area. See Block 132. The depth map is analyzed by the computer controller 42 to determine if the candidate area meets threshold criteria. See Block 134. The threshold criteria include, but are not limited to, a certain degree of flatness, the lack of obstructions, the lack of cracks, the lack of joints, and the lack of surface moisture. If the candidate area fails to meet the set criteria, then the operator selects another candidate area and the initial steps are repeated. See Block 136 and loop line 138. If the candidate area meets the initial criteria, then the robot changes the working head to a ranging head 62. See Block 140. The ranging head 62 includes a test electronics unit 98. The robot 38 positions the test electronics unit 98 in the position being considered for the real sensor assembly 10. See Block 142. Transmission tests are then run using the test electronics unit to ensure that communications are clear and unencumbered. See Block 144. If the transmission test fails, then the operator selects another candidate area and the initial steps are repeated. See Block 136. If the transmission test is successful, then the physical installation of the sensor assembly 10 begins. The robot 38 changes working heads to the drill head 64. See Block 146. The robot 38 knows the location of the candidate area from the data received using the surface scanning head 60. The robot 38 moves the drill head 64 to the candidate area and begins drilling two bolt holes 26 using the hammer drills 100. See Block 148. The hammer drills 100 are monitored for a minimum drill rate and a maximum drill force. For example, a minimum drill rate can be 5 millimeters per second. A maximum drill force can be one-hundred newtons. The hammer drills 100 are operated until the bolt holes 26 are deep enough to receive the anchor bolts 22 therein. If the hammer drills fail to meet the drilling criteria for minimum drill rate and maximum drill force, then it can be assumed that the drill site is inappropriate. This may be due to an obstruction, such as a segment of rebar set behind the concrete of the tunnel wall 12. If this is the case, the operator selects another candidate area and the initial steps are repeated. See Block 136. As the bolt holes 26 are drilled, the drilling debris is removed using the blow nozzles 108, wherein the debris is drawn into the evacuation port 110. The final drilled bolt holes 26 are also blown clean, to ensure no debris remains within the drilled bolt holes 26. See Block 150. Once the bolt holes 26 are drilled and cleaned, the robot 38 changes to the bracket fastening head 66. See Block 152. The bracket fastening head 66 is loaded with the mounting bracket 16 of a sensor assembly 10, two anchor bolts 22 and two nuts 24. The robot 38 moves the base plate 18 against the tunnel wall 12 and aligns the holes in the base plate 18 with the bolt holes 26 drilled into the tunnel wall 12. See Block 154. Once the base plate 18 of the mounting bracket 16 is aligned, the anchor bolts 22 are driven into the bolt holes 26 using the drive hammers 120. See Block 156. After the anchor bolts 22 are fully set into the bolt holes 26, the nut runner 116 on the bracket fastening head 66 threads the nuts 24 onto the anchor bolts 22, therein bolting the mounting bracket 16 into place. See Block 158. Once the mounting bracket 16 is in place, the robot 38 changes working heads to the sensor installation head 68. See Block 160. The sensor installation head 68 has a clamp 124 that is loaded with the electronics unit 14. See Block 162. The robot 38 manipulates the sensor installation head 68 until the electronics unit 14 engages the previously installed mounting bracket 16. See Block 164. The nut runner 126 on the sensor installation head 68 engages and tightens the nut and bolt 28 on the mounting bracket 16. This locks the electronics unit 14 in place on the mounting bracket 16, therein completing the installation. Once the installation is complete, the railway car 32 is advanced along the track and the process is repeated. See Block 168. Many railway tunnels have I-beam supports spaced along the length of the tunnel. The I-beam supports protrude into the tunnel and often block otherwise good mounting locations for sensor assemblies. Referring to 10, a column bracket head 70 is shown. The column bracket head 70 is used to mount an electronics unit 14 directly to an I-beam support, rather than to the wall of the railway tunnel. The use of the column bracket head 70 requires a specialized mounting bracket 170 be used to hold the electronics unit 14. The mounting bracket 170 has a clamping base plate 172 with opposing jaws 174 that can be selectively tightened and loosened by turning a nut 176. The column bracket head 70 has a coupler 177 that can be selectively engaged by the articulating arm 40 of the robot 38. A support ledge 178 and gripper 180 are used to hold the specialized mounting bracket 170 in place. The robot 38 manipulates the column bracket head 70 and the specialized mounting bracket 170 until the clamping base plate 172 is pressed against the face of an I-beam. The jaws 174 are then tightened, therein attaching the specialized mounting bracket 170 to the I-beam. The robot 38 can then use the sensor installation head 68 8 to attach an electronics unit 14 to the specialized mounting bracket 170 in the manner previously described. The attachment of electronics units 14 to I-beams can be integrated with the described methodology of attaching electronics units to walls, should the attachment to an I-beam be more practical at a given location. It will be understood that the embodiment of the present invention that is illustrated and described is merely exemplary and that a person skilled in the art can make many variations to that embodiment. All such embodiments are intended to be included within the scope of the present invention as defined by the claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/84/793/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9190995693206787,
                    "F_SPEC_PARAMS": [
                        "exposed to many contaminants",
                        "coated in thick deposits of dirt",
                        "grime",
                        "cracks,"
                    ],
                    "S_SPEC_PARAMS": [
                        "cost in time, labor and line closures,",
                        "large commitment of equipment and labor"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "In this long period of time, the walls of the tunnel have been exposed to many contaminants.",
                        "As such, many of the tunnel walls between stations are coated in thick deposits of dirt and grime.",
                        "Furthermore, walls of many tunnels are riddled with cracks, old equipment mounts, running cables, and the like."
                    ],
                    "S_SENTS": [
                        "The cost in time, labor and line closures, therefore, makes the modernization of railway lines with electronic sensors unappealing to many railway operators.",
                        "For the above referenced reasons, the installation of sensors in a railway tunnel requires a large commitment of equipment and labor.",
                        "If the work is only performed during overnight, low traffic time, it can take many months, possibly years to install electronic sensors and other wayside equipment along any one railway line.",
                        "This further limits the number of locations that a sensor unit can be placed because many selected locations get damaged or otherwise are discovered to be inappropriate during the mechanical installation process."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object"
                    ],
                    "F_SIM_SCORE": 0.5205106735229492,
                    "S_TRIZ_PARAMS": [
                        "Productivity"
                    ],
                    "S_SIM_SCORE": 0.544265866279602,
                    "GLOBAL_SCORE": 1.6514878392219543
                },
                "sort": [
                    1.6514878
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10919152-20210216",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10919152-20210216",
                    "KIND_CODE": "B1",
                    "APPLICATION_DATE": "2018-04-16",
                    "PUBLICATION_DATE": "2021-02-16",
                    "INVENTORS": [
                        "Simon Kalouche"
                    ],
                    "APPLICANTS": [
                        "Nimble Robotics, Inc.    ( San Francisco , US )"
                    ],
                    "INVENTION_TITLE": "Teleoperating of robots with tasks by mapping to human operator pose",
                    "DOMAIN": "B25J 9163",
                    "ABSTRACT": "A system enables teleoperation of a robot based on a pose of a subject. The system includes an image capturing device and an operator system controller that are remotely located from a robotic system controller and a robot. The image capturing device captures images of the subject. The operator system controller maps a processed version of the captured image to a three-dimensional skeleton model of the subject and generates body pose information of the subject in the captured image. The robotic system controller communicates with the operator system controller over a network. The robotic system controller generates a plurality of kinematic parameters for the robot and causes the robot to take a pose corresponding to the pose of the subject in the captured image.",
                    "CLAIMS": "1. A system comprising: an imitation learning engine; an operator system controller coupled to an image capturing device, the operator system controller configured to: process an image of a subject, captured by the image capturing device, using a machine learning algorithm to identify one or more body parts of the subject during execution of a task; and generate, based on the one or more body parts of the subject, body pose information of the subject in the captured image, the body pose information indicating a pose or motion trajectory of the subject in the captured image; and a robotic system controller communicating with the operator system controller over a network, the robotic system controller coupled to a second image capturing device, the robotic system controller configured to: receive one or more images of a robot and/or an environment surrounding the robot, captured by the second image capturing device, during execution of the task; generate one or more pose and/or motion commands by processing the body pose information received from the operator system controller; control one or more actuators of the robot according to the one or more pose and/or motion commands to cause the robot to take a pose or motion trajectory corresponding to the pose or motion trajectory of the subject in the captured image; and provide the one or more images of the robot and/or the environment surrounding the robot, the one or more pose and/or motion commands, and/or information identifying the task to the imitation learning engine. 2. The system of claim 1, wherein processing the image of the subject includes localizing the one or more body parts of the subject in the captured image. 3. The system of claim 1, wherein the operator system controller is further configured to: map at least one of the one or more body parts of the subject to a three-dimensional skeleton model of the subject, wherein the three-dimensional skeleton model of the subject is an intermediate mapping to estimate the pose of the subject in the captured image. 4. The system of claim 3, wherein the three-dimensional skeleton model of the subject includes, one or more of i three-dimensional coordinate positions of the one or more body parts of the subject with respect to a coordinate system of the subject, ii orientation of the one or more body parts of the subject, and iii one or more joint angles between a pair of adjacent body parts of the one or more body parts. 5. The system of claim 1, wherein the robotic system controller is further configured to process the body pose information based on a configuration of the robot by aligning the one or more body parts and/or joint angles of the subject to one or more segments and joints of the robot. 6. The system of claim 5, wherein the robotic system controller is configured to align the one or more body parts and joint angles of the subject to one or more segments and joints of the robot in accordance with direct mapping, indirect mapping, or end-effector mapping. 7. The system of claim 1, wherein the one or more pose and/or motion commands comprise one or more of i three dimensional coordinates, ii orientation, and iii joint angles for one or more segments and one or more joints of the robot. 8. The system of claim 1, wherein the operator system controller and the robotic system controller communicate via a network. 9. The system of claim 1, wherein, the robotic system controller is configured to: identify one or more objects in the one or more images of the robot and/or the environment surrounding the robot; and generate the one or more pose and/or motion commands based in part on an interaction between the robot and the one or more identified objects. 10. The system of claim 1, wherein the imitation learning engine is further configured to: receive identifications of a plurality of tasks and corresponding kinematic parameters that change over time to perform the plurality of tasks using a plurality of robots of different configurations at different locations; and train a machine learning algorithm using the received identification of the tasks and the kinematic parameters of the plurality of robots to predict a series of kinematic parameters for a predetermined robot to perform a selected task of the plurality of tasks. 11. The system of claim 10, wherein training the machine learning algorithm comprises: calculating a difference between the predicted kinematic parameters and received kinematic parameters; and adjusting, based on the difference, one or more coefficients of the machine learning algorithm to reduce the difference. 12. The system of claim 10, wherein the a plurality of tasks and corresponding kinematic parameters include the task and the plurality of kinematic parameters, respectively. 13. A method of operating a robot comprising: by an operator system controller: processing an image of a subject using a machine learning algorithm to identify one or more body parts of the subject during execution of a task; generating, based on the one or more body parts of the subject, body pose information of the subject in the captured image, the body pose information indicating a pose or motion trajectory of the subject in the image; and by a robotic system controller: receiving one or more images of the robot and/or an environment surrounding the robot, captured by a second image capturing device, during execution of the task; generating one or more pose and/or motion commands by processing the body pose information received from the operator system controller; controlling one or more actuators of the robot according to the one or more pose and/or motion commands to cause the robot to take a pose or motion trajectory corresponding to the pose or motion trajectory of the subject in the image and providing the one or more images of the robot and/or the environment surrounding the robot, the one or more pose and/or motion commands, and/or information identifying the task to the imitation learning engine. 14. The method of claim 13, wherein processing the image of the subject includes localizing the one or more body parts of the subject in the image. 15. The method of claim 13, further comprising: by the operator system controller: mapping the subject and at least one body part of the one or more body parts of the subject to a three-dimensional skeleton model of the subject, wherein the three-dimensional skeleton model of the subject is an intermediate mapping to estimate the pose of the subject in the image, and wherein the three-dimensional skeleton model of the subject includes one or more of i three-dimensional coordinate positions of the one or more body parts of the subject with respect to a coordinate system of the subject, ii orientation of the one or more body parts of the robot, and iii one or more joint angles between a pair of adjacent body parts of the one or more body parts. 16. The method of claim 13, further comprising: by the robotic system controller: processing the generated body pose information based on a configuration of the robot by aligning the one or more body parts and joint angles of the subject to one or more segments and joints of the robot. 17. The method of claim 16, further comprising aligning the one or more body parts and joint angles of the subject to one or more segments and joints of the robot in accordance with one or more of the following control modes: direct mapping, indirect mapping, and end-effector mapping. 18. The method of claim 13, wherein the one or more pose and/or motion commands comprise one or more of i three-dimensional coordinates, ii orientation, and iii joint angles for one or more segments and one or more joints of the robot. 19. The method of claim 13, further comprising: identifying one or more objects in the one or more images of the robot and/or the environment surrounding the robot; and generating the one or more pose and/or motion commands based in part on an interaction between the robot and the one or more identified objects. 20. A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising: processing an image of a subject using a machine learning algorithm to identify one or more body parts of the subject during execution of a task; generating, based on the identified one or more body parts of the subject, body pose information of the subject in the image, the body pose information indicating a pose or motion trajectory of the subject in the image; receiving one or more images of a robot and/or an environment surrounding the robot during execution of the task; generating one or more pose and/or motion commands by processing the body pose information; controlling one or more actuators of the robot according to the one or more pose and/or motion commands to cause the robot to take a pose or motion trajectory corresponding to the pose or motion trajectory of the subject in the image; and providing the one or more images of the robot and/or the environment surrounding the robot, the one or more pose and/or motion commands, and/or information identifying the task to an imitation learning engine. 21. The computer-readable storage medium of claim 20, wherein the instructions, when executed by the one or more processors, further cause the one or more processors to: process the generated body pose information based on a configuration of the robot by aligning the one or more body parts and joint angles of the subject to one or more segments and joints of the robot. 22. The computer-readable storage medium of claim 21, wherein the instructions, when executed by the one or more processors, further cause the one or more processors to: align the one or more body parts and joint angles of the subject to one or more segments and joints of the robot in accordance with one or more of the following control modes: direct mapping, indirect mapping, and end-effector mapping. 23. The computer-readable storage medium of claim 20, wherein the one or more pose and/or motion commands comprise one or more of the following: x-, y-, and z-coordinates; roll, pitch, and yaw; and joint angles for one or more segments and one or more joints of the robot. 24. The computer-readable storage medium of claim 20, wherein the instructions, when executed by the one or more processors, further cause the one or more processors to: identify one or more objects in the one or more images the robot and/or the environment surrounding the robot; and generate the one or more pose and/or motion commands based in part on an interaction between the robot and the one or more identified objects. 25. A system comprising: an imitation learning engine an operator system controller coupled to an image capturing device, the operator system controller configured to: generate body pose information of a subject captured in an image by the image capturing device, the body pose information indicating a pose or motion trajectory of the subject in the captured image; and a robotic system controller communicating with the operator system controller over a network, the robotic system controller configured to: generate one or more pose and/or motion commands by processing the body pose information received from the operator system controller; control one or more actuators of the robot according to the one or more pose and/or motion commands to cause the robot to take a pose or motion trajectory corresponding to the pose or motion trajectory of the subject in the captured image; and provide the one or more pose and/or motion commands to the imitation learning engine.",
                    "STATE_OF_THE_ART": "The disclosure relates generally to teleoperation of robots and specifically to teleoperation of robots based on a pose of a human operator. Traditionally, teleoperation of robots having multiple degrees of freedom DOF is accomplished using complex controllers that may be specifically designed for a particular robot arm. In some instances, these controllers may be as simple as using a joystick, but more commonly these controllers are complicated devices, such as body worn exoskeletons that map the exoskeleton's joint angles to the robot's joint angles. In both situations, handheld or worn hardware is used to teleoperate the robot. In the case of handheld joysticks or traditional remote controllers, the teleoperation of a high DOF robot is challenging, not intuitive, and slow because of the lack of direct mapping from joysticks and buttons to the many degrees of freedom of the robot. While these controllers provide a relatively cheap method of teleoperating a robot, they require significant training or automation to handle low-level functionality and are typically not time efficient. For example, a robot having two or more legs a high DOF system operated in real-time using a controller would require low-level algorithms for balancing the robot to be autonomously handled, while the controller or joystick would be used for high-level commands , which direction and speed the robot should ambulate in. Similarly, controlling a robot arm using joysticks requires the joystick to map 6 DOF or more into 2 or 3 DOF interfaces of the joystick, which is not intuitive and can lead to slow teleoperating speeds for even simple tasks. Alternatively, an exoskeleton can be worn to control a robot, which may allow for more intuitive and direct control of a robot arm with a morphology that is similar to the arm of a human operator. This method of teleoperation is easier for the operator to learn and can integrate haptic feedback to allow the operator to feel forces that the robot is sensing when it interacts with its environment. However, exoskeletons are complex systems that are expensive, not easily donned or doffed, not portable or mobile, and typically not accommodating for differences in limb or body size from one operator to another. Another alternative for teleoperation is the use of motion capture systems. However, current motion capture systems rely on either 1 optical systems that require retrofitting a room with an array of calibrated cameras and tagging the operator with reflective markers at body locations of interest for tracking or 2 wearable inertial measurement units IMUs that require precise calibration, are susceptible to drifting, and are tedious to don and doff.",
                    "SUMMARY": [
                        "Embodiments relate to teleoperation of a robot of a robotic system based on a pose of an operator. Teleoperation indicates operation of a system or machine at a distance. The system includes an image capturing device and an operator system controller that are remotely located from a robotic system controller and a robot. In one embodiment, the image capturing device captures an image of a subject i. e. , operator. The operator system controller is coupled to the image capturing device and maps a processed version of the captured image to a three-dimensional skeleton model of the subject. The operator system controller generates body pose information of the subject in the captured image. The body pose information indicates a pose of the subject in the captured image. The robotic system controller communicates with the operator system controller over a network. The robotic system controller generates a plurality of kinematic parameters of a robot by processing the body pose information received from the operator system controller based on a configuration of the robot. The robotic system controller controls one or more actuators of the robot according to the plurality of kinematic parameters, causing the robot to take a pose corresponding to the pose of the subject in the captured image.",
                        "1 illustrates a block diagram of a system for teleoperation of robotic systems, according to an embodiment. 2 illustrates a block diagram of an operator system controller, according to one embodiment. 3 illustrates a block diagram of a robotic system controller, according to one embodiment. 4 illustrates a flowchart of a method for teleoperating a robot by mapping a pose of an operator, according to one embodiment. 5 illustrates a schematic block diagram of a training phase of an imitation learning engine, according to one embodiment. 6 illustrates a schematic block diagram of an operational phase of the imitation learning engine, according to one embodiment. The figures depict embodiments of the present disclosure for purposes of illustration only. Alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles, or benefits touted, of the disclosure described herein."
                    ],
                    "DESCRIPTION": "Embodiments relate to allowing an operator to wirelessly and intuitively control the joint space and/or end-effector space of a remotely located robot by simply moving one's hands, arms, legs, etc. without the need for traditional external calibrated motion capture systems, worn exoskeletons/sensors, or traditional but unintuitive joysticks. In a crowd-sourced teleoperation application, tasks that robots are currently unable to accomplish autonomously can be executed semi-autonomously via human teleoperation while the recorded data of how the human operator guided the robot to accomplish the arbitrary task can be used as training examples to use to enable robots to learn how to accomplish similar tasks in the future. One embodiment for a method of teleoperating a robot based on a pose of a subject includes two major steps: i generating body pose information of the subject in a captured image, and ii generating a plurality of kinematic parameters of the robot based on the generated body pose information of the subject in the captured image. In the step of generating body pose information, an algorithm is used to localize an array of body parts of the subject in the captured image. The algorithm then projects the localized body parts of the subject onto a three-dimensional 3D skeleton model of the subject. The 3D skeleton model is output as an estimate of the pose and is used for estimating and tracking the poses of the subject in a next captured image. In the step of generating the plurality of kinematic parameters, the 3D skeleton model is then mapped, directly or indirectly, to a configuration of the robot to determine a plurality of joint angles of the robot that correspond to the position and/or orientation of the subject's pose. A subject herein refers to any moving objects that have more than one pose. The moving objects include, among other objects, animals, people, and robots. Although embodiments herein are described with reference to humans as the subject, note that the present invention can be applied essentially in the same manner to any other object or animal having more than one pose. In several instances, the subject may also be referred to as an operator. The localized body parts herein refer to any portion of the subject that can be conceptually identified as one or more joints and links. For example, in a human subject, the localized body parts include, among other parts, a head, a torso, a left arm, a right arm, a left hand, a right hand, a left leg, and a right leg. The localized body parts can be subdivided into other parts , a left arm has a left upper arm and a left forearm, a left hand has a left thumb and left fingers. The one or more body parts may be localized relative to a camera, an external landmark, or another point on the subject's body. Note that the number of localized body parts is not limited and can be increased or decreased according to the purposes of the pose estimation and tracking. Body parts may also be referred to herein as limbs, segments, and links, and vice versa. A model herein refers to a representation of the subject by joints and links. In one embodiment, the model is a human body represented as a hierarchy of and links with a skin mesh attached. Various models with joints and links can be used as the model of the subject. In alternative embodiments, the model is a subset of joints and links of the human body. For example, the model may be a hand that includes one or more of the following: a palm, a thumb, and a finger. For the sake of clarity, the skeleton model is referred to throughout, but it is understood that the skeleton model may not represent the full human body and instead may represent a portion of the human body. 1 illustrates a block diagram of a system 100 for teleoperation of robotic systems 115a-115d, according to an embodiment. The system 100 includes, among other components, a network 105 that connects operator systems 110a-110d collectively referred to as operator systems 110 and also individually referred to as operator system 110, robotic systems 115a-115d collectively referred to as robotic systems 115 and also individually referred to as robotic system 115, and a processing server 120. In the embodiment of 1, four operator systems 110a, 110b, 110c, 110d and four corresponding robotic systems 115a, 115b, 115c, 115d are illustrated, but it is understood that the number of each system is not limited and can be increased or decreased. Some embodiments of the system 100 have different components than those described here. Similarly, in some cases, functions can be distributed among the components in a different manner than is described here. The network 105 provides a communication infrastructure between the operator systems 110, the robotic systems 115, and the processing server 120. The network 105 is typically the Internet, but may be any network, including but not limited to a Local Area Network LAN, a Metropolitan Area Network MAN, a Wide Area Network WAN, a mobile wired or wireless network, a private network, or a virtual private network. The network 105 enables users in different locations to teleoperate robots of robotic systems, for example, for the purposes of robotic labor. The operator system 110 enables an operator to teleoperate one or more corresponding robotic systems 115. The operator system 110 may be located at a distance from its corresponding one or more robotic systems 115. In the embodiment of 1, the operator system 110 is controlled by the operator, who may be the subject of one or more captured images. For the sake of clarity, it is understood that the subject and the operator are referred to interchangeably, but it is also understood that, in some embodiments, the subject in the captured images may be a separate subject from the operator of the operator system 110. Generally, the operator takes one or more poses, and a robot mimics a processed mapping of the poses. The operator may take a specific series of continuous or non-continuous poses that causes the robot to accomplish a certain task. As the operator takes the one or more poses, the operator system 110 captures images of the subject and generates body pose information of the subject in the captured images. The generated body pose information is a representation of the pose of the subject in the captured images, which dictates a pose that a robot of a corresponding robotic system 115 takes. The operator system 110 then transmits the generated body pose information to the corresponding robotic system 115 via the network 105. In the embodiment of 1, the operator system 110a corresponds to robotic system 115a, the operator system 110b corresponds to robotic system 115b, the operator system 110c corresponds to robotic system 115c, and the operator system 110d corresponds to robotic system 115d. In alternative embodiments, one operator system 110 may correspond to two or more robotic systems 115. In the embodiment of 1, the operator system 110 includes an image capturing device 125 and an operator system controller 130. The image capturing device 125 captures images and/or video of the subject whose pose is to be mapped to a robot of a corresponding robotic system 115. The image capturing device 125 may comprise one or more cameras positioned and/or oriented to capture part or all of the subject's body. The image capturing device 125 may be positioned on the subject's body and oriented such that segments of the subject's body are within a field of view of the image capturing device 125. Alternatively, the image capturing device 125 may be positioned external to the subject's body such that all or portions of the subject's body are within the field of view of the image capturing device 125. For example, the image capturing device 125 may be part of a camera assembly, an external mobile device, a virtual reality VR or augmented reality AR headset, a standalone VR or AR camera assembly, a similar portable imaging device, or some combination thereof. The field of view of the image capturing device 125 may vary to capture more or less of the subject's body. For example, the image capturing device 125 may comprise standard lenses or wide angle lenses , a fisheye lens. The image capturing device 125 may capture two-dimensional 2D images. In alternative embodiments, the image capturing device 125 may comprise one or more depth cameras or cameras in stereo to capture images with depth information. The image capturing device 125 may capture images of the operator at a random or specified interval. In some embodiments, the operator may take a series of poses that cause the robot to accomplish a task. The image capturing device 125 may capture images as it detects movement of the operator. In some embodiments, the image capturing device 125 sends the captured images to the operator system controller 130. In alternative embodiments, the image capturing device 125 is integrated with the operator system controller 130. In some embodiments, the image capturing device 125 captures images/and or video of equipment that is worn or manipulated by an operator. For example, the operator may be wearing a glove or holding a wand or a controller that includes visual markers. The image capturing device 125 may detect and capture a pose or motion of the visual markers, which can then be mapped to the robot of the corresponding robotic system 115. This configuration may be beneficial for robots including an end-effector or an instrument that resembles the glove or wand/controller manipulated by the operator. In some embodiments, the wand/controller may include buttons or switches as additional input for robot control, which may improve intuitive control and/or efficiency of the operator. The operator system controller 130 generates body pose information of the subject in the captured image. The generated body pose information indicates a pose of the subject in the captured image. The operator system controller 130 may be a desktop, a laptop, a mobile device, or a similar computing device. In the embodiment of 1, the operator system controller 130 receives the captured images from the image capturing device 125. The operator system controller 130 may execute an algorithm that localizes an array of body parts of the subject in the captured image. The algorithm then projects the localized body parts of the subject onto a three-dimensional 3D skeleton model of the subject. The 3D skeleton model is output as the estimate of the pose and is used for estimating and tracking the poses of the subject in a next captured image. Alternatively, the operator system controller 13 may execute an algorithm that directly predicts an estimate of the pose of the subject. The operator system controller 130 transmits the body pose information of the subject to the corresponding robotic system 115. The operator system controller 130 may transmit additional teleoperation data to one or more corresponding robotic systems 115. The teleoperation data may be parameters associated with each captured image and/or processed image that are transmitted throughout teleoperation or may be calibration parameters that are transmitted before or during initial stages of teleoperation. In some embodiments, the parameters may be manually set by an operator , via a user interface, automatically determined by the operator system 110 or robotic system 115, and/or could be updated throughout teleoperation. The teleoperation data may be transmitted as a set of one or more parameters. Parameters may relate to motion scaling or sensitivity, pause functionality, origin reset, Cartesian or joint axis locking and unlocking, bounding volumes, home positions and orientations, quick-snap orientations and positions and other similar features. Pause functionality enables the teleoperator to perform a gesture or use a specific pose that, when detected by the image capturing device 125, pauses motion and/or operation of the robot arm, which effectively pauses tracking between the teleoperator pose and the robot arm. A counter-gesture or counter-pose may be performed by the teleoperator to resume motion and/or operation of the robot arm. This feature may be used by the teleoperator to change or adjust their position, for example, to improve their comfort during teleoperation. Origin reset enables the teleoperator to modify the reference point to which the robot's motion or pose is relative. In one embodiment, this enables the teleoperator to keep the robot's motion within a comfortable range of human arm motion. Motion scaling enables motion from the operator to be mapped to motion of the robot on a different scale. For example, certain precise tasks performed by the robot may include small-scale motion , sub-millimeter motion while the operator may move on a relatively larger scale , a centimeter scale; by scaling the motion of the operator, a robot may then move on a relatively smaller scale , a micron scale. As another example, a large robot may perform large motions; motion of the operator may occur on a relatively smaller scale , the centimeter scale, which may be scaled to correspond to motion of the robot on a relatively larger scale , a meter scale. Motion scaling may be applied linearly or non-linearly to individual axes in Cartesian space or joint space. Cartesian or joint-axis locking enables an operator to constrain the motion of a robot to a plane, a line, or point in 3D space. It may also be used to lock orientation of one or more segments and/or end-effectors of the robot along one or more axes. Bounding volumes may constrain a robot to only move within a certain subspace of its total workspace. Quick-snap orientations or positions may enable the robot to take a predefined pose or a pose calculated based on a vision system of the robot. If the vision system of the robot identifies a target object in the environment, the operator system controller 130 may suggest a pose based on the target object to the teleoperator who can then select for the robot to snap to the suggested pose. These features may be used in any combination and may apply to the entire robot or a portion of the robot , one or more segments and/or end-effectors. The operator system controller 130 is discussed in further detail with regards to 2. The robotic system 115 controls the robot and causes the robot to move in accordance with a pose of the operator. The robotic system 115 receives the generated body pose information of the subject in the captured images and, based on the generated body pose information, determines mapping parameters and one or more kinematic parameters of the robot. In the embodiment of 1, the robotic system 115 includes a robot 135, an image capturing device 140, and a robotic system controller 145. The robot 135 is a machine comprising one or more segments and one or more joints that are designed to manipulate, ambulate, or both in the case of mobile manipulation. The robot 135 may have an anthropomorphic design having a human morphology or similarly dimensioned segments resembling a human operator. For example, the robot 135 may have segments and joints that resemble body parts , limbs such as an arm, a leg, of the human operator and are designed to ambulate in a similar way. In some embodiments, the robot 135 may have an end-effector that resembles a human hand , having several fingers, joints, and degrees of freedom or that functions similar to a hand , a claw, a 3-finger gripper, an adaptive gripper, an internal or external gripper, . In other embodiments, the robot may not have an anthropomorphic design, where the robot's joints and segments do not closely align to joints and segments on the human operator's body. Generally, the robot 135 may have one or more ambulating segments achieving mobility via wheels, legs, wheeled legs, or similar methods, a stationary arm with an end-effector, a combination of one or more ambulating segments and an end-effector, or some combination thereof. To move the robot 135, each joint may have one or more actuators. In some embodiments, the robot 135 may include a gripper at the end-effector. The robot end-effector is gripper agnostic and can be used with several existing or custom grippers with varying number of degrees of freedom. The robot or robot arm may be equipped with a mobile base for locomoting around its environment using wheels, tracks, legs, or a multi-modal design incorporating legs with wheels or treads or any combination thereof. The teleoperation interface is robot agnostic and need not be paired with any particular robot arm to work as intended. The image capturing device 140 captures images and/or video of the robot 135 and a local area surrounding the robot 135. The local area is the environment that surrounds the robot 135. For example, the local area may be a room that the robot 135 is inside. The image capturing device 140 captures images of the local area to identify objects that are near the robot 135. Identifying nearby objects enables the robotic system 115 to determine if there are any objects the robot will interact with to perform a task or if there are any constraints to the range of motion of the robot 135. For example, the robot 135 may be located in a small room near one or more walls, near one or more other robots, or other similar objects that the robot 135 aims to avoid during ambulation or manipulation. This enables safe use of the robot 135, especially if the robot 135 is in the presence of humans. The image capturing device 140 may capture images at a random, continuous, or specified interval to determine changes in the environment and subsequently update any constraints that need to be placed on the range of motion of the robot 135. The image capturing device 140 may be positioned and/or oriented to capture all or a portion of the robot 135 and its environment. Embodiments in which the image capturing device 140 comprises one or more cameras, the cameras may be located or mounted directly on varying parts of the robot or can be external to the robot. Similar to the image capturing device 125, the image capturing device 135 may be part of an imaging assembly, an external mobile device, a virtual reality headset, a standalone virtual reality camera assembly, a similar portable imaging device, a computer webcam, dedicated high-resolution cameras, or some combination thereof. The field of view of the image capturing device 135 may vary to capture more or less of the robot 135. For example, the image capturing device 135 may comprise standard lenses or wide angle lenses , a fisheye lens. The image capturing device 135 may capture two-dimensional images. In alternative embodiments, the image capturing device 135 may comprise one or more depth cameras or cameras in stereo to capture images with depth information. The robotic system controller 145 receives the generated body pose information from its corresponding operator system 110 and accordingly determines a set of mapping parameters and kinematic parameters to control the motion of the robot 135. As previously described, the body pose information may be in the form of a 3D skeleton model of the subject based on a pose of the subject in one or more captured images. The robotic system controller 115 maps the 3D skeleton model to the configuration of the robot 135. The robotic system controller 145 may have one or more control modes for mapping the arm and/or leg poses and joint angles to segments and joint angles of the robot 135. For example, a first control mode may be a direct mapping if the robot 135 has an anthropomorphic design or similarly dimensioned arms and/or legs to the operator. A second control mode may be an indirect mapping if the robot 135 does not have an anthropomorphic design. As such, the robotic system controller 145 is able to map an operator pose to a robot with any type of configuration. By mapping the 3D skeleton model to the configuration of the robot 135, the robotic system controller 145 determines one or more kinematic parameters for the robot 135. These kinematic parameters may include x-, y-, and z-coordinates; roll, pitch, and yaw; and joint angles for each segment and joint of the robot 135. The workspace coordinates of the robot 135 may be selected or pre-determined. The robotic system controller 145 may also receive and process force and/or haptic feedback from sensors on the robot 135; the robotic system controller 145 may transmit the force and/or haptic feedback to the operator system 110, which enables the operator to feel forces that the robot 135 is sensing as it moves and interacts with its environment. In an alternative embodiment, the force and/or haptic feedback from the robot 135 may be conveyed to the operator by visual or audible modalities, for example, in the form of augmented reality features on the operator system 110. The robotic system controller 145 may be a desktop, a laptop, a mobile device, or a similar computing device. The robotic system controller 145 is discussed in further detail with regards to 3. The processing server 120 enables users to operate the operator systems 110 and robotic systems 115 via the network 105. The processing server 120 may be embodied in a single server or multiple servers. Further, each server may be located at different geographic locations to serve users of the operator system 110 or the robotic system 115 in different geographic locations. In the embodiment of 1, the processing server 120 may host the platform that allows users of the operator system 110 and the robotic system 115 to access and control each system without needing to install or download the platform onto their own devices. In addition, the processing server 120 processes the data collected from the operator systems 110 and robotic systems 115. The processing server 120 executes a machine learning algorithm that learns from examples of robots being teleoperated to accomplish a variety of tasks in various environments and applications. In an example application, the system 100 may be used as a control input to crowdsourcing teleoperation of robotic labor. Because crowdsourcing leverages the network effect, the teleoperative nature of the system 100 enables the creation of a large data set of diverse demonstration tasks in diverse environments which does not currently exist and is difficult/expensive to generate. In this configuration, the system 100 enables the use of powerful tools such as crowdsourcing data collection and deep imitation learning and meta-learning algorithms which requires large amounts of data to teach a robot to accomplish certain tasks. This learning process becomes possible when a robot is exposed to thousands of examples of how to properly and not properly accomplish a task. In the embodiment of 1, the processing server 120 includes the imitation learning engine 150. The imitation learning engine 150 implements an algorithm to learn how a robot can perform different tasks based on the examples from human operators. The imitation learning engine 150 inputs into its model the data consisting of thousands of examples of robots executing a pose or performing a task based on the subject performing the tasks through teleoperation. A few examples of specific algorithms that may be employed are neural networks, imitation learning, meta-learning, deep multi-modal embedding, deep reinforcement learning, and other similar learning algorithms. The imitation learning engine 150 learns and extracts representations from these examples to determine appropriate movements for the robot to perform similar and unseen tasks in the same or different environments as provided in the demonstration training dataset. Accordingly, the imitation learning engine 150 stores a label corresponding to each task that includes the determined appropriate movements for each task. The imitation learning engine 150 can exist locally on the robotic system controller of a robot, on the operator system controller of an operator, or in the cloud running on a cloud server. In any embodiment, the data collected from each robot-teleoperator pair can be shared collectively in a database that enables data sharing for parallelized learning such that a first robot in a first environment performs a task, and, once the task is learned by the imitation learning engine 150, a second robot in a second environment may also learn the motions to perform the same task as well as a third robot in a third environment, a fourth robot in a fourth environment, and so on, until an Nth robot in an Nth environment. 2 illustrates a block diagram of the operator system controller 130, according to one embodiment. As described with regards to 1, the operator system controller 130 generates body pose information of a subject in a captured image. The operator system controller 130 may be a desktop, a laptop, a mobile device, or a similar computing device. One or more of the components in the operator system controller 130 may be embodied as software that may be stored in a computer-readable storage medium, such as memory 205. In the embodiment of 2, the memory 205 stores, among others, a user device communication module 210, a pose estimation module 215, a user interface module 220, a robotic system controller interface 225, and an imitation learning system interface 230. Instructions of the software modules are retrieved and executed by a processor 235. The computer-readable storage medium for storing the software modules may be volatile memory such as RAM, non-volatile memory such as a flash memory or a combination thereof. A bus 240 couples the memory 205 and the processor 235. The bus 240 additionally couples the memory 205 to an image capturing device interface 245, a user interface circuit 250, and a network interface 255. Some embodiments of the operator system controller 130 have different components than those described here. Similarly, in some cases, functions can be distributed among the components in a different manner than is described here. The user device communication module 210 is software, firmware, or a combination thereof for communicating with user devices via the network 105. A user device may be a device that an operator uses as part of the operator system 110. For example, a user device may be a mobile computing device, and the operator system controller 130 may be a desktop or a laptop that communicates with the user device. The user device communication module 210 receives commands and requests from the user device to access and control the operator system 110. The pose estimation module 215 estimates a body pose of a subject in a captured image. In the embodiment of 2, the pose estimation module 215 may include, among others, an image processor 260, a skeletal model mapper 265, and a tracking module 270 as described below in detail. The image processor 260 receives and processes the images captured by the image capturing device 125. The image processor 260 identifies a subject and the subject's body parts in a captured image. For example, the image processor 260 identifies hands, fingers, arms, elbows, shoulders, legs, knees, a head, etc. of the subject. The image processor 260 may use a machine learning model , a pre-trained deep learning model or convolutional neural network to identify these body parts in each captured image. Additionally, the machine learning model localizes body parts and the dimensions between adjacent body parts or joints. Embodiments in which the captured images are without depth information, the localized body parts are two-dimensional characteristics of the pose of the subject. The machine learning model may use spatial motion information from an IMU on the mobile device from the relationship between a changing image perspective and the 6-axis motion of the image capturing device 125 in an embodiment in which the image capturing device and the IMU are embedded in the same device and do not move relative to one another. In alternative embodiments, the operator may manually set the subject's body part dimensions. In some embodiments, the machine learning model may track certain body parts, joints, or segments relative to other body joints, parts, or segments, relative to an external landmark, or relative to the image capturing device 140. The skeletal model mapper 265 projects the two-dimensional localized body parts to a three-dimensional skeleton model of the operator. In the embodiment of 2, the skeletal model mapper 265 executes an algorithm that enhances the alignment between a 2D pixel location of each body part in the captured image and the 3D skeleton model. The 3D skeleton model of the operator may be calibrated for operators of different sizes. In the embodiment of 2, the 3D skeleton model may include several parameters, such as body part dimensions , limb lengths, joint angles between adjacent body parts , limbs, and other relevant pose information. An output of the 3D skeleton model may be estimated pose information, which may include x-, y-, and z-coordinate positions with respect to a coordinate system i. e. , workspace of each body part of the operator; roll, pitch, and yaw of the one or more body parts of the operator; and joint angles between adjacent body parts. In some embodiments, the skeletal model mapper 265 creates the 3D skeleton model during a calibration process, where the 3D skeleton model represents an initial estimated pose of the operator. The 3D skeleton model may receive as input the two-dimensional localized body parts from subsequent captured images of the subject and may output pose information for the pose of the subject in the subsequent captured images. In this configuration, the 3D skeleton model can be used to estimate and track poses of the subject based on subsequent captured images of the subject. The tracking module 270 tracks the poses of the subject in subsequent images captured by the image capturing device 125. The tracking module 270 receives one or more processed images from the image processor 260, and uses it to estimate pose information of the subject in the processed images. In some embodiments, the one or more processed images may be images that were captured subsequent to the captured images used to generate the 3D skeleton model. In this configuration, the pose estimation module 215 is able to estimate a pose of a subject in real-time as images are captured by the image capturing device 125. The pose estimation of the subject is transmitted to the corresponding robotic system controller 145. This enables a robot of a corresponding robotic system to take a pose in accordance with the subject in real-time. In alternative embodiments, the pose estimation module 215 may directly input one or more captured images into a machine learning model. The machine learning model may then output an estimation of the pose of the subject in the captured images or may then output a prediction of a pose or a motion of the robot. In this configuration, the pose estimation module 215 does not separately localize body parts of the subject in the captured images and generate a corresponding 3D skeleton model. The user interface module 220 may update a user interface that allows the user to interact with and control the operator system 110. In the embodiment of 2, the user interface module 220 may provide a graphical user interface GUI that displays the robot 135. The GUI may display the robot 135 in its current environment and/or a simulated model of the robot in a simulated environment. The GUI may include a manual controller that allows individual control of each of the robot's joint angles as well as the position and orientation of an end-effector of the robot 135. The GUI may additionally include a point-and-click function that enables the operator to select, via a mouse or a touchscreen on the user device, objects in the robot's environment. Based on the object in the environment and past experiences with similar objects, the system 100 may infer how the operator would like that object manipulated or handled by the robot. A simulation of that action may then be shown to the user via the user interface , mobile screen, monitor, AR/VR, before the robot executes the task. The GUI may include options for the user to approve or reject the simulated action. In this configuration, the operator ensures that the autonomy of completing the specified task is correct before allowing the robot to move. The GUI may include options to enable or disable modes that dictate the autonomy of the robot 135. For example, the operator system controller 130 or the corresponding robotic system controller 145 may store automated motions that have been pre-defined, programmed, or previously-learned. These modes may increase the speed and efficiency of the operator. Similarly, the GUI may provide suggestions to an operator that may further streamline teleoperation of the robot 135. Suggestions may include poses or snap poses for the robot 135 to take. These poses may be poses that pre-defined, programmed, or previously-learned poses. A snap pose may snap one or more segments and/or end-effectors of the robot 135 into a pose or to an object to perform a dexterous task. For example, learned graspable objects , door handles, writing instruments, utensils, may have corresponding snap poses that enable the robot 135 to grasp the object. In this configuration, the robot 135 may be able to manipulate objects quickly and minimize fine robot control by an operator. In one embodiment, the user interface module 220 may present an image and/or video stream of the robot 135 in the GUI on a monitor, mobile device, a head set AR, VR, and/or MR, or similar. The user interface module 220 may overlay onto the video stream a simulation of the robot 135 or a portion of the robot 135 , an end-effector of the robot 135. Using the GUI, an operator may be able to position and/or orient the robot 135 in 6D space. An operator may be able to add one or more set points that define a pose or motion of the robot 135. The set points may be ordered in a defined sequence. Each set point may be associated with one or more types that each indicate an action that the robot may take at the set point. The robot 135 may then move through the set points in the defined sequence. The user interface module 220 may provide a simulation of the defined sequence in the GUI as an overlay on the image and/or video stream of the robot 135. Example set point types may include contact, grasping, trajectory, or other similar actions, or some combination thereof. A contact set point may define that the robot 135 contacts an object, tool, or area within its environment. A grasping set point may define that the robot 135 grasp an object when it reaches the set point. A trajectory set point may be used as a waypoint in a trajectory to ensure that the robot 135 moves through a target trajectory, for example, to avoid collisions with itself and/or the environment. In this embodiment, the user interface module 220 may also provide one or more suggestions for snap poses that each correspond to a target pose. The user interface module 220 may also provide one or more snap regions that correspond to each snap pose. An operator may select a snap pose and, in some embodiments, a snap region. The GUI may provide a simulation of the robot 135 snapping to the pose. The operator may select to accept or reject the simulation. If the simulation is accepted, the user interface module 220 may add the snap pose as a set point. The user interface module 220 may additionally communicate depth information of the robot 135 and its environment to the operator. In one embodiment, a VR headset may be used to project stereo images into each eye that were captured using a stereo image capturing device on the robot 135. In this configuration, the human brain perceives depth information as human eyes naturally do without a VR headset. In an alternative embodiment, the user interface module 220 may use a mobile device, a monitor, or a head set AR, VR, and/or MR to display a video stream from the image capturing device 140 of the robot 135 to the operator. In these embodiments, additional features may be added to enhance depth perception of a 3D world projected onto a 2D computer monitor or mobile device. A processed depth stream from a depth camera may be displayed in depth form or as a point cloud to the operator. Multiple videos may be displayed from the image capturing device 140 of the robot 135, which may include multiple cameras with different perspectives top view, side view, isometric view, gripper camera view, of the robot 135. Augmented reality AR features may be overlaid in real-time onto the video stream from the image capturing device 140 of the robot 135 to enhance depth perception. Example AR features may include depth-based augmented reality boxes, lines, shapes, and highlighting; square grids that align with 3D features in the environment of the robot 135; real or augmented laser pointer projected from an end-effector of the robot 135 to objects in the environment of the robot 135 with a measured distance reading to that object; use of background, foreground, stripes, and masking to distinguish objects of interest from the background; use of chromostereopsis methods where glasses with different colored lenses and processed display videos may be used to create an illusion of depth; use of processed images via spatio-temporal blur and focus rendering; use of a homunculus control panel with one or more camera feeds; a simulated robot configuration rendered over a transformed perspective of the point cloud image; and/or one or more of the previously described features depth enhancing features. These features may be integrated into the user interface module 220 individually or in some combination thereof. The AR features may be generated using stereo or depth sensing cameras of the image capturing device 140. The robotic system controller interface 225 couples the operator system controller 130 to the robotic system 115 via the network 105. The robotic system controller interface 225 may transmit data to the robotic system controller 145 and receive data from the robotic system controller 145. In the embodiment of 2, the robotic system controller interface 225 transmits the generated pose estimation of the subject and tracking information to the robotic system 115. In some embodiments, the robotic system controller interface 225 may transmit additional data, such as the images captured by the image capturing device 125 and/or commands or requests input by the user via the user device. The robotic system controller interface 225 may receive captured images of the robot 135 captured by the image capturing device 140 and haptic feedback from the robotic system controller 145. The robotic system controller interface 225 may transmit data in real-time or at specified or random intervals. The imitation learning system interface 230 provides data from the operator system 110 to the imitation learning engine 150 online or offline. The imitation learning system interface 230 transmits data associated with a subject performing a task, such as the captured images, the 3D skeleton model, the pose tracking information, and/or other relevant information. The imitation learning system interface 230 may transmit this data in real-time or at specified or random intervals. This enables the imitation learning engine 150 to continually improve online in real-time in a parallelized framework with every additional teleoperational task completed, which enables the robots connected within the system 100 to become more capable of autonomously performing tasks and requires fewer human interventions. The image capturing device interface 245 is software, firmware, hardware, or a combination thereof that couples the operator system controller 130 to the image capturing device 125. For example, the image capturing device interface 245 may be a USB cable that couples to the bus 240. In another embodiment, image capturing device interface 245 may enable a wireless connection to the image capturing device 125, , via the network 105, Bluetooth, or a similar connection. The user interface circuit 250 is software, firmware, hardware, or a combination thereof that couples the user interface to the operator system controller 130. For example, the user interface circuit 250 may couple a keyboard and/or a mouse to the operator system controller 130 via the bus 240. In another embodiment, the user interface circuit 250 may enable a touchscreen or monitor on a user device of the operator system 110. The network interface 255 is a hardware component that couples the operator system controller 130 to the network 105. For example, the network interface 255 may be a network interface card, a network adapter, a LAN adapter, or a physical network interface that couples to the bus 240. 3 illustrates a block diagram of a robotic system controller, according to one embodiment. As described with regards to 1, the robotic system controller 145 receives the generated body pose information from its corresponding operator system 110 and accordingly determines a set of kinematic parameters to move the robot 135. The robotic system controller 145 may be a desktop, a laptop, custom computer, a mobile device, or a similar computing device. The robotic system controller 145 includes components that are stored in a computer-readable storage medium, such as memory 305. In the embodiment of 3, the memory 305 stores an operator system controller interface 310, a robot mapping module 315, a robot kinematics module 320, a feedback module 325, and an imitation learning system interface 330. Instructions of the software modules are retrieved and executed by a processor 335. The computer-readable storage medium for storing the software modules may be volatile memory such as RAM, non-volatile memory such as a flash memory or a combination thereof. A bus 340 couples the memory 305 and the processor 335. The bus 340 additionally couples the memory 305 to an image capturing device interface 345, a robot interface 350, and a network interface 355. Some embodiments of the operator system controller 130 have different components than those described here. Similarly, in some cases, functions can be distributed among the components in a different manner than is described here. The operator system controller interface 310 enables communication between the robotic system 115 and the operator system controller 130 via the network 105. The operator system controller interface 310 may transmit data to the operator system controller 130 and receive data from the operator system controller 130. In the embodiment of 3, the operator system controller interface 310 receives the generated pose estimation of the subject and tracking information from the operator system 110. The operator system controller interface 310 may transmit captured images of the robot 135 and its environment captured by the image capturing device 140 and feedback from the robot 135 including but not limited to force, torque, position, velocity, and other sensory feedback from the robot's joints, end-effector, segments, or externally in the robot's environment. In some embodiments, the operator system controller interface 310 transmits additional data, such as the configuration of the robot 135, current or previous states of the robot 135 including kinematic parameters for each state, information regarding the local area surrounding the robot 135, or some combination thereof. The operator system controller interface 310 may transmit data in real-time or at specified or random intervals. The robot mapping module 315 maps the estimated pose of the operator to the configuration of the robot 135. In one embodiment, mapping the estimated pose to the robot 135 is performed by aligning and potentially scaling the limbs and joint angles of the operator to the segments and joint angles of the robot 135. The robot mapping module 315 may create a set of mapping parameters, which may include scaling coefficients, relationships of corresponding joints or segments, and other relevant information. In the embodiment of 3, the robot mapping module may have several control modes for mapping. For example, in a first control mode, direct mapping may be employed if the robot 135 has an anthropomorphic design or similarly dimensioned arms, legs, and/or fingers. Direct mapping maps the limbs and joint angles of the operator directly to the segments and joint angles of the robot 135. In this configuration, control of the robot 135 may be intuitive to the operator, especially if a virtual reality headset is used by the operator. In a second control mode, indirect mapping may be employed if the robot 135 does not have an anthropomorphic design or similarly dimensioned arms, legs, and/or fingers. Indirect mapping may use a linear or non-linear function to map an estimate of the limbs and joint angles of the operator to the segments and joint angles of the robot 135. Indirect mapping may be used if 1 the robot's dimensions are on a different scale compared to the operator's body, 2 the robot has a different kinematic configuration or number of joints compared to the operator's body, or 3 it is desired to have varying levels of control sensitivity in joint or end-effector space. In a third control mode, end-effector mapping may be employed if the robot 135 has an arm or leg that includes an end-effector where only the end-effector ambulates in accordance with the operator. End-effector mapping may track the poses of the operator's hand rather than the operator's limbs. The position and/or orientation of the fingers and/or the joint angles of the operator's hands are mapped to the position and/or orientation of the segments and/or joint angles of the end-effector. In this configuration, control of just the end-effector of the robot 135 may be intuitive when the robot 135 does not have an anthropomorphic design. In some embodiments, the arm or leg of the robot 135 may be stationary or may ambulate according to the first or second control mode. The robot mapping module 315 may use one or control modes simultaneously for different portions of the robot 135. In a fourth control mode, the operator's pose corresponds to a velocity or force controller rather than a position or pose tracker. In this embodiment, an origin position may be defined by the operator or automatically set to a default point in the operator's workspace. When the operator's hand or other body part is within a certain threshold distance from the origin, the robot 135 may not move. When the operator's hand is positioned at a distance greater than the threshold distance from the origin along one or more axes, the robot 135 may move at a velocity along an axis proportional to the distance the operator's hand is from the origin. To control robot orientation, the operator may use a user interface to toggle between position and orientation control. In one embodiment, a rotation vector connecting the origin to a point on the operator's body part , a palm center and a norm of the rotation vector control a rotation axis and a proportional angular velocity about that rotation vector. Alternatively, a hand tracker may set thresholds relating to the operator's hand orientation such that when the hand orientation is within an angular threshold in roll, pitch, and yaw, the angular velocity of the robot 135 is zero. If the hand orientation exceeds those thresholds, the angular velocity of the robot 135 becomes proportional to an angular pose of the operator's hand relative to a coordinate frame at the origin. In this configuration, the operator may control the position and orientation of the one or more segments and/or end-effectors of robot 135 in velocity mode, allowing the operator to maintain his/her hand in a comfortable position. The pose of the operator may still be tracked, but in this embodiment, the relative position of the operator's hand relative to a defined origin maps to velocity of the robot 135 as opposed to the position of the operator's body mapping to the position of the robot 135. A user interface may display this functionality to operator control more intuitive. For example, the user interface may display a marker , a dot, simulated hand, or coordinate frame that corresponds to the operator's hand position, which may have a coordinate frame overlaid onto it to illustrate the orientation of the operator's hand relative to a coordinate frame at the origin. The marker may be surrounded by a circle that defines the velocity threshold such that if the marker is within the circle, the robot 135 remains stationary in its current pose. If the marker is outside of the circle, then the robot 135 moves in the direction of the vector from the origin to the marker at a velocity proportional to a function of the norm of that vector. In a fifth control mode, the operator may provide third person demonstrations that the robot mapping module 315 interprets such that the robot 135 performs higher level task-related motions. In this embodiment, the operator may manipulate an object in his/her environment, which the robot mapping module 315 maps to the robot manipulating a corresponding object may or may not be the same object as the operator in its environment in accordance with a processed version of the operators motion. The robot mapping module 315 may not map the exact poses or trajectory of the operator but rather may infer poses or a trajectory to achieve a similar high level task. For example, the operator may have a test object in his/her environment. The operator may specify an object in the environment of the robot 135 that corresponds to the test object. In some embodiments, the robotic system controller 145 may infer the object correspondence. The operator may manipulate the test object in some way, such as picking it up and placing it in a bin which will provide high level task information to the robotic system controller 145 that the robot 135 place the corresponding object in its environment in a bin. The objects in the operator's environment may not correspond identically with those in the environment of the robot 135. In the example described, the bins in the operator's environment and the robot's environment might be different sizes, shapes, colors, may appear differently, and may be placed in different locations relative to the test/corresponding object and/or operator/robot. In this control mode, the robot 135 may have a higher level of intelligence and may be trained on extracting higher level task-related information from the operator demonstration as opposed to fine motor control commands from the operator mapping explicitly to motion. This task-mapping mode may be used to manipulate objects in lower-level control mode such that however the operator manipulates the test object, the robot 135 manipulates the corresponding object in the same or similar inferred way. The robot kinematics module 320 determines one or more kinematic parameters for the robot 135. In the embodiment of 3, the kinematic parameters correspond to a position and an orientation for each segment and/or joint of the robot 135. The kinematic parameters may include one or more of the following: a set of x-, y-, and z-coordinates with respect to the coordinate system i. e. , workspace of the robot 135; roll, pitch, and yaw describing orientation of one or more segments of the robot 135; joint angles between adjacent segments; a set of transformation coefficients between the body of the operator and the configuration of the robot 135. The robot kinematics module 320 determines these kinematic parameters based on the mapping parameters from the robot mapping module 315 that maps the body pose of the operator to the configuration of the robot 135. The robot kinematics module 320 may send the kinematic parameters to the robot interface 350 for motion of the robot 135 in accordance with the kinematic parameters. As the operator takes a series of poses that collectively cause the robot 135 to perform a task, the robot kinematics module 320 determines a set of kinematic parameters for each subsequent pose. For the subsequent poses that the robot 135 may take, the robot kinematics module 230 may consider an initial state of the robot 135 , current pose and a target state of the robot 135 corresponding to the pose of the subject to determine a movement to transition the robot 135 from the current state to the target state. The robot kinematics module 320 may generate an intermediate set of parameters that represent the transitional movement i. e. , a motion trajectory. In the embodiment of 3, the robot kinematics module 320 may perform an optimization algorithm to determine the optimal transitional movement. The robot kinematics module 320 may consider any constraints placed on the robot 135, for example to prevent self-collision or collisions with objects in the local area of the robot 135 as determined from the image capturing device 140. In some embodiments, the operator system controller interface 310 sends the kinematic parameters and intermediate parameters to the operator system controller 130 such that a simulation of the movement is displayed in a user interface of the user device, enabling the operator to approve or reject the simulated movement before the robot 135 takes the pose. The feedback module 325 receives and processes feedback from the robot 135. In the embodiments of 1-3, the robot 135 may include sensors on each segment or at each joint, such as torque sensors, encoders, cameras, IMUs, and other possible sensors. The feedback module 325 may monitor the feedback from the sensors to ensure that the detected feedback stays within an acceptable range. For example, monitoring feedback from the torque sensors ensures that the segments and/or joints of the robot 135 do not experience excessive load-bearing forces. In some embodiments, the feedback module 325 may constrain a motion or a pose of the robot 135 if the feedback module 325 detects feedback that is outside of an acceptable range. In the embodiment of 3, the operator system controller interface 310 may transmit force or haptic feedback from the feedback module 325 to the operator system 110, which may enable the operator to feel forces that the robot 135 is sensing as it moves and interacts with its environment. In some embodiments, the operator system 110 may update a user interface of the user device to inform the operator of the feedback and if any detected feedback is outside of an acceptable range. The operator system 110 may provide multisensory feedback , visual or audio feedback through, for example, AR or display features. The imitation learning system interface 330 provides data from the robotic system 115 to the imitation learning engine 150. The imitation learning system interface 330 transmits data such as images captured by the image capturing device 140 of the robot 135 and its environment, images captured by the image capturing device 125 of the operator, mapping parameters, kinematic parameters, corresponding initial and target states and the associated intermediate parameters, sensor feedback, and other relevant information such as an embedding or information of the type of task being performed. Based on the tasks performed by the operator and the corresponding states and kinematic parameters of the robot 135, the imitation learning engine 150 learns and labels the poses for a robot to accomplish each task. The imitation learning system interface 330 may transmit this data in real-time or at specified or random intervals. This enables the imitation learning engine 150 to continually improve online in real-time, in a parallelized framework where the robotic systems 115 collectively learn from their own and other's demonstrations and experiences. With every additional teleoperational task completed, the robots become more capable of autonomously performing tasks and require fewer human interventions. The image capturing device interface 345 is software, firmware, hardware, or a combination thereof that couples the operator system controller 130 to the image capturing device 140. For example, the image capturing device interface 345 may be a USB cable that couples to the bus 340. In another embodiment, image capturing device interface 345 may enable a wireless connection to the image capturing device 140, , via the network 105, Bluetooth, or a similar connection. The robot interface 350 may be software, firmware, hardware, or a combination thereof that couples the robotic system controller 145 to the robot 135. For example, the robot interface 350 may be a power cable, USB cable, or a similar connection. In alternative embodiments, the robot interface 350 may be a wireless connection via the network 105, Bluetooth, or a similar wireless connection. In the embodiment of 3, the robotic system controller 145 transmits the intermediate parameters and the kinematic parameters to one or more actuators at the respective joints of the robot 135. In this configuration, the actuators move the robot 135 in accordance with the parameters received. The robot 135 may additionally send sensor feedback to the robotic system controller 145 via the robot interface 350. The network interface 355 is a hardware component that couples the robotic system controller 145 to the network 105. For example, the network interface 355 may be a network interface card, a network adapter, a LAN adapter, or a physical network interface that couples to the bus 340. 4 illustrates a flowchart of a method 400 for teleoperating a robot by mapping a pose of an operator, according to one embodiment. The method 400 can be performed using a computer system , system 100. An image capturing device , image capturing device 125 captures 405 an image of a subject. The image capturing device may be part of an imaging assembly, an external mobile device, a virtual reality headset, a standalone virtual reality camera assembly, a webcam, a similar portable imaging device, or some combination thereof. The image capturing device may be positioned on the subject's body and oriented such that segments of the subject's body are within a field of view of the image capturing device, or the image capturing device may be positioned external to the subject's body such that all or portions of the subject's body are within the field of view of the image capturing device. In the embodiment of 4, the image capturing device captures images that are two-dimensional i. e. , without depth information. The image capturing device captures 405 images of the subject as the subject takes a series of poses, which are to be mapped to a robot of a robotic system, causing the robot to perform a task. An image processor , image processor 260 processes 410 the captured images to localize one or more body parts of the subject. The image processor identifies the subject and the subject's body parts in the captured image. For example, the image processor identifies hands, fingers, arms, elbows, shoulders, legs, knees, a head, etc. of the subject. The image processor may use a machine learning model , a pre-trained deep learning model or convolutional neural network to identify these body parts in each captured image. Additionally, the machine learning model localizes body parts and the dimensions between adjacent body parts or joints. A skeletal model mapper , skeletal model mapper 265 maps 415 the localized body parts to a human body skeletal model. The skeletal model mapper projects the two-dimensional localized body parts to a three-dimensional skeleton model of the operator. In the embodiment of 4, the skeletal model mapper executes an optimization algorithm that maximizes the alignment between a 2D pixel location of each body part in the captured image and the 3D skeleton model. The 3D skeleton model represents an initial estimated pose of the operator. In the embodiment of 4, the 3D skeleton model may include several parameters, such as body part dimensions , limb lengths, joint angles between adjacent body parts , limbs, and other relevant pose information. A pose estimation module , pose estimation module 215 generates 420 body pose information of the subject. In some embodiments, the body pose information of the subject is generated based on the skeletal model. In alternative embodiments, a machine learning model estimates the body pose information based on the captured images or a processed version of the captured images of the subject. The machine learning model is used to estimate and track poses of the subject for subsequently received captured images of the subject. A robot mapping module , robot mapping module 315 maps 425 the body pose estimates to a configuration of a robot , robot 135. The robot mapping module maps the body pose estimates of the operator to the configuration of the robot. The robot mapping module may create a set of mapping parameters, which may include scaling coefficients, relationships of corresponding joints or segments, and other relevant information. In the embodiment of 4, the robot mapping module may use one or more control modes , direct mapping, indirect mapping, end-effector mapping for mapping. A robot kinematics module , robot kinematics module 320 generates 430 kinematic parameters of the robot , robot 135. In the embodiment of 4, the kinematic parameters correspond to a position and an orientation for each segment and/or joint of the robot. The kinematic parameters may include one or more of the following: a set of x-, y-, and z-coordinates with respect to the coordinate system i. e. , workspace of the robot 135; roll, pitch, and yaw of one or more segments of the robot; joint angles between adjacent segments; a set of transformation coefficients between the body of the operator and the configuration of the robot. The robot kinematics module determines these kinematic parameters based on the mapping parameters from the robot mapping module that maps the 3D skeleton model of the operator to the configuration of the robot. A robotic system controller , robotic system controller 145 sends 435 the generated kinematic parameters to one or more actuators of the robot , robot 135. In accordance with the generated kinematic parameters, the actuators ambulate the one or more segments and joints to a target pose corresponding to the pose of the subject. A feedback module , feedback module 325 detects 440 sensor feedback of the robot , robot 135. The feedback module monitors the feedback from sensors on the robot to ensure that the detected feedback stays within an acceptable range. In some instances, the feedback module may constrain a motion or a pose of the robot if the feedback module detects feedback that is outside of an acceptable range. Various modifications or changes may be made to the method 400 illustrated in 4. For example, steps 410, 415, and 440 may be omitted. Also, the sequence of steps 430, 435, and 440 may be modified. 5 illustrates a schematic block diagram of a training phase of the imitation learning engine 150, according to one embodiment. During the training phase, the imitation learning engine 150 implements a learning algorithm to learn how a robot can perform different tasks based on example demonstrations from human operators. The imitation learning engine 150 inputs into its model a large number of examples of robots executing a pose or performing a task based on the subject performing the tasks. The imitation learning engine 150 learns using these examples to determine appropriate movements for the robot to perform the same tasks. Accordingly, the imitation learning engine 150 stores a label for each task that includes the determined appropriate movements for each task. In the embodiment of 5, the imitation learning engine inputs data from several examples of a human operator teleoperating a robot to perform a task. Each example includes a series of poses by the subject and by the robot that occurred over a period of time, t=0 to t=Z, where Z indicates the amount of time to complete the task. As illustrated in 5, an example includes a task label 505 associated with the task performed by the robot, captured images 510, object information 515, a robot state 520 of the robot before taking a pose, and kinematic parameters 525 associated with each robot state 520. The task label 505 indicates the task performed by the robot. The captured images 510 are one or more images captured of the local area surrounding the robot. The object information 515 includes data regarding objects located in the local area surrounding the robot. The robot state 520 is an initial configuration of the robot before taking the pose corresponding to the pose of the subject. The kinematic parameters 525 are the kinematic parameters associated with the configuration of the robot taking the pose corresponding to the pose of the subject. The imitation learning engine 150 receives as input the task label 505, the captured images 510, the object information 515, and the robot state 520 before each pose, and then, for each pose in the sequence of poses to complete the task, outputs a prediction of the kinematic parameters to achieve each pose or robot motion trajectory. The imitation learning engine 150 performs error detection 530 and compares the predicted kinematic parameters to the actual kinematic parameters for each pose or robot motion trajectory. Based on a calculated difference 535, the imitation learning engine 150 may adjust the coefficients of its machine learning model to reduce the detected error. The imitation learning engine 150 may perform the training process multiple times for one or more task examples that it receives. 6 illustrates a schematic block diagram of an operational phase of the imitation learning engine 150, according to one embodiment. During the operational phase, the imitation learning engine 150 determines the configuration of a robot at several time steps such that when executed in sequence enable the robot to perform a task. The imitation learning engine 150 analyzes a current configuration of a robot , at time=t to determine a configuration of a robot at a next time step , time=t+1. The imitation learning engine 150 may be executed for one or more remotely located robots. As illustrated in 6, the input data associated with time=t includes a task label 605 associated with the task to be performed by the robot, captured images 610, object information 615, a robot state 620, and kinematic parameters 625. The task label 605 indicates the task to be performed by the robot. The captured images 610 are one or more images captured of the local area surrounding the robot. The object information 615 includes data regarding objects located in the local area surrounding the robot , objects that the robot will interact with or will avoid. The robot state 620 is the configuration of the robot , at a current time step, time=t. The kinematic parameters 625 are the kinematic parameters associated with the configuration of the robot , at a current time step, time=t. Based on the input data, the imitation learning engine 150 may output kinematic parameters 630, a robot state 635, and object information 635 for the robot at the subsequent time step. These kinematic parameters 630 may include x-, y-, and z-coordinates; roll, pitch, and yaw; and joint angles for each segment and joint of the robot. The robot state 635 represents the subsequent configuration of the robot. The object information 635 may change from the previous time-step, for example, if the robot interacted with any objects in its environment or if the position or orientation of the robot changed with respect to the objects. The imitation learning engine 150 may perform this process for the next time step , time=t+2 using the kinematic parameters 630, the robot state 635, and the object information 640. The imitation learning engine 150 may repeat this process for each subsequent time step, enabling the robot to accomplish the task associated with the task label 605. The foregoing description of the embodiments of the disclosure has been presented for the purpose of illustration; it is not intended to be exhaustive or to limit the disclosure to the precise forms disclosed. Many modifications and variations are possible in light of the above disclosure. Some portions of this description describe the embodiments of the disclosure in terms of algorithms and symbolic representations of operations on information. These algorithmic descriptions and representations are used to convey the substance of the work effectively. These operations, while described functionally, computationally, or logically, are understood to be implemented by computer programs or equivalent electrical circuits, microcode, or the like. Furthermore, it has also proven convenient at times, to refer to these arrangements of operations as modules, without loss of generality. The described operations and their associated modules may be embodied in software, firmware, hardware, or any combinations thereof. Any of the steps, operations, or processes described herein may be performed or implemented with one or more hardware or software modules, alone or in combination with other devices. In one embodiment, a software module is implemented with a computer program product comprising a computer-readable medium containing computer program code, which can be executed by a computer processor for performing any or all of the steps, operations, or processes described. Embodiments of the disclosure may also relate to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, and/or it may comprise a general-purpose computing device selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a non-transitory, tangible computer readable storage medium, or any type of media suitable for storing electronic instructions, which may be coupled to a computer system bus. Furthermore, any computing systems referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability. Finally, the language used in the specification has been principally selected for readability and instructional purposes, and it may not have been selected to delineate or circumscribe the inventive subject matter. It is therefore intended that the scope of the disclosure be limited not by this detailed description, but rather by any claims that issue on an application based hereon. Accordingly, the disclosure of the embodiments is intended to be illustrative, but not limiting, of the scope of the disclosure, which is set forth in the following claims.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/52/191/109/0.pdf",
                    "CONTRADICTION_SCORE": 0.99405437707901,
                    "F_SPEC_PARAMS": [
                        "challenging,",
                        "intuitive,",
                        "slow"
                    ],
                    "S_SPEC_PARAMS": [
                        "expensive,",
                        "easily donned or doffed, not portable or mobile,",
                        "accommodating for differences in limb or body size",
                        "require precise calibration,",
                        "susceptible to drifting,",
                        "tedious to don and doff"
                    ],
                    "A_PARAMS": [
                        "handheld joysticks or traditional remote controllers,"
                    ],
                    "F_SENTS": [
                        "In the case of handheld joysticks or traditional remote controllers, the teleoperation of a high DOF robot is challenging, not intuitive, and slow because of the lack of direct mapping from joysticks and buttons to the many degrees of freedom of the robot."
                    ],
                    "S_SENTS": [
                        "However, exoskeletons are complex systems that are expensive, not easily donned or doffed, not portable or mobile, and typically not accommodating for differences in limb or body size from one operator to another.",
                        "However, current motion capture systems rely on either 1 optical systems that require retrofitting a room with an array of calibrated cameras and tagging the operator with reflective markers at body locations of interest for tracking or 2 wearable inertial measurement units IMUs that require precise calibration, are susceptible to drifting, and are tedious to don and doff."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Power",
                        "Speed"
                    ],
                    "F_SIM_SCORE": 0.6524183750152588,
                    "S_TRIZ_PARAMS": [
                        "Waste of Time",
                        "Adaptability",
                        "Accuracy of Measurement",
                        "Stability of Object"
                    ],
                    "S_SIM_SCORE": 0.5290186703205109,
                    "GLOBAL_SCORE": 1.6514395664135615
                },
                "sort": [
                    1.6514395
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US10890913-20210112",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US10890913-20210112",
                    "KIND_CODE": "B1",
                    "APPLICATION_DATE": "2020-05-01",
                    "PUBLICATION_DATE": "2021-01-12",
                    "INVENTORS": [
                        "Scott C. Wiley"
                    ],
                    "APPLICANTS": [
                        "Facebook, Inc.    ( Menlo Park , US )"
                    ],
                    "INVENTION_TITLE": "Differential drive robotic platform and assembly",
                    "DOMAIN": "G05D 10088",
                    "ABSTRACT": "A robotic platform may include a chassis, left and right wheel assemblies, and a controller. The left and right wheel assemblies may include a caster wheel, a motor, a shaft, and a bevel gear. The wheel may be mounted to an axle for rotation about a drive axis and steering about a steering axis. The drive shaft may have one end coupled to the axle and another end wrapped by a respective belt to control rotation of the shaft about the steering axis. The bevel gear may couple the shaft to the axle so rotation of the shaft about the steering axis controls rotation of the wheel about the drive axis to drive the platform in a substantially horizontal direction. The controller may control the left and right drive motors independently, to provide differential drive. Various other assemblies, robots, and methods are also disclosed.",
                    "CLAIMS": "1. A platform comprising: a chassis having a fore end and an aft end; a driven wheel assembly comprising: a caster wheel positioned at the aft end of the chassis, and mounted to an axle for rotation about a drive axis and steering about a substantially vertical steering axis; a drive shaft extending along the steering axis from a first end to a second end; a drive motor: positioned at the fore end of the chassis, coupled to the second end of the drive shaft, and rotating about a substantially vertical drive motor axis that extends substantially parallel to the steering axis; a steer shaft extending longitudinally along the steering axis; and a bevel gear connecting the first end of the drive shaft to the axle; and a steer assembly comprising a steer motor: positioned at the aft end of the chassis, and controlling the steer shaft to steer the caster wheel, which in turn steers the platform. 2. The platform of claim 1, wherein the bevel gear connects the first end of the drive shaft to the axle such that rotation of the drive shaft about the steering axis controls rotation of the caster wheel about the drive axis to drive the platform in a substantially horizontal direction. 3. The platform of claim 1, wherein the drive motor is coupled to the second end of the drive shaft by way of a drive belt. 4. The platform of claim 1, further comprising a controller for controlling the drive motor. 5. The platform of claim 1, wherein the drive shaft is concentrically arranged inside of the steer shaft. 6. The platform of claim 1, wherein the driven wheel assembly further comprises a brake: arranged on the second end of the drive shaft, and dimensioned to retard motion of the drive shaft to thereby retard motion of the caster wheel. 7. The platform of claim 6, wherein the brake comprises a disc brake comprising: a disc disposed on the drive shaft, wherein the disc is configured to rotate with the drive shaft about the steering axis; and a pair of pads on a top side of the disc and on a bottom side of the disc, wherein the pair of pads are compressible against the disc to retard rotation of the disc and thereby retard rotation of the drive shaft. 8. The platform of claim 1, wherein the drive axis and the steering axis together f an oblique angle such that the caster wheel is tilted relative to the steering axis. 9. The platform of claim 1, wherein the caster wheel is configured to freely rotate 360 degrees about the steering axis. 10. The platform of claim 1, further comprising a drive assembly comprising: a motor pulley coupled to the drive motor; a drive shaft pulley coupled to the drive shaft; and a drive belt extended between the motor pulley and the drive shaft pulley and within a plane substantially perpendicular to the steering axis. 11. The platform of claim 10, wherein the drive motor causes the motor pulley to move about the drive motor axis, which in turn causes the drive belt to move about the motor pulley and the drive shaft pulley, which in turn causes the drive shaft pulley and the drive shaft to rotate about the steering axis. 12. The platform of claim 10, wherein the drive assembly, the steer assembly, the second end of the drive shaft, and the drive motor are disposed on a top of the chassis. 13. A method of assembling a platform comprising: arranging a driven wheel assembly on a chassis having a fore end and an aft end, the arranging comprising: positioning a caster wheel at the aft end of the chassis; mounting the caster wheel to an axle for rotation about a drive axis and steering about a substantially vertical steering axis; coupling a first end of a drive shaft to the axle with a bevel gear, the drive shaft extending along the steering axis from the first end to a second end; positioning a steer shaft to extend longitudinally along the steering axis; positioning a drive motor at the fore end of the chassis; and coupling the drive motor to the second end of the drive shaft such that the drive motor rotates about a substantially vertical drive motor axis that extends substantially parallel to the steering axis; and arranging a steer assembly, the arranging comprising positioning a steer motor the at the aft end of the chassis such that the steer motor controls the steer shaft to steer the caster wheel when driving the platform. 14. The method of claim 13, wherein the bevel gear couples the first end of the drive shaft to the axle such that rotation of the drive shaft about the steering axis controls rotation of the caster wheel about the drive axis to drive the platform in a substantially horizontal direction. 15. The method of claim 13, wherein arranging the driven wheel assembly further comprises coupling the drive motor to the second end of the drive shaft by way of a drive belt. 16. The method of claim 13, further comprising positioning the drive shaft such that the drive shaft is concentrically arranged inside of the steer shaft. 17. The method of claim 13, wherein arranging the driven wheel assembly further comprises arranging a brake on the second end of the drive shaft, wherein the brake is dimensioned to retard motion of the drive shaft to thereby retard motion of the caster wheel. 18. The method of claim 17, wherein the brake comprises a disc brake comprising: a disc disposed on the drive shaft, wherein the disc is configured to rotate with the drive shaft about the steering axis; and a pair of pads on a top side of the disc and on a bottom side of the disc, wherein the pair of pads are compressible against the disc to retard rotation of the disc and thereby retard rotation of the drive shaft. 19. The method of claim 3, further comprising arranging a drive assembly on the chassis, the arranging comprising: coupling a motor pulley to the drive motor; coupling a drive shaft pulley to the drive shaft; and extending a drive belt between the motor pulley and the drive shaft pulley, wherein the drive belt is extended within a plane substantially perpendicular to the steering axis. 20. A robot comprising: a platform mounted to the robot, the platform comprising: a chassis having a fore end and an aft end; a driven wheel assembly comprising: a caster wheel positioned at the aft end of the chassis, and mounted to an axle for rotation about a drive axis and steering about a substantially vertical steering axis; a drive shaft extending along the steering axis from a first end to a second end; a drive motor: positioned at the fore end of the chassis, coupled to the second end of the drive shaft, and rotating about a substantially vertical drive motor axis that extends substantially parallel to the steering axis; a steer shaft extending longitudinally along the steering axis; and a bevel gear connecting the first end of the drive shaft to the axle such that rotation of the drive shaft about the steering axis controls rotation of the caster wheel about the drive axis to drive the robot in a substantially horizontal direction; and a steer assembly comprising a steer motor: positioned at the aft end of the chassis, and controlling the steer shaft to steer the caster wheel when driving the robot.",
                    "STATE_OF_THE_ART": "Robots have found use in a nearly unlimited number of applications, from mapping and exploring unknown terrain, to performing rescue missions in extreme environmental conditions, to performing surgeries in hospitals, and to managing data centers, among a host of other scenarios. Robots are typically mounted to a robotic platform to support the robot on a floor or other surface. Caster wheels may be mounted to the robotic platform to enable rolling or otherwise linear movement of the robot in a generally horizontal direction. Some robotic platforms may have caster wheels that are powered or driven. For example, a power assembly such as an electric motor may control rotation of a caster wheel's axle, which may in turn control rotation of the wheel thereabout to move the robot. Some powered or driven caster wheels may also include brake assemblies designed to retard rotation of the wheel axle and thus slow or stop rotation of the wheel, thereby slowing or stopping the robot. Traditionally, at least a portion of this power assembly , the motor, as well as the brake, is disposed within the circumference of the wheel, near the wheel hub and adjacent to the axle. The placement of a power or brake assembly within the circumference of a caster wheel, however, can lead to various disadvantages. For example, when an electrical component of the power assembly , a motor is disposed within the circumference of the wheel, the motor may be exposed to whatever conditions the wheel moves through. This is particularly problematic for robots that are intended to move through treacherous conditions or water. In addition, when all or some of the power assembly is located within the circumference of the wheel, the electrical wires that provide electricity to the power assembly may limit the maneuverability of the robotic platform since the wheel may be unable to rotate 360 degrees without entangling the wires. This is especially disadvantageous for robots intended to perform highly maneuverable functions. Positioning all or some of the power assembly within the circumference of the wheel may also lead to an increased risk of damage to the power assembly. For instance, when the driven caster wheel is part of a robot or other object that is susceptible to falling or being dropped, the power assembly may be damaged by such forceful impacts. Similar issues arise when positioning brake assemblies within the circumference of a caster wheel. As such, the instant disclosure identifies and addresses a need for improved caster wheels and robotic platforms.",
                    "SUMMARY": [
                        "As will be described in greater detail below, the instant disclosure describes a driven or powered robotic platform. The robotic platform may be driven by a power assembly that drives left and right driven wheel assemblies independently. The power assembly may drive each of the left and right driven wheel assemblies by controlling rotation of a respective drive shaft, which may be connected to a wheel axle via a bevel gear. Thus, the entire power assembly may be located outside the circumference of the wheel. The power assembly may include left and right drive motors that may enable differential drive to provide improved steering and maneuverability. In one example, a robotic platform may include a chassis, left and right driven wheel assemblies, and a controller. The chassis may have oppositely disposed top and bottom sides, left and right sides, and a fore end and an aft end. The left and right driven wheel assemblies may be disposed proximate the aft end of the chassis. Each of the left and right driven wheel assemblies may include a caster wheel, a drive shaft, a drive motor, and a bevel gear. The caster wheels may each be mounted to an axle for rotation about a drive axis and for steering about a substantially vertical steering axis. The drive shafts may extend along the steering axis from a first drive shaft end to a second drive shaft end. The second drive shaft end may be coupled to the motor, and the bevel gear may couple the first drive shaft end to the axle such that rotation of the drive shaft about the steering axis controls rotation of the caster wheel about the drive axis to drive the robotic platform in a substantially horizontal direction. The controller may control the left and right drive motors independently of one another. In some embodiments, the robotic platform may also include a steer assembly disposed on the chassis. The steer assembly may include a steer motor and a steer belt. Each driven wheel assembly may include a steer shaft extending along the steering axis from a first steer shaft end to a second steer shaft end. The first steer shaft end may be coupled to the wheel axle and the second shaft end may be wrapped by the steer belt such that the steer belt controls rotation of the steer shaft about the steering axis, thereby steering the caster wheels about the steering axis. The steer belt may be wrapped around the steer shaft of each of the left and right driven wheel assemblies such that the steer shafts rotate synchronously. The drive shaft may be concentrically arranged inside of the steer shaft. In some embodiments, the robotic platform may include, for at least one of the left and right driven wheel assemblies, a disc brake disposed on the drive shaft dimensioned to retard motion of the drive shaft and thereby retard motion of the wheel. The disc brake may include 1 a disc disposed on the drive shaft and configured to rotate with the drive shaft about the steering axis, and 2 a pair of pads on top and bottom sides of the disc. The pads may be compressible against the disc to retard rotation of the disc and thereby retard rotation of the drive shaft. In some embodiments, the drive axis and the steering axis together form an oblique angle such that the caster wheel is tilted relative to the steering axis. The caster wheel may be configured to freely rotate 360 degrees about the steering axis. The robotic platform may also include at least one idle caster wheel disposed proximate the fore end of the chassis. The second drive shaft end and the drive belt may be disposed on top of the chassis. A robot having a chassis, left and right driven wheel assemblies, and a controller is also disclosed. The chassis may have oppositely disposed top and bottom sides, left and right sides, and a fore end and an aft end. The left and right driven wheel assemblies may be disposed proximate the aft end of the chassis. Each of the left and right driven wheel assemblies may include a caster wheel, a drive shaft, a motor, and a bevel gear. For each left and right driven wheel assembly, the caster wheel may be mounted to an axle for rotation about a drive axis and steering about a steering axis. The drive shaft may extend along the steering axis from a first drive shaft end to a second drive shaft end. The first drive shaft end may be coupled to the wheel axle and the second drive shaft end may be coupled to the motor. The bevel gear may couple the first drive shaft end to the axle such that rotation of the drive shaft about the steering axis controls rotation of the wheel about the drive axis to drive the robot in a substantially horizontal direction. The controller may be configured to control the left and right drive motors independently of one another. A method of assembling a robotic platform is also disclosed. The method may include arranging a drive assembly on a chassis, assembling left and right driven wheel assemblies, mounting the left driven wheel assembly to a left side of the chassis, and mounting the right driven wheel assembly to the right side of the chassis. Assembling left and right driven wheel assemblies may include mounting a caster wheel to a respective axle for rotation about a drive axis and steering about a substantially vertical steering axis and coupling a first end of a drive shaft to the axle with a bevel gear. The drive assembly may include a left drive motor and a left drive belt that is controlled by the left drive motor, and a right drive motor and a right drive belt that is controlled by the right drive motor. Each drive shaft may extend along the respective steering axis from the first drive shaft end to a second drive shaft end. The method may include wrapping the left drive belt around the drive shaft of the left caster wheel assembly, and wrapping the right drive belt around the drive shaft of the right caster wheel assembly. The method may also include electrically coupling a controller to the left and right drive motors to control the left and right drive motors independently from one another. Each bevel gear may couple the respective drive shaft to the axle such that rotation of the drive shaft about the steering axis controls rotation of the wheel about the drive axis to drive the robot in a substantially horizontal direction. Features from any of the above-mentioned embodiments may be used in combination with one another in accordance with the general principles described herein. These and other embodiments, features, and advantages will be more fully understood upon reading the following detailed description in conjunction with the accompanying drawings and claims.",
                        "The accompanying drawings illustrate a number of exemplary embodiments and are a part of the specification. Together with the following description, these drawings demonstrate and explain various principles of the instant disclosure. 1 is a rear, left perspective view of a differential drive robotic platform, according to an embodiment. 2 is a top view of the robotic platform of 1. 3 is front, right perspective view of the robotic platform of 1. 4 is a right side view of the robotic platform of 1. 5 is a view of the robotic platform of 1 with the chassis removed. 6 is a right side view of the robotic platform of 1, with the chassis and a casting removed. 7 is a perspective view of two of the robotic platforms from 1 being used to support an accessibility robot. 8 is a flow diagram of a method of assembling a differential drive robotic platform, according to an embodiment. Throughout the drawings, identical reference characters and descriptions indicate similar, but not necessarily identical, elements. While the exemplary embodiments described herein are susceptible to various modifications and alternative forms, specific embodiments have been shown by way of example in the drawings and will be described in detail herein. However, the exemplary embodiments described herein are not intended to be limited to the particular forms disclosed. Rather, the instant disclosure covers all modifications, equivalents, and alternatives falling within the scope of the appended claims."
                    ],
                    "DESCRIPTION": "The present disclosure is generally directed to robotic platforms that are driven by caster wheel assemblies in a manner that may provide differential drive and/or a variety of other features and advantages. As will be explained in greater detail below, embodiments of the instant disclosure may include a robotic platform having left and right driven or powered caster wheels mounted on an aft end of a chassis. The caster wheels may each be driven by a respective left or right power assembly via a drive shaft and bevel gear arrangement. The left and right power assemblies may each drive rotation of the respective driven wheel about an axle to move the wheel in a substantially horizontal direction, while the wheel may be steerable about a drive shaft that extends along a substantially vertical steering axis. The respective power assembly may be connected to one end of the drive shaft to control its rotation about the steering axis. The other end of the drive shaft may be connected to the wheel axle via a bevel gear. The left and right power assemblies may each include a drive motor coupled to the drive shaft. The drive motors may thus control the drive shafts independently of one another. The drive shafts may each extend from the respective axle to a location above the chassis. The devices and assemblies described herein may provide a number of features and advantages over traditional systems. For example, in some configurations, the entire power assemblies of driven caster wheels may be located distally from the wheels. As such, components of the power assemblies may be protected, for example, from environmental conditions that may surround the wheels. This may be particularly advantageous for robotic platforms intended to move through extreme weather conditions or through water. Also, a power assembly that is distally located from the wheel may be better protected from damaging forceful impacts that may occur, for instance, in configurations where the driven caster wheels are mounted to a robot that is susceptible to falling or being dropped. Another advantage of the driven caster wheel disclosed herein may be that configuration of the driven caster wheel may allow a caster wheel to freely rotate 360 degrees about its steering axis without becoming entangled by electrical wires or other components of the drive assembly. This driven caster wheel may also have a brake assembly that is disposed distally from the caster wheel , entirely above the chassis, which may similarly protect the brake assembly from impact and environmental damage. Furthermore, the left and right drive assemblies may be controlled independently of one another to enable differential drive, which may provide improved steering and maneuverability. The following will provide, with reference to 1-6, detailed descriptions of a robotic platform. 1 is a rear, left perspective view of a robotic platform 100, according to an embodiment. The robotic platform 100 may include a chassis 120, a right drive assembly 106a arranged on a right side of the chassis 120, a left drive assembly 106b arranged on a left side of the chassis 120, left and right wheel assemblies 101a, 101b disposed on the left and right sides of the chassis 120, and an idle caster wheel 158 disposed on the fore end of the chassis 120. While a single idle caster wheel 158 is shown coupled to robotic platform 100, an additional fixed or rotatable idle caster wheel or multiple idle caster wheels may be coupled to robotic platform 100. Furthermore, while the idle caster wheel 158 is shown to be rotatable around a vertical axis, the idle caster wheel 158 may also be configured to be fixed such that it does not rotate around a vertical axis. The chassis 120 may have oppositely disposed top and bottom sides, a fore end and an aft end. The terms fore and aft, in some embodiments, generally refer to relative positions on a robotic platform and are not necessarily indicative of a primary direction of movement of the robotic platform or of a configuration of a robot mounted on the robotic platform the front of the robot on the platform may be positioned facing the fore end, the aft end, the right or left sides, . Furthermore, the chassis 120 may have a variety of shapes, sizes, thicknesses, etc. in different embodiments of the instant disclosure. The drive assemblies 106a, 106b may each include drive motors 122a, 122b as referenced in 3 and drive belts 124a, 124b as references in 2 and 3 that are controlled by drive motors 122a, 122b. In some embodiments, the drive motors 122a, 122b generally represent any type or form of machine powered in any suitable manner , by electricity, internal combustion, to supply motive power for the robotic platform 100. Also, the drive belts 124a, 124b generally represent any type or form of transmission mechanism , belts, chains, driveshafts, for transferring power from a motor to a wheel assembly. Furthermore, while 1-6 show the motors 122a, 122b as being positioned remotely from wheel assemblies 101a, 101b, the motors 122a, 122b may also be coupled directly above the wheel assemblies 101a, 101b such that rotating shafts of the motors 122a, 122b are coupled directly to wheel assemblies 101a, 101b. While 1-6 show and describe a robotic platform with two driven caster wheel assemblies, any suitable number of driven caster wheel assemblies may be provided while remaining within the scope of this disclosure. Furthermore, while the following description at times refers to only one of the wheel assemblies , the left wheel assembly 101a, it should be well understood that the description of the one-wheel assembly , the left wheel assembly 101a may also apply to some or all of the other wheel assemblies , the right wheel assembly 101b. A more detailed description and discussion of the driven wheel assemblies described herein may be found in patent application Ser. 15/828,349, titled DRIVEN CASTER WHEEL AND ASSEMBLY, filed on 30 Nov. 2017, the entirety of which is incorporated herein by reference. With reference to 6, wheel assembly 101a may include a wheel 102a mounted to an axle 104a, a drive shaft 108a, and a bevel gear 114a. The wheel 102a may be mounted to its respective axle 104a for rotation about a drive axis D. In this example, the rotation of the wheel 102a about the drive axis may be controlled by a drive assembly 106a, as shown in 1, that is located distally from the wheel 102a and on the chassis 120. The drive shaft 108a may extend substantially vertically , vertically or almost vertically from the drive assembly 106a to the axle 104a and rotate about a substantially vertical steering axis The bevel gear 114a may connect the drive shaft 108a to the axle 104a to translate torque and power from the drive assembly 106a to the wheel 102a. As shown in 4, the entire drive assembly 106a may be located distally from the wheel 102a. In this configuration, pulleys 126a, 128a and the drive belt 124a may be disposed on top of the chassis 120, which may protect the robotic platform from damage. For example, by positioning the power assembly's electrical components distally from the caster wheel, the robotic platform 100 may be able to drive through deep water or survive other environmental conditions. With reference to 3, the drive assemblies 106a, 106b may each include drive motors 122a, 122b and drive belts 124a, 124b that rotate about the motor pulleys 126a, 126b and the drive shaft pulleys 128a, 128b. The power of the wheel assemblies 101a, 101b may be controlled by the respective drive shaft pulleys 128a, 128b that are mounted to or integral within the drive shafts 108a, 108b. As shown in 4, in some embodiments, the drive motor 122a may rotate about a substantially vertical drive motor axis M that extends substantially parallel , parallel or approximately parallel to the steering axis Drive motor pulley 126a may be mounted to the upper portion of the drive motor 122a, and the respective drive shaft pulley 128a may be mounted to an upper portion of respective drive shaft 108a. The drive belt 124a may extend between the pulleys 126a, 128a and within a plane substantially perpendicular to the drive motor axis M and/or the steering axis Thus, the drive motor 122a may cause the drive motor pulley 126a to move about the drive motor axis M, which may in turn cause the drive belt 124a to move about the drive pulleys 126a, 128a, thereby causing the pulley 128a and the drive shaft 108a to rotate about the steering axis The drive motors 122a, 122b may be controlled independently of one another by a controller , an electrical system programmed with logic, with encoders, and/or with other electronic elements to control robotic platform 100 to provide differential power. The terms differential power and/or differential drive, as used herein, generally refers to supplying independent power and/or control to at least two different wheels of a robotic assembly. Referring to 4-6, the torque and power from the drive motor 122a may be transferred from the drive shaft 108a to the respective axle 104a via the bevel gear 114a. The drive shaft 108a and axle 104a may function as the shafts of the bevel gear 114a. Mating gears 130a, 132a may be disposed on the lower portion of the drive shaft 108a and axle 104a. Thus, the bevel gear 114a may transfer and change the direction of the power and torque from the drive assembly 106a to the wheel 102a. As shown in 5, the bevel gear 114a is a miter gear, and the mating gears 130a, 132a provide a gear ratio of 1:1. The bevel gear 114a, however, can have other suitable configurations and gear ratios while remaining within the scope of this disclosure. For example, the mating gears 130a, 132a may include any suitable number of teeth. Also, the intersection of the drive and steering axes can form any suitable angle while remaining within the scope of this disclosure. As described herein, drive power may be provided to the wheel 102a by way of the substantially vertical drive shaft 108a that extends along only one side of the wheel 102a from the axle 104a to a location entirely above the wheel , above chassis 120. As such, the wheel 102a may be able to freely rotate 360 degrees about the steering axis This configuration may offer many advantages from a steering and maneuverability standpoint. In addition, the configuration of the drive shaft 108a and the bevel gear 114a may result in a robotic platform 100 that may be free of wires or other electrical components extending close to the wheels 102a, 102b. For example, since the drive shaft 108a may receive power at its upper end and transfer power to the wheel 102a at its lower end via the bevel gear 114a, the drive belt 124a may be located entirely above the wheel, and the electrical components may be disposed distally from the wheel 102a, leaving a simple gearing configuration proximate the wheel 102a. In some embodiments, brakes 116a, 116b may be disposed entirely above the respective wheels 102a, 102b. The brakes 116a, 116b may each be disposed on the respective upper drive shaft ends and configured to retard motion of the respective drive shafts 108a, 108b about the steering axis, thereby retarding motion of the respective wheels 102a, 102b about the drive axis. Any suitable brake configuration may be used. In one example, the brake 116a may represent a disc brake, including a disc 134a arranged on the upper drive shaft end and configured to rotate about the steering axis along with the drive shaft 108a. As shown in 2-5, the disc 134a may be mounted to the upper drive shaft end by being mounted to the drive shaft pulley 128a. The disc 134a may have top and bottom surfaces, with each extending in a horizontal plane that is substantially perpendicular to the steering axis. A retarding mechanism may be provided to retard motion of the disc 134a. Any suitable type of retarding mechanism may be provided. In the embodiment herein described, the retarding mechanism includes top and bottom pads 136a, 136b disposed proximate the top and bottom surfaces of the disc 134a, respectively. At least one of the pads 136a, 136b may be compressible towards the disc 134a such that the pads 136a, 136b pinch the disc 134a to thereby retard its motion about the steering axis. In the embodiment shown and herein described, a steer assembly 118 may be provided to control the steering of the wheels 102a, 102b about the steering axis. For each of the wheel assemblies 101a, 101b steer shafts 138a, 138b may extend along the steering axis from a bottom steer shaft end to a top steer shaft end. The bottom steer shaft end may be connected to the axle 104a, and the top steer shaft end may be coupled to the steer assembly 118, such that the steer assembly 118 controls rotation of the steer shaft 138a about the steering axis, which in turn steers the wheel 102a about the steering axis. The bottom steer shaft end may be mounted to the axle 104a via a casting 152a such that the axle 104a rotates about the steering axis along with the steer shaft 138a, thus causing the wheel 102a to rotate about the steering axis. As described in further detail below, the wheel 102a may be tilted with respect to the steering axis. In one example, the steer assembly 118 may include a steer motor 144, steer belt 146, a steer motor pulley 150, and steer shaft pulleys 148a, 148b, the operation of which may be similar to that of the drive assembly 106a. The steer motor 144 and steer belt 146 may control the steer shafts of both wheel assemblies 101a, 101b, to thus steer the wheels 102a, 102b synchronously. The steer motor 144 may extend substantially vertically. The steer motor 144 and drive motors 122a, 122b may be disposed on opposite ends or sides of the chassis 120 from one another , on fore and aft ends, on left and right sides. In the embodiment shown in 1-6, the drive motors 122a, 122b may be disposed on the fore end of the chassis 120 and the steer motor 144 on the aft end of the chassis 120. The steer belt 146 and steer pulleys 150, 148a, 148b, may rotate within a plane that is substantially parallel to and below the plane in which the drive motors 122a, 122b, drive belts 124a, 124b, and drive pulleys 126a, 126b, 128a, 128b rotate. The steer shaft pulley 148a that is arranged on the steer shaft 138a may be disposed beneath the drive shaft pulley 128a that is arranged on the drive shaft 108a. With reference to 6, in some embodiments, the steer shaft 138a may be arranged concentrically with respect to the drive shaft 108a. More particularly, the steer shaft 138a and drive shaft 108a may each extend along the steering axis, with the drive shaft 108a being disposed within the steer shaft 138a. As such, and as shown in 5 and 6, the drive shaft 108a may be connected to the axle 104a via the bevel gear 114a, and the steer shaft 138a may be mounted to the axle 104a via a casting 152a that is disposed around the bevel gear 114a. As shown in 6, in some embodiments, the bevel gear 114a may be configured so that the wheel 102a is tilted with respect to the steering axis. This tilt may reduce scrubbing action when the wheel turns, and thus improve durability and steering capabilities. As shown in 6, a wheel axis may extend along the length of the wheel, perpendicular to the drive axis. In this example, the wheel axis does not extend parallel to the steering axis, but rather is tilted at an angle relative thereto. As such, while the drive shaft and steering axis extend in a substantially vertical direction, the axle and drive axis may extend in a direction that is not horizontal. In other words, the bevel gear 114a may be configured such that the steering axis and drive axis form an angle that is oblique. In some examples, the wheel 102a is tilted at an angle so that when the robotic platform 100 is on a surface, the wheel rotates about the steering axis at a point that is its center of mass. For example, for a wheel and tire configuration having a diameter substantially equal to 16 inches, the bevel gear may be configured so that the angle between the drive axis and steering axis is about 110 degrees and the angle between the steering axis and the wheel is equal to about 20 degrees. As shown in 1-6, the entire chassis 120 may be arranged above the wheels 102a, 102b, and the drive and steer shafts 108a, 108b, 138a, 138b may extend from the respective axle through the surface of chassis 120 to a location at or above a top surface of the chassis 120. In addition, the drive and steer belts 124a, 124b, 146 and pulleys 126a, 126b, 128a, 128b, 150, 148a, 148b, may be located at or above the top surface of the chassis 120. Also, the drive and steer motors 122a, 122b, 144 may be located distally from the wheels 102a, 102b. Thus, with the chassis disposed between the wheel and the drive and steer belts and pulleys, and the drive and steer motors 122a, 122b, 144 located distally from the wheels 102a, 102b, the drive and steer assemblies may be protected from environmental conditions that may surround the wheel. In some embodiments, drive belts 124a, 124b may be wrapped directly around the circumference of at least one of the drive motor 122a, 122b or drive shaft 108a without a separate pulley component , the bottom portion of the drive motors 122a, 122b and/or top portion of the drive shafts 108a, 108b may function as a pulley. The term wheel, in some examples, generally refers to any suitable type of disc or other object that is rotatable about an axis. Examples of wheels include, without limitation, a disc-shaped object that is configured to roll along a surface, a propeller capable of converting rotational movement to thrust , to drive a boat through the water, or any other type or form of rotatable object. The robotic platform described herein can have a multitude of applications. For example, the robotic platform may support a robot that is configured to navigate through extreme environmental conditions, perform surgery in a hospital, manage a data center, etc. While the robotic platform 100 is referred to herein as having a fore end with an idle caster wheel 158 and an aft end with driven wheel assemblies 101a, 101b, the disclosed robotic platform 100 may move in multiple directions such that the aft end, right side, and left side may also be located at the front of the robotic platform 100. For example, the disclosed robotic platform may be capable of moving in all directions on a substantially horizontal surface. In one instance, the wheels 102a, 102b may both turn about the steering axis 90 degrees, such that the right side of the robotic platform 100 becomes the front of the vehicle as it moves horizontally across the surface. This may be particularly advantageous when navigating the robotic platform 100 between objects or barriers. For example, in order to parallel park the robotic platform 100 into a space between two objects, the platform does not need to maneuver to angle forward and backwards in the traditional process for parallel parking. Rather, the robotic platform 100 may be positioned beside and parallel to a space, turn the wheels 102a, 102b to be directed to the space , turn the wheels 102a, 102b 90 degrees, and then simply move linearly , in a sideways direction into the space. Also, the robotic platform 100 may move in a backward direction without using a reverse gear. Thus, the robotic platform 100 may be capable of moving in the backward direction with the same speed as in the forward direction. The robotic platform 100 may be used in a variety of contexts and/or may be used to support a variety of different types of robots. For example, the robotic platform 100 may support an autonomous or semi-autonomous data center robot. In this example, the robotic platform 100 may enable the data center robot to move through aisles of a data center to install, remove, and/or service rack mount units and modules within the data center. Additionally or alternatively, multiple instances of robotic platform 100 may support a robotic gantry capable of moving among data center aisles and servicing modules within the aisles at various different heights. As another example, the robotic platform 100 may support a telepresence robot. In this example, the robotic platform 100 may enable to telepresence robot to move from one location to another, to move to the optimal location for interacting with another robot or human, and/or to move in any other suitable manner. A third example is provided in 7, which shows an accessibility robot 700 mounted on two instances of robotic platform 100, shown as platforms 100a, 100b. In this example, a chair 704 may be coupled to platform 100a at ball joint 190a and platform 100b at a ball joint not visible in 7. Accessibility robot 700 may also include arms 706a, 706b coupled to platforms 100a, 100b, respectively. A user 702 may control arms 706a, 706b via any suitable control interface , using voice control, using a joystick or control pad, to perform a variety of tasks. Similarly, the user 702 may use a control interface to direct platforms 100a, 100b to move accessibility robot 700 from one location to another. While several distinct examples of using robotic platform 100 have been presented, robotic platform 100 may be utilized in any other suitable context and/or to support any other suitable type or form of robot. Referring now to 8, also disclosed herein is a method 800 of assembling a differential drive robotic platform, such as the robotic platform 100 described above with reference to 1-6. In step 810, a right drive motor and right drive belt may be arranged on a right side of a chassis, and a left drive motor and left drive belt may be arranged on a left side of the chassis. In steps 820 and 830, left and right wheel assemblies may be assembled. In step 820, for each left and right wheel assembly, a caster wheel may be mounted to a respective axle for rotation about a drive axis and steering about a substantially vertical steering axis. In step 830, for each left and right wheel assembly, a first end of a drive shaft may be coupled to the axle with a bevel gear, such that each drive shaft extends along the steering axis from the first end to a second end. In step 840, the left and right wheel assemblies may be mounted to left and right sides of the chassis, respectively. In step 850, the left and right drive belts may be wrapped around the drive shaft of the left and right wheel assemblies, respectively, such that the drive belt controls rotation of the drive shaft about the steering axis. Each bevel gear may couple the respective drive shaft to the axle such that rotation of the drive shaft about the steering axis controls rotation of the wheel about the drive axis to move the robotic platform in a substantially horizontal direction. In step 860, a controller may be electrically coupled to the left and right drive motors. The controller may be coupled to the left and right drive motors independently of one another, to provide differential power. The disclosed robotic platform may provide many advantages. For example, the robotic platform's power assembly may be located entirely outside the circumference of the wheel, thus protecting the power assembly from forceful impacts as well as environmental conditions that may surround the robotic platform. Similar benefits may be achieved by disposing a brake assembly distally from the wheel. The robotic platform disclosed herein may also allow the wheel to freely rotate 360 degrees about its steering axis without becoming entangled by electrical wires or other components of the drive assembly. The disclosed robotic platform may provide differential power, which may enable improved steering and maneuverability. The methods and sequence of the steps described and/or illustrated herein are given by way of example only and can be varied as desired. For example, while the steps illustrated and/or described herein may be shown or discussed in a particular order, these steps do not necessarily need to be performed in the order illustrated or discussed. The various exemplary methods described and/or illustrated herein may also omit one or more of the steps described or illustrated herein or include additional steps in addition to those disclosed. The preceding description has been provided to enable others skilled in the art to best utilize various aspects of the exemplary embodiments disclosed herein. This exemplary description is not intended to be exhaustive or to be limited to any precise form disclosed. Many modifications and variations are possible without departing from the spirit and scope of the instant disclosure. The embodiments disclosed herein should be considered in all respects illustrative and not restrictive. Reference should be made to the appended claims and their equivalents in determining the scope of the instant disclosure. Unless otherwise noted, the terms connected to and coupled to and their derivatives, as used in the specification and claims, are to be construed as permitting both direct and indirect i. e. , via other elements or components connection. In addition, the terms a or an, as used in the specification and claims, are to be construed as meaning at least one of. Finally, for ease of use, the terms including and having and their derivatives, as used in the specification and claims, are interchangeable with and have the same meaning as the word comprising.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/13/909/108/0.pdf",
                    "CONTRADICTION_SCORE": 0.9460012316703796,
                    "F_SPEC_PARAMS": [
                        "highly maneuverable functions",
                        "maneuverability of the robotic platform",
                        "unable to rotate 360 degrees without entangling the wires",
                        "risk of damage",
                        "power assembly may be damaged"
                    ],
                    "S_SPEC_PARAMS": [
                        "risk of damage"
                    ],
                    "A_PARAMS": [
                        "placement of a power or brake assembly within the circumference of a caster wheel,",
                        "Positioning all or some of the power assembly within the circumference of the wheel",
                        "Positioning all or some of the power assembly within the circumference of the wheel"
                    ],
                    "F_SENTS": [
                        "The placement of a power or brake assembly within the circumference of a caster wheel, however, can lead to various disadvantages.",
                        "For example, when an electrical component of the power assembly , a motor is disposed within the circumference of the wheel, the motor may be exposed to whatever conditions the wheel moves through.",
                        "This is especially disadvantageous for robots intended to perform highly maneuverable functions.",
                        "In addition, when all or some of the power assembly is located within the circumference of the wheel, the electrical wires that provide electricity to the power assembly may limit the maneuverability of the robotic platform since the wheel may be unable to rotate 360 degrees without entangling the wires.",
                        "Positioning all or some of the power assembly within the circumference of the wheel may also lead to an increased risk of damage to the power assembly.",
                        "For instance, when the driven caster wheel is part of a robot or other object that is susceptible to falling or being dropped, the power assembly may be damaged by such forceful impacts."
                    ],
                    "S_SENTS": [
                        "Positioning all or some of the power assembly within the circumference of the wheel may also lead to an increased risk of damage to the power assembly."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Durability of Moving Object",
                        "Force Torque",
                        "Harmful Factors Acting on Object"
                    ],
                    "F_SIM_SCORE": 0.5288056135177612,
                    "S_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object"
                    ],
                    "S_SIM_SCORE": 0.6811132431030273,
                    "GLOBAL_SCORE": 1.650960659980774
                },
                "sort": [
                    1.6509607
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11273091-20220315",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11273091-20220315",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2018-05-24",
                    "PUBLICATION_DATE": "2022-03-15",
                    "INVENTORS": [
                        "Haizhong Zhang",
                        "Yutong Zhang",
                        "Lejun Xing",
                        "Youbai Chen",
                        "Tingting Jia",
                        "Hang Feng",
                        "Yao Duan",
                        "Xiaoguang Tian",
                        "Rui Zhao",
                        "Liang Zhu",
                        "Fanhao Meng",
                        "Bo Qiao",
                        "Zheng Zhou",
                        "Suixin Hu",
                        "Nenghao Jin",
                        "Wenjun Yu"
                    ],
                    "APPLICANTS": [
                        "Haizhong Zhang    ( Beijing , CN )"
                    ],
                    "INVENTION_TITLE": "Robot system for oral cavity and tooth treatment",
                    "DOMAIN": "A61G 1514",
                    "ABSTRACT": "A robot system for oral cavity and tooth treatment. The system includes a console, a seat device, a robot and a navigation device. The console includes a display device, an operating device and an emergency stop device. The seat device includes a body support portion, an orientation adjustment portion and a head fixing portion. The robot comprises a main controller, a robotic arm, a manipulator. The navigation device performs navigation inside a tooth in a root canal treatment operation according to pre-stored tooth information. The treatment instrument in the manipulator can be replaceable. The seat device can be adjusted such that a patient can face any direction between forward and downward, or lie prostrate.",
                    "CLAIMS": "1. A manipulator for oral cavity and tooth treatment, comprising: a shell, detachable and provided thereon with a fixing portion and a water spray outlet, wherein the fixing portion fixes a base end of a treatment instrument on the shell; a treatment instrument, used for treating the oral cavity and tooth of a patient, replaceable, and capable of being replaced with different treatment instruments according to specific requirements for treatment, and wherein the treatment instrument can be retracted in the manipulator and can extend out from the manipulator when in use; a water or air sprayer, extending out from the water spray outlet, for spraying water or air having a certain pressure; a lighting device, for illuminating the oral cavity and tooth of the patient; and an imaging device, disposed on the shell, for imaging the oral cavity and tooth of the patient. 2. The manipulator as claimed in claim 1, wherein the treatment instrument is selected from the group consisting of a laser head, an ultrasonic tooth cleaning head, a drill needle, an enlarge needle, a polishing head, a filling material injection head, and a filling and pressing head. 3. The manipulator as claimed in claim 1, wherein the imaging device comprises more than two cameras configured to stereo-image a tooth. 4. The manipulator as claimed in claim 3, wherein the cameras enlarge an image by 1-20 times. 5. A robot system for oral cavity and tooth treatment, comprising: a main body, mounted with a controller and an actuator to control and drive the robot system to move; at least one arm, having one end thereof movably connected to the main body via a joint portion and another end movably connected to a manipulator of claim 1 via a joint portion, and comprising at least one intermediate joint portion, thus allowing the main body to control the manipulator to do free motion in three-dimensional space; and a console, comprising an operating device and a display device, wherein the operating device transmits operation information to the main body to control and drive the movement of the at least one arm and the manipulator; and the display device receives from the manipulator images from the imaging device, and displays the images via the display device or a visual observation device. 6. The robot system as claimed in claim 5, wherein the manipulator can extend, retract, incline and rotate by 180 degrees relative to the movably connected at least one arm; and the manipulator further comprises a pressure sensor, for detecting a resistance encountered during treatment when the treatment instrument operates on a tooth, and transmitting a detection result of the resistance to the console. 7. The robot system as claimed in claim 6, wherein the display device comprises at least a display for use by a patient and a display for use by a doctor; the display for use by the patient is used for displaying a two-dimensional treatment image for the patient; the display device further comprises two eye lenses; wherein the two eye lenses and the display for use by the doctor are utilized to perform stereo-imaging; and the display for use by the doctor comprises a waveform display interface for displaying a detection result of the pressure sensor. 8. The robot system as claimed in claim 5, wherein the operating device comprises at least one virtual operating handle connected to the console in a wireless or wired manner. 9. The robot system as claimed in claim 5, further comprising an emergency stop device for stopping the movement of the treatment instrument and slowly withdrawing the manipulator. 10. The robot system as claimed in claim 9, wherein the emergency stop device comprises an emergency stop device for use by the doctor and an emergency stop device for use by the patient. 11. The robot system as claimed in claim 5, further comprising a navigation device, wherein the navigation device comprises a body portion and a navigation tracker; wherein the body portion is fixed to the robot system; wherein the navigation tracker is connected to the robot system in a wireless or wired manner, such that the robot system can be controlled to move to observe a tooth of the patient; and wherein according to tooth data acquired from the patient before an operation, the navigation device locates the tooth of the patient during treatment, and displays the depth and direction of the treatment instrument entering the tooth and a root canal. 12. The robot system as claimed in claim 11, wherein the navigation tracker is disposed on an independent mechanical arm. 13. The robot system as claimed in claim 11, wherein the console is configured to set a moving range of the manipulator according to the tooth data of the navigation device, such that the manipulator does not move beyond the set moving range.",
                    "FIELD_OF_INVENTION": "The present invention relates to a robot system for oral cavity and tooth treatment, in particular to a robot system for controlling a robot to treat a tooth.",
                    "STATE_OF_THE_ART": "In dental treatment procedures currently practiced, a doctor generally performs the therapy and operation manually, which requires the doctor to observe closely to perform an operation. To complete an operation, the doctor need to spend a large amount of physical energy, and need to maintain a posture for a long time, such as keeping his or her head low, among others. As a result, is prone to cause fatigue for doctors, the doctor working for a long time is easy to get tired, thus reducing operation quality. At present, various dental handpieces are already proposed in related fields to replace the manual operation of a dentist, thereby reducing the workload of the dentist, and realizing intelligent dental treatment. In such dental handpieces, through a camera and a lighting device installed, image of a tooth in the oral cavity of a patient is transmitted to a display, and then the doctor performs an operation based on what is shown on the display. Therefore, the doctor does not need to observe the oral cavity of the patient at a short distance to perform the operation. As a result, the physical energy and stamina required of the doctor is greatly reduced for the operation, the fatigue is much lessened, and the operation quality is improved. However, the above-described dental handpieces still have the following problems, among others: the dental handpieces comprise only an operating manipulator such as a drill bit; a doctor can only perform an operation through simple control using the manipulator, and cannot intelligently perform different types of dental procedures; the scope of application of such handpieces is narrow, and intelligent control cannot be realized.",
                    "SUMMARY": [
                        "The present invention provides a robot system for oral cavity and tooth treatment. On the basis of the original dental handpiece, the present invention provides a complete robot system, wherein the system can act as an integrated dental treatment device based on which the entire operating procedure of various dental treatments can be completed. In addition, in the robot system, the manipulator, as an operating main body, comprises at least one manipulator capable of being replaced with a treatment instrument, which ensures that the robot system can be used to perform different dental treatments. Furthermore, in the robot system, the orientation of the seat can be adjusted to allow the patient to face forward or downward, or allow the patient to lie prostrate to receive a dental treatment, in which case the sprayed water in the oral cavity of the patient can naturally flow out during dental treatment instead of being left in the oral cavity, reducing the discomfort of the patient due to the dirty water. Specifically, according to one aspect of the present invention, provided is a manipulator for treating oral cavity and teeth of a patient, comprising:a shell, detachable and provided thereon a fixing portion and a water spray outlet, wherein the fixing portion fixes a base end of a treatment instrument on the shell;a treatment instrument, used for treating the oral cavity and tooth of a patient, replaceable, and capable of being replaced with different treatment instruments as required by specific therapies;a water sprayer, extending out from the water spray outlet, and used for spraying water or air with a certain pressure;an lighting device, used for illuminating the oral cavity and tooth of the patient;andan imaging device, disposed on the shell, for imaging the oral cavity and teeth of the patient. Further, the treatment instruments are enclosed in the manipulator, and the selected treatment instrument extends out from the manipulator when in use. Further, the treatment instrument comprises a laser head, an ultrasonic tooth cleaning head, a drill needle, an enlargement needle, a polishing head, and a filling material injection head or a filling and pressing head. Further, the imaging device comprises more than two cameras configured to obtain a stereo image of the teeth. Further, the cameras enlarge an image by 1-20 times. Further, the manipulator further comprises an elastic flap, wherein the elastic flap is disposed on an outer periphery of the shell on one side that is provided with the treatment instrument, for preventing the tongue of the patient from touching the manipulator. According to another aspect of the present invention, provided is a robot system for treating the oral cavity and teeth, comprising:a main body, mounted with a controller and an actuator to control and drive the movement of the robot system;at least one mechanical arm, having one end thereof movably connected to the main body via a joint and the other end movably connected to the manipulator via a joint portion, and comprising at least one intermediate joint portion, thus allowing the main body to control the manipulator to move freely in three-dimensional space;any one manipulator as described above, movably connected to the mechanical arm via a joint portion; anda console, comprising an operating device and a display device, wherein the operating device transmits operation information to the main body to control and drive the movement of the mechanical arm and the manipulator; and the display device receives from the manipulator portion images from the imaging device, and displays the images via the display device or a visual observation device. Further, the manipulator can extend, retract, incline and rotate by 180 degrees relative to the movably connected arm. Further, the display device comprises at least two displays, one for use by the patient, and the other for use by the doctor, wherein the display for patient is used for displaying a 2-dimensional image of the treatment for the patient, and the display for the doctor is used for displaying stereo images for the doctor. Further, the display device further comprises two eye lenses; the two eye lenses and display for the doctor are utilized to realize stereo-imaging. Further, the manipulator further comprises a pressure sensor for detecting a resistance encountered during treatment when the treatment instrument operates on a tooth, and transmitting the detection result to the console. Further, the display for the doctor comprises a waveform display interface which displays the detection result of the pressure sensor. Further, the operating device comprises at least one virtual operating handle connected to the console in a wireless or wired manner. Further, the treatment robot system further comprises an emergency stop device for stopping the rotation of a drill bit and slowly withdrawing the manipulator. Further, the treatment robot system further comprises a navigation device, wherein the navigation device comprises a body portion and a navigation tracker; the body portion is fixed to the robot system; the navigation tracker is connected to the robot system in a wireless or wired manner, such that the robot system can be controlled to move to observe the tooth of the patient; and according to tooth data acquired from the patient before an operation, the navigation device locates the tooth of the patient during treatment, and displays the depth and direction of the treatment instrument entering the tooth and root canal. Further, the navigation tracker is disposed on an independent mechanical arm. Further, the console allows setting a moving range for the manipulator according to the tooth data of the navigation device; and when the manipulator moves beyond the set moving range, the manipulator stops moving. According to another aspect of the present invention, further provided is a seat device of the robot system for oral cavity and tooth treatment, comprising:a body support portion, remaining in a fixed position relative to the position of the treatment robot system;a head fixing portion, for fixing the head of a patient, so that the head of the patient is in a fixed position relative to the position of a seat portion; and. an orientation adjustment portion, for adjusting the inclination angle of the seat portion, and enabling the patient to face downwards. Further, the patient may face any directions between forward and downward. According to another aspect of the present invention, further provided is a treatment couch of the robot system for oral cavity and tooth treatment, comprising:a couch body, for a patient to lie prostrate, comprising a face exposure portion from which the face of the patient exposes downwards; anda head fixing portion, for fixing the head of the patient after the patient lies prostrate, so that the head of the patient is in a fixed position relative to the position of the couch body. Further, the treatment couch further comprises an orientation adjustment portion for adjusting the inclination angle of the treatment couch. Further, the manipulator of the robot system extends into the mouth of the patient from below. The technical solution of the present invention will be further described in detail hereafter in connection with drawings and preferred embodiments of the present invention. And the beneficial effects of the present invention will be further specified.",
                        "The drawings in the specification provide a further understanding to the present invention and constitute a part of the present invention. And the description thereof is used for explaining the present invention only, and not for unduly limiting the present invention. 1 is a schematic view of the robot system for oral cavity and tooth treatment according to the first embodiment of the present invention; 2 is a perspective view of the manipulator of the robot system for oral cavity and tooth treatment according to the first embodiment of the present invention; 3 is a bottom view of the manipulator of the robot system for oral cavity and tooth treatment according to the first embodiment of the present invention; 4 is a bottom view of the internal structure of an upper shell after the shell of the manipulator of the robot system for oral cavity and tooth treatment according to the first embodiment of the present invention is disassembled; 5 is a schematic view of the robot system for oral cavity and tooth treatment according to the first embodiment of the present invention in the state as shown in 4 after the treatment instrument is removed; 6 is a schematic view of the seat device of the robot system for oral cavity and tooth treatment according to the first embodiment of the present invention; and 7 is a schematic view of the treatment couch of the robot system for oral cavity and tooth treatment according to the second embodiment of the present invention."
                    ],
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/91/730/112/0.pdf",
                    "CONTRADICTION_SCORE": 0.9874961376190186,
                    "F_SPEC_PARAMS": [
                        "fatigue",
                        "easy to get tired,",
                        "operation quality"
                    ],
                    "S_SPEC_PARAMS": [
                        "cannot intelligently perform different types of dental procedures;",
                        "scope of application of such handpieces is narrow,",
                        "intelligent control cannot be realized"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "As a result, is prone to cause fatigue for doctors, the doctor working for a long time is easy to get tired, thus reducing operation quality."
                    ],
                    "S_SENTS": [
                        "However, the above-described dental handpieces still have the following problems, among others: the dental handpieces comprise only an operating manipulator such as a drill bit; a doctor can only perform an operation through simple control using the manipulator, and cannot intelligently perform different types of dental procedures; the scope of application of such handpieces is narrow, and intelligent control cannot be realized."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Strength",
                        "Waste of Time",
                        "Reliability"
                    ],
                    "F_SIM_SCORE": 0.5206025739510854,
                    "S_TRIZ_PARAMS": [
                        "Complexity of Control"
                    ],
                    "S_SIM_SCORE": 0.6042050123214722,
                    "GLOBAL_SCORE": 1.6498999307552975
                },
                "sort": [
                    1.6499
                ]
            },
            {
                "_index": "patents",
                "_type": "_doc",
                "_id": "US11407104-20220809",
                "_score": null,
                "_ignored": [
                    "IMAGE.keyword"
                ],
                "_source": {
                    "REF_PATENT": "US11407104-20220809",
                    "KIND_CODE": "B2",
                    "APPLICATION_DATE": "2019-07-08",
                    "PUBLICATION_DATE": "2022-08-09",
                    "INVENTORS": [
                        "Shuusuke Watanabe",
                        "Shinji Mizokami"
                    ],
                    "APPLICANTS": [
                        "FANUC Corporation    ( Yamanashi , JP )"
                    ],
                    "INVENTION_TITLE": "Tooth contact position adjustment amount estimation device, machine learning device, and robot system",
                    "DOMAIN": "B25J 9163",
                    "ABSTRACT": "A tooth contact position adjustment amount estimation device that performs processing with respect to estimating a tooth contact position adjustment amount for dimensional data of parts constituting a power transmission mechanism according to the present invention, comprising: a machine learning device that performs processing with respect to estimating the tooth contact position adjustment amount for the dimensional data of parts constituting the power transmission mechanism, wherein the machine learning device observes part dimensional data, which is the dimensional data of parts constituting the power transmission mechanism, as a state variable indicating a current state of an environment, and performs processing with respect to estimating the tooth contact position adjustment amount for the dimensional data of parts constituting the power transmission mechanism in assembling the power transmission mechanism by performing processing with respect to machine learning based on the observed state variable.",
                    "CLAIMS": "1. A tooth contact position adjustment amount estimation device configured to perform processing with respect to estimating a tooth contact position adjustment amount for dimensional data of parts constituting a power transmission mechanism, the tooth contact position adjustment amount estimation device comprising: a machine learning device configured to perform processing with respect to estimating the tooth contact position adjustment amount for the dimensional data of parts constituting the power transmission mechanism, a tooth contact between the parts changing due to a dimensional error of the parts, wherein the machine learning device is configured to observe part dimensional data, which is the dimensional data of the parts constituting the power transmission mechanism, as a state variable indicating a current state of an environment, before the parts are supplied to assemble the power transmission mechanism, store the observed part dimensional data in a memory, and perform processing with respect to estimating the tooth contact position adjustment amount for the dimensional data of the parts constituting the power transmission mechanism before the power transmission mechanism is assembled, by performing processing with respect to machine learning based on the state variable observed and stored in the memory before the parts are supplied. 2. The tooth contact position adjustment amount estimation device according to claim 1, wherein the machine learning device comprises: a state observation unit configured to observe the part dimensional data, which is dimensional data of parts constituting the power transmission mechanism, as the state variable indicating the current state of the environment; a label data acquisition unit configured to acquire tooth contact position adjustment amount data adjusted in assembling the power transmission mechanism as label data; and a learning unit configured to learn the tooth contact position adjustment amount in assembling the power transmission mechanism for the dimensional data of parts constituting the power transmission mechanism by use of the state variable and the label data. 3. The tooth contact position adjustment amount estimation device according to claim 2, wherein the learning unit comprises: an error calculation unit configured to calculate an error between a correlation model for estimating the tooth contact position adjustment amount in assembling the power transmission mechanism from the state variable and a correlation characteristic identified by previously-prepared teaching data; and a model update unit configured to update the correlation model to reduce the error. 4. The tooth contact position adjustment amount estimation device according to claim 2, wherein the learning unit is configured to calculate the state variable and the label data in a multilayered structure. 5. The tooth contact position adjustment amount estimation device according to claim 1, wherein the machine learning device comprises: a state observation unit configured to observe part dimensional data as dimensional data of parts constituting a power transmission mechanism as a state variable indicating a current state of an environment; a learning unit that has learned a tooth contact position adjustment amount in assembling a power transmission mechanism for dimensional data of parts constituting the power transmission mechanism; and an estimation result output unit configured to estimate and output a tooth contact position adjustment amount in assembling a power transmission mechanism on the basis of a state variable observed by the state observation unit and a learning result by the learning unit. 6. A machine learning device configured to perform processing with respect to estimating a tooth contact position adjustment amount for dimensional data of parts constituting a power transmission mechanism, wherein the machine learning device is configured to observe part dimensional data, which is the dimensional data of the parts constituting the power transmission mechanism, as a state variable indicating a current state of an environment, before the parts are supplied to assemble the power transmission mechanism, store the observed part dimensional data in a memory, and perform processing with respect to estimating the tooth contact position adjustment amount for the dimensional data of the parts constituting the power transmission mechanism before the parts are assembled into the power transmission mechanism, by performing processing with respect to machine learning based on the state variable observed and stored in the memory before the parts are supplied, wherein a tooth contact between the parts changes due to a dimensional error of the parts. 7. The machine learning device according to claim 6, further comprising: a state observation unit configured to observe the part dimensional data as the dimensional data of parts constituting the power transmission mechanism as the state variable indicating the current state of the environment; a label data acquisition unit configured to acquire the tooth contact position adjustment amount data adjusted in assembling the power transmission mechanism as the label data; and a learning unit configured to learn the tooth contact position adjustment amount in assembling the power transmission mechanism for the dimensional data of parts constituting the power transmission mechanism by use of the state variable and the label data. 8. A machine learning device according to claim 6, further comprising: a state observation unit configured to observe the part dimensional data as the dimensional data of parts constituting the power transmission mechanism as the state variable indicating the current state of the environment; a learning unit that has learned the tooth contact position adjustment amount in assembling the power transmission mechanism for the dimensional data of parts constituting the power transmission mechanism; and an estimation result output unit configured to estimate and output the tooth contact position adjustment amount in assembling the power transmission mechanism on the basis of the state variable observed by the state observation unit and a learning result by the learning unit. 9. A robot system, comprising: a robot; and a controller configured to on the basis of the tooth contact position adjustment amount estimated by the tooth contact position adjustment amount estimation device according to claim 5, determine a number of shims to be used for assembling the power transmission mechanism from the parts, and instruct the robot to insert the determined number of shims between the parts to obtain the power transmission mechanism. 10. A tooth contact position adjustment amount estimation system, comprising: a plurality of devices connected to each other via a network, wherein the plurality of devices include at least one tooth contact position adjustment amount estimation device according to claim 2, and the at least one tooth contact position adjustment amount estimation device comprises a first tooth contact position adjustment amount estimation device. 11. The tooth contact position adjustment amount estimation system according to claim 10, wherein the plurality of devices include a computer provided with a machine learning device, the computer is configured to acquire a learning model as at least one of results of learning by the first tooth contact position adjustment amount estimation device, and the machine learning device is configured to perform optimization or efficiency enhancement based on the learning model. 12. The tooth contact position adjustment amount estimation system according to claim 10, wherein the at least one tooth contact position adjustment amount estimation device further comprises a second tooth contact position adjustment amount estimation device different from the first tooth contact position adjustment amount estimation device, and the first tooth contact position adjustment amount estimation device is configured to share a result of learning with the second tooth contact position adjustment amount estimation device. 13. The tooth contact position adjustment amount estimation system according to claim 10, wherein the at least one tooth contact position adjustment amount estimation device further comprises a second tooth contact position adjustment amount estimation device different from the first tooth contact position adjustment amount estimation device, and the second tooth contact position adjustment amount estimation device is configured to provide, via the network, data observed in the second tooth contact position adjustment amount estimation device to be used for in the first tooth contact position adjustment amount estimation device. 14. The tooth contact position adjustment amount estimation device according to claim 1, wherein the parts constituting the power transmission mechanism include a first part having a first teeth and a second part having a second teeth engageable with the first teeth of the first part in assembling the power transmission mechanism, and the tooth contact position is a relative contact position between the first teeth of the first part and the second teeth of the second part when the first teeth engages with the second teeth. 15. The tooth contact position adjustment amount estimation device according to claim 14, wherein an axial line of the first part and an axial line of the second part are arranged at a right angle. 16. The machine learning device according to claim 6, wherein the parts constituting the power transmission mechanism include a first part having a first teeth and a second part having a second teeth engageable with the first teeth of the first part in assembling the power transmission mechanism, and the tooth contact position is a relative contact position between the first teeth of the first part and the second teeth of the second part when the first teeth engages with the second teeth. 17. The machine learning device according to claim 16, wherein an axial line of the first part and an axial line of the second part are arranged at a right angle. 18. The tooth contact position adjustment amount estimation device according to claim 1, wherein the parts constituting the power transmission mechanism include a first bevel gear with a first axial line and a second bevel gear with a second axial line, and the first axial line and the second axial line are arranged at a right angle when the first and the second bevel gears are assembled in the power transmission mechanism. 19. The machine learning device according to claim 6, wherein the parts constituting the power transmission mechanism include a first bevel gear with a first axial line and a second bevel gear with a second axial line, and the first axial line and the second axial line are arranged at a right angle when the first and the second bevel gears are assembled in the power transmission mechanism. 20. The tooth contact position adjustment amount estimation device according to claim 1, wherein the tooth contact position adjustment amount determines, before assembling the power transmission mechanism, a number of shims to be inserted between the parts when the power transmission mechanism is assembled from the parts.",
                    "FIELD_OF_INVENTION": "The present invention relates to a tooth contact position adjustment amount estimation device, a machine learning device, and a robot system. The present invention particularly relates to a tooth contact position adjustment amount estimation device, a machine learning device, and a robot system for estimating a tooth contact position adjustment amount of gears used in a structure for transmitting power via shafts the axial lines of which cross each other.",
                    "STATE_OF_THE_ART": "A bevel gear is used in a structure for transmitting power via shafts the axial lines of which cross each other. A worm gear is used in a structure for transmitting power via shafts the axial lines of which are offset with each other. When the tooth contact of the gears, which are assembled on each shaft, and which mesh with each other, is not appropriate, abnormal noise, vibration, and abnormal wear of the teeth can occur. It is important to optimize the shape of the gears for appropriate tooth contact. The positions where the gears are assembled may be different with each other due to an error of dimensions of the parts assembled with the gears. 6 is a diagram illustrating a gear structure using bevel gears. In 6, power on the input side is transmitted to the output side by the bevel gears the axial lines of which are arranged at right angle. The bevel gears are assembled on a reducer and a holder, respectively. The reducer or the holder is assembled to a casing. Thus, the tooth contact of the bevel gears changes due to a dimensional error of the assembled parts such as the bevel gears or the holder. For example, when the shaft assembled with the bevel gear on the input side is short in 6, the bevel gear on the input side and the bevel gear on the output side are different with each other in terms of their meshing position. In this case, there arises a problem that abnormal noise, vibration, or abnormal wear of the teeth occurs. In such a case, the tooth contact needs to be adjusted. A tooth contact position adjustment shim is inserted between the parts such as the gears or the holder in order to adjust the tooth contact JP 2016-142379 A, JP 2013-160361 A. The publication of Japanese Patent 3586997 discloses a technique for determining tooth contact adjustment in an image processing, for example, as a conventional technique for adjusting tooth contact between gears. However, a dimensional variation is in the parts such as the gears or the holder. Thus, an optimum assembly position is different for each part. In the tooth contact adjustment method, the optimum tooth contact position adjustment amount such as the thickness of shim or the number of shims is finally determined by an experienced assembly worker. Thus, there is a problem that it takes much time for an unexperienced worker to determine the optimum tooth contact position adjustment amount. Tooth contact between gears is evaluated by an image in JP 2016-142379 A, but tooth contact needs to be evaluated while the gears are actually assembled. When it is determined that the tooth contact position is not appropriate, a step of removing the assembled parts and inserting a shim is caused. Thus, productivity or work efficiency lowers in the technique disclosed in JP 2016-142379 A. It is therefore an object of the present invention to provide a tooth contact position adjustment amount estimation device capable of determining the thickness or number of shims to be used before assembling a power transmission mechanism, a machine learning device, and a robot system.",
                    "SUMMARY": [
                        "A tooth contact position adjustment amount estimation device, a machine learning device, and a robot system according to the present invention solve at least some of the above problems. According to one embodiment of the present invention, machine learning is performed in which input data and output data corresponding to the input data are used as teaching data. The input data is dimensional data of parts such as gears constituting a structure for transmitting power via shafts the axial lines of which cross each other. The output data indicates the optimum tooth contact position adjustment amount when assembling the parts. A learned model acquired as a result of machine learning is used to estimate the tooth contact position adjustment amount from the part dimensional data. The thickness or number of shims is determined on the basis of the estimated tooth contact position adjustment amount. Thereby, the assembly position rearrangement step is alleviated and the assembly is efficiently performed. According to one aspect of the present invention, there is provided a tooth contact position adjustment amount estimation device that performs processing with respect to estimating a tooth contact position adjustment amount for dimensional data of parts constituting a power transmission mechanism, comprising: a machine learning device that performs processing with respect to estimating the tooth contact position adjustment amount for dimensional data of parts constituting a power transmission mechanism, wherein the machine learning device observes part dimensional data, which is dimensional data of parts constituting the power transmission mechanism, as a state variable indicating a current state of an environment, and performs processing with respect to estimating the tooth contact position adjustment amount for dimensional data of parts constituting the power transmission mechanism in assembling the power transmission mechanism by performing processing with respect to machine learning based on the observed state variable. According to another aspect of the present invention, there is provided a machine learning device that performs processing with respect to estimating a tooth contact position adjustment amount for dimensional data of parts constituting a power transmission mechanism, wherein the machine learning device observes part dimensional data, which is dimensional data of parts constituting the power transmission mechanism, as a state variable indicating a current state of an environment, and performs processing with respect to estimating the tooth contact position adjustment amount for dimensional data of parts constituting the power transmission mechanism in assembling the power transmission mechanism by performing processing with respect to machine learning based on the observed state variable. According to another aspect of the present invention, there is provided a robot system including a controller that determines the number of shims used for assembling a power transmission mechanism and instructs a robot to assemble a determined number of shims on the parts on the basis of the tooth contact position adjustment amount estimated by the tooth contact position adjustment amount estimation device. According to another aspect of the present invention, there is provided a tooth contact position adjustment amount estimation system in which a plurality of devices are connected to each other via a network, wherein the plurality of devices include at least one tooth contact position adjustment amount estimation device provided with a learning unit. According to the present invention, the thickness or number of shims is previously determined on the basis of the estimated tooth contact position adjustment amount. Thus, loads on a worker in the assembly position rearrangement step are alleviated and the assembly is efficiently performed.",
                        "The above and other objects and characteristics of the present invention will be apparent from the following description of embodiments with reference to the accompanying drawings: 1 is a schematic hardware configuration diagram of a tooth contact position adjustment amount estimation device according to a first embodiment; 2 is a diagram illustrating a schematic configuration of a robot system; 3 is a schematic functional block diagram of the tooth contact position adjustment amount estimation device according to the first embodiment; 4 is a diagram illustrating exemplary part dimensional data; 5 is a schematic functional block diagram illustrating one form of the tooth contact position adjustment amount estimation device according to a second embodiment; 6 is a diagram illustrating an exemplary power transmission mechanism; 7 is a diagram illustrating an exemplary system of three-layer structure including a cloud server, a fog computer and an edge computer; 8 is a schematic hardware configuration diagram of a tooth contact position adjustment amount estimation device implemented on a computer; 9 is a schematic configuration diagram of a tooth contact position adjustment amount estimation device according to a third embodiment; 10 is a schematic configuration diagram of a tooth contact position adjustment amount estimation device according to a fourth embodiment; and 11 is a schematic configuration diagram of a tooth contact position adjustment amount estimation device according to a fifth embodiment."
                    ],
                    "DESCRIPTION": "Embodiments of the present invention will be described below with reference to the drawings. 1 is a schematic hardware configuration diagram illustrating a tooth contact position adjustment amount estimation device 1 according to the first embodiment. The tooth contact position adjustment amount estimation device 1 is implemented on a computer such as a personal computer provided for a robot system for manufacturing a machine, or a cell computer, a host computer, an edge server, and a Cloud server which are connected to the robot system via a wired/wireless network. According to the present embodiment, the tooth contact position adjustment amount estimation device 1 is implemented on a personal computer provided for a robot system 2. The tooth contact position adjustment amount estimation device 1 is connected to the robot system 2 illustrated in 2 via an interface 19. The robot system 2 includes a controller 2A and a robot 2B controlled by the controller 2A. The robot 2B assembles the parts previously supplied and arranged by a part supply device or the like thereby to assemble a power transmission mechanism. When assembling the parts, the robot system 2 assembles only the required gear adjustment shims on the parts. The tooth contact position adjustment amount estimation device 1 acquires dimensional data of the respective parts previously input into the controller 2A via the interface 19. The tooth contact position adjustment amount estimation device 1 outputs the tooth contact position adjustment amount as an estimation result to the controller 2A via the interface 19. The controller 2A determines the number of shims to be assembled on the parts or the thickness of a shim on the basis of the tooth contact position adjustment amount input from the tooth contact position adjustment amount estimation device 1. A detailed configuration of the robot system 2 for assembling a power transmission mechanism is known, and thus a detailed description thereof will be omitted in the present specification. A central processing unit CPU 11 provided for the tooth contact position adjustment amount estimation device 1 is a processor for totally controlling the tooth contact position adjustment amount estimation device 1. The CPU 11 reads a system program stored in a read only memory ROM 12 via a bus 20. The CPU 11 controls the entire tooth contact position adjustment amount estimation device 1 according to the system program. Temporary calculation data or display data, or various items of data input by a worker from an input device 71 are temporarily stored in a random access memory RAM 13. A nonvolatile memory 14 is backed up by a battery not illustrated, for example. The nonvolatile memory 14 is a memory in which the stored states are held even if the tooth contact position adjustment amount estimation device 1 is powered off. The nonvolatile memory 14 has a setting area in which the setting information for the operations of the tooth contact position adjustment amount estimation device 1 is stored. The nonvolatile memory 14 stores the programs input from the input device 71, the programs read from an external storage device not illustrated, or the dimensional data of the respective parts acquired from the robot system 2 via the interface 19. The programs or various items of data stored in the nonvolatile memory 14 may be loaded into the RAM 13 on execution/in use. Further, various system programs such as known analysis program including a system program for controlling communication with a machine learning device 100 described below are previously written in the ROM 12. A display device 70 displays thereon each item of data read into the memory, or data acquired as a result of an executed program or the like. Data or the like output from the machine learning device 100 described below is input into the display device 70 via an interface 17. The display device 70 displays the input data or the like thereon. The input device 71 is configured of a keyboard or a pointing device. The input device 71 receives commands based on worker's operations, data, and the like. The input device 71 passes the received commands, data, and the like to the CPU 11 via an interface 18. An interface 21 connects each part of the tooth contact position adjustment amount estimation device 1 with the machine learning device 100 in the tooth contact position adjustment amount estimation device 1. The machine learning device 100 includes a processor 101, a ROM 102, a RAM 103, and a nonvolatile memory 104. The processor 101 controls the entire machine learning device 100. The ROM 102 stores a system program and the like. The RAM 103 stores temporary data in each processing for machine learning. The nonvolatile memory 104 sores a learning model and the like. The machine learning device 100 observes each item of information such as dimensional data of each part acquired from the robot system 2 capable of being acquired by the tooth contact position adjustment amount estimation device 1 via the interface 21. Further, the tooth contact position adjustment amount estimation device 1 causes the display device 70 to display the information on the tooth contact position adjustment amount estimated by the machine learning device 100. The tooth contact position adjustment amount estimation device 1 outputs the information on the tooth contact position adjustment amount estimated by the machine learning device 100 to the robot system 2. 3 is a schematic functional block diagram of the tooth contact position adjustment amount estimation device 1 according to the first embodiment. The CPU 11 provided for the tooth contact position adjustment amount estimation device 1 and the processor 101 of the machine learning device 100, which are illustrated in 1, execute the system programs, respectively, thereby to control the operation of each part so that each function of a functional block illustrated in 3 is realized. The tooth contact position adjustment amount estimation device 1 includes an output unit 34 for outputting the information on the tooth contact position adjustment amount estimated by the machine learning device 100 to the outside. The output unit 34 may output the estimation result of the tooth contact position adjustment amount output from the machine learning device 100 to the display device 70, for example. The configuration is used when a worker needs to set the number of shims used for assembling a power transmission mechanism in the robot system 2, for example. Further, the output unit 34 may directly output the estimation result of the tooth contact position adjustment amount output from the machine learning device 100 to the robot system 2, for example. The configuration is used when the tooth contact position adjustment amount estimation device 1 sets the number of shims used for assembling a power transmission mechanism in the robot system 2 via a communication line. On the other hand, the machine learning device 100 provided for the tooth contact position adjustment amount estimation device 1 performs machine learning in order to estimate the tooth contact position adjustment amount for the dimensional data of the parts constituting a power transmission mechanism assembled by the robot system 2. The machine learning device 100 includes software such as learning algorithm and hardware such as the processor 101 for performing machine learning. The machine learning device 100 learns a model structure indicating a correlation between the dimensional data of the parts constituting the power transmission mechanism and the tooth contact position adjustment amount. As illustrated in 3, the machine learning device 100 includes a state observation unit 106, a label data acquisition unit 108, a learning unit 110, and an estimation result output unit 122. The state observation unit 106 observes part dimensional data S1 indicating the dimensions of parts constituting a power transmission mechanism assembled by the robot system 2 as a status variable S indicating a current state of an environment. The label data acquisition unit 108 acquires label data L including tooth contact position adjustment amount data L1. The learning unit 110 associates and learns the dimensional data of the parts constituting the power transmission mechanism and the estimated tooth contact position adjustment amount by use of the state variable S and the label data L. The part dimensional data S1 in the state variable S observed by the state observation unit 106 is dimensional data of the parts constituting the power transmission mechanism acquired from the robot system 2. As illustrated in 4, the dimensional data of the parts constituting the power transmission mechanism can be defined as a matrix with the dimensional data of the respective parts as elements. The dimensions of the parts are observed by use of a 3D measurement device or the like before the parts are supplied to the robot system 2. The dimensions of the parts constituting the power transmission mechanism may be measured by an image of each part previously taken by an imaging device. The measured dimensional data of the parts is stored in the memory of the robot system 2 via a network or an external device such as USB device. When the robot system 2 is provided with a sensor such as 3D measurement device or imaging device, the dimensional data of each part may be measured when the part is supplied to the robot system 2. The label data acquisition unit 108 acquires, as the adjustment amount data L1, the data on the tooth contact position adjustment amount acquired from the robot system 2 at the time of assembly of a power transmission mechanism during learning. The label data acquisition unit 108 may acquire, as the adjustment amount data L1, the data on the tooth contact position adjustment amount input by the worker from the input device 71. The adjustment amount data L1 may indicate a distance between a reference position for adjusting the tooth contact position and a predetermined part of the gear. Further, the adjustment amount data L1 may indicate the number of inserted shims. A plurality of items of adjustment amount data L1 may be acquired from one power transmission mechanism. For example, in the case of the power transmission mechanism illustrated in 6, the label data acquisition unit 108 acquires two adjustment amounts including the adjustment amount y1 on the input side and the adjustment amount y2 on the output side as the adjustment amount data L1. The label data acquisition unit 108 is used for learning. Thus, the label data acquisition unit 108 may not be essential for the machine learning device 100 after the learning is completed. The learning unit 110 learns a correlation between the state variable S the part dimensional data S1 indicating the dimensions of the parts constituting the power transmission mechanism and the label data L the tooth contact position adjustment amount data L1 according to any learning algorithm. For example, the learning unit 110 learns a correlation between the part dimensional data S1 included in the state variable S and the adjustment amount data L1 included in the label data L. The learning unit 110 repeatedly performs learning based on a collection of data including the state variable S and the label data L. The learning unit 110 performs machine learning by use of the adjustment amount data L1 acquired by the label data acquisition unit 108 and the part dimensional data S1 of the parts constituting the power transmission mechanism corresponding to the adjustment amount data L1. The learning unit 110 performs machine learning online with the robot system 2. Further, the learning unit 110 may acquire the data accumulated when a power transmission mechanism is assembled by the robot system 2 via an external device such as USB device not illustrated. That is, the learning unit may perform learning offline. In this case, after the data acquired from the external device not illustrated is stored in the nonvolatile memory 14, for example, the state observation unit 106 and the label data acquisition unit 108 acquire the state variable S and the label data L, respectively, from the stored data. The learning unit 110 performs machine learning on the basis of the acquired data. The learning cycle is repeated so that the learning unit 110 automatically interprets a correlation between the dimensional data the part dimensional data S1 of the parts constituting the power transmission mechanism and the tooth contact position adjustment amount the adjustment amount data L1. The correlation between the part dimensional data S1 and the adjustment amount data L1 is substantially unknown at the start of the learning algorithm. The learning unit 110 gradually interprets the correlation between the part dimensional data S1 and the adjustment amount data L1 as the learning progresses. Consequently, the learning unit 110 acquires a learned model indicating the correlation between the part dimensional data S1 and the adjustment amount data L1. The estimation result output unit 122 estimates the tooth contact position adjustment amount on the basis of the dimensional data of the parts constituting the power transmission mechanism by use of the result learned model learned by the learning unit 110. The estimation result output unit 122 outputs the estimated tooth contact position adjustment amount. The estimation result of the tooth contact position adjustment amount output by the estimation result output unit 122 is output to the display device 70 by the output unit 34 to be displayed thereon. The worker confirms the estimation result displayed on the display device 70, and sets the tooth contact position adjustment amount. Further, the output unit 34 may directly output the estimation result of the tooth contact position adjustment amount output by the estimation result output unit 122 to the robot system 2. In this case, the controller 2A of the robot system 2 determines the number of shims assembled for assembling the power transmission mechanism on the basis of the input estimation result of the tooth contact position adjustment amount. The controller 2A of the robot system 2 gives a control command for assembling a determined number of shims to the robot 2B. The robot 2B assembles the power transmission mechanism in response to the control command. The estimation result output unit 122 is used for estimating the tooth contact position adjustment amount. Thus, the estimation result output unit 122 may not be essential for the machine learning device 100 while only the learning unit 110 performs learning. The learning algorithm executed by the learning unit 110 is not particularly limited, and a known learning algorithm is employed for machine learning. 5 illustrates a second embodiment of the tooth contact position adjustment amount estimation device 1 illustrated in 3. The learning unit 110 performs learning by use of a supervised learning algorithm. In the supervised learning, a known dataset called teaching data indicating an input and an output corresponding to the input is given to the learning unit 110. The teaching data is a dataset including dimensional data input of each of the parts acquired in the past and data output indicating the tooth contact position adjustment amount adjusted for each item of dimensional data, for example. The learning unit 110 identifies the characteristics indicating a correlation between the input and the output from the teaching data. Thereby, the learning unit 110 derives a correlation model for estimating an output for a new input. The learning unit 110 includes an error calculation unit 112 and a model update unit 114. The error calculation unit 112 calculates an error E between a correlation model M for estimating the tooth contact position adjustment amount and a correlation characteristic identified by teaching data T. The model update unit 114 updates the correlation model M to reduce the error E. The model update unit 114 in the learning unit 110 repeatedly updates the correlation model M thereby to learn estimating the tooth contact position adjustment amount. That is, the learning unit 110 performs machine learning thereby to derive the correlation model M indicating the correlation between the part dimensional data and the optimum tooth contact position adjustment amount for the part dimensional data. For example, the correlation between the state variable S and the label data L is simplified by a linear function, for example and expressed as the correlation model M at the start of learning. The simplified correlation model M is given to the learning unit 110 before the start of supervised learning. According to the present embodiment, the part dimensional data of the power transmission mechanism acquired in the past and the data on the tooth contact position adjustment amount are used for the teaching data T. When the tooth contact position adjustment amount estimation device 1 is operated, the teaching data T is given to the learning unit 110 regularly. The error calculation unit 112 identifies the correlation characteristics indicating the correlation between the part dimensional data of the power transmission mechanism and the tooth contact position adjustment amount by the teaching data T given to the learning unit 110. The error calculation unit 112 finds the error E between the identified correlation characteristic and the correlation model M corresponding to the state variable S and the label data L in the current state. The model update unit 114 updates the correlation model M to reduce the error E according to a predefined update rule, for example. In the next learning cycle, the error calculation unit 112 estimates the tooth contact position adjustment amount by use of the state variable S according to the updated correlation model M. The error calculation unit 112 finds the error E between the estimation result of the tooth contact position adjustment amount and the actually-acquired label data L. The model update unit 114 updates the correlation model M to reduce the error again. In this way, the unknown correlation between the current state of the environment and the estimation therefor gradually becomes clear. That is, the correlation between the dimensional data of the parts constituting the power transmission mechanism and the tooth contact position adjustment amount data for the dimensional data gradually becomes clear. The learning unit 110 may use a neural network for performing the supervised learning. A neural network including the three layers of input layer, middle layer, and output layer may be used as the neural network, for example. Further, the learning unit 110 may perform learning using a neural network with three or more layers, or deep learning. The learning unit 110 can perform more effective learning and estimation by use of deep learning. A configuration of the machine learning device 100 can be described as a machine learning method or software performed by the processor 101. The machine learning method is to learn estimating the tooth contact position adjustment amount from the dimensional data of the parts constituting the power transmission mechanism assembled by the robot system 2, and the processor 101 executes, in the machine learning method, a step of observing the dimensional data part dimensional data S1 of the parts constituting the power transmission mechanism assembled by the robot system 2 as a state variable S indicating a current state, a step of acquiring the tooth contact position adjustment amount adjustment amount data L1 as label data L, and a step of associating and learning the dimensional data of the parts constituting the power transmission mechanism assembled by the robot system 2 and the tooth contact position adjustment amount by use of the state variable S and the label data L. In third to fifth embodiments described below, description is made to the case in which the tooth contact position adjustment amount estimation device 1 according to the first and second embodiments is implemented as a part of a system in which the tooth contact position adjustment amount estimation device 1 is connected via a wired/wireless network to a plurality of devices such as a cloud server, a host computer, a fog computer, and an edge computer robot controller, controller, and so forth. As exemplarily depicted in 7, in the following third to fifth embodiments, each of a plurality of devices is configured to be logically divided into three layers, that is, a layer including a cloud server 6 and so forth, a layer including fog computers 7 and so forth, and a layer including edge computers 8 robot controller included in a cell 9, controller, and so forth and so forth, with each of the plurality of devices connected to the network. In the system as described above, the tooth contact position adjustment amount estimation device 1 can be implemented on any of the cloud server 6, the fog computers 7, and the edge computers 8. The tooth contact position adjustment amount estimation device 1 can share learning data used in processing with respect to machine learning among the plurality of devices via the network and can execute distributed learning, for example. The tooth contact position adjustment amount estimation device 1 can collect the generated learning model in the fog computers 7 or the cloud server 6 for large-scale analysis. Furthermore, the tooth contact position adjustment amount estimation device 1 can mutually reuse the generated learning model. In a system exemplarily depicted in 7, a plurality of cells 9 are provided to each factory at many places. The fog computers 7 which is the upper layer on the cell 9 manage the respective cells 9 in predetermined units such as by factory or by a plurality of factories of the same manufacturer. The cloud server 6 which is the upper layer on the fog computer 7 then collects and analyze data collected and analyzed by these fog computers 7. The resultant information by analyzing can be utilized for control and so forth of each edge computer. 8 is a schematic hardware configuration diagram illustrating a tooth contact position adjustment amount estimation device 1 implemented on a computer such as a cloud server, a fog computer and so forth. A central processing unit CPU 311 provided for the tooth contact position adjustment amount estimation device 1 implemented on a computer according to this embodiment is a processor for totally controlling the tooth contact position adjustment amount estimation device 1. The CPU 311 reads a system program stored in a read only memory ROM 312 via a bus 320. The CPU 311 controls the entire tooth contact position adjustment amount estimation device 1 according to the system program. Temporary calculation data or display data, or various items of data input by a worker from an input device 371 are temporarily stored in a random access memory RAM 313. A nonvolatile memory 314 is backed up by a battery not illustrated, for example. The nonvolatile memory 314 is a memory in which the stored states are held even if the tooth contact position adjustment amount estimation device 1 is powered off. The nonvolatile memory 314 stores the programs input from the input device 371. The nonvolatile memory 314 stores various items of data acquired from the robot system 2 and the like via each part of the tooth contact position adjustment amount estimation device 1 or a network 5. The programs or various items of data stored in the nonvolatile memory 14 may be loaded into the RAM 313 on execution/in use. Further, various system programs such as known analysis program including a system program for controlling communication with a machine learning device 100 described below are previously written in the ROM 312. The tooth contact position adjustment amount estimation device 1 is connected to a wired/wireless network 5 via an interface 319. At least one robot system 2, another tooth contact position adjustment amount estimation device 1, an edge computer 8, a fog computer 7, a cloud server 6 and so forth are connected to the network 5. These devices connected to the network 5 mutually exchange data with the tooth contact position adjustment amount estimation device 1. A display device 370 displays thereon each item of data read into the memory, or data acquired as a result of an executed program or the like, which are output via an interface 317. The input device 371 is configured of a keyboard or a pointing device. The input device 371 passes commands based on worker's operations, data, and the like to the CPU 311 via an interface 318. An interface 321 is an interface which connects each part of the tooth contact position adjustment amount estimation device 1 with the machine learning device 100. The machine learning device 100 has the same configuration as that described in 1. As described above, in a case where the tooth contact position adjustment amount estimation device 1 is implemented on a computer such as a cloud server and a fog computer, functions provided with the tooth contact position adjustment amount estimation device 1 are the same as the functions described in the first and the second embodiments, except that information from the robot system 2 and commands for setting a tooth contact position adjustment amount to the robot system are exchanged vie a network 5. 9 is a schematic hardware configuration diagram according to the third embodiment illustrating a tooth contact position adjustment amount estimation system provided with the tooth contact position adjustment amount estimation device 1. A tooth contact position adjustment amount estimation system 500 is provided with a plurality of tooth contact position adjustment amount estimation devices 1, 1, a plurality of robot systems 2 and a network 5 connecting the plurality of tooth contact position adjustment amount estimation devices 1, 1 and the plurality of robot systems 2. In the tooth contact position adjustment amount estimation system 500, the tooth contact position adjustment amount estimation device 1 provided with a machine learning device 100 estimates tooth contact position adjustment amount in assembling a power transmission mechanism by the robot system 2 using a learning result by the learning unit 110. At least one tooth contact position adjustment amount estimation device 1 learns the tooth contact position adjustment amount, which is common to all of the tooth contact position adjustment amount estimation devices 1, 1, in assembling a power transmission mechanism by the robot system 2 based on a state variable and label data acquired by each of the plurality of the other tooth contact position adjustment amount estimation devices 1, 1. The tooth contact position adjustment amount estimation device 1 is configured to share the learning result among all of the tooth contact position adjustment amount estimation devices 1, 1. Therefore, according to the tooth contact position adjustment amount estimation system 500 can improve speed and reliability of the learning using a various collection of data including the state variable S and label data L as an input. 10 is a schematic configuration diagram according to the fourth embodiment illustrating a system in which a machine learning device and a tooth contact position adjustment amount estimation device are implemented on different devices. A tooth contact position adjustment amount estimation system 500 includes at least one machine learning device 100 implemented on a part of a computer such as a cloud server, a host computer, a fog computer and so forth 10 illustrates an example of a machine learning device 100 implemented as a part of a fog computer 7 and a plurality of tooth contact position adjustment amount estimation devices 1. Further, the tooth contact position adjustment amount estimation system 500 is provided with a network 5 connecting the plurality of tooth contact position adjustment amount estimation devices 1 and a computer. Hardware configuration of the computer is the same as schematic hardware configuration of the tooth contact position adjustment amount estimation devices 1 described in 8 in which hardware such as a CPU 311, a RAM 313 and a nonvolatile memory 314 provided for a general computer is connected with each other via a bus 320. In the above-structured tooth contact position adjustment amount estimation system 500, based on the state variable S and the label data L acquired for each of the plurality of tooth contact position adjustment amount estimation devices 1, the machine learning device 100 of the tooth contact position adjustment amount estimation system 500 learns the tooth contact position adjustment amount, which is common to all the tooth contact position adjustment amount estimation devices 1, in assembling a power transmission mechanism by the robot system 2. The tooth contact position adjustment amount estimation system 500, using the learning result, can adjust the tooth contact position adjustment amount in assembling a power transmission mechanism by the robot system 2. According to the tooth contact position adjustment amount estimation system 500 with respect to the embodiment, each of the required number of the tooth contact position adjustment amount estimation devices 1 can be connected to the machine learning device 100 when necessary regardless of existing place or time. 11 is a schematic configuration diagram according to the fifth embodiment illustrating a tooth contact position adjustment amount estimation system 500 provided with a machine learning device 100 and a tooth contact position adjustment amount estimation device 1. A tooth contact position adjustment amount estimation system 500 is provided with at least one machine learning device 100 implemented on a computer such as an edge computer, a fog computer, host computer and a cloud server 11 illustrates an example of a machine learning device 100 implemented as a part of a fog computer 7 and a plurality of tooth contact position adjustment amount estimation devices 1. Further, the tooth contact position adjustment amount estimation system 500 is provided with a wired/wireless network 5 connecting the tooth contact position adjustment amount estimation devices 1 and the computer. In the above-structured tooth contact position adjustment amount estimation system 500, the fog computer 7 provided with the machine learning device 100 acquires, from each of the tooth contact position adjustment amount estimation devices 1, learning models acquired as a result of machine learning by the machine learning device 100 included in the tooth contact position adjustment amount estimation devices 1. The machine learning device 100 included in the fog computer 7 then optimizes knowledge and performs processing for enhancing efficiency based on the plurality of these learning models to generate new optimized, efficiency-enhanced learning models. The machine learning device 100 distributes the generated learning models to the each of the tooth contact position adjustment amount estimation devices 1. An example of optimization or efficiency enhancement of learning models performed by the machine learning device 100 is generation of a distilled model based on a plurality of learning models acquired from each of the tooth contact position adjustment amount estimation devices 1. The machine learning device 100 according to the present embodiment generates input data to be inputted to the learning models and then learns by using an output acquired as a result of inputting the input data to each learning model, thereby generating a new learning model distilled model. As described above, thus generated distilled model is distributed to an external storage medium, or distributed to the tooth contact position adjustment amount estimation devices 1 and other computers via a network and utilized. Another example of optimization or efficiency enhancement of learning models performed by the machine learning device 100 can be as follows. First, in the course of performing distillation on a plurality of learning models acquired from each of the tooth contact position adjustment amount estimation devices 1, an output data distribution of each learning model for input data is analyzed by a general statistical scheme. Next, outliers of sets of input data and output data are extracted. Next, distillation is performed by using sets of input data and output data except the outliers. By undergoing this course, exceptional estimation results are excluded from the sets of input data and output data acquired from each learning model. Namely, a distilled model can be generated by using the sets of input data and output data with the exceptional estimation results excluded. A distilled model generated in this manner can more generally be utilized compared with a learning model generated in a plurality of the tooth contact position adjustment amount estimation devices 1. Note that any of other general schemes of optimization or efficiency enhancement of learning models such as analyzing each learning model and optimizing a hyperparameter of the learning models based on the analysis result can be introduced as appropriate. In the tooth contact position adjustment amount estimation system 500 according to the present embodiment, the machine learning device 100 is arranged on a fog computer 7 installed for a plurality of the tooth contact position adjustment amount estimation devices 1 as edge computers. Learning models generated by the tooth contact position adjustment amount estimation devices 1 are collectively stored in the fog computer 7. Optimization and enhancement of efficiency are performed based on the plurality of stored learning models, and then the optimized, efficiency-enhanced learning models can be re-distributed to each of the tooth contact position adjustment amount estimation devices 1 as required. Also, in the tooth contact position adjustment amount estimation system 500 according to the present embodiment, for example, learning models collectively stored on the fog computer 7 and learning models optimized or efficiency-enhanced on the fog computer 7 can be collected onto a further upper host computer or cloud server. In this case, by using these learning models, application to intellectual tasks at factories or the manufacturer of the tooth contact position adjustment amount estimation devices 1 can be made such as construction and re-distribution of further versatile learning models at an upper server, support for maintenance based on the result of analysis on the learning models, analysis on performance of each of the tooth contact position adjustment amount estimation devices 1 and others, and application to new machine development. The embodiments of the present invention have been described above, but the present invention is not limited to the above embodiments, and can be changed as needed to be implemented in various aspects. For example, the learning algorithm or calculation algorithm executed by the machine learning device 100, the control algorithm executed by the tooth contact position adjustment amount estimation device 1, and the like are not limited to the above ones, and various algorithms may be employed. Further, the tooth contact position adjustment amount estimation device 1 and the machine learning device 100 have the different CPUs, respectively, in the above embodiments, but the machine learning device 100 may be realized by the CPU 11 provided for the tooth contact position adjustment amount estimation device 1 and the system program stored in the ROM 12.",
                    "IMAGE": "data:image/jpeg;base64,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",
                    "PDF_URL": "https://pdfpiw.uspto.gov/fdd/04/071/114/0.pdf",
                    "CONTRADICTION_SCORE": 0.9740425944328308,
                    "F_SPEC_PARAMS": [
                        "noise, vibration,",
                        "abnormal wear of the teeth"
                    ],
                    "S_SPEC_PARAMS": [
                        "optimum assembly position is different for each part",
                        "takes much time for an unexperienced worker to determine the optimum tooth contact position adjustment amount"
                    ],
                    "A_PARAMS": [],
                    "F_SENTS": [
                        "When the tooth contact of the gears, which are assembled on each shaft, and which mesh with each other, is not appropriate, abnormal noise, vibration, and abnormal wear of the teeth can occur."
                    ],
                    "S_SENTS": [
                        "However, a dimensional variation is in the parts such as the gears or the holder.",
                        "Thus, an optimum assembly position is different for each part.",
                        "Thus, there is a problem that it takes much time for an unexperienced worker to determine the optimum tooth contact position adjustment amount.",
                        "When it is determined that the tooth contact position is not appropriate, a step of removing the assembled parts and inserting a shim is caused."
                    ],
                    "F_TRIZ_PARAMS": [
                        "Harmful Factors Acting on Object"
                    ],
                    "F_SIM_SCORE": 0.4868423044681549,
                    "S_TRIZ_PARAMS": [
                        "Accuracy of Manufacturing"
                    ],
                    "S_SIM_SCORE": 0.4631194770336151,
                    "GLOBAL_SCORE": 1.6490234851837158
                },
                "sort": [
                    1.6490235
                ]
            }
        ]
    }
}